[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.16166v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.16166v2",
                "title": "Terahertz third-harmonic generation of lightwave driven Weyl fermions far from equilibrium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz third-harmonic generation of lightwave driven Weyl fermions far from equilibrium"
                },
                "updated": "2025-11-13T15:44:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    44,
                    30,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.16166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.16166v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We report on time-resolved ultrafast terahertz third-harmonic generation spectroscopy of nonequilibrium dynamics of Weyl fermions in a nanometer thin film of the Weyl semimetal TaP. Terahertz third-harmonic generation is observed at room temperature under the drive of a multicycle narrowband terahertz pulse with a peak field strength of down to tens of kV/cm. The observed terahertz third-harmonic generation exhibits a perturbative cubic power-law dependence on the terahertz drive. By varying the polarization of the drive pulse from linear to elliptical, we realize a sensitive tuning of the third harmonic yield. By carrying out theoretical analysis based on the Boltzmann transport theory, we can properly describe the experimental results and ascribe the observed THz nonlinearity to field-driven kinetics of the Weyl fermions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on time-resolved ultrafast terahertz third-harmonic generation spectroscopy of nonequilibrium dynamics of Weyl fermions in a nanometer thin film of the Weyl semimetal TaP. Terahertz third-harmonic generation is observed at room temperature under the drive of a multicycle narrowband terahertz pulse with a peak field strength of down to tens of kV/cm. The observed terahertz third-harmonic generation exhibits a perturbative cubic power-law dependence on the terahertz drive. By varying the polarization of the drive pulse from linear to elliptical, we realize a sensitive tuning of the third harmonic yield. By carrying out theoretical analysis based on the Boltzmann transport theory, we can properly describe the experimental results and ascribe the observed THz nonlinearity to field-driven kinetics of the Weyl fermions."
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-22T07:42:10Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    42,
                    10,
                    4,
                    234,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el"
                },
                "authors": [
                    {
                        "name": "Patrick Pilch"
                    },
                    {
                        "name": "Changqing Zhu"
                    },
                    {
                        "name": "Sergey Kovalev"
                    },
                    {
                        "name": "Renato M. A. Dantas"
                    },
                    {
                        "name": "Amilcar Bedoya-Pinto"
                    },
                    {
                        "name": "Stuart S. P. Parkin"
                    },
                    {
                        "name": "Zhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Wang"
                },
                "author": "Zhe Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.10394v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10394v1",
                "title": "LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components"
                },
                "updated": "2025-11-13T15:14:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    14,
                    34,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10394v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The health condition of wind turbine (WT) components is crucial for ensuring stable and reliable operation. However, existing fault detection methods are largely limited to visual recognition, producing structured outputs that lack semantic interpretability and fail to support maintenance decision-making. To address these limitations, this study proposes an integrated framework that combines YOLOMS with a large language model (LLM) for intelligent fault analysis and diagnosis. Specifically, YOLOMS employs multi-scale detection and sliding-window cropping to enhance fault feature extraction, while a lightweight key-value (KV) mapping module bridges the gap between visual outputs and textual inputs. This module converts YOLOMS detection results into structured textual representations enriched with both qualitative and quantitative attributes. A domain-tuned LLM then performs semantic reasoning to generate interpretable fault analyses and maintenance recommendations. Experiments on real-world datasets demonstrate that the proposed framework achieves a fault detection accuracy of 90.6\\% and generates maintenance reports with an average accuracy of 89\\%, thereby improving the interpretability of diagnostic results and providing practical decision support for the operation and maintenance of wind turbines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The health condition of wind turbine (WT) components is crucial for ensuring stable and reliable operation. However, existing fault detection methods are largely limited to visual recognition, producing structured outputs that lack semantic interpretability and fail to support maintenance decision-making. To address these limitations, this study proposes an integrated framework that combines YOLOMS with a large language model (LLM) for intelligent fault analysis and diagnosis. Specifically, YOLOMS employs multi-scale detection and sliding-window cropping to enhance fault feature extraction, while a lightweight key-value (KV) mapping module bridges the gap between visual outputs and textual inputs. This module converts YOLOMS detection results into structured textual representations enriched with both qualitative and quantitative attributes. A domain-tuned LLM then performs semantic reasoning to generate interpretable fault analyses and maintenance recommendations. Experiments on real-world datasets demonstrate that the proposed framework achieves a fault detection accuracy of 90.6\\% and generates maintenance reports with an average accuracy of 89\\%, thereby improving the interpretability of diagnostic results and providing practical decision support for the operation and maintenance of wind turbines."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:14:34Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    14,
                    34,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Journal resubmission",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yaru Li"
                    },
                    {
                        "name": "Yanxue Wang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Xinming Li"
                    },
                    {
                        "name": "Jianbo Feng"
                    }
                ],
                "author_detail": {
                    "name": "Jianbo Feng"
                },
                "author": "Jianbo Feng"
            },
            {
                "id": "http://arxiv.org/abs/2511.05534v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05534v2",
                "title": "FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference"
                },
                "updated": "2025-11-13T14:25:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    25,
                    8,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05534v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05534v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-29T13:20:16Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    20,
                    16,
                    2,
                    302,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kunxi Li"
                    },
                    {
                        "name": "Yufan Xiong"
                    },
                    {
                        "name": "Zhonghua Jiang"
                    },
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Zhaode Wang"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.19248v2",
                "title": "Reconfigurable Intelligent Surface-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surface-Assisted Multiple-Antenna Coded Caching"
                },
                "updated": "2025-11-13T11:36:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    36,
                    29,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.19248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.19248v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reconfigurable Intelligent Surface (RIS) has emerged as a promising technology to enhance the wireless propagation environment for next-generation wireless communication systems. This paper introduces a new RIS-assisted multiple-antenna coded caching problem. Unlike the existing multi-antenna coded caching models, our considered model incorporates a passive RIS with a limited number of elements aimed at enhancing the multicast gain (i.e., Degrees of Freedom (DoF)). The system consists of a server equipped with multiple antennas and several single-antenna users. The RIS, which functions as a passive and configurable relay, improves communication by selectively erasing certain transmission paths between transmit and receive antennas, thereby reducing interference. We first propose a new RIS-assisted interference nulling algorithm to determine the phase-shift coefficients of the RIS. This algorithm achieves faster convergence compared to the existing approach. By strategically nulling certain interference paths in each time slot, the transmission process is divided into multiple interference-free groups. Each group consists of a set of transmit antennas that serve a corresponding set of users without any interference from other groups. The optimal grouping strategy to maximize the DoF is formulated as a combinatorial optimization problem. To efficiently solve this, we design a low-complexity algorithm that identifies the optimal solution and develops a corresponding coded caching scheme to achieve the maximum DoF. Building on the optimal grouping strategy, we introduce a new framework, referred to as RIS-assisted Multiple-Antenna Placement Delivery Array (RMAPDA), to construct the cache placement and delivery phases. Then we propose a general RMAPDA design to achieve the maximum DoF under the optimal grouping strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surface (RIS) has emerged as a promising technology to enhance the wireless propagation environment for next-generation wireless communication systems. This paper introduces a new RIS-assisted multiple-antenna coded caching problem. Unlike the existing multi-antenna coded caching models, our considered model incorporates a passive RIS with a limited number of elements aimed at enhancing the multicast gain (i.e., Degrees of Freedom (DoF)). The system consists of a server equipped with multiple antennas and several single-antenna users. The RIS, which functions as a passive and configurable relay, improves communication by selectively erasing certain transmission paths between transmit and receive antennas, thereby reducing interference. We first propose a new RIS-assisted interference nulling algorithm to determine the phase-shift coefficients of the RIS. This algorithm achieves faster convergence compared to the existing approach. By strategically nulling certain interference paths in each time slot, the transmission process is divided into multiple interference-free groups. Each group consists of a set of transmit antennas that serve a corresponding set of users without any interference from other groups. The optimal grouping strategy to maximize the DoF is formulated as a combinatorial optimization problem. To efficiently solve this, we design a low-complexity algorithm that identifies the optimal solution and develops a corresponding coded caching scheme to achieve the maximum DoF. Building on the optimal grouping strategy, we introduce a new framework, referred to as RIS-assisted Multiple-Antenna Placement Delivery Array (RMAPDA), to construct the cache placement and delivery phases. Then we propose a general RMAPDA design to achieve the maximum DoF under the optimal grouping strategy."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "arxiv_comment": "Submitted to IEEE Trans. Information Theory, 40 pages",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire"
            },
            {
                "id": "http://arxiv.org/abs/2511.10116v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10116v1",
                "title": "Lanthanides-Based Nanoparticles Conjugated with Rose Bengal for FRET-Mediated X-Ray-Induced PDT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lanthanides-Based Nanoparticles Conjugated with Rose Bengal for FRET-Mediated X-Ray-Induced PDT"
                },
                "updated": "2025-11-13T09:20:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    9,
                    20,
                    38,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10116v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In order to find a good candidate for F{ö}rster Resonance Energy Transfer (FRET)-mediated X-ray-induced photodynamic therapy (X-PDT) for the treatment of cancer, lanthanide (Ln)-based AGuIX nanoparticles (NPs) conjugated with Rose Bengal (RB) as a photosensitizer (PS) were synthesized. X-PDT overcomes the problem of the poor penetration of visible light into tissues, which limits the efficacy of PDT in the treatment of deep-seated tumors. It is essential to optimize FRET efficiency by maximizing the overlap integral between donor emission and acceptor absorption and lengthening the duration of the donor emission. In this study, we optimized energy transfer between a scintillator (Sc) as a donor and a PS as an acceptor. Terbium (Tb) and Gadolinium (Gd) as Scs and Rose RB as a PS were chosen. The study of energy transfer between Tb, Gd and RB in solution and chelated on AGuIX NPs proved to be FRET-like. RB was conjugated directly onto AGuIX NPs (i.e., AGuIX Ln@RB), and the use of a spacer arm (i.e., AGuIX Ln@spacer arm-RB) increased FRET efficiency. Singlet oxygen production by these NPs was observed under UV--visible illumination and X-ray irradiation. The in vitro bioassay demonstrated 52% cell death of U-251MG derived from human malignant glioblastoma multiforme at a concentration of 1 $μ$M RB after illumination and irradiation (2 Gy, 320 kV, 10 mA, 3 Gy/min at 47 cm). In addition, the RB-coupled NRP-1-targeting peptide (i.e., K(RB)DKPPR) was conjugated onto AGuIX NPs by a thiol-maleimide click chemistry reaction, and an affinity in the nM range was observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to find a good candidate for F{ö}rster Resonance Energy Transfer (FRET)-mediated X-ray-induced photodynamic therapy (X-PDT) for the treatment of cancer, lanthanide (Ln)-based AGuIX nanoparticles (NPs) conjugated with Rose Bengal (RB) as a photosensitizer (PS) were synthesized. X-PDT overcomes the problem of the poor penetration of visible light into tissues, which limits the efficacy of PDT in the treatment of deep-seated tumors. It is essential to optimize FRET efficiency by maximizing the overlap integral between donor emission and acceptor absorption and lengthening the duration of the donor emission. In this study, we optimized energy transfer between a scintillator (Sc) as a donor and a PS as an acceptor. Terbium (Tb) and Gadolinium (Gd) as Scs and Rose RB as a PS were chosen. The study of energy transfer between Tb, Gd and RB in solution and chelated on AGuIX NPs proved to be FRET-like. RB was conjugated directly onto AGuIX NPs (i.e., AGuIX Ln@RB), and the use of a spacer arm (i.e., AGuIX Ln@spacer arm-RB) increased FRET efficiency. Singlet oxygen production by these NPs was observed under UV--visible illumination and X-ray irradiation. The in vitro bioassay demonstrated 52% cell death of U-251MG derived from human malignant glioblastoma multiforme at a concentration of 1 $μ$M RB after illumination and irradiation (2 Gy, 320 kV, 10 mA, 3 Gy/min at 47 cm). In addition, the RB-coupled NRP-1-targeting peptide (i.e., K(RB)DKPPR) was conjugated onto AGuIX NPs by a thiol-maleimide click chemistry reaction, and an affinity in the nM range was observed."
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T09:20:38Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    9,
                    20,
                    38,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph"
                },
                "arxiv_journal_ref": "Pharmaceuticals, 2025, 18 (5), pp.672",
                "authors": [
                    {
                        "name": "Batoul Dhaini"
                    },
                    {
                        "name": "Joël Daouk"
                    },
                    {
                        "name": "Hervé Schohn"
                    },
                    {
                        "name": "Philippe Arnoux"
                    },
                    {
                        "name": "Valérie Jouan-Hureaux"
                    },
                    {
                        "name": "Albert Moussaron"
                    },
                    {
                        "name": "Agnès Hagege"
                    },
                    {
                        "name": "Mathilde Achard"
                    },
                    {
                        "name": "Samir Acherar"
                    },
                    {
                        "name": "Tayssir Hamieh"
                    },
                    {
                        "name": "Céline Frochot"
                    }
                ],
                "author_detail": {
                    "name": "Céline Frochot"
                },
                "arxiv_affiliation": "LRGP",
                "author": "Céline Frochot"
            },
            {
                "id": "http://arxiv.org/abs/2511.09956v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09956v1",
                "title": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction"
                },
                "updated": "2025-11-13T04:37:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    4,
                    37,
                    52,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09956v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T04:37:52Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    4,
                    37,
                    52,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Mani Tofigh"
                    },
                    {
                        "name": "Edward Guo"
                    },
                    {
                        "name": "Weiwei Jia"
                    },
                    {
                        "name": "Xiaoning Ding"
                    },
                    {
                        "name": "Jianchen Shan"
                    }
                ],
                "author_detail": {
                    "name": "Jianchen Shan"
                },
                "author": "Jianchen Shan"
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.17741v2",
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments"
                },
                "updated": "2025-11-12T22:17:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    22,
                    17,
                    44,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.17741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.17741v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3725843.3756083",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their deployment, imposing substantial demands on inference clusters. These clusters must handle numerous concurrent queries for different LLM downstream tasks. To handle multi-task settings with vast LLM parameter counts, methods like Low-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most of the base LLM model across tasks. Hence, they allow concurrent task serving with minimal memory requirements. However, existing LLM serving systems face inefficiencies: they overlook workload heterogeneity, impose high link bandwidth from frequent adapter loading, and suffer from head-of-line blocking in their schedulers. To address these challenges, we present Chameleon, a novel LLM serving system optimized for many adapter environments, that relies on two core ideas: adapter caching and adapter-aware scheduling. First, Chameleon caches popular adapters in GPU memory, minimizing the adapter loading times. Importantly, it uses the otherwise idle GPU memory, avoiding extra memory costs. Second, Chameleon uses a non-preemptive multi-queue scheduling to efficiently account for workload heterogeneity. In this way, Chameleon simultaneously prevents head of line blocking and starvation. We implement Chameleon on top of a state-of-the-art LLM serving platform and evaluate it with real-world production traces and open-source LLMs. Under high loads, Chameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively, while improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their deployment, imposing substantial demands on inference clusters. These clusters must handle numerous concurrent queries for different LLM downstream tasks. To handle multi-task settings with vast LLM parameter counts, methods like Low-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most of the base LLM model across tasks. Hence, they allow concurrent task serving with minimal memory requirements. However, existing LLM serving systems face inefficiencies: they overlook workload heterogeneity, impose high link bandwidth from frequent adapter loading, and suffer from head-of-line blocking in their schedulers. To address these challenges, we present Chameleon, a novel LLM serving system optimized for many adapter environments, that relies on two core ideas: adapter caching and adapter-aware scheduling. First, Chameleon caches popular adapters in GPU memory, minimizing the adapter loading times. Importantly, it uses the otherwise idle GPU memory, avoiding extra memory costs. Second, Chameleon uses a non-preemptive multi-queue scheduling to efficiently account for workload heterogeneity. In this way, Chameleon simultaneously prevents head of line blocking and starvation. We implement Chameleon on top of a state-of-the-art LLM serving platform and evaluate it with real-world production traces and open-source LLMs. Under high loads, Chameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively, while improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "arxiv_comment": "Accepted at MICRO '25",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "arxiv_journal_ref": "MICRO 58, 2025, 15",
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "arxiv_doi": "10.1145/3725843.3756083"
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.15212v3",
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning"
                },
                "updated": "2025-11-12T16:02:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    16,
                    2,
                    3,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.15212v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.15212v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "arxiv_comment": "accepted to AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.09052v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09052v1",
                "title": "Efficient Distributed Exact Subgraph Matching via GNN-PE: Load Balancing, Cache Optimization, and Query Plan Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Distributed Exact Subgraph Matching via GNN-PE: Load Balancing, Cache Optimization, and Query Plan Ranking"
                },
                "updated": "2025-11-12T07:06:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    7,
                    6,
                    33,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09052v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Exact subgraph matching on large-scale graphs remains a challenging problem due to high computational complexity and distributed system constraints. Existing GNN-based path embedding (GNN-PE) frameworks achieve efficient exact matching on single machines but lack scalability and optimization for distributed environments. To address this gap, we propose three core innovations to extend GNN-PE to distributed systems: (1) a lightweight dynamic correlation-aware load balancing and hot migration mechanism that fuses multi-dimensional metrics (CPU, communication, memory) and guarantees index consistency; (2) an online incremental learning-based multi-GPU collaborative dynamic caching strategy with heterogeneous GPU adaptation and graph-structure-aware replacement; (3) a query plan ranking method driven by dominance embedding pruning potential (PE-score) that optimizes execution order. Through METIS partitioning, parallel offline preprocessing, and lightweight metadata management, our approach achieves \"minimum edge cut + load balancing + non-interruptible queries\" in distributed scenarios (tens of machines), significantly improving the efficiency and stability of distributed subgraph matching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact subgraph matching on large-scale graphs remains a challenging problem due to high computational complexity and distributed system constraints. Existing GNN-based path embedding (GNN-PE) frameworks achieve efficient exact matching on single machines but lack scalability and optimization for distributed environments. To address this gap, we propose three core innovations to extend GNN-PE to distributed systems: (1) a lightweight dynamic correlation-aware load balancing and hot migration mechanism that fuses multi-dimensional metrics (CPU, communication, memory) and guarantees index consistency; (2) an online incremental learning-based multi-GPU collaborative dynamic caching strategy with heterogeneous GPU adaptation and graph-structure-aware replacement; (3) a query plan ranking method driven by dominance embedding pruning potential (PE-score) that optimizes execution order. Through METIS partitioning, parallel offline preprocessing, and lightweight metadata management, our approach achieves \"minimum edge cut + load balancing + non-interruptible queries\" in distributed scenarios (tens of machines), significantly improving the efficiency and stability of distributed subgraph matching."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T07:06:33Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    7,
                    6,
                    33,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "10 pages",
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiake Ge"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.06029v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06029v2",
                "title": "Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving"
                },
                "updated": "2025-11-12T03:53:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    3,
                    53,
                    30,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06029v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T14:52:43Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    14,
                    52,
                    43,
                    5,
                    312,
                    0
                ],
                "arxiv_comment": "aaai26 camera-ready version, 12 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Hui Zeng"
                    },
                    {
                        "name": "Daming Zhao"
                    },
                    {
                        "name": "Pengfei Yang"
                    },
                    {
                        "name": "WenXuan Hou"
                    },
                    {
                        "name": "Tianyang Zheng"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Weiye Ji"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai"
            },
            {
                "id": "http://arxiv.org/abs/2511.08923v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08923v1",
                "title": "TiDAR: Think in Diffusion, Talk in Autoregression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TiDAR: Think in Diffusion, Talk in Autoregression"
                },
                "updated": "2025-11-12T02:59:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    2,
                    59,
                    33,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08923v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T02:59:33Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    2,
                    59,
                    33,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "NVIDIA-Tech Report",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Zhifan Ye"
                    },
                    {
                        "name": "Rishabh Mehta"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Vartika Singh"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov"
            },
            {
                "id": "http://arxiv.org/abs/2511.06838v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06838v2",
                "title": "P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats"
                },
                "updated": "2025-11-12T01:39:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    1,
                    39,
                    19,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06838v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The substantial memory bandwidth and computational demands of large language models (LLMs) present critical challenges for efficient inference. To tackle this, the literature has explored heterogeneous systems that combine neural processing units (NPUs) with DRAM-based processing-in-memory (PIM) for LLM acceleration. However, existing high-precision (e.g., FP16) PIM compute units incur significant area and power overhead in DRAM technology, limiting the effective computation throughput. In this paper, we introduce P3-LLM, a novel NPU-PIM integrated accelerator for LLM inference using hybrid numerical formats. Our approach is threefold: First, we propose a flexible mixed-precision quantization scheme, which leverages hybrid numerical formats to quantize different LLM operands with high compression efficiency and minimal accuracy loss. Second, we architect an efficient PIM accelerator for P3-LLM, featuring enhanced compute units to support hybrid numerical formats. Our careful choice of numerical formats allows to co-design low-precision PIM compute units that significantly boost the computation throughput under iso-area constraints. Third, we optimize the low-precision dataflow of different LLM modules by applying operator fusion to minimize the overhead of runtime dequantization. Evaluation on a diverse set of representative LLMs and tasks demonstrates that P3-LLM achieves state-of-the-art accuracy in terms of both KV-cache quantization and weight-activation quantization. Combining the proposed quantization scheme with PIM architecture co-design, P3-LLM yields an average of $4.9\\times$, $2.0\\times$, and $3.4\\times$ speedups over the state-of-the-art LLM accelerators HBM-PIM, Ecco, and Pimba, respectively. Our quantization code is available at https://github.com/yc2367/P3-LLM.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The substantial memory bandwidth and computational demands of large language models (LLMs) present critical challenges for efficient inference. To tackle this, the literature has explored heterogeneous systems that combine neural processing units (NPUs) with DRAM-based processing-in-memory (PIM) for LLM acceleration. However, existing high-precision (e.g., FP16) PIM compute units incur significant area and power overhead in DRAM technology, limiting the effective computation throughput. In this paper, we introduce P3-LLM, a novel NPU-PIM integrated accelerator for LLM inference using hybrid numerical formats. Our approach is threefold: First, we propose a flexible mixed-precision quantization scheme, which leverages hybrid numerical formats to quantize different LLM operands with high compression efficiency and minimal accuracy loss. Second, we architect an efficient PIM accelerator for P3-LLM, featuring enhanced compute units to support hybrid numerical formats. Our careful choice of numerical formats allows to co-design low-precision PIM compute units that significantly boost the computation throughput under iso-area constraints. Third, we optimize the low-precision dataflow of different LLM modules by applying operator fusion to minimize the overhead of runtime dequantization. Evaluation on a diverse set of representative LLMs and tasks demonstrates that P3-LLM achieves state-of-the-art accuracy in terms of both KV-cache quantization and weight-activation quantization. Combining the proposed quantization scheme with PIM architecture co-design, P3-LLM yields an average of $4.9\\times$, $2.0\\times$, and $3.4\\times$ speedups over the state-of-the-art LLM accelerators HBM-PIM, Ecco, and Pimba, respectively. Our quantization code is available at https://github.com/yc2367/P3-LLM.git"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T08:29:34Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    8,
                    29,
                    34,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "Preprint. Under review",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Chao Fang"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Yuheng Wu"
                    },
                    {
                        "name": "Thierry Tambe"
                    },
                    {
                        "name": "Marian Verhelst"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah"
            },
            {
                "id": "http://arxiv.org/abs/2510.25977v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.25977v3",
                "title": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium"
                },
                "updated": "2025-11-11T23:18:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    23,
                    18,
                    58,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.25977v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.25977v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "AI accelerators, customized to AI workloads, provide cost-effective and high-performance solutions for training and inference. Trainium, an AI accelerator recently developed by Amazon Web Services (AWS), provides an attractive option for LLM training and inference through its heterogeneous architecture. However, leveraging Trainium architecture for high performance can be challenging because of its systolic array architecture and special requirement on data layout. In this paper, we design high-performance matrix multiplication (matmul), a critical compute kernel, for LLM inference on Trainium. We introduce a series of techniques customized to Trainium based on kernel fusion and novel caching strategies to reduce data movement across the software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive matrix transpose. Evaluating with nine datasets and four recent LLMs, we show that our system largely outperforms the state-of-the-art matmul implemented by AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x speedup (up to 2.22x), which translates to an average 1.66x speedup (up to 2.49x) for end-to-end LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI accelerators, customized to AI workloads, provide cost-effective and high-performance solutions for training and inference. Trainium, an AI accelerator recently developed by Amazon Web Services (AWS), provides an attractive option for LLM training and inference through its heterogeneous architecture. However, leveraging Trainium architecture for high performance can be challenging because of its systolic array architecture and special requirement on data layout. In this paper, we design high-performance matrix multiplication (matmul), a critical compute kernel, for LLM inference on Trainium. We introduce a series of techniques customized to Trainium based on kernel fusion and novel caching strategies to reduce data movement across the software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive matrix transpose. Evaluating with nine datasets and four recent LLMs, we show that our system largely outperforms the state-of-the-art matmul implemented by AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x speedup (up to 2.22x), which translates to an average 1.66x speedup (up to 2.49x) for end-to-end LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-29T21:22:08Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    22,
                    8,
                    2,
                    302,
                    0
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Jierui Xu"
                    },
                    {
                        "name": "Weichu Yang"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li"
            },
            {
                "id": "http://arxiv.org/abs/2510.25979v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.25979v3",
                "title": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache"
                },
                "updated": "2025-11-11T23:07:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    23,
                    7,
                    38,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.25979v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.25979v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model encodes input sequences without performing autoregressive decoding. In these prefill only scenarios, the self-attention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length. In this paper, we observe that semantically different sentences often produce similar attention maps across layers and heads. Building on this insight, we propose AttnCache, a framework that accelerates the prefill stage of LLM inference by retrieving and reusing similar attention maps. Based on an attention map memorization database, AttnCache employs efficient caching and similarity search techniques to identify and reuse pre-cached attention maps during inference, thereby reducing the computational overhead of self-attention. Experimental results show that AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model encodes input sequences without performing autoregressive decoding. In these prefill only scenarios, the self-attention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length. In this paper, we observe that semantically different sentences often produce similar attention maps across layers and heads. Building on this insight, we propose AttnCache, a framework that accelerates the prefill stage of LLM inference by retrieving and reusing similar attention maps. Based on an attention map memorization database, AttnCache employs efficient caching and similarity search techniques to identify and reuse pre-cached attention maps during inference, thereby reducing the computational overhead of self-attention. Experimental results show that AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-29T21:26:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    26,
                    17,
                    2,
                    302,
                    0
                ],
                "arxiv_comment": "10 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Shangye Chen"
                    },
                    {
                        "name": "Cyril Guyot"
                    },
                    {
                        "name": "Filip Blagojevic"
                    },
                    {
                        "name": "Hyeran Jeon"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.08826v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08826v1",
                "title": "FlashMap: A Flash Optimized Key-Value Store",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashMap: A Flash Optimized Key-Value Store"
                },
                "updated": "2025-11-11T22:48:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    22,
                    48,
                    29,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08826v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Key-value stores are a fundamental class of NoSQL databases that offer a simple yet powerful model for data storage and retrieval, representing information as pairs of unique keys and associated values. Their minimal structure enables exceptionally fast access times, scalability, and flexibility in storing diverse data types, making them ideal for high-performance applications such as caching, session management, and distributed systems. As modern computing increasingly demands responsiveness and scalability, key-value stores have become a critical component of the data infrastructure in both industry and research contexts. In this work, we present FlashMap, a high-performance key-value store optimized for Flash-based solid-state drives (SSDs). Experiments show that FlashMap achieves outstanding throughput, averaging 19.8 million inserts and 23.8 million random lookups per second with a 100-byte payload, all on a single data center-grade server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value stores are a fundamental class of NoSQL databases that offer a simple yet powerful model for data storage and retrieval, representing information as pairs of unique keys and associated values. Their minimal structure enables exceptionally fast access times, scalability, and flexibility in storing diverse data types, making them ideal for high-performance applications such as caching, session management, and distributed systems. As modern computing increasingly demands responsiveness and scalability, key-value stores have become a critical component of the data infrastructure in both industry and research contexts. In this work, we present FlashMap, a high-performance key-value store optimized for Flash-based solid-state drives (SSDs). Experiments show that FlashMap achieves outstanding throughput, averaging 19.8 million inserts and 23.8 million random lookups per second with a 100-byte payload, all on a single data center-grade server."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T22:48:29Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    22,
                    48,
                    29,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "6 pages, 2 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Zonglin Guo"
                    },
                    {
                        "name": "Tony Givargis"
                    }
                ],
                "author_detail": {
                    "name": "Tony Givargis"
                },
                "author": "Tony Givargis"
            },
            {
                "id": "http://arxiv.org/abs/2511.08568v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08568v1",
                "title": "Machine Learning-Guided Memory Optimization for DLRM Inference on Tiered Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning-Guided Memory Optimization for DLRM Inference on Tiered Memory"
                },
                "updated": "2025-11-11T18:49:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    18,
                    49,
                    53,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08568v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep learning recommendation models (DLRMs) are widely used in industry, and their memory capacity requirements reach the terabyte scale. Tiered memory architectures provide a cost-effective solution but introduce challenges in embedding-vector placement due to complex embedding-access patterns. We propose RecMG, a machine learning (ML)-guided system for vector caching and prefetching on tiered memory. RecMG accurately predicts accesses to embedding vectors with long reuse distances or few reuses. The design of RecMG focuses on making ML feasible in the context of DLRM inference by addressing unique challenges in data labeling and navigating the search space for embedding-vector placement. By employing separate ML models for caching and prefetching, plus a novel differentiable loss function, RecMG narrows the prefetching search space and minimizes on-demand fetches. Compared to state-of-the-art temporal, spatial, and ML-based prefetchers, RecMG reduces on-demand fetches by 2.2x, 2.8x, and 1.5x, respectively. In industrial-scale DLRM inference scenarios, RecMG effectively reduces end-to-end DLRM inference time by up to 43%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning recommendation models (DLRMs) are widely used in industry, and their memory capacity requirements reach the terabyte scale. Tiered memory architectures provide a cost-effective solution but introduce challenges in embedding-vector placement due to complex embedding-access patterns. We propose RecMG, a machine learning (ML)-guided system for vector caching and prefetching on tiered memory. RecMG accurately predicts accesses to embedding vectors with long reuse distances or few reuses. The design of RecMG focuses on making ML feasible in the context of DLRM inference by addressing unique challenges in data labeling and navigating the search space for embedding-vector placement. By employing separate ML models for caching and prefetching, plus a novel differentiable loss function, RecMG narrows the prefetching search space and minimizes on-demand fetches. Compared to state-of-the-art temporal, spatial, and ML-based prefetchers, RecMG reduces on-demand fetches by 2.2x, 2.8x, and 1.5x, respectively. In industrial-scale DLRM inference scenarios, RecMG effectively reduces end-to-end DLRM inference time by up to 43%."
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T18:49:53Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    18,
                    49,
                    53,
                    1,
                    315,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF"
                },
                "authors": [
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Shuangyan Yang"
                    },
                    {
                        "name": "Benjamin Francis"
                    },
                    {
                        "name": "Ehsan K. Ardestani"
                    },
                    {
                        "name": "Min Si"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li"
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.15683v3",
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models"
                },
                "updated": "2025-11-11T13:52:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    13,
                    52,
                    57,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.15683v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.15683v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.10069v3",
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism"
                },
                "updated": "2025-11-11T12:44:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    12,
                    44,
                    32,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.10069v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.10069v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs)."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "arxiv_comment": "Accepted at NeurIPS 2025 Oral (Thirty-Ninth Conference on Neural Information Processing Systems)",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao"
            },
            {
                "id": "http://arxiv.org/abs/2502.20969v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.20969v3",
                "title": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval"
                },
                "updated": "2025-11-11T09:41:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    9,
                    41,
                    30,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.20969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.20969v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, creating a significant system challenge: achieving high throughput and low latency is difficult, especially when GPU memory is limited. To address these challenges, we propose TeleRAG, an efficient inference system that reduces latency and improves throughput with minimal GPU memory requirements. The core innovation of TeleRAG is lookahead retrieval, a prefetching mechanism that predicts required data and transfers them from CPU to GPU in parallel with LLM generation. In addition, TeleRAG adopts a prefetching scheduler and a cache-aware scheduler to support efficient multi-GPU inference with minimal overhead. Evaluations show TeleRAG achieves up to a 1.53x average end-to-end latency reduction (single-query) and 1.83x higher average throughput (batched), as well as good scalability in throughput. This confirms the practical utility of TeleRAG for faster and more memory-efficient deployments of RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, creating a significant system challenge: achieving high throughput and low latency is difficult, especially when GPU memory is limited. To address these challenges, we propose TeleRAG, an efficient inference system that reduces latency and improves throughput with minimal GPU memory requirements. The core innovation of TeleRAG is lookahead retrieval, a prefetching mechanism that predicts required data and transfers them from CPU to GPU in parallel with LLM generation. In addition, TeleRAG adopts a prefetching scheduler and a cache-aware scheduler to support efficient multi-GPU inference with minimal overhead. Evaluations show TeleRAG achieves up to a 1.53x average end-to-end latency reduction (single-query) and 1.83x higher average throughput (batched), as well as good scalability in throughput. This confirms the practical utility of TeleRAG for faster and more memory-efficient deployments of RAG applications."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-28T11:32:22Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    32,
                    22,
                    4,
                    59,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Keisuke Kamahori"
                    },
                    {
                        "name": "Yiyu Liu"
                    },
                    {
                        "name": "Xiaoxiang Shi"
                    },
                    {
                        "name": "Madhav Kashyap"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Rulin Shao"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Rohan Kadekodi"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci"
            },
            {
                "id": "http://arxiv.org/abs/2511.08003v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08003v1",
                "title": "Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning"
                },
                "updated": "2025-11-11T09:07:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    9,
                    7,
                    40,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08003v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T09:07:40Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    9,
                    7,
                    40,
                    1,
                    315,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jialong Qin"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Di Lu"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu"
            },
            {
                "id": "http://arxiv.org/abs/2511.07969v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07969v1",
                "title": "Unified Work Embeddings: Contrastive Learning of a Bidirectional Multi-task Ranker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Work Embeddings: Contrastive Learning of a Bidirectional Multi-task Ranker"
                },
                "updated": "2025-11-11T08:28:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    8,
                    28,
                    26,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07969v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks. To this end, we introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, establishing a common ground for multi-task progress. Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction. UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks. To this end, we introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, establishing a common ground for multi-task progress. Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction. UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T08:28:26Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    8,
                    28,
                    26,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "Preprint",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Matthias De Lange"
                    },
                    {
                        "name": "Jens-Joris Decorte"
                    },
                    {
                        "name": "Jeroen Van Hautte"
                    }
                ],
                "author_detail": {
                    "name": "Jeroen Van Hautte"
                },
                "author": "Jeroen Van Hautte"
            },
            {
                "id": "http://arxiv.org/abs/2511.07762v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07762v1",
                "title": "Kerr Polarization Transport: Accuracy and Performance in General Relativistic Light Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kerr Polarization Transport: Accuracy and Performance in General Relativistic Light Propagation"
                },
                "updated": "2025-11-11T02:26:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    2,
                    26,
                    27,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07762v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a compact and reproducible method for general relativistic polarization transport in the Kerr metric that achieves median electric vector position angle (EVPA) residuals of $\\langle Δ\\mathrm{PA} \\rangle \\approx 0.09^\\circ$, a 95th percentile of $0.31^\\circ$, and a worst case $Δ\\mathrm{PA} \\lesssim 0.32^\\circ$ for spins up to $|a/M|=0.9$, while maintaining a fivefold or greater speedup relative to a strict reference integrator. Across the benchmark grid, typical residuals remain at the sub-tenth-degree level, with only modest degradation ($Δ\\mathrm{PA} \\lesssim 2^\\circ$) near the Thorne spin limit. Photon four-momenta $k^μ$ and polarization four-vectors $f^μ$ are advanced using a fourth order Runge-Kutta scheme with cached Christoffel symbols, maintaining the constraints $u\\cdot f=0$ and $n\\cdot f=0$, where $u^μ$ is the ZAMO four-velocity and $n^μ$ is the disk normal, while keeping $k\\cdot f \\simeq 0$. A physically motivated gauge is enforced by projecting the polarization into the local zero-angular-momentum observer (ZAMO) screen at every substep, ensuring numerical stability of the orthogonality constraints. Accuracy and performance are benchmarked over a representative grid in spin, inclination, image-plane azimuth, and radius. The method comfortably meets IXPE and NICER polarization tolerances and approaches EHT requirements. The approach provides a practical foundation for future general relativistic polarimetry and simulation pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a compact and reproducible method for general relativistic polarization transport in the Kerr metric that achieves median electric vector position angle (EVPA) residuals of $\\langle Δ\\mathrm{PA} \\rangle \\approx 0.09^\\circ$, a 95th percentile of $0.31^\\circ$, and a worst case $Δ\\mathrm{PA} \\lesssim 0.32^\\circ$ for spins up to $|a/M|=0.9$, while maintaining a fivefold or greater speedup relative to a strict reference integrator. Across the benchmark grid, typical residuals remain at the sub-tenth-degree level, with only modest degradation ($Δ\\mathrm{PA} \\lesssim 2^\\circ$) near the Thorne spin limit. Photon four-momenta $k^μ$ and polarization four-vectors $f^μ$ are advanced using a fourth order Runge-Kutta scheme with cached Christoffel symbols, maintaining the constraints $u\\cdot f=0$ and $n\\cdot f=0$, where $u^μ$ is the ZAMO four-velocity and $n^μ$ is the disk normal, while keeping $k\\cdot f \\simeq 0$. A physically motivated gauge is enforced by projecting the polarization into the local zero-angular-momentum observer (ZAMO) screen at every substep, ensuring numerical stability of the orthogonality constraints. Accuracy and performance are benchmarked over a representative grid in spin, inclination, image-plane azimuth, and radius. The method comfortably meets IXPE and NICER polarization tolerances and approaches EHT requirements. The approach provides a practical foundation for future general relativistic polarimetry and simulation pipelines."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T02:26:27Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    2,
                    26,
                    27,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "11 pages, 2 figures. Submitted to arXiv; to be submitted to The Astrophysical Journal",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "Shakibul Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Shakibul Chowdhury"
                },
                "author": "Shakibul Chowdhury"
            },
            {
                "id": "http://arxiv.org/abs/2511.07399v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07399v1",
                "title": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation"
                },
                "updated": "2025-11-10T18:51:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    18,
                    51,
                    28,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07399v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T18:51:28Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    18,
                    51,
                    28,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "Project Page: http://streamdiffusionv2.github.io",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Lvmin Zhang"
                    },
                    {
                        "name": "Keting Yang"
                    },
                    {
                        "name": "Kelly Peng"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Maneesh Agrawala"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Akio Kodaira"
                    },
                    {
                        "name": "Chenfeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenfeng Xu"
                },
                "author": "Chenfeng Xu"
            },
            {
                "id": "http://arxiv.org/abs/2511.07278v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07278v1",
                "title": "StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression"
                },
                "updated": "2025-11-10T16:25:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    16,
                    25,
                    3,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07278v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Video Large Language Models (Video-LLMs) have demonstrated significant potential in the areas of video captioning, search, and summarization. However, current Video-LLMs still face challenges with long real-world videos. Recent methods have introduced a retrieval mechanism that retrieves query-relevant KV caches for question answering, enhancing the efficiency and accuracy of long real-world videos. However, the compression and retrieval of KV caches are still not fully explored. In this paper, we propose \\textbf{StreamKV}, a training-free framework that seamlessly equips Video-LLMs with advanced KV cache retrieval and compression. Compared to previous methods that used uniform partitioning, StreamKV dynamically partitions video streams into semantic segments, which better preserves semantic information. For KV cache retrieval, StreamKV calculates a summary vector for each segment to retain segment-level information essential for retrieval. For KV cache compression, StreamKV introduces a guidance prompt designed to capture the key semantic elements within each segment, ensuring only the most informative KV caches are retained for answering questions. Moreover, StreamKV unifies KV cache retrieval and compression within a single module, performing both in a layer-adaptive manner, thereby further improving the effectiveness of streaming video question answering. Extensive experiments on public StreamingVQA benchmarks demonstrate that StreamKV significantly outperforms existing Online Video-LLMs, achieving superior accuracy while substantially improving both memory efficiency and computational latency. The code has been released at https://github.com/sou1p0wer/StreamKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (Video-LLMs) have demonstrated significant potential in the areas of video captioning, search, and summarization. However, current Video-LLMs still face challenges with long real-world videos. Recent methods have introduced a retrieval mechanism that retrieves query-relevant KV caches for question answering, enhancing the efficiency and accuracy of long real-world videos. However, the compression and retrieval of KV caches are still not fully explored. In this paper, we propose \\textbf{StreamKV}, a training-free framework that seamlessly equips Video-LLMs with advanced KV cache retrieval and compression. Compared to previous methods that used uniform partitioning, StreamKV dynamically partitions video streams into semantic segments, which better preserves semantic information. For KV cache retrieval, StreamKV calculates a summary vector for each segment to retain segment-level information essential for retrieval. For KV cache compression, StreamKV introduces a guidance prompt designed to capture the key semantic elements within each segment, ensuring only the most informative KV caches are retained for answering questions. Moreover, StreamKV unifies KV cache retrieval and compression within a single module, performing both in a layer-adaptive manner, thereby further improving the effectiveness of streaming video question answering. Extensive experiments on public StreamingVQA benchmarks demonstrate that StreamKV significantly outperforms existing Online Video-LLMs, achieving superior accuracy while substantially improving both memory efficiency and computational latency. The code has been released at https://github.com/sou1p0wer/StreamKV."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T16:25:03Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    16,
                    25,
                    3,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Xiang Bai"
                    },
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Chengyu Bai"
                    },
                    {
                        "name": "Yuhan Dai"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.07229v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07229v1",
                "title": "LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and Serving Techniques in LLM Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and Serving Techniques in LLM Infrastructure"
                },
                "updated": "2025-11-10T15:47:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    15,
                    47,
                    53,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07229v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/LCA.2025.3628325",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This paper introduces LLMServingSim2.0, a system simulator designed for exploring heterogeneous hardware in large-scale LLM serving systems. LLMServingSim2.0 addresses two key limitations of its predecessor: (1) integrating hardware models into system-level simulators is non-trivial due to the lack of a clear abstraction, and (2) existing simulators support only a narrow subset of serving techniques, leaving no infrastructure that captures the breadth of approaches in modern LLM serving. To overcome these issues, LLMServingSim2.0 adopts trace-driven performance modeling, accompanied by an operator-level latency profiler, enabling the integration of new accelerators with a single command. It further embeds up-to-date serving techniques while exposing flexible interfaces for request routing, cache management, and scheduling policies. In a TPU case study, our profiler requires 18.5x fewer LoC and outperforms the predecessor's hardware-simulator integration, demonstrating LLMServingSim2.0's low-effort hardware extensibility. Our experiments further show that LLMServingSim2.0 reproduces GPU-based LLM serving with 1.9% error, while maintaining practical simulation time, making it a comprehensive platform for both hardware developers and LLM service providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LLMServingSim2.0, a system simulator designed for exploring heterogeneous hardware in large-scale LLM serving systems. LLMServingSim2.0 addresses two key limitations of its predecessor: (1) integrating hardware models into system-level simulators is non-trivial due to the lack of a clear abstraction, and (2) existing simulators support only a narrow subset of serving techniques, leaving no infrastructure that captures the breadth of approaches in modern LLM serving. To overcome these issues, LLMServingSim2.0 adopts trace-driven performance modeling, accompanied by an operator-level latency profiler, enabling the integration of new accelerators with a single command. It further embeds up-to-date serving techniques while exposing flexible interfaces for request routing, cache management, and scheduling policies. In a TPU case study, our profiler requires 18.5x fewer LoC and outperforms the predecessor's hardware-simulator integration, demonstrating LLMServingSim2.0's low-effort hardware extensibility. Our experiments further show that LLMServingSim2.0 reproduces GPU-based LLM serving with 1.9% error, while maintaining practical simulation time, making it a comprehensive platform for both hardware developers and LLM service providers."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T15:47:53Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    15,
                    47,
                    53,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "4 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "arxiv_journal_ref": "IEEE Computer Architecture Letters (CAL) 2025",
                "authors": [
                    {
                        "name": "Jaehong Cho"
                    },
                    {
                        "name": "Hyunmin Choi"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_doi": "10.1109/LCA.2025.3628325"
            },
            {
                "id": "http://arxiv.org/abs/2510.19670v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.19670v2",
                "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation"
                },
                "updated": "2025-11-10T14:37:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    14,
                    37,
                    47,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.19670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.19670v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-22T15:16:56Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "arxiv_comment": "19 pages,8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Aina Binti Abdullah"
                    },
                    {
                        "name": "Pieter van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Pieter van der Merwe"
                },
                "author": "Pieter van der Merwe"
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.18480v3",
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices"
                },
                "updated": "2025-11-10T13:10:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    13,
                    10,
                    25,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.18480v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.18480v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2506.08009v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.08009v2",
                "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion"
                },
                "updated": "2025-11-10T04:36:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    4,
                    36,
                    27,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.08009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.08009v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-09T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 spotlight. Project website: http://self-forcing.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xun Huang"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Guande He"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Eli Shechtman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shechtman"
                },
                "author": "Eli Shechtman"
            },
            {
                "id": "http://arxiv.org/abs/2507.02397v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.02397v2",
                "title": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission Dynamics"
                },
                "updated": "2025-11-10T03:55:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    3,
                    55,
                    22,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.02397v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While field-driven electron emission is theoretically understood down to the subcycle regime, its direct experimental temporal characterization using long-wavelength terahertz (THz) fields remains elusive. Here, by driving a graphite tip with phase-stable quasi-single-cycle THz pulses, we reveal distinct subcycle electron emission dynamics including: (1) At a carrier-envelope phase (CEP) zero, spectral peaks scale linearly with THz field strength, characteristic of subcycle emission; (2) At nearly opposite CEP, dominant deceleration fields generate stationary low-energy peaks. Crucially, we develop a pump-probe-free, direct reconstruction method extracting electron pulse profiles solely from measured energy spectra, obtaining durations from 73.0 to 81.0 fs as the field increases (191-290 kV/cm). Phase-resolved simulations further reveal a 72.8% modulation in the cutoff energy and a near-total (99.7%) suppression of the emission current. This work not only validates the field-emisssion theory under THz excitation but also establishes a general framework for the direct temporal characterization of subcycle electron emission, opening pathways for precise electron control in ultrafast electron sources and lightwave nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While field-driven electron emission is theoretically understood down to the subcycle regime, its direct experimental temporal characterization using long-wavelength terahertz (THz) fields remains elusive. Here, by driving a graphite tip with phase-stable quasi-single-cycle THz pulses, we reveal distinct subcycle electron emission dynamics including: (1) At a carrier-envelope phase (CEP) zero, spectral peaks scale linearly with THz field strength, characteristic of subcycle emission; (2) At nearly opposite CEP, dominant deceleration fields generate stationary low-energy peaks. Crucially, we develop a pump-probe-free, direct reconstruction method extracting electron pulse profiles solely from measured energy spectra, obtaining durations from 73.0 to 81.0 fs as the field increases (191-290 kV/cm). Phase-resolved simulations further reveal a 72.8% modulation in the cutoff energy and a near-total (99.7%) suppression of the emission current. This work not only validates the field-emisssion theory under THz excitation but also establishes a general framework for the direct temporal characterization of subcycle electron emission, opening pathways for precise electron control in ultrafast electron sources and lightwave nanoelectronics."
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-03T07:49:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "arxiv_comment": "18 pages, 5 figures, references added",
                "arxiv_primary_category": {
                    "term": "physics.optics"
                },
                "authors": [
                    {
                        "name": "Jiakang Mao"
                    },
                    {
                        "name": "Yushan Zeng"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Liwei Song"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ruxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruxin Li"
                },
                "author": "Ruxin Li"
            },
            {
                "id": "http://arxiv.org/abs/2510.27070v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.27070v2",
                "title": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review"
                },
                "updated": "2025-11-10T02:03:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    2,
                    3,
                    6,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.27070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.27070v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The security and efficiency of modern computing systems are fundamentally undermined by the absence of a native architectural mechanism to propagate high-level program semantics, such as object identity, bounds, and lifetime, across the hardware/software interface. This paper presents a comprehensive survey of the architectural paradigm designed to bridge this semantic gap: descriptor-based, object-aware memory systems. By elevating the descriptor to a first-class architectural abstraction, this paradigm enables hardware to dynamically acquire and enforce the rich semantics of software-defined objects. This survey systematically charts the evolution and current landscape of this approach. We establish the foundational concepts of memory objects and descriptors and introduce a novel taxonomy of descriptor addressing modes, providing a structured framework for analyzing and comparing diverse implementations. Our unified analysis reveals how this paradigm holistically addresses the intertwined challenges of memory protection, management, and processing. As a culminating case study, we re-examine the CentroID model, demonstrating how its hybrid tagged-pointer encoding and descriptor processing mechanisms embody the path toward practical and efficient object-aware designs. Finally, we outline how the explicit cross-layer communication of object semantics provides a foundational research direction for next-generation cache hierarchies, unified virtual memory, and even 128-bit architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security and efficiency of modern computing systems are fundamentally undermined by the absence of a native architectural mechanism to propagate high-level program semantics, such as object identity, bounds, and lifetime, across the hardware/software interface. This paper presents a comprehensive survey of the architectural paradigm designed to bridge this semantic gap: descriptor-based, object-aware memory systems. By elevating the descriptor to a first-class architectural abstraction, this paradigm enables hardware to dynamically acquire and enforce the rich semantics of software-defined objects. This survey systematically charts the evolution and current landscape of this approach. We establish the foundational concepts of memory objects and descriptors and introduce a novel taxonomy of descriptor addressing modes, providing a structured framework for analyzing and comparing diverse implementations. Our unified analysis reveals how this paradigm holistically addresses the intertwined challenges of memory protection, management, and processing. As a culminating case study, we re-examine the CentroID model, demonstrating how its hybrid tagged-pointer encoding and descriptor processing mechanisms embody the path toward practical and efficient object-aware designs. Finally, we outline how the explicit cross-layer communication of object semantics provides a foundational research direction for next-generation cache hierarchies, unified virtual memory, and even 128-bit architectures."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-31T00:39:27Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Dong Tong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Tong"
                },
                "author": "Dong Tong"
            },
            {
                "id": "http://arxiv.org/abs/2511.06605v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06605v1",
                "title": "DMA Collectives for Efficient ML Communication Offloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DMA Collectives for Efficient ML Communication Offloads"
                },
                "updated": "2025-11-10T01:28:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    1,
                    28,
                    58,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06605v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Offloading machine learning (ML) communication collectives to direct memory access (DMA) engines has emerged as an interesting and low-cost solution to efficiently overlap computation and communication in inference and training. Doing so delivers superior concurrent performance by freeing up all GPU cores for computation and also lowers interference in the memory sub-system (caches). While DMA collectives show strong promise, prior works have only studied them in limited context (bandwidth-bound transfer sizes only, performance-only).\n  To address this, we provide a comprehensive performance, power/energy and synchronization costs analysis of offloading ML communication collectives (all-gather, all-to-all) to DMA engines on state-of-the-art AMD Instinct MI300X GPUs. Our analysis reveals that, compared to the state-of-the-art RCCL communication collectives library, DMA collectives are at-par or better for large sizes (10s of MB to GB) in terms of both performance (16% better) and power (32% better). However, they significantly lag for latency-bound small sizes; 4.5X and 2.5X slower for all-gather and all-to-all, respectively. We provide a detailed latency breakdown of a DMA transfer and identify that DMA command scheduling and synchronization costs can limit DMA collective performance. To tackle this, we harness existing DMA architecture innovations, hitherto untapped, to build optimized DMA collectives and demonstrate their efficacy on real hardware. Our optimized implementations considerably close the performance gap for DMA collectives at smaller sizes (30% slower and 20% faster all-gather and all-to-all, respectively) and further improves performance (by 7%) and power savings at larger sizes (3-10%). Overall, this work represents a significant step toward making DMA collectives suitable for adoption in mainstream collective libraries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offloading machine learning (ML) communication collectives to direct memory access (DMA) engines has emerged as an interesting and low-cost solution to efficiently overlap computation and communication in inference and training. Doing so delivers superior concurrent performance by freeing up all GPU cores for computation and also lowers interference in the memory sub-system (caches). While DMA collectives show strong promise, prior works have only studied them in limited context (bandwidth-bound transfer sizes only, performance-only).\n  To address this, we provide a comprehensive performance, power/energy and synchronization costs analysis of offloading ML communication collectives (all-gather, all-to-all) to DMA engines on state-of-the-art AMD Instinct MI300X GPUs. Our analysis reveals that, compared to the state-of-the-art RCCL communication collectives library, DMA collectives are at-par or better for large sizes (10s of MB to GB) in terms of both performance (16% better) and power (32% better). However, they significantly lag for latency-bound small sizes; 4.5X and 2.5X slower for all-gather and all-to-all, respectively. We provide a detailed latency breakdown of a DMA transfer and identify that DMA command scheduling and synchronization costs can limit DMA collective performance. To tackle this, we harness existing DMA architecture innovations, hitherto untapped, to build optimized DMA collectives and demonstrate their efficacy on real hardware. Our optimized implementations considerably close the performance gap for DMA collectives at smaller sizes (30% slower and 20% faster all-gather and all-to-all, respectively) and further improves performance (by 7%) and power savings at larger sizes (3-10%). Overall, this work represents a significant step toward making DMA collectives suitable for adoption in mainstream collective libraries."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T01:28:58Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    1,
                    28,
                    58,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Mohamed Assem Ibrahim"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Assem Ibrahim"
                },
                "author": "Mohamed Assem Ibrahim"
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.16002v4",
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "updated": "2025-11-10T00:11:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    0,
                    11,
                    57,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.16002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.16002v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we investigate a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation when using KV caches computed independently for each document, KVLink introduces two key techniques: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, and using trainable special tokens to restore self-attention across independently encoded documents. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 96% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Additionally, KVLink can be combined with KV cache compression to further save cache loading and storage overhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we investigate a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation when using KV caches computed independently for each document, KVLink introduces two key techniques: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, and using trainable special tokens to restore self-attention across independently encoded documents. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 96% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Additionally, KVLink can be combined with KV cache compression to further save cache loading and storage overhead while outperforming the baselines."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang"
            },
            {
                "id": "http://arxiv.org/abs/2510.13797v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13797v2",
                "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons"
                },
                "updated": "2025-11-10T00:06:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    0,
                    6,
                    46,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13797v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13797v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T17:57:21Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Yair Feldman"
                    },
                    {
                        "name": "Shankar Padmanabhan"
                    },
                    {
                        "name": "Kianté Brantley"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi"
            },
            {
                "id": "http://arxiv.org/abs/2511.06460v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06460v1",
                "title": "Guidelines for Building Indexes on Partially Cache-Coherent CXL Shared Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guidelines for Building Indexes on Partially Cache-Coherent CXL Shared Memory"
                },
                "updated": "2025-11-09T16:55:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    9,
                    16,
                    55,
                    0,
                    6,
                    313,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06460v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The \\emph{Partial Cache-Coherence (PCC)} model maintains hardware cache coherence only within subsets of cores, enabling large-scale memory sharing with emerging memory interconnect technologies like Compute Express Link (CXL). However, PCC's relaxation of global cache coherence compromises the correctness of existing single-machine software.\n  This paper focuses on building consistent and efficient indexes on PCC platforms. We present that existing indexes designed for cache-coherent platforms can be made consistent on PCC platforms following SP guidelines, i.e., we identify \\emph{sync-data} and \\emph{protected-data} according to the index's concurrency control mechanisms, and synchronize them accordingly. However, conversion with SP guidelines introduces performance overhead. To mitigate the overhead, we identify several unique performance bottlenecks on PCC platforms, and propose P$^3$ guidelines (i.e., using Out-of-\\underline{P}lace update, Re\\underline{P}licated shared variable, S\\underline{P}eculative Reading) to improve the efficiency of converted indexes on PCC platforms.\n  With SP and P$^3$ guidelines, we convert and optimize two indexes (CLevelHash and BwTree) for PCC platforms. Evaluation shows that converted indexes' throughput improves up to 16$\\times$ following P$^3$ guidelines, and the optimized indexes outperform their message-passing-based and disaggregated-memory-based counterparts by up to 16$\\times$ and 19$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \\emph{Partial Cache-Coherence (PCC)} model maintains hardware cache coherence only within subsets of cores, enabling large-scale memory sharing with emerging memory interconnect technologies like Compute Express Link (CXL). However, PCC's relaxation of global cache coherence compromises the correctness of existing single-machine software.\n  This paper focuses on building consistent and efficient indexes on PCC platforms. We present that existing indexes designed for cache-coherent platforms can be made consistent on PCC platforms following SP guidelines, i.e., we identify \\emph{sync-data} and \\emph{protected-data} according to the index's concurrency control mechanisms, and synchronize them accordingly. However, conversion with SP guidelines introduces performance overhead. To mitigate the overhead, we identify several unique performance bottlenecks on PCC platforms, and propose P$^3$ guidelines (i.e., using Out-of-\\underline{P}lace update, Re\\underline{P}licated shared variable, S\\underline{P}eculative Reading) to improve the efficiency of converted indexes on PCC platforms.\n  With SP and P$^3$ guidelines, we convert and optimize two indexes (CLevelHash and BwTree) for PCC platforms. Evaluation shows that converted indexes' throughput improves up to 16$\\times$ following P$^3$ guidelines, and the optimized indexes outperform their message-passing-based and disaggregated-memory-based counterparts by up to 16$\\times$ and 19$\\times$."
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-09T16:55:00Z",
                "published_parsed": [
                    2025,
                    11,
                    9,
                    16,
                    55,
                    0,
                    6,
                    313,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Fangnuo Wu"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Wenjun Cai"
                    },
                    {
                        "name": "Jingsheng Yan"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.06446v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06446v1",
                "title": "SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention"
                },
                "updated": "2025-11-09T16:27:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    9,
                    16,
                    27,
                    55,
                    6,
                    313,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06446v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries. Unlike traditional retrieval-augmented generation methods that rely heavily on the performance of external retrievers and multi-stage pipelines, SR-KI supports end-to-end inference by performing retrieval entirely within the models latent space. This design enables efficient compression of injected knowledge and facilitates dynamic knowledge updates. Comprehensive experiments demonstrate that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries. Unlike traditional retrieval-augmented generation methods that rely heavily on the performance of external retrievers and multi-stage pipelines, SR-KI supports end-to-end inference by performing retrieval entirely within the models latent space. This design enables efficient compression of injected knowledge and facilitates dynamic knowledge updates. Comprehensive experiments demonstrate that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-09T16:27:55Z",
                "published_parsed": [
                    2025,
                    11,
                    9,
                    16,
                    27,
                    55,
                    6,
                    313,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.06174v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06174v1",
                "title": "LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs"
                },
                "updated": "2025-11-09T01:17:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    9,
                    1,
                    17,
                    8,
                    6,
                    313,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06174v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid progress of large language models (LLMs) has advanced numerous applications, yet efficient single-batch inference remains vital for on-device intelligence. While FPGAs offer fine-grained data control and high energy efficiency, recent GPU optimizations have narrowed their advantage, especially under arithmetic-based computation. To overcome this, we leverage FPGAs' abundant on-chip memory to shift LLM inference from arithmetic- to memory-based computation through table lookups. We present LUT-LLM, the first FPGA accelerator enabling 1B+ LLM inference via vector-quantized memory operations. Our analysis identifies activation-weight co-quantization as the most effective scheme, supported by (1) bandwidth-aware parallel centroid search, (2) efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency gain over A100.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of large language models (LLMs) has advanced numerous applications, yet efficient single-batch inference remains vital for on-device intelligence. While FPGAs offer fine-grained data control and high energy efficiency, recent GPU optimizations have narrowed their advantage, especially under arithmetic-based computation. To overcome this, we leverage FPGAs' abundant on-chip memory to shift LLM inference from arithmetic- to memory-based computation through table lookups. We present LUT-LLM, the first FPGA accelerator enabling 1B+ LLM inference via vector-quantized memory operations. Our analysis identifies activation-weight co-quantization as the most effective scheme, supported by (1) bandwidth-aware parallel centroid search, (2) efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency gain over A100."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-09T01:17:08Z",
                "published_parsed": [
                    2025,
                    11,
                    9,
                    1,
                    17,
                    8,
                    6,
                    313,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Zifan He"
                    },
                    {
                        "name": "Shengyu Ye"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong"
            },
            {
                "id": "http://arxiv.org/abs/2511.06010v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06010v1",
                "title": "MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference"
                },
                "updated": "2025-11-08T13:40:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    8,
                    13,
                    40,
                    16,
                    5,
                    312,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06010v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/LCA.2025.3627539",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The escalating context length in Large Language Models (LLMs) creates a severe performance bottleneck around the Key-Value (KV) cache, whose memory-bound nature leads to significant GPU under-utilization. This paper introduces Mixture of Shared KV Attention (MoSKA), an architecture that addresses this challenge by exploiting the heterogeneity of context data. It differentiates between per-request unique and massively reused shared sequences. The core of MoSKA is a novel Shared KV Attention mechanism that transforms the attention on shared data from a series of memory-bound GEMV operations into a single, compute-bound GEMM by batching concurrent requests. This is supported by an MoE-inspired sparse attention strategy that prunes the search space and a tailored Disaggregated Infrastructure that specializes hardware for unique and shared data. This comprehensive approach demonstrates a throughput increase of up to 538.7x over baselines in workloads with high context sharing, offering a clear architectural path toward scalable LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The escalating context length in Large Language Models (LLMs) creates a severe performance bottleneck around the Key-Value (KV) cache, whose memory-bound nature leads to significant GPU under-utilization. This paper introduces Mixture of Shared KV Attention (MoSKA), an architecture that addresses this challenge by exploiting the heterogeneity of context data. It differentiates between per-request unique and massively reused shared sequences. The core of MoSKA is a novel Shared KV Attention mechanism that transforms the attention on shared data from a series of memory-bound GEMV operations into a single, compute-bound GEMM by batching concurrent requests. This is supported by an MoE-inspired sparse attention strategy that prunes the search space and a tailored Disaggregated Infrastructure that specializes hardware for unique and shared data. This comprehensive approach demonstrates a throughput increase of up to 538.7x over baselines in workloads with high context sharing, offering a clear architectural path toward scalable LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T13:40:16Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    13,
                    40,
                    16,
                    5,
                    312,
                    0
                ],
                "arxiv_comment": "4 pages, 5 figures, accepted for publication at IEEE Computer Architecture Letters (IEEE CAL), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Myunghyun Rhee"
                    },
                    {
                        "name": "Sookyung Choi"
                    },
                    {
                        "name": "Euiseok Kim"
                    },
                    {
                        "name": "Joonseop Sim"
                    },
                    {
                        "name": "Youngpyo Joo"
                    },
                    {
                        "name": "Hoshik Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hoshik Kim"
                },
                "author": "Hoshik Kim",
                "arxiv_doi": "10.1109/LCA.2025.3627539"
            },
            {
                "id": "http://arxiv.org/abs/2508.02240v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.02240v3",
                "title": "Forecasting When to Forecast: Accelerating Diffusion Models with Confidence-Gated Taylor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting When to Forecast: Accelerating Diffusion Models with Confidence-Gated Taylor"
                },
                "updated": "2025-11-08T11:53:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    8,
                    11,
                    53,
                    33,
                    5,
                    312,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.02240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.02240v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in visual generation tasks. However, their low inference speed limits their deployment in low-resource applications. Recent training-free approaches exploit the redundancy of features across timesteps by caching and reusing past representations to accelerate inference. Building on this idea, TaylorSeer instead uses cached features to predict future ones via Taylor expansion. However, its module-level prediction across all transformer blocks (e.g., attention or feedforward modules) requires storing fine-grained intermediate features, leading to notable memory and computation overhead. Moreover, it adopts a fixed caching schedule without considering the varying accuracy of predictions across timesteps, which can lead to degraded outputs when prediction fails. To address these limitations, we propose a novel approach to better leverage Taylor-based acceleration. First, we shift the Taylor prediction target from the module level to the last block level, significantly reducing the number of cached features. Furthermore, observing strong sequential dependencies among Transformer blocks, we propose to use the error between the Taylor-estimated and actual outputs of the first block as an indicator of prediction reliability. If the error is small, we trust the Taylor prediction for the last block; otherwise, we fall back to full computation, thereby enabling a dynamic caching mechanism. Empirical results show that our method achieves a better balance between speed and quality, achieving a 3.17x acceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible quality drop. The Project Page is \\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in visual generation tasks. However, their low inference speed limits their deployment in low-resource applications. Recent training-free approaches exploit the redundancy of features across timesteps by caching and reusing past representations to accelerate inference. Building on this idea, TaylorSeer instead uses cached features to predict future ones via Taylor expansion. However, its module-level prediction across all transformer blocks (e.g., attention or feedforward modules) requires storing fine-grained intermediate features, leading to notable memory and computation overhead. Moreover, it adopts a fixed caching schedule without considering the varying accuracy of predictions across timesteps, which can lead to degraded outputs when prediction fails. To address these limitations, we propose a novel approach to better leverage Taylor-based acceleration. First, we shift the Taylor prediction target from the module level to the last block level, significantly reducing the number of cached features. Furthermore, observing strong sequential dependencies among Transformer blocks, we propose to use the error between the Taylor-estimated and actual outputs of the first block as an indicator of prediction reliability. If the error is small, we trust the Taylor prediction for the last block; otherwise, we fall back to full computation, thereby enabling a dynamic caching mechanism. Empirical results show that our method achieves a better balance between speed and quality, achieving a 3.17x acceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible quality drop. The Project Page is \\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-04T09:39:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    31,
                    0,
                    216,
                    0
                ],
                "arxiv_comment": "15 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Jiaxing Yan"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Zetao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu"
            },
            {
                "id": "http://arxiv.org/abs/2511.05958v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05958v1",
                "title": "MT4G: A Tool for Reliable Auto-Discovery of NVIDIA and AMD GPU Compute and Memory Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MT4G: A Tool for Reliable Auto-Discovery of NVIDIA and AMD GPU Compute and Memory Topologies"
                },
                "updated": "2025-11-08T10:27:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    8,
                    10,
                    27,
                    32,
                    5,
                    312,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05958v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3731599.3767518",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Understanding GPU topology is essential for performance-related tasks in HPC or AI. Yet, unlike for CPUs with tools like hwloc, GPU information is hard to come by, incomplete, and vendor-specific.\n  In this work, we address this gap and present MT4G, an open-source and vendor-agnostic tool that automatically discovers GPU compute and memory topologies and configurations, including cache sizes, bandwidths, and physical layouts. MT4G combines existing APIs with a suite of over 50 microbenchmarks, applying statistical methods, such as the Kolmogorov-Smirnov test, to automatically and reliably identify otherwise programmatically unavailable topological attributes.\n  We showcase MT4G's universality on ten different GPUs and demonstrate its impact through integration into three workflows: GPU performance modeling, GPUscout bottleneck analysis, and dynamic resource partitioning. These scenarios highlight MT4G's role in understanding system performance and characteristics across NVIDIA and AMD GPUs, providing an automated, portable solution for modern HPC and AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding GPU topology is essential for performance-related tasks in HPC or AI. Yet, unlike for CPUs with tools like hwloc, GPU information is hard to come by, incomplete, and vendor-specific.\n  In this work, we address this gap and present MT4G, an open-source and vendor-agnostic tool that automatically discovers GPU compute and memory topologies and configurations, including cache sizes, bandwidths, and physical layouts. MT4G combines existing APIs with a suite of over 50 microbenchmarks, applying statistical methods, such as the Kolmogorov-Smirnov test, to automatically and reliably identify otherwise programmatically unavailable topological attributes.\n  We showcase MT4G's universality on ten different GPUs and demonstrate its impact through integration into three workflows: GPU performance modeling, GPUscout bottleneck analysis, and dynamic resource partitioning. These scenarios highlight MT4G's role in understanding system performance and characteristics across NVIDIA and AMD GPUs, providing an automated, portable solution for modern HPC and AI systems."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T10:27:32Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    10,
                    27,
                    32,
                    5,
                    312,
                    0
                ],
                "arxiv_comment": "14 pages, including Appendix and References, 5 figures, to be published in Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC Workshops '25)",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Stepan Vanecek"
                    },
                    {
                        "name": "Manuel Walter Mussbacher"
                    },
                    {
                        "name": "Dominik Groessler"
                    },
                    {
                        "name": "Urvij Saroliya"
                    },
                    {
                        "name": "Martin Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Martin Schulz"
                },
                "author": "Martin Schulz",
                "arxiv_doi": "10.1145/3731599.3767518"
            },
            {
                "id": "http://arxiv.org/abs/2511.05814v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05814v1",
                "title": "In-depth Analysis on Caching and Pre-fetching in Mixture of Experts Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-depth Analysis on Caching and Pre-fetching in Mixture of Experts Offloading"
                },
                "updated": "2025-11-08T03:04:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    8,
                    3,
                    4,
                    11,
                    5,
                    312,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05814v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In today's landscape, Mixture of Experts (MoE) is a crucial architecture that has been used by many of the most advanced models. One of the major challenges of MoE models is that they usually require much more memory than their dense counterparts due to their unique architecture, and hence are harder to deploy in environments with limited GPU memory, such as edge devices. MoE offloading is a promising technique proposed to overcome this challenge, especially if it is enhanced with caching and pre-fetching, but prior work stopped at suboptimal caching algorithm and offered limited insights. In this work, we study MoE offloading in depth and make the following contributions: 1. We analyze the expert activation and LRU caching behavior in detail and provide traces. 2. We propose LFU caching optimization based on our analysis and obtain strong improvements from LRU. 3. We implement and experiment speculative expert pre-fetching, providing detailed trace showing its huge potential . 4. In addition, our study extensively covers the behavior of the MoE architecture itself, offering information on the characteristic of the gating network and experts. This can inspire future work on the interpretation of MoE models and the development of pruning techniques for MoE architecture with minimal performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In today's landscape, Mixture of Experts (MoE) is a crucial architecture that has been used by many of the most advanced models. One of the major challenges of MoE models is that they usually require much more memory than their dense counterparts due to their unique architecture, and hence are harder to deploy in environments with limited GPU memory, such as edge devices. MoE offloading is a promising technique proposed to overcome this challenge, especially if it is enhanced with caching and pre-fetching, but prior work stopped at suboptimal caching algorithm and offered limited insights. In this work, we study MoE offloading in depth and make the following contributions: 1. We analyze the expert activation and LRU caching behavior in detail and provide traces. 2. We propose LFU caching optimization based on our analysis and obtain strong improvements from LRU. 3. We implement and experiment speculative expert pre-fetching, providing detailed trace showing its huge potential . 4. In addition, our study extensively covers the behavior of the MoE architecture itself, offering information on the characteristic of the gating network and experts. This can inspire future work on the interpretation of MoE models and the development of pruning techniques for MoE architecture with minimal performance loss."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T03:04:11Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    3,
                    4,
                    11,
                    5,
                    312,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Shuning Lin"
                    },
                    {
                        "name": "Yifan He"
                    },
                    {
                        "name": "Yitong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yitong Chen"
                },
                "author": "Yitong Chen"
            },
            {
                "id": "http://arxiv.org/abs/2504.06319v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.06319v2",
                "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching"
                },
                "updated": "2025-11-08T02:40:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    8,
                    2,
                    40,
                    48,
                    5,
                    312,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.06319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.06319v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-08T09:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "arxiv_comment": "8 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yanhao Dong"
                    },
                    {
                        "name": "Yubo Miao"
                    },
                    {
                        "name": "Weinan Li"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Jiesheng Wu"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu"
            },
            {
                "id": "http://arxiv.org/abs/2508.17032v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.17032v2",
                "title": "Learned Structure in Cartridges: Keys as Shareable Routers in Self-Studied Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Structure in Cartridges: Keys as Shareable Routers in Self-Studied Representations"
                },
                "updated": "2025-11-07T21:01:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    21,
                    1,
                    10,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.17032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.17032v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A bottleneck for long-context LLM inference is the linearly growing KV cache. Recent work has proposed Cartridges, an approach which leverages offline compute to train a much smaller KV cache than is typically required for a full document (up to 40x less memory usage at inference time). In this paper, we present the first mechanistic exploration of the learned Cartridge key-value cache structure. In particular, we propose that (1) Cartridge keys act as stable, shareable retrieval routers for the compressed corpora and (2) most of the learned compression occurs within the Cartridge value vectors. We present empirical evidence of our routing theory across tasks, model families, and model sizes; for example, we can ablate the learned Cartridge key vectors between tasks with little performance loss. Finally, we propose a slight improvement in initialization called Sampled Chunk Initialization (SCI). We suggest that SCI can lead to faster Cartridge convergence than previously demonstrated in the literature. Our findings lay the groundwork for broader empirical study of Cartridge training optimization which may be crucial for further scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A bottleneck for long-context LLM inference is the linearly growing KV cache. Recent work has proposed Cartridges, an approach which leverages offline compute to train a much smaller KV cache than is typically required for a full document (up to 40x less memory usage at inference time). In this paper, we present the first mechanistic exploration of the learned Cartridge key-value cache structure. In particular, we propose that (1) Cartridge keys act as stable, shareable retrieval routers for the compressed corpora and (2) most of the learned compression occurs within the Cartridge value vectors. We present empirical evidence of our routing theory across tasks, model families, and model sizes; for example, we can ablate the learned Cartridge key vectors between tasks with little performance loss. Finally, we propose a slight improvement in initialization called Sampled Chunk Initialization (SCI). We suggest that SCI can lead to faster Cartridge convergence than previously demonstrated in the literature. Our findings lay the groundwork for broader empirical study of Cartridge training optimization which may be crucial for further scaling."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-23T14:20:06Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Maurizio Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Diaz"
                },
                "author": "Maurizio Diaz"
            },
            {
                "id": "http://arxiv.org/abs/2511.03092v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.03092v3",
                "title": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators"
                },
                "updated": "2025-11-07T19:27:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    19,
                    27,
                    58,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.03092v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.03092v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-05T00:38:31Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    0,
                    38,
                    31,
                    2,
                    309,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Nasim Farahini"
                    },
                    {
                        "name": "Evgenii Iuliugin"
                    },
                    {
                        "name": "Magnus Vesterlund"
                    },
                    {
                        "name": "Christian Häggström"
                    },
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Ayush Sachdeva"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Faline Fu"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Ayesha Siddiqua"
                    },
                    {
                        "name": "John Long"
                    },
                    {
                        "name": "Tuowen Zhao"
                    },
                    {
                        "name": "Matheen Musaddiq"
                    },
                    {
                        "name": "Håkan Zeffer"
                    },
                    {
                        "name": "Yun Du"
                    },
                    {
                        "name": "Mingran Wang"
                    },
                    {
                        "name": "Qinghua Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    },
                    {
                        "name": "Raghu Prabhakar"
                    }
                ],
                "author_detail": {
                    "name": "Raghu Prabhakar"
                },
                "author": "Raghu Prabhakar"
            },
            {
                "id": "http://arxiv.org/abs/2406.06483v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2406.06483v4",
                "title": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection Correctness, Security, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection Correctness, Security, and Performance"
                },
                "updated": "2025-11-07T18:03:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    3,
                    32,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2406.06483v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2406.06483v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The battle for a more secure Internet is waged on many fronts, including the most basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an IPv4 header field as old as the Internet with an equally long history as an exploited side channel for scanning network properties, inferring off-path connections, and poisoning DNS caches. This article taxonomizes the 25-year history of IPID-based exploits and the corresponding changes to IPID selection methods. By mathematically analyzing these methods' correctness and security and empirically evaluating their performance, we reveal recommendations for best practice as well as shortcomings of current operating system implementations, emphasizing the value of systematic evaluations in network security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The battle for a more secure Internet is waged on many fronts, including the most basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an IPv4 header field as old as the Internet with an equally long history as an exploited side channel for scanning network properties, inferring off-path connections, and poisoning DNS caches. This article taxonomizes the 25-year history of IPID-based exploits and the corresponding changes to IPID selection methods. By mathematically analyzing these methods' correctness and security and empirically evaluating their performance, we reveal recommendations for best practice as well as shortcomings of current operating system implementations, emphasizing the value of systematic evaluations in network security."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-06-10T17:22:17Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    22,
                    17,
                    0,
                    162,
                    0
                ],
                "arxiv_comment": "37 pages, 11 figures, 2 tables, 1 algorithm",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Joshua J. Daymude"
                    },
                    {
                        "name": "Antonio M. Espinoza"
                    },
                    {
                        "name": "Holly Bergen"
                    },
                    {
                        "name": "Benjamin Mixon-Baca"
                    },
                    {
                        "name": "Jeffrey Knockel"
                    },
                    {
                        "name": "Jedidiah R. Crandall"
                    }
                ],
                "author_detail": {
                    "name": "Jedidiah R. Crandall"
                },
                "author": "Jedidiah R. Crandall"
            },
            {
                "id": "http://arxiv.org/abs/2506.05345v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.05345v2",
                "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Hyper-Scaling with KV Cache Compression"
                },
                "updated": "2025-11-07T16:42:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    42,
                    30,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.05345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.05345v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8$\\times$ compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference latency and memory load. For instance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and 9.7 on LiveCodeBench on average for an equivalent number of memory reads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8$\\times$ compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference latency and memory load. For instance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and 9.7 on LiveCodeBench on average for an equivalent number of memory reads."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti"
            },
            {
                "id": "http://arxiv.org/abs/2511.05299v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05299v1",
                "title": "LiveStar: Live Streaming Assistant for Real-World Online Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveStar: Live Streaming Assistant for Real-World Online Video Understanding"
                },
                "updated": "2025-11-07T15:00:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    0,
                    37,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05299v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-07T15:00:37Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    0,
                    37,
                    4,
                    311,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 Accepted",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Kairui Zhang"
                    },
                    {
                        "name": "Yuhang Hu"
                    },
                    {
                        "name": "Bing Wang"
                    },
                    {
                        "name": "Shengsheng Qian"
                    },
                    {
                        "name": "Bin Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Weiming Dong"
                    },
                    {
                        "name": "Changsheng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Changsheng Xu"
                },
                "author": "Changsheng Xu"
            },
            {
                "id": "http://arxiv.org/abs/2511.05060v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05060v1",
                "title": "kV-Class Lateral NiOx/GaN Super-Heterojunction Diode via Ammonia Molecular Beam Epitaxy (NH3-MBE)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "kV-Class Lateral NiOx/GaN Super-Heterojunction Diode via Ammonia Molecular Beam Epitaxy (NH3-MBE)"
                },
                "updated": "2025-11-07T08:07:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    8,
                    7,
                    19,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05060v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work reports the demonstration of lateral p-NiOx/p-GaN/n-GaN-based super-heterojunction (SHJ) diodes using p-GaN with additional sputtered p-type nickel oxide (NiOx) layers to realize charge-balanced structures. The heterojunction diode capacitance-voltage (C-V) model is applied to extract effective the acceptor concentration from the p-NiOx. Net donor and acceptor concentration in n-GaN and p-GaN are extracted by using metal-oxide-semiconductor (MOS) test structures. The fabricated p-NiOx/p-GaN/n-GaN SHJ diodes with charge-balanced region between anode and cathode exhibit a forward on-state current density of 10-30 mA/mm across an anode-to-cathode distance (LAC) from 16 μm to 80 μm. The SHJ diodes show rectifying behavior with a maximum on/off ratio of 10^9 and a low reverse leakage density. The highest breakdown voltage achieved for the SHJ diodes is ~2.8 kV with reverse leakage density of 10^-4 mA/mm at ~80% of devices catastrophic breakdown voltage. The SHJ diodes across all types of dimensions exhibit significant breakdown voltage improvements (~6X on average) with ultra-low reverse leakage current compared to corresponding reference structures without a charge-balanced extension, clearly demonstrating the superjunction effect for devices fabricated on GaN epitaxial layer with ~10^17 cm^-3 electron density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the demonstration of lateral p-NiOx/p-GaN/n-GaN-based super-heterojunction (SHJ) diodes using p-GaN with additional sputtered p-type nickel oxide (NiOx) layers to realize charge-balanced structures. The heterojunction diode capacitance-voltage (C-V) model is applied to extract effective the acceptor concentration from the p-NiOx. Net donor and acceptor concentration in n-GaN and p-GaN are extracted by using metal-oxide-semiconductor (MOS) test structures. The fabricated p-NiOx/p-GaN/n-GaN SHJ diodes with charge-balanced region between anode and cathode exhibit a forward on-state current density of 10-30 mA/mm across an anode-to-cathode distance (LAC) from 16 μm to 80 μm. The SHJ diodes show rectifying behavior with a maximum on/off ratio of 10^9 and a low reverse leakage density. The highest breakdown voltage achieved for the SHJ diodes is ~2.8 kV with reverse leakage density of 10^-4 mA/mm at ~80% of devices catastrophic breakdown voltage. The SHJ diodes across all types of dimensions exhibit significant breakdown voltage improvements (~6X on average) with ultra-low reverse leakage current compared to corresponding reference structures without a charge-balanced extension, clearly demonstrating the superjunction effect for devices fabricated on GaN epitaxial layer with ~10^17 cm^-3 electron density."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-07T08:07:19Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    8,
                    7,
                    19,
                    4,
                    311,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Zachary J. Biegler"
                    },
                    {
                        "name": "Ashley E. Wissel-Garcia"
                    },
                    {
                        "name": "James S. Speck"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy"
            },
            {
                "id": "http://arxiv.org/abs/2511.05022v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05022v1",
                "title": "AWARE: Evaluating PriorityFresh Caching for Offline Emergency Warning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AWARE: Evaluating PriorityFresh Caching for Offline Emergency Warning Systems"
                },
                "updated": "2025-11-07T06:53:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    53,
                    48,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05022v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "PriorityFresh is a semantic, actionability-first caching policy designed for offline emergency warning systems. Within the AWARE system's simulation environment, PriorityFresh optimizes which alerts to retain and surface under constrained connectivity. Experiments indicate improved actionability-first performance without harming efficiency. A separate Priority Forecasting model is used only to synthesize realistic alert sequences for controlled experiments and does not influence caching or push decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PriorityFresh is a semantic, actionability-first caching policy designed for offline emergency warning systems. Within the AWARE system's simulation environment, PriorityFresh optimizes which alerts to retain and surface under constrained connectivity. Experiments indicate improved actionability-first performance without harming efficiency. A separate Priority Forecasting model is used only to synthesize realistic alert sequences for controlled experiments and does not influence caching or push decisions."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-07T06:53:48Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    53,
                    48,
                    4,
                    311,
                    0
                ],
                "arxiv_comment": "Preprint version",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Charles Melvin"
                    },
                    {
                        "name": "N. Rich Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "N. Rich Nguyen"
                },
                "author": "N. Rich Nguyen"
            },
            {
                "id": "http://arxiv.org/abs/2511.05017v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05017v1",
                "title": "Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings"
                },
                "updated": "2025-11-07T06:39:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    39,
                    54,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05017v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations -- and to show that refining textual embeddings with visual information mitigates this issue -- we leave exploration of advanced fusion strategies for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations -- and to show that refining textual embeddings with visual information mitigates this issue -- we leave exploration of advanced fusion strategies for future work."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-07T06:39:54Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    39,
                    54,
                    4,
                    311,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Aakriti Agrawal"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Rohith Aralikatti"
                    },
                    {
                        "name": "Gauri Jagatap"
                    },
                    {
                        "name": "Jiaxin Yuan"
                    },
                    {
                        "name": "Vijay Kamarshi"
                    },
                    {
                        "name": "Andrea Fanelli"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang"
            },
            {
                "id": "http://arxiv.org/abs/2505.24095v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.24095v2",
                "title": "SkyWalker: A Locality-Aware Cross-Region Load Balancer for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyWalker: A Locality-Aware Cross-Region Load Balancer for LLM Inference"
                },
                "updated": "2025-11-06T21:05:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    21,
                    5,
                    23,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.24095v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.24095v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Serving Large Language Models (LLMs) efficiently in multi-region setups remains a challenge. Due to cost and GPU availability concerns, providers typically deploy LLMs in multiple regions using instance with long-term commitments, like reserved instances or on-premise clusters, which are often underutilized due to their region-local traffic handling and diurnal traffic variance. In this paper, we introduce SkyWalker, a multi-region load balancer for LLM inference that aggregates regional diurnal patterns through cross-region traffic handling. By doing so, SkyWalker enables providers to reserve instances based on expected global demand, rather than peak demand in each individual region. Meanwhile, SkyWalker preserves KV-Cache locality and load balancing, ensuring cost efficiency without sacrificing performance. SkyWalker achieves this with a cache-aware cross-region traffic handler and a selective pushing based load balancing mechanism. Our evaluation on real-world workloads shows that it achieves 1.12-2.06x higher throughput and 1.74-6.30x lower latency compared to existing load balancers, while reducing total serving cost by 25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) efficiently in multi-region setups remains a challenge. Due to cost and GPU availability concerns, providers typically deploy LLMs in multiple regions using instance with long-term commitments, like reserved instances or on-premise clusters, which are often underutilized due to their region-local traffic handling and diurnal traffic variance. In this paper, we introduce SkyWalker, a multi-region load balancer for LLM inference that aggregates regional diurnal patterns through cross-region traffic handling. By doing so, SkyWalker enables providers to reserve instances based on expected global demand, rather than peak demand in each individual region. Meanwhile, SkyWalker preserves KV-Cache locality and load balancing, ensuring cost efficiency without sacrificing performance. SkyWalker achieves this with a cache-aware cross-region traffic handler and a selective pushing based load balancing mechanism. Our evaluation on real-world workloads shows that it achieves 1.12-2.06x higher throughput and 1.74-6.30x lower latency compared to existing load balancers, while reducing total serving cost by 25%."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-30T00:46:18Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    46,
                    18,
                    4,
                    150,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Jamison Kerney"
                    },
                    {
                        "name": "Ethan J. Jackson"
                    },
                    {
                        "name": "Zhifei Li"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica"
            },
            {
                "id": "http://arxiv.org/abs/2511.04804v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04804v1",
                "title": "Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator"
                },
                "updated": "2025-11-06T20:49:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    20,
                    49,
                    13,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04804v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial predictor that represents f: R^d -> R^k as a globally C^r finite-element field on a learned simplicial mesh in an optionally warped input space. Each query activates exactly one simplex and at most d+1 basis functions via barycentric coordinates, yielding explicit locality, controllable smoothness, and cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with a light invertible warp and trains end-to-end with shape regularization, semi-discrete OT coverage, and differentiable edge flips. Under standard shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic approximation tasks, tabular regression/classification, and as a drop-in head on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter budgets, improves calibration (lower ECE/Brier), and reduces inference latency due to geometric locality. These properties make SiFEN a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial predictor that represents f: R^d -> R^k as a globally C^r finite-element field on a learned simplicial mesh in an optionally warped input space. Each query activates exactly one simplex and at most d+1 basis functions via barycentric coordinates, yielding explicit locality, controllable smoothness, and cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with a light invertible warp and trains end-to-end with shape regularization, semi-discrete OT coverage, and differentiable edge flips. Under standard shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic approximation tasks, tabular regression/classification, and as a drop-in head on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter budgets, improves calibration (lower ECE/Brier), and reduces inference latency due to geometric locality. These properties make SiFEN a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T20:49:13Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    20,
                    49,
                    13,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Chaymae Yahyati"
                    },
                    {
                        "name": "Ismail Lamaakal"
                    },
                    {
                        "name": "Khalid El Makkaoui"
                    },
                    {
                        "name": "Ibrahim Ouahbi"
                    },
                    {
                        "name": "Yassine Maleh"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Maleh"
                },
                "author": "Yassine Maleh"
            },
            {
                "id": "http://arxiv.org/abs/2511.04791v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04791v1",
                "title": "DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing"
                },
                "updated": "2025-11-06T20:18:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    20,
                    18,
                    34,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04791v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern LLM serving systems must sustain high throughput while meeting strict latency SLOs across two distinct inference phases: compute-intensive prefill and memory-bound decode phases. Existing approaches either (1) aggregate both phases on shared GPUs, leading to interference between prefill and decode phases, which degrades time-between-tokens (TBT); or (2) disaggregate the two phases across GPUs, improving latency but wasting resources through duplicated models and KV cache transfers. We present DuetServe, a unified LLM serving framework that achieves disaggregation-level isolation within a single GPU. DuetServe operates in aggregated mode by default and dynamically activates SM-level GPU spatial multiplexing when TBT degradation is predicted. Its key idea is to decouple prefill and decode execution only when needed through fine-grained, adaptive SM partitioning that provides phase isolation only when contention threatens latency service level objectives (SLOs). DuetServe integrates (1) an attention-aware roofline model to forecast iteration latency, (2) a partitioning optimizer that selects the optimal SM split to maximize throughput under TBT constraints, and (3) an interruption-free execution engine that eliminates CPU-GPU synchronization overhead. Evaluations show that DuetServe improves total throughput by up to 1.3x while maintaining low generation latency compared to state-of-the-art frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM serving systems must sustain high throughput while meeting strict latency SLOs across two distinct inference phases: compute-intensive prefill and memory-bound decode phases. Existing approaches either (1) aggregate both phases on shared GPUs, leading to interference between prefill and decode phases, which degrades time-between-tokens (TBT); or (2) disaggregate the two phases across GPUs, improving latency but wasting resources through duplicated models and KV cache transfers. We present DuetServe, a unified LLM serving framework that achieves disaggregation-level isolation within a single GPU. DuetServe operates in aggregated mode by default and dynamically activates SM-level GPU spatial multiplexing when TBT degradation is predicted. Its key idea is to decouple prefill and decode execution only when needed through fine-grained, adaptive SM partitioning that provides phase isolation only when contention threatens latency service level objectives (SLOs). DuetServe integrates (1) an attention-aware roofline model to forecast iteration latency, (2) a partitioning optimizer that selects the optimal SM split to maximize throughput under TBT constraints, and (3) an interruption-free execution engine that eliminates CPU-GPU synchronization overhead. Evaluations show that DuetServe improves total throughput by up to 1.3x while maintaining low generation latency compared to state-of-the-art frameworks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T20:18:34Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    20,
                    18,
                    34,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram"
            },
            {
                "id": "http://arxiv.org/abs/2506.05410v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.05410v2",
                "title": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs"
                },
                "updated": "2025-11-06T17:09:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    9,
                    52,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.05410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.05410v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in Large Language Models (LLMs) have highlighted the critical importance of extending context length, yet the quadratic complexity of attention mechanisms poses significant challenges for efficient long-context modeling. KV cache compression has emerged as a key approach to address this challenge. Through extensive empirical analysis, we reveal a fundamental yet previously overlooked asymmetry in KV caches: while adjacent keys receive similar attention weights ({\\it local homogeneity}), adjacent values demonstrate distinct {\\it heterogeneous} distributions. This key-value asymmetry reveals a critical limitation in existing compression methods that treat keys and values uniformly. To address the limitation, we propose a training-free compression framework (AsymKV) that combines homogeneity-based key merging with a mathematically proven lossless value compression. Extensive experiments demonstrate that AsymKV consistently outperforms existing long-context methods across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing SOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in this link:https://github.com/the-scale-lab/Asymkv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have highlighted the critical importance of extending context length, yet the quadratic complexity of attention mechanisms poses significant challenges for efficient long-context modeling. KV cache compression has emerged as a key approach to address this challenge. Through extensive empirical analysis, we reveal a fundamental yet previously overlooked asymmetry in KV caches: while adjacent keys receive similar attention weights ({\\it local homogeneity}), adjacent values demonstrate distinct {\\it heterogeneous} distributions. This key-value asymmetry reveals a critical limitation in existing compression methods that treat keys and values uniformly. To address the limitation, we propose a training-free compression framework (AsymKV) that combines homogeneity-based key merging with a mathematically proven lossless value compression. Extensive experiments demonstrate that AsymKV consistently outperforms existing long-context methods across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing SOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in this link:https://github.com/the-scale-lab/Asymkv."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-04T16:10:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    10,
                    44,
                    2,
                    155,
                    0
                ],
                "arxiv_comment": "14 pages,7 figures;Accepted by NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Wanyun Cui"
                    },
                    {
                        "name": "Mingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mingwei Xu"
                },
                "author": "Mingwei Xu"
            },
            {
                "id": "http://arxiv.org/abs/2511.04489v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04489v1",
                "title": "Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear Fusion"
                },
                "updated": "2025-11-06T16:08:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    8,
                    24,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04489v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "EIRENE [1] is a Monte Carlo neutral transport solver heavily used in the fusion community. EIRENE does not implement domain decomposition, making it impossible to use for simulations where the grid data does not fit on one compute node (see e.g. [2]). This paper presents a domain-decomposed Monte Carlo (DDMC) algorithm implemented in a new open source Monte Carlo code, Eiron. Two parallel algorithms currently used in EIRENE are also implemented in Eiron, and the three algorithms are compared by running strong scaling tests, with DDMC performing better than the other two algorithms in nearly all cases. On the supercomputer Mahti [3], DDMC strong scaling is superlinear for grids that do not fit into an L3 cache slice (4 MiB). The DDMC algorithm is also scaled up to 16384 cores in weak scaling tests, with a weak scaling efficiency of 45% in a high-collisional (heavier compute load) case, and 26% in a low-collisional (lighter compute load) case. We conclude that implementing this domain decomposition algorithm in EIRENE would improve performance and enable simulations that are currently impossible due to memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EIRENE [1] is a Monte Carlo neutral transport solver heavily used in the fusion community. EIRENE does not implement domain decomposition, making it impossible to use for simulations where the grid data does not fit on one compute node (see e.g. [2]). This paper presents a domain-decomposed Monte Carlo (DDMC) algorithm implemented in a new open source Monte Carlo code, Eiron. Two parallel algorithms currently used in EIRENE are also implemented in Eiron, and the three algorithms are compared by running strong scaling tests, with DDMC performing better than the other two algorithms in nearly all cases. On the supercomputer Mahti [3], DDMC strong scaling is superlinear for grids that do not fit into an L3 cache slice (4 MiB). The DDMC algorithm is also scaled up to 16384 cores in weak scaling tests, with a weak scaling efficiency of 45% in a high-collisional (heavier compute load) case, and 26% in a low-collisional (lighter compute load) case. We conclude that implementing this domain decomposition algorithm in EIRENE would improve performance and enable simulations that are currently impossible due to memory constraints."
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T16:08:24Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    8,
                    24,
                    3,
                    310,
                    0
                ],
                "arxiv_comment": "19 pages, 3 figures, submitted to Journal of Computational Physics",
                "arxiv_primary_category": {
                    "term": "physics.comp-ph"
                },
                "authors": [
                    {
                        "name": "Oskar Lappi"
                    },
                    {
                        "name": "Huw Leggate"
                    },
                    {
                        "name": "Yannick Marandet"
                    },
                    {
                        "name": "Jan Åström"
                    },
                    {
                        "name": "Keijo Heljanko"
                    },
                    {
                        "name": "Dmitriy V. Borodin"
                    }
                ],
                "author_detail": {
                    "name": "Dmitriy V. Borodin"
                },
                "author": "Dmitriy V. Borodin"
            },
            {
                "id": "http://arxiv.org/abs/2511.04464v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04464v1",
                "title": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context"
                },
                "updated": "2025-11-06T15:37:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    37,
                    11,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04464v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditional vehicle routing systems efficiently optimize singular metrics like time or distance, and when considering multiple metrics, they need more processes to optimize . However, they lack the capability to interpret and integrate the complex, semantic, and dynamic contexts of human drivers, such as multi-step tasks, situational constraints, or urgent needs. This paper introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a hybrid agentic assistant designed to augment classical pathfinding algorithms with contextual reasoning. Our approach employs a Large Language Model (LLM) agent that operates on a candidate set of routes generated by a multi-objective (time, CO2) Dijkstra algorithm. The agent evaluates these options against user-provided tasks, preferences, and avoidance rules by leveraging a pre-processed geospatial cache of urban Points of Interest (POIs). In a benchmark of realistic urban scenarios, PAVe successfully used complex user intent into appropriate route modifications, achieving over 88% accuracy in its initial route selections with a local model. We conclude that combining classical routing algorithms with an LLM-based semantic reasoning layer is a robust and effective approach for creating personalized, adaptive, and scalable solutions for urban mobility optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional vehicle routing systems efficiently optimize singular metrics like time or distance, and when considering multiple metrics, they need more processes to optimize . However, they lack the capability to interpret and integrate the complex, semantic, and dynamic contexts of human drivers, such as multi-step tasks, situational constraints, or urgent needs. This paper introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a hybrid agentic assistant designed to augment classical pathfinding algorithms with contextual reasoning. Our approach employs a Large Language Model (LLM) agent that operates on a candidate set of routes generated by a multi-objective (time, CO2) Dijkstra algorithm. The agent evaluates these options against user-provided tasks, preferences, and avoidance rules by leveraging a pre-processed geospatial cache of urban Points of Interest (POIs). In a benchmark of realistic urban scenarios, PAVe successfully used complex user intent into appropriate route modifications, achieving over 88% accuracy in its initial route selections with a local model. We conclude that combining classical routing algorithms with an LLM-based semantic reasoning layer is a robust and effective approach for creating personalized, adaptive, and scalable solutions for urban mobility optimization."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T15:37:11Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    37,
                    11,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Carnot Braun"
                    },
                    {
                        "name": "Rafael O. Jarczewski"
                    },
                    {
                        "name": "Gabriel U. Talasso"
                    },
                    {
                        "name": "Leandro A. Villas"
                    },
                    {
                        "name": "Allan M. de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Allan M. de Souza"
                },
                "author": "Allan M. de Souza"
            },
            {
                "id": "http://arxiv.org/abs/2511.04421v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04421v1",
                "title": "Temporal Action Selection for Action Chunking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Action Selection for Action Chunking"
                },
                "updated": "2025-11-06T14:52:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    52,
                    54,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04421v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Action chunking is a widely adopted approach in Learning from Demonstration (LfD). By modeling multi-step action chunks rather than single-step actions, action chunking significantly enhances modeling capabilities for human expert policies. However, the reduced decision frequency restricts the utilization of recent observations, degrading reactivity - particularly evident in the inadequate adaptation to sensor noise and dynamic environmental changes. Existing efforts to address this issue have primarily resorted to trading off reactivity against decision consistency, without achieving both. To address this limitation, we propose a novel algorithm, Temporal Action Selector (TAS), which caches predicted action chunks from multiple timesteps and dynamically selects the optimal action through a lightweight selector network. TAS achieves balanced optimization across three critical dimensions: reactivity, decision consistency, and motion coherence. Experiments across multiple tasks with diverse base policies show that TAS significantly improves success rates - yielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a base policy with residual reinforcement learning (RL) substantially enhances training efficiency and elevates the performance plateau. Experiments in both simulation and physical robots confirm the method's efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action chunking is a widely adopted approach in Learning from Demonstration (LfD). By modeling multi-step action chunks rather than single-step actions, action chunking significantly enhances modeling capabilities for human expert policies. However, the reduced decision frequency restricts the utilization of recent observations, degrading reactivity - particularly evident in the inadequate adaptation to sensor noise and dynamic environmental changes. Existing efforts to address this issue have primarily resorted to trading off reactivity against decision consistency, without achieving both. To address this limitation, we propose a novel algorithm, Temporal Action Selector (TAS), which caches predicted action chunks from multiple timesteps and dynamically selects the optimal action through a lightweight selector network. TAS achieves balanced optimization across three critical dimensions: reactivity, decision consistency, and motion coherence. Experiments across multiple tasks with diverse base policies show that TAS significantly improves success rates - yielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a base policy with residual reinforcement learning (RL) substantially enhances training efficiency and elevates the performance plateau. Experiments in both simulation and physical robots confirm the method's efficacy."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T14:52:54Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    52,
                    54,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Yueyang Weng"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    },
                    {
                        "name": "Yongjin Mu"
                    },
                    {
                        "name": "Yingcong Zhu"
                    },
                    {
                        "name": "Yanjie Li"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.04406v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04406v1",
                "title": "Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning"
                },
                "updated": "2025-11-06T14:33:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    33,
                    29,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04406v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Data quality and its effective selection are fundamental to improving the performance of machine translation models, serving as cornerstones for achieving robust and reliable translation systems. This paper presents a data selection methodology specifically designed for fine-tuning machine translation systems, which leverages the synergy between a learner model and a pre-trained reference model to enhance overall training effectiveness. By defining a learnability score, our approach systematically evaluates the utility of data points for training, ensuring that only the most relevant and impactful examples contribute to the fine-tuning process. Furthermore, our method employs a batch selection strategy which considers interdependencies among data points, optimizing the efficiency of the training process while maintaining a focus on data relevance. Experiments on English to Persian and several other language pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that our method can achieve up to a fivefold improvement in data efficiency compared to an iid baseline. Experimental results indicate that our approach improves computational efficiency by 24 when utilizing cached embeddings, as it requires fewer training data points. Additionally, it enhances generalization, resulting in superior translation performance compared to random selection method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data quality and its effective selection are fundamental to improving the performance of machine translation models, serving as cornerstones for achieving robust and reliable translation systems. This paper presents a data selection methodology specifically designed for fine-tuning machine translation systems, which leverages the synergy between a learner model and a pre-trained reference model to enhance overall training effectiveness. By defining a learnability score, our approach systematically evaluates the utility of data points for training, ensuring that only the most relevant and impactful examples contribute to the fine-tuning process. Furthermore, our method employs a batch selection strategy which considers interdependencies among data points, optimizing the efficiency of the training process while maintaining a focus on data relevance. Experiments on English to Persian and several other language pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that our method can achieve up to a fivefold improvement in data efficiency compared to an iid baseline. Experimental results indicate that our approach improves computational efficiency by 24 when utilizing cached embeddings, as it requires fewer training data points. Additionally, it enhances generalization, resulting in superior translation performance compared to random selection method."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T14:33:29Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    33,
                    29,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mohammad Amin Ghanizadeh"
                    },
                    {
                        "name": "Mohammad Javad Dousti"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Javad Dousti"
                },
                "author": "Mohammad Javad Dousti"
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.01110v3",
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory"
                },
                "updated": "2025-11-06T13:44:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    44,
                    12,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.01110v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.01110v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Thomas Köhler"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger"
            },
            {
                "id": "http://arxiv.org/abs/2511.04002v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04002v1",
                "title": "Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing"
                },
                "updated": "2025-11-06T02:55:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    2,
                    55,
                    7,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04002v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have achieved near-human performance across diverse reasoning tasks, yet their deployment on resource-constrained Internet-of-Things (IoT) devices remains impractical due to massive parameter footprints and memory-intensive autoregressive decoding. While split computing offers a promising solution by partitioning model execution between edge devices and cloud servers, existing approaches fail to address the unique challenges of autoregressive inference, particularly the iterative token generation process and expanding key-value (KV) cache requirements. This work introduces the first autoregressive-aware split computing framework designed explicitly for LLM deployment on edge devices. Our approach makes three key contributions. First, we develop one-point split compression (OPSC), a mixed-precision quantization scheme that prevents out-of-memory failures by strategically partitioning models into front-end and back-end segments with different precision levels. Second, we propose a two-stage intermediate compression pipeline that combines threshold splitting (TS) and token-wise adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations while dramatically reducing communication overhead. Third, we formulate a unified optimization framework that jointly selects optimal split points, quantization settings, and sequence lengths to satisfy strict memory and latency constraints. Extensive evaluations across diverse LLMs and hardware platforms demonstrate superior performance compared to state-of-the-art quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework achieves a 1.49 inference speedup and significant communication overhead reduction while maintaining or improving model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved near-human performance across diverse reasoning tasks, yet their deployment on resource-constrained Internet-of-Things (IoT) devices remains impractical due to massive parameter footprints and memory-intensive autoregressive decoding. While split computing offers a promising solution by partitioning model execution between edge devices and cloud servers, existing approaches fail to address the unique challenges of autoregressive inference, particularly the iterative token generation process and expanding key-value (KV) cache requirements. This work introduces the first autoregressive-aware split computing framework designed explicitly for LLM deployment on edge devices. Our approach makes three key contributions. First, we develop one-point split compression (OPSC), a mixed-precision quantization scheme that prevents out-of-memory failures by strategically partitioning models into front-end and back-end segments with different precision levels. Second, we propose a two-stage intermediate compression pipeline that combines threshold splitting (TS) and token-wise adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations while dramatically reducing communication overhead. Third, we formulate a unified optimization framework that jointly selects optimal split points, quantization settings, and sequence lengths to satisfy strict memory and latency constraints. Extensive evaluations across diverse LLMs and hardware platforms demonstrate superior performance compared to state-of-the-art quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework achieves a 1.49 inference speedup and significant communication overhead reduction while maintaining or improving model accuracy."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T02:55:07Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    2,
                    55,
                    7,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Mingyu Sung"
                    },
                    {
                        "name": "Vikas Palakonda"
                    },
                    {
                        "name": "Suhwan Im"
                    },
                    {
                        "name": "Sunghwan Moon"
                    },
                    {
                        "name": "Il-Min Kim"
                    },
                    {
                        "name": "Sangseok Yun"
                    },
                    {
                        "name": "Jae-Mo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Mo Kang"
                },
                "author": "Jae-Mo Kang"
            },
            {
                "id": "http://arxiv.org/abs/2505.24722v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.24722v2",
                "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts"
                },
                "updated": "2025-11-06T01:18:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    1,
                    18,
                    3,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.24722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.24722v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-30T15:42:42Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Neil He"
                    },
                    {
                        "name": "Rishabh Anand"
                    },
                    {
                        "name": "Hiren Madhu"
                    },
                    {
                        "name": "Ali Maatouk"
                    },
                    {
                        "name": "Smita Krishnaswamy"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying"
            },
            {
                "id": "http://arxiv.org/abs/2511.03944v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.03944v1",
                "title": "From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies"
                },
                "updated": "2025-11-06T00:42:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    0,
                    42,
                    29,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.03944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.03944v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a simple, storage-memory-economics-based heuristic for deciding when data should live in DRAM rather than on storage. Subsequent revisits to the rule largely retained that economics-only view, leaving host costs, feasibility limits, and workload behavior out of scope. This paper revisits the rule from first principles, integrating host costs, DRAM bandwidth/capacity, and physics-grounded models of SSD performance and cost, and then embedding these elements in a constraint- and workload-aware framework that yields actionable provisioning guidance. We show that, for modern AI platforms, especially GPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained random access, the DRAM-to-flash caching threshold collapses from minutes to a few seconds. This shift reframes NAND flash memory as an active data tier and exposes a broad research space across the hardware-software stack. We further introduce MQSim-Next, a calibrated SSD simulator that supports validation and sensitivity analysis and facilitates future architectural and system research. Finally, we present two concrete case studies that showcase the software system design space opened by such memory hierarchy paradigm shift. Overall, we turn a classical heuristic into an actionable, feasibility-aware analysis and provisioning framework and set the stage for further research on AI-era memory hierarchy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a simple, storage-memory-economics-based heuristic for deciding when data should live in DRAM rather than on storage. Subsequent revisits to the rule largely retained that economics-only view, leaving host costs, feasibility limits, and workload behavior out of scope. This paper revisits the rule from first principles, integrating host costs, DRAM bandwidth/capacity, and physics-grounded models of SSD performance and cost, and then embedding these elements in a constraint- and workload-aware framework that yields actionable provisioning guidance. We show that, for modern AI platforms, especially GPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained random access, the DRAM-to-flash caching threshold collapses from minutes to a few seconds. This shift reframes NAND flash memory as an active data tier and exposes a broad research space across the hardware-software stack. We further introduce MQSim-Next, a calibrated SSD simulator that supports validation and sensitivity analysis and facilitates future architectural and system research. Finally, we present two concrete case studies that showcase the software system design space opened by such memory hierarchy paradigm shift. Overall, we turn a classical heuristic into an actionable, feasibility-aware analysis and provisioning framework and set the stage for further research on AI-era memory hierarchy."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T00:42:29Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    0,
                    42,
                    29,
                    3,
                    310,
                    0
                ],
                "arxiv_comment": "13 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Chris J. Newburn"
                    },
                    {
                        "name": "Teresa Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jiangpeng Li"
                    },
                    {
                        "name": "Hao Zhong"
                    },
                    {
                        "name": "Wen-Mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Mei Hwu"
                },
                "author": "Wen-Mei Hwu"
            },
            {
                "id": "http://arxiv.org/abs/2505.22913v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.22913v2",
                "title": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference"
                },
                "updated": "2025-11-06T00:11:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    0,
                    11,
                    18,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.22913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.22913v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We demonstrate that unstructured sparsity significantly improves KV cache compression for LLMs, enabling sparsity levels up to 70% without compromising accuracy or requiring fine-tuning. We conduct a systematic exploration of pruning strategies and find per-token magnitude-based pruning as highly effective for both Key and Value caches under unstructured sparsity, surpassing prior structured pruning schemes. The Key cache benefits from prominent outlier elements, while the Value cache surprisingly benefits from a simple magnitude-based pruning despite its uniform distribution. KV cache size is the major bottleneck in decode performance due to high memory overhead for large context lengths. To address this, we use a bitmap-based sparse format and a custom attention kernel capable of compressing and directly computing over compressed caches pruned to arbitrary sparsity patterns, significantly accelerating memory-bound operations in decode computations and thereby compensating for the overhead of runtime pruning and compression. Our custom attention kernel coupled with the bitmap-based format delivers substantial compression of KV cache upto 45% of dense inference and thereby enables longer context length and increased tokens/sec throughput of upto 2.23x compared to dense inference. Our pruning mechanism and sparse attention kernel is available at https://github.com/dhjoo98/mustafar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that unstructured sparsity significantly improves KV cache compression for LLMs, enabling sparsity levels up to 70% without compromising accuracy or requiring fine-tuning. We conduct a systematic exploration of pruning strategies and find per-token magnitude-based pruning as highly effective for both Key and Value caches under unstructured sparsity, surpassing prior structured pruning schemes. The Key cache benefits from prominent outlier elements, while the Value cache surprisingly benefits from a simple magnitude-based pruning despite its uniform distribution. KV cache size is the major bottleneck in decode performance due to high memory overhead for large context lengths. To address this, we use a bitmap-based sparse format and a custom attention kernel capable of compressing and directly computing over compressed caches pruned to arbitrary sparsity patterns, significantly accelerating memory-bound operations in decode computations and thereby compensating for the overhead of runtime pruning and compression. Our custom attention kernel coupled with the bitmap-based format delivers substantial compression of KV cache upto 45% of dense inference and thereby enables longer context length and increased tokens/sec throughput of upto 2.23x compared to dense inference. Our pruning mechanism and sparse attention kernel is available at https://github.com/dhjoo98/mustafar."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-28T22:32:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "arxiv_comment": "20 pages, 9 figures, NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Donghyeon Joo"
                    },
                    {
                        "name": "Helya Hosseini"
                    },
                    {
                        "name": "Ramyad Hadidi"
                    },
                    {
                        "name": "Bahar Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Bahar Asgari"
                },
                "author": "Bahar Asgari"
            },
            {
                "id": "http://arxiv.org/abs/2511.03830v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.03830v1",
                "title": "Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification"
                },
                "updated": "2025-11-05T19:53:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    19,
                    53,
                    51,
                    2,
                    309,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.03830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.03830v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce a method for efficient multi-label text classification with large language models (LLMs), built on reformulating classification tasks as sequences of dichotomic (yes/no) decisions. Instead of generating all labels in a single structured response, each target dimension is queried independently, which, combined with a prefix caching mechanism, yields substantial efficiency gains for short-text inference without loss of accuracy. To demonstrate the approach, we focus on affective text analysis, covering 24 dimensions including emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator model (DeepSeek-V3) provides multiple annotations per text, which are aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B, Gemma3-1B). The fine-tuned models show significant improvements over zero-shot baselines, particularly on the dimensions seen during training. Our findings suggest that decomposing multi-label classification into dichotomic queries, combined with distillation and cache-aware inference, offers a scalable and effective framework for LLM-based classification. While we validate the method on affective states, the approach is general and applicable across domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for efficient multi-label text classification with large language models (LLMs), built on reformulating classification tasks as sequences of dichotomic (yes/no) decisions. Instead of generating all labels in a single structured response, each target dimension is queried independently, which, combined with a prefix caching mechanism, yields substantial efficiency gains for short-text inference without loss of accuracy. To demonstrate the approach, we focus on affective text analysis, covering 24 dimensions including emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator model (DeepSeek-V3) provides multiple annotations per text, which are aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B, Gemma3-1B). The fine-tuned models show significant improvements over zero-shot baselines, particularly on the dimensions seen during training. Our findings suggest that decomposing multi-label classification into dichotomic queries, combined with distillation and cache-aware inference, offers a scalable and effective framework for LLM-based classification. While we validate the method on affective states, the approach is general and applicable across domains."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-05T19:53:51Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    19,
                    53,
                    51,
                    2,
                    309,
                    0
                ],
                "arxiv_comment": "9 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mikołaj Langner"
                    },
                    {
                        "name": "Jan Eliasz"
                    },
                    {
                        "name": "Ewa Rudnicka"
                    },
                    {
                        "name": "Jan Kocoń"
                    }
                ],
                "author_detail": {
                    "name": "Jan Kocoń"
                },
                "author": "Jan Kocoń"
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.02558v2",
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "updated": "2025-11-05T14:29:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    29,
                    12,
                    2,
                    309,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.02558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.02558v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness. The code is available at https://github.com/OpenMOSS/Sparse-dLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness. The code is available at https://github.com/OpenMOSS/Sparse-dLLM."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "arxiv_comment": "12 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu"
            },
            {
                "id": "http://arxiv.org/abs/2511.03475v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.03475v1",
                "title": "RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse"
                },
                "updated": "2025-11-05T13:59:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.03475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.03475v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with retrieved context but often suffers from downgraded prefill performance as modern applications demand longer and more complex inputs. Existing caching techniques either preserve accuracy with low cache reuse or improve reuse at the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG system that achieves high cache reuse without sacrificing accuracy through accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items across concurrent sessions and multi-turn interactions, using efficient context indexing, ordering, and de-duplication to maximize reuse, while lightweight contextual hints maintain reasoning fidelity. It integrates seamlessly with existing LLM inference engines and improves their prefill performance by 1.5-3X over state-of-the-art methods, while preserving or even enhancing reasoning accuracy across diverse RAG and agentic AI workloads. Our code is released at: https://github.com/Edinburgh-AgenticAI/RAGBoost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with retrieved context but often suffers from downgraded prefill performance as modern applications demand longer and more complex inputs. Existing caching techniques either preserve accuracy with low cache reuse or improve reuse at the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG system that achieves high cache reuse without sacrificing accuracy through accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items across concurrent sessions and multi-turn interactions, using efficient context indexing, ordering, and de-duplication to maximize reuse, while lightweight contextual hints maintain reasoning fidelity. It integrates seamlessly with existing LLM inference engines and improves their prefill performance by 1.5-3X over state-of-the-art methods, while preserving or even enhancing reasoning accuracy across diverse RAG and agentic AI workloads. Our code is released at: https://github.com/Edinburgh-AgenticAI/RAGBoost."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-05T13:59:01Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yinsicheng Jiang"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Liang Cheng"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xuan Sun"
                    },
                    {
                        "name": "Luo Mai"
                    }
                ],
                "author_detail": {
                    "name": "Luo Mai"
                },
                "author": "Luo Mai"
            },
            {
                "id": "http://arxiv.org/abs/2506.09045v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.09045v2",
                "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagCache: Fast Video Generation with Magnitude-Aware Cache"
                },
                "updated": "2025-11-05T09:18:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    18,
                    48,
                    2,
                    309,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.09045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.09045v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features. These approaches typically require extensive calibration with curated prompts and risk inconsistent outputs due to prompt-specific overfitting. In this paper, we introduce a novel and robust discovery: a unified magnitude law observed across different models and prompts. Specifically, the magnitude ratio of successive residual outputs decreases monotonically, steadily in most timesteps while rapidly in the last several steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache) that adaptively skips unimportant timesteps using an error modeling mechanism and adaptive caching strategy. Unlike existing methods requiring dozens of curated samples for calibration, MagCache only requires a single sample for calibration. Experimental results show that MagCache achieves 2.10x-2.68x speedups on Open-Sora, CogVideoX, Wan 2.1, and HunyuanVideo, while preserving superior visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM, and PSNR, under similar computational budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features. These approaches typically require extensive calibration with curated prompts and risk inconsistent outputs due to prompt-specific overfitting. In this paper, we introduce a novel and robust discovery: a unified magnitude law observed across different models and prompts. Specifically, the magnitude ratio of successive residual outputs decreases monotonically, steadily in most timesteps while rapidly in the last several steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache) that adaptively skips unimportant timesteps using an error modeling mechanism and adaptive caching strategy. Unlike existing methods requiring dozens of curated samples for calibration, MagCache only requires a single sample for calibration. Experimental results show that MagCache achieves 2.10x-2.68x speedups on Open-Sora, CogVideoX, Wan 2.1, and HunyuanVideo, while preserving superior visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM, and PSNR, under similar computational budgets."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "arxiv_comment": "Project Page: https://zehong-ma.github.io/MagCache Accepted by NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "arxiv_journal_ref": "In Proceedings of NeurIPS 2025",
                "authors": [
                    {
                        "name": "Zehong Ma"
                    },
                    {
                        "name": "Longhui Wei"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian"
            },
            {
                "id": "http://arxiv.org/abs/2511.03159v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.03159v1",
                "title": "Joint Optimization of DNN Model Caching and Request Routing in Mobile Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Optimization of DNN Model Caching and Request Routing in Mobile Edge Computing"
                },
                "updated": "2025-11-05T03:40:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    3,
                    40,
                    44,
                    2,
                    309,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.03159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.03159v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near end-users, providing low-latency services and improving users' quality of experience (QoE). However, caching all DNN models at edge servers with limited capacity is difficult, and the impact of model loading time on QoE remains underexplored. Hence, we introduce dynamic DNNs in edge scenarios, disassembling a complete DNN model into interrelated submodels for more fine-grained and flexible model caching and request routing solutions. This raises the pressing issue of jointly deciding request routing and submodel caching for dynamic DNNs to balance model inference precision and loading latency for QoE optimization. In this paper, we study the joint dynamic model caching and request routing problem in MEC networks, aiming to maximize user request inference precision under constraints of server resources, latency, and model loading time. To tackle this problem, we propose CoCaR, an offline algorithm based on linear programming and random rounding that leverages dynamic DNNs to optimize caching and routing schemes, achieving near-optimal performance. Furthermore, we develop an online variant of CoCaR, named CoCaR-OL, enabling effective adaptation to dynamic and unpredictable online request patterns. The simulation results demonstrate that the proposed CoCaR improves the average inference precision of user requests by 46\\% compared to state-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves an improvement of no less than 32.3\\% in user QoE over competitive baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near end-users, providing low-latency services and improving users' quality of experience (QoE). However, caching all DNN models at edge servers with limited capacity is difficult, and the impact of model loading time on QoE remains underexplored. Hence, we introduce dynamic DNNs in edge scenarios, disassembling a complete DNN model into interrelated submodels for more fine-grained and flexible model caching and request routing solutions. This raises the pressing issue of jointly deciding request routing and submodel caching for dynamic DNNs to balance model inference precision and loading latency for QoE optimization. In this paper, we study the joint dynamic model caching and request routing problem in MEC networks, aiming to maximize user request inference precision under constraints of server resources, latency, and model loading time. To tackle this problem, we propose CoCaR, an offline algorithm based on linear programming and random rounding that leverages dynamic DNNs to optimize caching and routing schemes, achieving near-optimal performance. Furthermore, we develop an online variant of CoCaR, named CoCaR-OL, enabling effective adaptation to dynamic and unpredictable online request patterns. The simulation results demonstrate that the proposed CoCaR improves the average inference precision of user requests by 46\\% compared to state-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves an improvement of no less than 32.3\\% in user QoE over competitive baselines."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-05T03:40:44Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    3,
                    40,
                    44,
                    2,
                    309,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Shuting Qiu"
                    },
                    {
                        "name": "Fang Dong"
                    },
                    {
                        "name": "Siyu Tan"
                    },
                    {
                        "name": "Ruiting Zhou"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Patrick P. C. Lee"
                    },
                    {
                        "name": "Qilin Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qilin Fan"
                },
                "author": "Qilin Fan"
            },
            {
                "id": "http://arxiv.org/abs/2511.02919v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02919v1",
                "title": "Cache Mechanism for Agent RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Mechanism for Agent RAG Systems"
                },
                "updated": "2025-11-04T19:02:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    19,
                    2,
                    29,
                    1,
                    308,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02919v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in Large Language Model (LLM)-based agents have been propelled by Retrieval-Augmented Generation (RAG), which grants the models access to vast external knowledge bases. Despite RAG's success in improving agent performance, agent-level cache management, particularly constructing, maintaining, and updating a compact, relevant corpus dynamically tailored to each agent's need, remains underexplored. Therefore, we introduce ARC (Agent RAG Cache Mechanism), a novel, annotation-free caching framework that dynamically manages small, high-value corpora for each agent. By synthesizing historical query distribution patterns with the intrinsic geometry of cached items in the embedding space, ARC automatically maintains a high-relevance cache. With comprehensive experiments on three retrieval datasets, our experimental results demonstrate that ARC reduces storage requirements to 0.015% of the original corpus while offering up to 79.8% has-answer rate and reducing average retrieval latency by 80%. Our results demonstrate that ARC can drastically enhance efficiency and effectiveness in RAG-powered LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Model (LLM)-based agents have been propelled by Retrieval-Augmented Generation (RAG), which grants the models access to vast external knowledge bases. Despite RAG's success in improving agent performance, agent-level cache management, particularly constructing, maintaining, and updating a compact, relevant corpus dynamically tailored to each agent's need, remains underexplored. Therefore, we introduce ARC (Agent RAG Cache Mechanism), a novel, annotation-free caching framework that dynamically manages small, high-value corpora for each agent. By synthesizing historical query distribution patterns with the intrinsic geometry of cached items in the embedding space, ARC automatically maintains a high-relevance cache. With comprehensive experiments on three retrieval datasets, our experimental results demonstrate that ARC reduces storage requirements to 0.015% of the original corpus while offering up to 79.8% has-answer rate and reducing average retrieval latency by 80%. Our results demonstrate that ARC can drastically enhance efficiency and effectiveness in RAG-powered LLM agents."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-04T19:02:29Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    19,
                    2,
                    29,
                    1,
                    308,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Zhencan Peng"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Xiao Lin"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.02761v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02761v1",
                "title": "Non-Contact Manipulation of Induced Magnetic Dipoles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Contact Manipulation of Induced Magnetic Dipoles"
                },
                "updated": "2025-11-04T17:40:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    40,
                    31,
                    1,
                    308,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02761v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Extending the field of magnetic manipulation to conductive, non-magnetic objects opens the door for a wide array of applications previously limited to hard or soft magnetic materials. Of particular interest is the recycling of space debris through the use of oscillating magnetic fields, which represent a cache of raw materials in an environment particularly suited to the low forces generated from inductive magnetic manipulation. Building upon previous work that demonstrated 3D open-loop position control by leveraging the opposing dipole moment created from induced eddy currents, this work demonstrates closed-loop position control of a semi-buoyant aluminum sphere in lab tests, and the efficacy of varying methods for force inversion is explored. The closed-loop methods represent a critical first step towards wider applications for 3-DOF position control of induced magnetic dipoles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the field of magnetic manipulation to conductive, non-magnetic objects opens the door for a wide array of applications previously limited to hard or soft magnetic materials. Of particular interest is the recycling of space debris through the use of oscillating magnetic fields, which represent a cache of raw materials in an environment particularly suited to the low forces generated from inductive magnetic manipulation. Building upon previous work that demonstrated 3D open-loop position control by leveraging the opposing dipole moment created from induced eddy currents, this work demonstrates closed-loop position control of a semi-buoyant aluminum sphere in lab tests, and the efficacy of varying methods for force inversion is explored. The closed-loop methods represent a critical first step towards wider applications for 3-DOF position control of induced magnetic dipoles."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-04T17:40:31Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    40,
                    31,
                    1,
                    308,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Seth Stewart"
                    },
                    {
                        "name": "Joseph Pawelski"
                    },
                    {
                        "name": "Steve Ward"
                    },
                    {
                        "name": "Andrew J. Petruska"
                    }
                ],
                "author_detail": {
                    "name": "Andrew J. Petruska"
                },
                "author": "Andrew J. Petruska"
            },
            {
                "id": "http://arxiv.org/abs/2511.02749v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02749v1",
                "title": "Using Span Queries to Optimize for Cache and Attention Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Span Queries to Optimize for Cache and Attention Locality"
                },
                "updated": "2025-11-04T17:22:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    49,
                    1,
                    308,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02749v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Clients are evolving beyond chat completion, and now include a variety of innovative inference-time scaling and deep reasoning techniques. At the same time, inference servers remain heavily optimized for chat completion. Prior work has shown that large improvements to KV cache hit rate are possible if inference servers evolve towards these non-chat use cases. However, they offer solutions that are also optimized for a single use case, RAG. In this paper, we introduce the span query to generalize the interface to the inference server. We demonstrate that chat, RAG, inference-time scaling, and agentic workloads can all be expressed as span queries. We show how the critical distinction that had been assumed by prior work lies in whether the order of the inputs matter -- do they commute? In chat, they do not. In RAG, they often do. This paper introduces span queries, which are expression trees of inference calls, linked together with commutativity constraints. We describe span query syntax and semantics. We show how they can be automatically optimized to improve KV cache locality. We show how a small change to vLLM (affecting only 492 lines) can enable high-performance execution of span queries. Using this stack, we demonstrate that span queries can achieve 10-20x reductions in TTFT for two distinct non-chat use cases. Finally, we show that span queries can also be optimized to improve attention locality, so as to avoid the so-called lost-in-the-middle problem. We demonstrate that an attention-optimized span query on a 2b parameter model vastly outperforms the accuracy of a stock inference server using an 8b model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clients are evolving beyond chat completion, and now include a variety of innovative inference-time scaling and deep reasoning techniques. At the same time, inference servers remain heavily optimized for chat completion. Prior work has shown that large improvements to KV cache hit rate are possible if inference servers evolve towards these non-chat use cases. However, they offer solutions that are also optimized for a single use case, RAG. In this paper, we introduce the span query to generalize the interface to the inference server. We demonstrate that chat, RAG, inference-time scaling, and agentic workloads can all be expressed as span queries. We show how the critical distinction that had been assumed by prior work lies in whether the order of the inputs matter -- do they commute? In chat, they do not. In RAG, they often do. This paper introduces span queries, which are expression trees of inference calls, linked together with commutativity constraints. We describe span query syntax and semantics. We show how they can be automatically optimized to improve KV cache locality. We show how a small change to vLLM (affecting only 492 lines) can enable high-performance execution of span queries. Using this stack, we demonstrate that span queries can achieve 10-20x reductions in TTFT for two distinct non-chat use cases. Finally, we show that span queries can also be optimized to improve attention locality, so as to avoid the so-called lost-in-the-middle problem. We demonstrate that an attention-optimized span query on a 2b parameter model vastly outperforms the accuracy of a stock inference server using an 8b model."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-04T17:22:49Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    49,
                    1,
                    308,
                    0
                ],
                "arxiv_comment": "12 pages, 17 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Paul Castro"
                    },
                    {
                        "name": "Nick Mitchell"
                    },
                    {
                        "name": "Nathan Ordonez"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Mudhakar Srivatsa"
                    },
                    {
                        "name": "Antoni Viros i Martin"
                    }
                ],
                "author_detail": {
                    "name": "Antoni Viros i Martin"
                },
                "author": "Antoni Viros i Martin"
            },
            {
                "id": "http://arxiv.org/abs/2511.02651v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02651v1",
                "title": "Apriel-H1: Towards Efficient Enterprise Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apriel-H1: Towards Efficient Enterprise Reasoning Models"
                },
                "updated": "2025-11-04T15:17:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    43,
                    1,
                    308,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02651v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) achieve remarkable reasoning capabilities through transformer architectures with attention mechanisms. However, transformers suffer from quadratic time and memory complexity in the attention module (MHA) and require caching key-value states during inference, which severely limits throughput and scalability. High inference throughput is critical for agentic tasks, long-context reasoning, efficient deployment under high request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with linear inference complexity and a constant memory footprint via recurrent computation with fixed-size hidden states. In this technical report we introduce the Apriel-H1 family of hybrid LLMs that combine transformer attention and SSM sequence mixers for efficient reasoning at 15B model size. These models are obtained through incremental distillation from a pretrained reasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing less critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with different SSM-to-MHA ratios and analyse how reasoning performance degrades as more Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant of Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces, achieving over 2x higher inference throughput when deployed in the production-ready vLLM environment, with minimal degradation in reasoning performance. This shows that distilled hybrid SSM-Transformer architectures can deliver substantial efficiency gains over the pretrained transformer equivalent without substantially compromising the reasoning quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve remarkable reasoning capabilities through transformer architectures with attention mechanisms. However, transformers suffer from quadratic time and memory complexity in the attention module (MHA) and require caching key-value states during inference, which severely limits throughput and scalability. High inference throughput is critical for agentic tasks, long-context reasoning, efficient deployment under high request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with linear inference complexity and a constant memory footprint via recurrent computation with fixed-size hidden states. In this technical report we introduce the Apriel-H1 family of hybrid LLMs that combine transformer attention and SSM sequence mixers for efficient reasoning at 15B model size. These models are obtained through incremental distillation from a pretrained reasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing less critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with different SSM-to-MHA ratios and analyse how reasoning performance degrades as more Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant of Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces, achieving over 2x higher inference throughput when deployed in the production-ready vLLM environment, with minimal degradation in reasoning performance. This shows that distilled hybrid SSM-Transformer architectures can deliver substantial efficiency gains over the pretrained transformer equivalent without substantially compromising the reasoning quality."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-04T15:17:43Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    43,
                    1,
                    308,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Oleksiy Ostapenko"
                    },
                    {
                        "name": "Luke Kumar"
                    },
                    {
                        "name": "Raymond Li"
                    },
                    {
                        "name": "Denis Kocetkov"
                    },
                    {
                        "name": "Joel Lamy-Poirier"
                    },
                    {
                        "name": "Shruthan Radhakrishna"
                    },
                    {
                        "name": "Soham Parikh"
                    },
                    {
                        "name": "Shambhavi Mishra"
                    },
                    {
                        "name": "Sebastien Paquet"
                    },
                    {
                        "name": "Srinivas Sunkara"
                    },
                    {
                        "name": "Valérie Bécaert"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Torsten Scholak"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Scholak"
                },
                "author": "Torsten Scholak"
            },
            {
                "id": "http://arxiv.org/abs/2511.02647v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02647v1",
                "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks"
                },
                "updated": "2025-11-04T15:14:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    14,
                    58,
                    1,
                    308,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02647v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are proliferating rapidly at the edge, delivering intelligent capabilities across diverse application scenarios. However, their practical deployment in collaborative scenarios confronts fundamental challenges: privacy vulnerabilities, communication overhead, and computational bottlenecks. To address these, we propose Federated Attention (FedAttn), which integrates the federated paradigm into the self-attention mechanism, creating a new distributed LLM inference framework that simultaneously achieves privacy protection, communication efficiency, and computational efficiency. FedAttn enables participants to perform local self-attention over their own token representations while periodically exchanging and aggregating Key-Value (KV) matrices across multiple Transformer blocks, collaboratively generating LLM responses without exposing private prompts. Further, we identify a structural duality between contextual representation refinement in FedAttn and parameter optimization in FL across private data, local computation, and global aggregation. This key insight provides a principled foundation for systematically porting federated optimization techniques to collaborative LLM inference. Building on this framework, we theoretically analyze how local self-attention computation within participants and heterogeneous token relevance among participants shape error propagation dynamics across Transformer blocks. Moreover, we characterize the fundamental trade-off between response quality and communication/computation efficiency, which is governed by the synchronization interval and the number of participants. Experimental results validate our theoretical analysis, and reveal significant optimization opportunities through sparse attention and adaptive KV aggregation, highlighting FedAttn's potential to deliver scalability and efficiency in real-world edge deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are proliferating rapidly at the edge, delivering intelligent capabilities across diverse application scenarios. However, their practical deployment in collaborative scenarios confronts fundamental challenges: privacy vulnerabilities, communication overhead, and computational bottlenecks. To address these, we propose Federated Attention (FedAttn), which integrates the federated paradigm into the self-attention mechanism, creating a new distributed LLM inference framework that simultaneously achieves privacy protection, communication efficiency, and computational efficiency. FedAttn enables participants to perform local self-attention over their own token representations while periodically exchanging and aggregating Key-Value (KV) matrices across multiple Transformer blocks, collaboratively generating LLM responses without exposing private prompts. Further, we identify a structural duality between contextual representation refinement in FedAttn and parameter optimization in FL across private data, local computation, and global aggregation. This key insight provides a principled foundation for systematically porting federated optimization techniques to collaborative LLM inference. Building on this framework, we theoretically analyze how local self-attention computation within participants and heterogeneous token relevance among participants shape error propagation dynamics across Transformer blocks. Moreover, we characterize the fundamental trade-off between response quality and communication/computation efficiency, which is governed by the synchronization interval and the number of participants. Experimental results validate our theoretical analysis, and reveal significant optimization opportunities through sparse attention and adaptive KV aggregation, highlighting FedAttn's potential to deliver scalability and efficiency in real-world edge deployments."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-04T15:14:58Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    14,
                    58,
                    1,
                    308,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xiumei Deng"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Binbin Chen"
                    },
                    {
                        "name": "Dong In Kim"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "H. Vincent Poor"
                    }
                ],
                "author_detail": {
                    "name": "H. Vincent Poor"
                },
                "author": "H. Vincent Poor"
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.02770v5",
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "updated": "2025-11-04T12:04:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    4,
                    6,
                    1,
                    308,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.02770v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.02770v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\\times$ acceleration in self-attention operations and $3.9\\times$ acceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\\times$ acceleration in self-attention operations and $3.9\\times$ acceleration in end-to-end per token latency in long context LLM decoding."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "arxiv_comment": "To appear on NeurIPS 2025 (spotlight)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao"
            },
            {
                "id": "http://arxiv.org/abs/2511.02381v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02381v1",
                "title": "Laser diagnostics for negative ion source optimization: insights from SPIDER at the ITER Neutral Beam Test Facility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser diagnostics for negative ion source optimization: insights from SPIDER at the ITER Neutral Beam Test Facility"
                },
                "updated": "2025-11-04T09:03:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    3,
                    9,
                    1,
                    308,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02381v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The ITER Heating Neutral Beams (HNBs) require large, high-energy H/D atom beams (285/330 A/m^2 extracted current density, and 1/0.87 MeV acceleration energy, respectively for H and D). To address the associated challenges, the SPIDER negative ion RF beam source at the Neutral Beam Test Facility (NBTF) in Padova (Italy) serves as a full-scale source prototype with a 100 kV triode accelerator, for design validation and performance verification. SPIDER is equipped with two advanced laser diagnostics to monitor key plasma parameters; Cavity Ring-Down Spectroscopy (CRDS) is used to measure H$^-$\\slash D$^-$ ion densities, while Laser Absorption Spectroscopy (LAS) tracks caesium neutral density in the source. These measurements are essential for optimizing negative ion production and meeting ITER source targets. We present diagnostic upgrade details, recent experimental results, and correlations with other machine parameters. Since CRDS relies on a single 4.637-meter-long optical cavity, the longest used in such sources, it has demonstrated sensitivity to alignment. Based on recent experimental experience, structural improvements are being implemented to enhance both stability and measurement reliability. LAS has mainly been employed as a tool to monitor the caesium conditioning status of SPIDER. Additionally, due to a distributed measurement over four lines of sight, LAS has proven effective in monitoring the caesium distribution within the source. This work demonstrates the essential role of laser diagnostics in developing ITER-relevant plasma sources and informs ongoing efforts to improve measurement accuracy in challenging environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ITER Heating Neutral Beams (HNBs) require large, high-energy H/D atom beams (285/330 A/m^2 extracted current density, and 1/0.87 MeV acceleration energy, respectively for H and D). To address the associated challenges, the SPIDER negative ion RF beam source at the Neutral Beam Test Facility (NBTF) in Padova (Italy) serves as a full-scale source prototype with a 100 kV triode accelerator, for design validation and performance verification. SPIDER is equipped with two advanced laser diagnostics to monitor key plasma parameters; Cavity Ring-Down Spectroscopy (CRDS) is used to measure H$^-$\\slash D$^-$ ion densities, while Laser Absorption Spectroscopy (LAS) tracks caesium neutral density in the source. These measurements are essential for optimizing negative ion production and meeting ITER source targets. We present diagnostic upgrade details, recent experimental results, and correlations with other machine parameters. Since CRDS relies on a single 4.637-meter-long optical cavity, the longest used in such sources, it has demonstrated sensitivity to alignment. Based on recent experimental experience, structural improvements are being implemented to enhance both stability and measurement reliability. LAS has mainly been employed as a tool to monitor the caesium conditioning status of SPIDER. Additionally, due to a distributed measurement over four lines of sight, LAS has proven effective in monitoring the caesium distribution within the source. This work demonstrates the essential role of laser diagnostics in developing ITER-relevant plasma sources and informs ongoing efforts to improve measurement accuracy in challenging environments."
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-04T09:03:09Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    3,
                    9,
                    1,
                    308,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph"
                },
                "authors": [
                    {
                        "name": "R. Agnello"
                    },
                    {
                        "name": "M. Barbisan"
                    },
                    {
                        "name": "R. Pasqualotto"
                    },
                    {
                        "name": "B. Pouradier-Duteil"
                    },
                    {
                        "name": "E. Sartori"
                    },
                    {
                        "name": "A. Tiso"
                    },
                    {
                        "name": "B. Zaniol"
                    }
                ],
                "author_detail": {
                    "name": "B. Zaniol"
                },
                "author": "B. Zaniol"
            },
            {
                "id": "http://arxiv.org/abs/2511.02230v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02230v1",
                "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live"
                },
                "updated": "2025-11-04T03:43:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    3,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02230v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Agentic LLM applications interleave LLM generation requests with tool calls. These tool calls break the continuity of the workflow by creating pauses between LLM requests, bringing many challenges for the serving system, especially under multi-turn scenarios. Each pause potentially causes KV cache eviction and extra waiting time before entering the continuous batch for the following LLM request. Since these pauses happen for each call, this problem becomes increasingly severe as turn number grow for agentic programs. Previous works either fail to incorporate information from the tool call, evicting KV cache that leads to repetitive prefill or loading, or ignore the continuity of a multi-turn program, creating waiting time between turns that increases per-request latency.\n  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by combining tool-aware KV cache timeout with program-level scheduling. By predicting tool call durations in agentic workflows, Continuum selectively pins the KV cache in GPU memory with a time-to-live value based on total turn number. When combined with program-level first-come-first-serve, Continuum prevents scheduling bubbles, preserves multi-turn continuity, and optimizes for throughput for complex agentic workflows. By modeling the variability of tool call and agent program continuity, Continuum outperforms state-of-the-art baselines. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models shows that Continuum significantly improves the average job completion times, and remains performant across different hardware setups and DRAM offloading schemes. Preview code is available at: https://github.com/Hanchenli/vllm-continuum",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic LLM applications interleave LLM generation requests with tool calls. These tool calls break the continuity of the workflow by creating pauses between LLM requests, bringing many challenges for the serving system, especially under multi-turn scenarios. Each pause potentially causes KV cache eviction and extra waiting time before entering the continuous batch for the following LLM request. Since these pauses happen for each call, this problem becomes increasingly severe as turn number grow for agentic programs. Previous works either fail to incorporate information from the tool call, evicting KV cache that leads to repetitive prefill or loading, or ignore the continuity of a multi-turn program, creating waiting time between turns that increases per-request latency.\n  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by combining tool-aware KV cache timeout with program-level scheduling. By predicting tool call durations in agentic workflows, Continuum selectively pins the KV cache in GPU memory with a time-to-live value based on total turn number. When combined with program-level first-come-first-serve, Continuum prevents scheduling bubbles, preserves multi-turn continuity, and optimizes for throughput for complex agentic workflows. By modeling the variability of tool call and agent program continuity, Continuum outperforms state-of-the-art baselines. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models shows that Continuum significantly improves the average job completion times, and remains performant across different hardware setups and DRAM offloading schemes. Preview code is available at: https://github.com/Hanchenli/vllm-continuum"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-04T03:43:05Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    3,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Qiuyang Mang"
                    },
                    {
                        "name": "Runyuan He"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Huanzhi Mao"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Alvin Cheung"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica"
            },
            {
                "id": "http://arxiv.org/abs/2511.02132v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02132v1",
                "title": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects"
                },
                "updated": "2025-11-03T23:48:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    23,
                    48,
                    39,
                    0,
                    307,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02132v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in large-scale attention workloads: non-uniform memory access (NUMA). As multi-chiplet designs become the norm for scaling compute capabilities, memory latency and bandwidth vary sharply across compute regions, undermining the performance of traditional GPU kernel scheduling strategies that assume uniform memory access. We identify how these NUMA effects distort locality in multi-head attention (MHA) and present Swizzled Head-first Mapping, a spatially-aware scheduling strategy that aligns attention heads with GPU NUMA domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our method achieves up to 50% higher performance over state-of-the-art attention algorithms using conventional scheduling techniques and sustains consistently high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware scheduling is now fundamental to achieving full efficiency on next-generation disaggregated GPUs, offering a path forward for scalable AI training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in large-scale attention workloads: non-uniform memory access (NUMA). As multi-chiplet designs become the norm for scaling compute capabilities, memory latency and bandwidth vary sharply across compute regions, undermining the performance of traditional GPU kernel scheduling strategies that assume uniform memory access. We identify how these NUMA effects distort locality in multi-head attention (MHA) and present Swizzled Head-first Mapping, a spatially-aware scheduling strategy that aligns attention heads with GPU NUMA domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our method achieves up to 50% higher performance over state-of-the-art attention algorithms using conventional scheduling techniques and sustains consistently high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware scheduling is now fundamental to achieving full efficiency on next-generation disaggregated GPUs, offering a path forward for scalable AI training and inference."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T23:48:39Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    23,
                    48,
                    39,
                    0,
                    307,
                    0
                ],
                "arxiv_comment": "11 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Mansi Choudhary"
                    },
                    {
                        "name": "Karthik Sangaiah"
                    },
                    {
                        "name": "Sonali Singh"
                    },
                    {
                        "name": "Muhammad Osama"
                    },
                    {
                        "name": "Lisa Wu Wills"
                    },
                    {
                        "name": "Ganesh Dasika"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Dasika"
                },
                "author": "Ganesh Dasika"
            },
            {
                "id": "http://arxiv.org/abs/2511.01815v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.01815v1",
                "title": "KV Cache Transform Coding for Compact Storage in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Transform Coding for Compact Storage in LLM Inference"
                },
                "updated": "2025-11-03T18:20:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    20,
                    35,
                    0,
                    307,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.01815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.01815v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Serving large language models (LLMs) at scale necessitates efficient key-value (KV) cache management. KV caches can be reused across conversation turns via shared-prefix prompts that are common in iterative code editing and chat. However, stale caches consume scarce GPU memory, require offloading, or force recomputation. We present KVTC, a lightweight transform coder that compresses KV caches for compact on-GPU and off-GPU storage. Drawing on classical media compression, KVTC combines PCA-based feature decorrelation, adaptive quantization, and entropy coding. It requires only a brief initial calibration and leaves model parameters unchanged. By exploiting redundancies in KV caches, KVTC achieves up to 20$\\times$ compression while maintaining reasoning and long-context accuracy, and 40$\\times$ or higher for specific use cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and MATH-500. It consistently outperforms inference-time baselines such as token eviction, quantization, and SVD-based methods, while achieving higher compression ratios. These results support KVTC as a practical building block for memory-efficient LLM serving with reusable KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) at scale necessitates efficient key-value (KV) cache management. KV caches can be reused across conversation turns via shared-prefix prompts that are common in iterative code editing and chat. However, stale caches consume scarce GPU memory, require offloading, or force recomputation. We present KVTC, a lightweight transform coder that compresses KV caches for compact on-GPU and off-GPU storage. Drawing on classical media compression, KVTC combines PCA-based feature decorrelation, adaptive quantization, and entropy coding. It requires only a brief initial calibration and leaves model parameters unchanged. By exploiting redundancies in KV caches, KVTC achieves up to 20$\\times$ compression while maintaining reasoning and long-context accuracy, and 40$\\times$ or higher for specific use cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and MATH-500. It consistently outperforms inference-time baselines such as token eviction, quantization, and SVD-based methods, while achieving higher compression ratios. These results support KVTC as a practical building block for memory-efficient LLM serving with reusable KV caches."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T18:20:35Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    20,
                    35,
                    0,
                    307,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Adrian Łańcucki"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Łańcucki"
                },
                "author": "Adrian Łańcucki"
            },
            {
                "id": "http://arxiv.org/abs/2511.01633v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.01633v1",
                "title": "Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving"
                },
                "updated": "2025-11-03T14:42:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    14,
                    42,
                    53,
                    0,
                    307,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.01633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.01633v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to perform step-by-step reasoning over graph-structured knowledge, but existing pipelines suffer from low accuracy, excessive token usage, high latency, and low throughput due to single-agent monolithic prompts, repeated context re-encoding, and inefficient serving execution. We present GLM, the first multi-agent Graph-CoT system co-designed with an optimized LLM serving architecture. GLM decomposes reasoning into specialized agents for classification, reasoning, action generation, and graph retrieval, enabling branching and selective context sharing to reduce prompt length and reasoning iterations while preserving reasoning quality, thereby improving accuracy and reducing overall token consumption. To scale inference, we introduce a Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache management, priority-based eviction, and pipelined execution to improve serving efficiency. Experiments demonstrate that GLM improves answer accuracy by up to 38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT baselines, enabling efficient adoption for complex real-world reasoning at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to perform step-by-step reasoning over graph-structured knowledge, but existing pipelines suffer from low accuracy, excessive token usage, high latency, and low throughput due to single-agent monolithic prompts, repeated context re-encoding, and inefficient serving execution. We present GLM, the first multi-agent Graph-CoT system co-designed with an optimized LLM serving architecture. GLM decomposes reasoning into specialized agents for classification, reasoning, action generation, and graph retrieval, enabling branching and selective context sharing to reduce prompt length and reasoning iterations while preserving reasoning quality, thereby improving accuracy and reducing overall token consumption. To scale inference, we introduce a Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache management, priority-based eviction, and pipelined execution to improve serving efficiency. Experiments demonstrate that GLM improves answer accuracy by up to 38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT baselines, enabling efficient adoption for complex real-world reasoning at scale."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T14:42:53Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    14,
                    42,
                    53,
                    0,
                    307,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Chengying Huan"
                    },
                    {
                        "name": "Ziheng Meng"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Zhengyi Yang"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yue Yun"
                    },
                    {
                        "name": "Shipeng Li"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Xiabao Wu"
                    },
                    {
                        "name": "Haitao Zhang"
                    },
                    {
                        "name": "Chuntao Hong"
                    },
                    {
                        "name": "Shaonan Ma"
                    },
                    {
                        "name": "Guihai Chen"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian"
            },
            {
                "id": "http://arxiv.org/abs/2506.21590v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.21590v2",
                "title": "Representation Consistency for Accurate and Coherent LLM Answer Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Consistency for Accurate and Coherent LLM Answer Aggregation"
                },
                "updated": "2025-11-03T11:32:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    11,
                    32,
                    13,
                    0,
                    307,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.21590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.21590v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Test-time scaling improves large language models' (LLMs) performance by allocating more compute budget during inference. To achieve this, existing methods often require intricate modifications to prompting and sampling strategies. In this work, we introduce representation consistency (RC), a test-time scaling method for aggregating answers drawn from multiple candidate responses of an LLM regardless of how they were generated, including variations in prompt phrasing and sampling strategy. RC enhances answer aggregation by not only considering the number of occurrences of each answer in the candidate response set, but also the consistency of the model's internal activations while generating the set of responses leading to each answer. These activations can be either dense (raw model activations) or sparse (encoded via pretrained sparse autoencoders). Our rationale is that if the model's representations of multiple responses converging on the same answer are highly variable, this answer is more likely to be the result of incoherent reasoning and should be down-weighted during aggregation. Importantly, our method only uses cached activations and lightweight similarity computations and requires no additional model queries. Through experiments with four open-source LLMs and four reasoning datasets, we validate the effectiveness of RC for improving task performance during inference, with consistent accuracy improvements (up to 4%) over strong test-time scaling baselines. We also show that consistency in the sparse activation signals aligns well with the common notion of coherent reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling improves large language models' (LLMs) performance by allocating more compute budget during inference. To achieve this, existing methods often require intricate modifications to prompting and sampling strategies. In this work, we introduce representation consistency (RC), a test-time scaling method for aggregating answers drawn from multiple candidate responses of an LLM regardless of how they were generated, including variations in prompt phrasing and sampling strategy. RC enhances answer aggregation by not only considering the number of occurrences of each answer in the candidate response set, but also the consistency of the model's internal activations while generating the set of responses leading to each answer. These activations can be either dense (raw model activations) or sparse (encoded via pretrained sparse autoencoders). Our rationale is that if the model's representations of multiple responses converging on the same answer are highly variable, this answer is more likely to be the result of incoherent reasoning and should be down-weighted during aggregation. Importantly, our method only uses cached activations and lightweight similarity computations and requires no additional model queries. Through experiments with four open-source LLMs and four reasoning datasets, we validate the effectiveness of RC for improving task performance during inference, with consistent accuracy improvements (up to 4%) over strong test-time scaling baselines. We also show that consistency in the sparse activation signals aligns well with the common notion of coherent reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-18T05:07:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "arxiv_comment": "Accepted at NeurIPS 2025. Camera-ready version",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Salim I. Amoukou"
                    },
                    {
                        "name": "Francesco Leofante"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni"
            },
            {
                "id": "http://arxiv.org/abs/2511.01385v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.01385v1",
                "title": "Memory-Efficient Training with In-Place FFT Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training with In-Place FFT Implementation"
                },
                "updated": "2025-11-03T09:36:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    36,
                    11,
                    0,
                    307,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.01385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.01385v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fast Fourier Transforms (FFT) are widely used to reduce memory and computational costs in deep learning. However, existing implementations, including standard FFT and real FFT (rFFT), cannot achieve true in-place computation. In particular, rFFT maps an input of size n to a complex output of size n/2+1, causing dimensional mismatch and requiring additional memory allocation. We propose the first real-domain, fully in-place FFT framework (rdFFT) that preserves input-output memory space consistency. By leveraging butterfly operation symmetry and conjugate properties in the frequency domain, we design an implicit complex encoding scheme that eliminates intermediate cache usage entirely. Experiments on multiple natural language understanding tasks demonstrate the method effectiveness in reducing training memory cost, offering a promising direction for frequency-domain lightweight adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Fourier Transforms (FFT) are widely used to reduce memory and computational costs in deep learning. However, existing implementations, including standard FFT and real FFT (rFFT), cannot achieve true in-place computation. In particular, rFFT maps an input of size n to a complex output of size n/2+1, causing dimensional mismatch and requiring additional memory allocation. We propose the first real-domain, fully in-place FFT framework (rdFFT) that preserves input-output memory space consistency. By leveraging butterfly operation symmetry and conjugate properties in the frequency domain, we design an implicit complex encoding scheme that eliminates intermediate cache usage entirely. Experiments on multiple natural language understanding tasks demonstrate the method effectiveness in reducing training memory cost, offering a promising direction for frequency-domain lightweight adaptation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T09:36:11Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    36,
                    11,
                    0,
                    307,
                    0
                ],
                "arxiv_comment": "Accepted at NeurIPS 2025. Presents a real-domain in-place FFT (rdFFT) operator for memory-efficient fine-tuning of large language models",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xinyu Ding"
                    },
                    {
                        "name": "Bangtian Liu"
                    },
                    {
                        "name": "Siyu Liao"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.01266v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.01266v1",
                "title": "MotionStream: Real-Time Video Generation with Interactive Motion Controls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionStream: Real-Time Video Generation with Interactive Motion Controls"
                },
                "updated": "2025-11-03T06:37:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    37,
                    53,
                    0,
                    307,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.01266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.01266v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons: (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons: (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T06:37:53Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    37,
                    53,
                    0,
                    307,
                    0
                ],
                "arxiv_comment": "Project webpage: https://joonghyuk.com/motionstream-web/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Joonghyuk Shin"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Jaesik Park"
                    },
                    {
                        "name": "Eli Schechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang"
            },
            {
                "id": "http://arxiv.org/abs/2505.13544v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.13544v3",
                "title": "Multi-head Temporal Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Temporal Latent Attention"
                },
                "updated": "2025-11-02T20:27:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    20,
                    27,
                    27,
                    6,
                    306,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.13544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.13544v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache size along the temporal dimension, greatly lowering the memory footprint of self-attention inference. MTLA employs a hyper-network to dynamically merge temporally adjacent KV cache vectors. To address the mismatch between the compressed KV cache and processed sequence lengths, a stride-aware causal mask is proposed to ensure efficient parallel training and consistency with inference behaviour. Experiments across tasks, including speech translation, speech recognition, speech understanding and text summarisation, demonstrate that MTLA achieves competitive performance compared to standard Multi-Head Attention (MHA), while greatly improving inference speed and GPU memory usage. For example, on a English-German speech translation task, MTLA achieves a 5.3x speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA, while maintaining translation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache size along the temporal dimension, greatly lowering the memory footprint of self-attention inference. MTLA employs a hyper-network to dynamically merge temporally adjacent KV cache vectors. To address the mismatch between the compressed KV cache and processed sequence lengths, a stride-aware causal mask is proposed to ensure efficient parallel training and consistency with inference behaviour. Experiments across tasks, including speech translation, speech recognition, speech understanding and text summarisation, demonstrate that MTLA achieves competitive performance compared to standard Multi-Head Attention (MHA), while greatly improving inference speed and GPU memory usage. For example, on a English-German speech translation task, MTLA achieves a 5.3x speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA, while maintaining translation quality."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-19T02:09:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    9,
                    41,
                    0,
                    139,
                    0
                ],
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Keqi Deng"
                    },
                    {
                        "name": "Philip C. Woodland"
                    }
                ],
                "author_detail": {
                    "name": "Philip C. Woodland"
                },
                "author": "Philip C. Woodland"
            },
            {
                "id": "http://arxiv.org/abs/2511.00868v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00868v1",
                "title": "FlexiCache: Leveraging Temporal Stability of Attention Heads for Efficient KV Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiCache: Leveraging Temporal Stability of Attention Heads for Efficient KV Cache Management"
                },
                "updated": "2025-11-02T09:33:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    9,
                    33,
                    12,
                    6,
                    306,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00868v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Model (LLM) serving is increasingly constrained by the growing size of the key-value (KV) cache, which scales with both context length and generation length. Prior work shows that attention is dominated by a small subset of critical tokens, yet existing systems struggle to exploit this efficiently without degrading accuracy, especially in long generation. We make a key observation: the temporal stability of these critical tokens varies significantly across KV heads: some heads consistently focus on the same tokens, while others shift frequently. Building on this insight, we introduce FlexiCache, a hierarchical KV-cache management system that leverages the temporal stability of KV heads to reduce GPU memory usage and computation overhead, while preserving model accuracy. FlexiCache classifies KV heads as stable or unstable: it retains all KV-cache pages from unstable heads in GPU memory, whereas for stable heads, it keeps only the top-K pages on the GPU and offloads the rest to host memory. By exploiting temporal stability, FlexiCache performs periodic reranking for stable heads to fetch newly promoted top pages. Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and lowers online token latency by 1.6-2.1x, all while maintaining accuracy in long-context, long-generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) serving is increasingly constrained by the growing size of the key-value (KV) cache, which scales with both context length and generation length. Prior work shows that attention is dominated by a small subset of critical tokens, yet existing systems struggle to exploit this efficiently without degrading accuracy, especially in long generation. We make a key observation: the temporal stability of these critical tokens varies significantly across KV heads: some heads consistently focus on the same tokens, while others shift frequently. Building on this insight, we introduce FlexiCache, a hierarchical KV-cache management system that leverages the temporal stability of KV heads to reduce GPU memory usage and computation overhead, while preserving model accuracy. FlexiCache classifies KV heads as stable or unstable: it retains all KV-cache pages from unstable heads in GPU memory, whereas for stable heads, it keeps only the top-K pages on the GPU and offloads the rest to host memory. By exploiting temporal stability, FlexiCache performs periodic reranking for stable heads to fetch newly promoted top pages. Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and lowers online token latency by 1.6-2.1x, all while maintaining accuracy in long-context, long-generation scenarios."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-02T09:33:12Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    9,
                    33,
                    12,
                    6,
                    306,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Nazmul Takbir"
                    },
                    {
                        "name": "Hamidreza Alikhani"
                    },
                    {
                        "name": "Nikil Dutt"
                    },
                    {
                        "name": "Sangeetha Abdu Jyothi"
                    }
                ],
                "author_detail": {
                    "name": "Sangeetha Abdu Jyothi"
                },
                "author": "Sangeetha Abdu Jyothi"
            },
            {
                "id": "http://arxiv.org/abs/2511.00819v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00819v1",
                "title": "Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies"
                },
                "updated": "2025-11-02T06:15:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    6,
                    15,
                    14,
                    6,
                    306,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00819v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this work, we conduct a systematic analysis of Native Sparse Attention (NSA) and propose targeted improvements that enhance long-context modeling. A key insight is that alternating between local (sliding-window) and global (compression, selective) attention across layers, rather than using fixed patterns, enables more effective propagation of long-range dependencies and substantially boosts performance on long-sequence tasks. Meanwhile, we further refine NSA's branches with Latent Attention that the sliding-window branch is enhanced with Multi-head Latent Attention (MLA) while compression and selective branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache memory by 50\\% versus NSA while improving the model's common-sense reasoning and long-text understanding capabilities. Experiments on models from 340M to 1.3B parameters (trained on 15B and 100B tokens) show our method matches or exceeds full attention and native sparse attention in both common-sense reasoning and long-context understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we conduct a systematic analysis of Native Sparse Attention (NSA) and propose targeted improvements that enhance long-context modeling. A key insight is that alternating between local (sliding-window) and global (compression, selective) attention across layers, rather than using fixed patterns, enables more effective propagation of long-range dependencies and substantially boosts performance on long-sequence tasks. Meanwhile, we further refine NSA's branches with Latent Attention that the sliding-window branch is enhanced with Multi-head Latent Attention (MLA) while compression and selective branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache memory by 50\\% versus NSA while improving the model's common-sense reasoning and long-text understanding capabilities. Experiments on models from 340M to 1.3B parameters (trained on 15B and 100B tokens) show our method matches or exceeds full attention and native sparse attention in both common-sense reasoning and long-context understanding tasks."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-02T06:15:14Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    6,
                    15,
                    14,
                    6,
                    306,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Wen Zan"
                    },
                    {
                        "name": "Pingwei Sun"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Yerui Sun"
                    },
                    {
                        "name": "Yuchen Xie"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.00745v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00745v1",
                "title": "High-Power Dual-Channel Field Chamber for High-Frequency Magnetic Neuromodulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Power Dual-Channel Field Chamber for High-Frequency Magnetic Neuromodulation"
                },
                "updated": "2025-11-02T00:04:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    0,
                    4,
                    54,
                    6,
                    306,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00745v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Several novel methods, including magnetogenetics and magnetoelectric stimulation, use high frequency alternating magnetic fields to precisely manipulate neural activity. To quantify the behavioral effects of such interventions in a freely moving mouse, we developed a dual-channel magnetic chamber, specifically designed for rate-sensitive magnetothermal-genetic stimulation, and adaptable for other uses of alternating magnetic fields. Through an optimized coil design, the system allows independent control of two spatially orthogonal uniform magnetic fields delivered at different frequencies within a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal frequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5 mT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV and currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling system enables magnetic field generation for second-level duration, and an observation port and camera allow video capture of the animal's behavior within the chamber. The system generates high-amplitude magnetic fields across two widely separated frequency channels with negligible interference (< 1%). Relatively uniform magnetic field distribution (+/-10% across 94% of the chamber volume) is maintained throughout the chamber, and temperature increase of the inner side of the coil enclosure during the operation is limited to < 0.35 °C/s to ensure in vivo safety. Using cobalt-doped and undoped iron oxide nanoparticles, we demonstrate channel-specific heating rates of 3.5 °C/s and 1.5 °C/s, respectively, validating frequency-selectivity. Both channels can run continuously for four seconds stably.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several novel methods, including magnetogenetics and magnetoelectric stimulation, use high frequency alternating magnetic fields to precisely manipulate neural activity. To quantify the behavioral effects of such interventions in a freely moving mouse, we developed a dual-channel magnetic chamber, specifically designed for rate-sensitive magnetothermal-genetic stimulation, and adaptable for other uses of alternating magnetic fields. Through an optimized coil design, the system allows independent control of two spatially orthogonal uniform magnetic fields delivered at different frequencies within a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal frequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5 mT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV and currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling system enables magnetic field generation for second-level duration, and an observation port and camera allow video capture of the animal's behavior within the chamber. The system generates high-amplitude magnetic fields across two widely separated frequency channels with negligible interference (< 1%). Relatively uniform magnetic field distribution (+/-10% across 94% of the chamber volume) is maintained throughout the chamber, and temperature increase of the inner side of the coil enclosure during the operation is limited to < 0.35 °C/s to ensure in vivo safety. Using cobalt-doped and undoped iron oxide nanoparticles, we demonstrate channel-specific heating rates of 3.5 °C/s and 1.5 °C/s, respectively, validating frequency-selectivity. Both channels can run continuously for four seconds stably."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-02T00:04:54Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    0,
                    4,
                    54,
                    6,
                    306,
                    0
                ],
                "arxiv_comment": "25 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Xiaoyang Tian"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Boshuo Wang"
                    },
                    {
                        "name": "Jinshui Zhang"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Jeannette Ingabire"
                    },
                    {
                        "name": "Samantha Coffler"
                    },
                    {
                        "name": "Guillaume Duret"
                    },
                    {
                        "name": "Quoc-Khanh Pham"
                    },
                    {
                        "name": "Gang Bao"
                    },
                    {
                        "name": "Jacob T. Robinson"
                    },
                    {
                        "name": "Stefan M. Goetz"
                    },
                    {
                        "name": "Angel V. Peterchev"
                    }
                ],
                "author_detail": {
                    "name": "Angel V. Peterchev"
                },
                "author": "Angel V. Peterchev"
            },
            {
                "id": "http://arxiv.org/abs/2510.26692v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.26692v2",
                "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi Linear: An Expressive, Efficient Attention Architecture"
                },
                "updated": "2025-11-01T12:05:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    12,
                    5,
                    18,
                    5,
                    305,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.26692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.26692v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.\n  To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.\n  To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T16:59:43Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    43,
                    3,
                    303,
                    0
                ],
                "arxiv_comment": "Kimi Linear tech report",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kimi Team"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zongyu Lin"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Fanqing Meng"
                    },
                    {
                        "name": "Chengyin Liu"
                    },
                    {
                        "name": "Xin Men"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Wentao Li"
                    },
                    {
                        "name": "Enzhe Lu"
                    },
                    {
                        "name": "Weizhou Liu"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Weixin Xu"
                    },
                    {
                        "name": "Longhui Yu"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Enming Yuan"
                    },
                    {
                        "name": "Dehao Zhang"
                    },
                    {
                        "name": "Yizhi Zhang"
                    },
                    {
                        "name": "T. Y. Liu"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Shengjun Fang"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Jiezhong Qiu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Zhejun Jiang"
                    },
                    {
                        "name": "Weixiao Huang"
                    },
                    {
                        "name": "Bohong Yin"
                    },
                    {
                        "name": "Jiacheng You"
                    },
                    {
                        "name": "Chu Wei"
                    },
                    {
                        "name": "Zhengtao Wang"
                    },
                    {
                        "name": "Chao Hong"
                    },
                    {
                        "name": "Yutian Chen"
                    },
                    {
                        "name": "Guanduo Chen"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Huabin Zheng"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Mengnan Dong"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Siyuan Pan"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Longyu Guan"
                    },
                    {
                        "name": "Jiawen Tao"
                    },
                    {
                        "name": "Guohong Fu"
                    },
                    {
                        "name": "Xinran Xu"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Yulun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Du"
                },
                "author": "Yulun Du"
            },
            {
                "id": "http://arxiv.org/abs/2511.00473v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00473v1",
                "title": "Drinfeld associators and Kashiwara-Vergne associators in higher genera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drinfeld associators and Kashiwara-Vergne associators in higher genera"
                },
                "updated": "2025-11-01T09:53:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    9,
                    53,
                    47,
                    5,
                    305,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00473v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "For $g\\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by Alekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in relation to the formality problem of the Goldman-Turaev Lie bialgebra on an oriented surface with a framing, is directly constructed from a genus $g$ analogue of a Drinfeld associator formulated by Gonzalez, which we call a Gonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus 0. The framing is automatically determined from the choice of a Gonzalez-Drinfeld associator, and in the case of genus 1, we show that only one particular framing is realised by our construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For $g\\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by Alekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in relation to the formality problem of the Goldman-Turaev Lie bialgebra on an oriented surface with a framing, is directly constructed from a genus $g$ analogue of a Drinfeld associator formulated by Gonzalez, which we call a Gonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus 0. The framing is automatically determined from the choice of a Gonzalez-Drinfeld associator, and in the case of genus 1, we show that only one particular framing is realised by our construction."
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-01T09:53:47Z",
                "published_parsed": [
                    2025,
                    11,
                    1,
                    9,
                    53,
                    47,
                    5,
                    305,
                    0
                ],
                "arxiv_comment": "40 pages, 2 figures",
                "arxiv_primary_category": {
                    "term": "math.QA"
                },
                "authors": [
                    {
                        "name": "Toyo Taniguchi"
                    }
                ],
                "author_detail": {
                    "name": "Toyo Taniguchi"
                },
                "author": "Toyo Taniguchi"
            },
            {
                "id": "http://arxiv.org/abs/2510.19755v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.19755v3",
                "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation"
                },
                "updated": "2025-11-01T08:49:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    8,
                    49,
                    20,
                    5,
                    305,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.19755v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.19755v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \\textit{multi-step iterations} and \\textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis.\n  Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \\textit{Efficient Generative Intelligence}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \\textit{multi-step iterations} and \\textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis.\n  Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \\textit{Efficient Generative Intelligence}."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-22T16:46:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    46,
                    5,
                    2,
                    295,
                    0
                ],
                "arxiv_comment": "22 pages,2 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Zhikai Wang"
                    },
                    {
                        "name": "Peiru Wang"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Zhengan Yan"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Zhengyi Shi"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2510.12872v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.12872v2",
                "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems"
                },
                "updated": "2025-11-01T08:26:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    8,
                    26,
                    24,
                    5,
                    305,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.12872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.12872v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for complex language processing tasks that require communication and coordination among agents. However, these systems often suffer substantial overhead from repeated reprocessing of overlapping contexts across agents. In typical pipelines, once an agent receives a message from its predecessor, the full context-including prior turns-must be reprocessed from scratch, leading to inefficient processing. While key-value (KV) caching is an effective solution for avoiding redundant computation in single-agent settings where prefixes remain unchanged, it cannot be directly reused in multi-agent scenarios due to diverging prefixes introduced by agent-specific context extensions. We identify that the core challenge lies in the offset variance of KV-caches across agents. To address this, we propose KVCOMM, a training-free framework that enables efficient prefilling in multi-agent inference by reusing KV-caches and aligning cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM estimates and adjusts KV-caches for shared content by referencing a pool of cached examples-termed anchors-that store observed cache deviations under varying prefixes. The anchor pool is maintained and updated online, allowing dynamic adaptation to distinct user requests and context structures. KVCOMM achieves over 70% reuse rate across diverse multi-agent workloads, including retrieval-augmented generation, math reasoning, and collaborative coding tasks, all without quality degradation. Particularly, when each fully-connected agent receives 1K input tokens with 512 prefix tokens and 512 output tokens under a five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard prefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent large language model (LLM) systems are increasingly adopted for complex language processing tasks that require communication and coordination among agents. However, these systems often suffer substantial overhead from repeated reprocessing of overlapping contexts across agents. In typical pipelines, once an agent receives a message from its predecessor, the full context-including prior turns-must be reprocessed from scratch, leading to inefficient processing. While key-value (KV) caching is an effective solution for avoiding redundant computation in single-agent settings where prefixes remain unchanged, it cannot be directly reused in multi-agent scenarios due to diverging prefixes introduced by agent-specific context extensions. We identify that the core challenge lies in the offset variance of KV-caches across agents. To address this, we propose KVCOMM, a training-free framework that enables efficient prefilling in multi-agent inference by reusing KV-caches and aligning cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM estimates and adjusts KV-caches for shared content by referencing a pool of cached examples-termed anchors-that store observed cache deviations under varying prefixes. The anchor pool is maintained and updated online, allowing dynamic adaptation to distinct user requests and context structures. KVCOMM achieves over 70% reuse rate across diverse multi-agent workloads, including retrieval-augmented generation, math reasoning, and collaborative coding tasks, all without quality degradation. Particularly, when each fully-connected agent receives 1K input tokens with 512 prefix tokens and 512 output tokens under a five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard prefill pipeline, reducing TTFT from ~430 ms to ~55 ms."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-14T18:00:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "arxiv_comment": "Accepted for publication in NeurIPS2025. Code is available at \\url{https://github.com/FastMAS/KVCOMM}",
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Hancheng Ye"
                    },
                    {
                        "name": "Zhengqi Gao"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Qinsi Wang"
                    },
                    {
                        "name": "Yuzhe Fu"
                    },
                    {
                        "name": "Ming-Yu Chung"
                    },
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen"
            },
            {
                "id": "http://arxiv.org/abs/2510.22765v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.22765v2",
                "title": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval"
                },
                "updated": "2025-11-01T07:01:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    7,
                    1,
                    0,
                    5,
                    305,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.22765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.22765v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid development of Vision-language models (VLMs) enables open-ended perception and reasoning. Recent works have started to investigate how to adapt general-purpose VLMs into personalized assistants. Even commercial models such as ChatGPT now support model personalization by incorporating user-specific information. However, existing methods either learn a set of concept tokens or train a VLM to utilize user-specific information. However, both pipelines struggle to generate accurate answers as personalized assistants. We introduce Jarvis, an innovative framework for a personalized AI assistant through personal KV-Cache retrieval, which stores user-specific information in the KV-Caches of both textual and visual tokens. The textual tokens are created by summarizing user information into metadata, while the visual tokens are produced by extracting distinct image patches from the user's images. When answering a question, Jarvis first retrieves related KV-Caches from personal storage and uses them to ensure accuracy in responses. We also introduce a fine-grained benchmark built with the same distinct image patch mining pipeline, emphasizing accurate question answering based on fine-grained user-specific information. Jarvis is capable of providing more accurate responses, particularly when they depend on specific local details. Jarvis achieves state-of-the-art results in both visual question answering and text-only tasks across multiple datasets, indicating a practical path toward personalized AI assistants. The code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Vision-language models (VLMs) enables open-ended perception and reasoning. Recent works have started to investigate how to adapt general-purpose VLMs into personalized assistants. Even commercial models such as ChatGPT now support model personalization by incorporating user-specific information. However, existing methods either learn a set of concept tokens or train a VLM to utilize user-specific information. However, both pipelines struggle to generate accurate answers as personalized assistants. We introduce Jarvis, an innovative framework for a personalized AI assistant through personal KV-Cache retrieval, which stores user-specific information in the KV-Caches of both textual and visual tokens. The textual tokens are created by summarizing user information into metadata, while the visual tokens are produced by extracting distinct image patches from the user's images. When answering a question, Jarvis first retrieves related KV-Caches from personal storage and uses them to ensure accuracy in responses. We also introduce a fine-grained benchmark built with the same distinct image patch mining pipeline, emphasizing accurate question answering based on fine-grained user-specific information. Jarvis is capable of providing more accurate responses, particularly when they depend on specific local details. Jarvis achieves state-of-the-art results in both visual question answering and text-only tasks across multiple datasets, indicating a practical path toward personalized AI assistants. The code and dataset will be released."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-26T17:28:05Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    17,
                    28,
                    5,
                    6,
                    299,
                    0
                ],
                "arxiv_comment": "19 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Binxiao Xu"
                    },
                    {
                        "name": "Junyu Feng"
                    },
                    {
                        "name": "Shaolin Lu"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v7",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.16242v7",
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising 1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising 1-Consistency"
                },
                "updated": "2025-11-01T04:26:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    4,
                    26,
                    3,
                    5,
                    305,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.16242v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.16242v7",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce excessive computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only O(1) additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce excessive computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only O(1) additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "arxiv_comment": "Accepted to NeurIPS 2025. https://neurips.cc/virtual/2025/poster/116615",
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng"
            },
            {
                "id": "http://arxiv.org/abs/2511.00321v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00321v1",
                "title": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits"
                },
                "updated": "2025-10-31T23:50:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    23,
                    50,
                    44,
                    4,
                    304,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00321v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The expansion of context windows in large language models (LLMs) to multi-million tokens introduces severe memory and compute bottlenecks, particularly in managing the growing Key-Value (KV) cache. While Compute Express Link (CXL) enables non-eviction frameworks that offload the full KV-cache to scalable external memory, these frameworks still suffer from costly data transfers when recalling non-resident KV tokens to limited GPU memory as context lengths increase. This work proposes scalable Processing-Near-Memory (PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that coordinates memory and computation beyond GPU limits. Our design offloads token page selection to a PNM accelerator within CXL memory, eliminating costly recalls and enabling larger GPU batch sizes. We further introduce a hybrid parallelization strategy and a steady-token selection mechanism to enhance compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM system, our solution delivers consistent performance gains for LLMs with up to 405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV) and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x throughput improvement, up to 60x lower energy per token, and up to 7.3x better total cost efficiency than the baseline, demonstrating that CXL-enabled multi-PNM architectures can serve as a scalable backbone for future long-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of context windows in large language models (LLMs) to multi-million tokens introduces severe memory and compute bottlenecks, particularly in managing the growing Key-Value (KV) cache. While Compute Express Link (CXL) enables non-eviction frameworks that offload the full KV-cache to scalable external memory, these frameworks still suffer from costly data transfers when recalling non-resident KV tokens to limited GPU memory as context lengths increase. This work proposes scalable Processing-Near-Memory (PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that coordinates memory and computation beyond GPU limits. Our design offloads token page selection to a PNM accelerator within CXL memory, eliminating costly recalls and enabling larger GPU batch sizes. We further introduce a hybrid parallelization strategy and a steady-token selection mechanism to enhance compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM system, our solution delivers consistent performance gains for LLMs with up to 405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV) and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x throughput improvement, up to 60x lower energy per token, and up to 7.3x better total cost efficiency than the baseline, demonstrating that CXL-enabled multi-PNM architectures can serve as a scalable backbone for future long-context LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-31T23:50:44Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    23,
                    50,
                    44,
                    4,
                    304,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Dowon Kim"
                    },
                    {
                        "name": "MinJae Lee"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "HyuckSung Kwon"
                    },
                    {
                        "name": "Hyeonggyu Jeong"
                    },
                    {
                        "name": "Sang-Soo Park"
                    },
                    {
                        "name": "Minyong Yoon"
                    },
                    {
                        "name": "Si-Dong Roh"
                    },
                    {
                        "name": "Yongsuk Kwon"
                    },
                    {
                        "name": "Jinin So"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi"
            },
            {
                "id": "http://arxiv.org/abs/2510.27641v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.27641v1",
                "title": "SpecAttn: Speculating Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecAttn: Speculating Sparse Attention"
                },
                "updated": "2025-10-31T17:12:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.27641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.27641v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) face significant computational bottlenecks during inference due to the quadratic complexity of self-attention mechanisms, particularly as context lengths increase. We introduce SpecAttn, a novel training-free approach that seamlessly integrates with existing speculative decoding techniques to enable efficient sparse attention in pre-trained transformers. Our key insight is to exploit the attention weights already computed by the draft model during speculative decoding to identify important tokens for the target model, eliminating redundant computation while maintaining output quality. SpecAttn employs three core techniques: KL divergence-based layer alignment between draft and target models, a GPU-optimized sorting-free algorithm for top-p token selection from draft attention patterns, and dynamic key-value cache pruning guided by these predictions. By leveraging the computational work already performed in standard speculative decoding pipelines, SpecAttn achieves over 75% reduction in key-value cache accesses with a mere 15.29% increase in perplexity on the PG-19 dataset, significantly outperforming existing sparse attention methods. Our approach demonstrates that speculative execution can be enhanced to provide approximate verification without significant performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational bottlenecks during inference due to the quadratic complexity of self-attention mechanisms, particularly as context lengths increase. We introduce SpecAttn, a novel training-free approach that seamlessly integrates with existing speculative decoding techniques to enable efficient sparse attention in pre-trained transformers. Our key insight is to exploit the attention weights already computed by the draft model during speculative decoding to identify important tokens for the target model, eliminating redundant computation while maintaining output quality. SpecAttn employs three core techniques: KL divergence-based layer alignment between draft and target models, a GPU-optimized sorting-free algorithm for top-p token selection from draft attention patterns, and dynamic key-value cache pruning guided by these predictions. By leveraging the computational work already performed in standard speculative decoding pipelines, SpecAttn achieves over 75% reduction in key-value cache accesses with a mere 15.29% increase in perplexity on the PG-19 dataset, significantly outperforming existing sparse attention methods. Our approach demonstrates that speculative execution can be enhanced to provide approximate verification without significant performance degradation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-31T17:12:34Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "arxiv_comment": "Accepted to NeurIPS 2025 Workshop on Structured Probabilistic Inference & Generative Modeling",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Harsh Shah"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Shah"
                },
                "author": "Harsh Shah"
            },
            {
                "id": "http://arxiv.org/abs/2510.27617v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.27617v1",
                "title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation"
                },
                "updated": "2025-10-31T16:40:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.27617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.27617v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automation of Register Transfer Level (RTL) design can help developers meet increasing computational demands. Large Language Models (LLMs) show promise for Hardware Description Language (HDL) generation, but face challenges due to limited parametric knowledge and domain-specific constraints. While prompt engineering and fine-tuning have limitations in knowledge coverage and training costs, multi-agent architectures offer a training-free paradigm to enhance reasoning through collaborative generation. However, current multi-agent approaches suffer from two critical deficiencies: susceptibility to noise propagation and constrained reasoning space exploration. We propose VeriMoA, a training-free mixture-of-agents (MoA) framework with two synergistic innovations. First, a quality-guided caching mechanism to maintain all intermediate HDL outputs and enables quality-based ranking and selection across the entire generation process, encouraging knowledge accumulation over layers of reasoning. Second, a multi-path generation strategy that leverages C++ and Python as intermediate representations, decomposing specification-to-HDL translation into two-stage processes that exploit LLM fluency in high-resource languages while promoting solution diversity. Comprehensive experiments on VerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves 15--30% improvements in Pass@1 across diverse LLM backbones, especially enabling smaller models to match larger models and fine-tuned alternatives without requiring costly training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation of Register Transfer Level (RTL) design can help developers meet increasing computational demands. Large Language Models (LLMs) show promise for Hardware Description Language (HDL) generation, but face challenges due to limited parametric knowledge and domain-specific constraints. While prompt engineering and fine-tuning have limitations in knowledge coverage and training costs, multi-agent architectures offer a training-free paradigm to enhance reasoning through collaborative generation. However, current multi-agent approaches suffer from two critical deficiencies: susceptibility to noise propagation and constrained reasoning space exploration. We propose VeriMoA, a training-free mixture-of-agents (MoA) framework with two synergistic innovations. First, a quality-guided caching mechanism to maintain all intermediate HDL outputs and enables quality-based ranking and selection across the entire generation process, encouraging knowledge accumulation over layers of reasoning. Second, a multi-path generation strategy that leverages C++ and Python as intermediate representations, decomposing specification-to-HDL translation into two-stage processes that exploit LLM fluency in high-resource languages while promoting solution diversity. Comprehensive experiments on VerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves 15--30% improvements in Pass@1 across diverse LLM backbones, especially enabling smaller models to match larger models and fine-tuned alternatives without requiring costly training."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-31T16:40:58Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Heng Ping"
                    },
                    {
                        "name": "Arijit Bhattacharjee"
                    },
                    {
                        "name": "Peiyu Zhang"
                    },
                    {
                        "name": "Shixuan Li"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Anzhe Cheng"
                    },
                    {
                        "name": "Xiaole Zhang"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Ali Jannesari"
                    },
                    {
                        "name": "Nesreen Ahmed"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan"
            },
            {
                "id": "http://arxiv.org/abs/2510.04525v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.04525v2",
                "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion"
                },
                "updated": "2025-10-31T05:31:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    5,
                    31,
                    58,
                    4,
                    304,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.04525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.04525v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Masked diffusion models have shown promising performance in generating high-quality samples in a wide range of domains, but accelerating their sampling process remains relatively underexplored. To investigate efficient samplers for masked diffusion, this paper theoretically analyzes the MaskGIT sampler for image modeling, revealing its implicit temperature sampling mechanism. Through this analysis, we introduce the \"moment sampler,\" an asymptotically equivalent but more tractable and interpretable alternative to MaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking positions before sampling tokens. In addition, we improve the efficiency of choose-then-sample algorithms through two key innovations: a partial caching technique for transformers that approximates longer sampling trajectories without proportional computational cost, and a hybrid approach formalizing the exploration-exploitation trade-off in adaptive unmasking. Experiments in image and text domains demonstrate our theory as well as the efficiency of our proposed methods, advancing both theoretical understanding and practical implementation of masked diffusion samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion models have shown promising performance in generating high-quality samples in a wide range of domains, but accelerating their sampling process remains relatively underexplored. To investigate efficient samplers for masked diffusion, this paper theoretically analyzes the MaskGIT sampler for image modeling, revealing its implicit temperature sampling mechanism. Through this analysis, we introduce the \"moment sampler,\" an asymptotically equivalent but more tractable and interpretable alternative to MaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking positions before sampling tokens. In addition, we improve the efficiency of choose-then-sample algorithms through two key innovations: a partial caching technique for transformers that approximates longer sampling trajectories without proportional computational cost, and a hybrid approach formalizing the exploration-exploitation trade-off in adaptive unmasking. Experiments in image and text domains demonstrate our theory as well as the efficiency of our proposed methods, advancing both theoretical understanding and practical implementation of masked diffusion samplers."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-06T06:30:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "arxiv_comment": "23 pages, fixed cleveref-related issue",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Satoshi Hayakawa"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Masaaki Imaizumi"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji"
            },
            {
                "id": "http://arxiv.org/abs/2510.27171v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.27171v1",
                "title": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance Acceleration of Generative Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance Acceleration of Generative Diffusion Models"
                },
                "updated": "2025-10-31T04:47:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    47,
                    14,
                    4,
                    304,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.27171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.27171v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have emerged as state-of-the-art in image generation, but their practical deployment is hindered by the significant computational cost of their iterative denoising process. While existing caching techniques can accelerate inference, they often create a challenging trade-off between speed and fidelity, suffering from quality degradation and high computational overhead. To address these limitations, we introduce H2-Cache, a novel hierarchical caching mechanism designed for modern generative diffusion model architectures. Our method is founded on the key insight that the denoising process can be functionally separated into a structure-defining stage and a detail-refining stage. H2-cache leverages this by employing a dual-threshold system, using independent thresholds to selectively cache each stage. To ensure the efficiency of our dual-check approach, we introduce pooled feature summarization (PFS), a lightweight technique for robust and fast similarity estimation. Extensive experiments on the Flux architecture demonstrate that H2-cache achieves significant acceleration (up to 5.08x) while maintaining image quality nearly identical to the baseline, quantitatively and qualitatively outperforming existing caching methods. Our work presents a robust and practical solution that effectively resolves the speed-quality dilemma, significantly lowering the barrier for the real-world application of high-fidelity diffusion models. Source code is available at https://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as state-of-the-art in image generation, but their practical deployment is hindered by the significant computational cost of their iterative denoising process. While existing caching techniques can accelerate inference, they often create a challenging trade-off between speed and fidelity, suffering from quality degradation and high computational overhead. To address these limitations, we introduce H2-Cache, a novel hierarchical caching mechanism designed for modern generative diffusion model architectures. Our method is founded on the key insight that the denoising process can be functionally separated into a structure-defining stage and a detail-refining stage. H2-cache leverages this by employing a dual-threshold system, using independent thresholds to selectively cache each stage. To ensure the efficiency of our dual-check approach, we introduce pooled feature summarization (PFS), a lightweight technique for robust and fast similarity estimation. Extensive experiments on the Flux architecture demonstrate that H2-cache achieves significant acceleration (up to 5.08x) while maintaining image quality nearly identical to the baseline, quantitatively and qualitatively outperforming existing caching methods. Our work presents a robust and practical solution that effectively resolves the speed-quality dilemma, significantly lowering the barrier for the real-world application of high-fidelity diffusion models. Source code is available at https://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-31T04:47:14Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    47,
                    14,
                    4,
                    304,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Mingyu Sung"
                    },
                    {
                        "name": "Il-Min Kim"
                    },
                    {
                        "name": "Sangseok Yun"
                    },
                    {
                        "name": "Jae-Mo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Mo Kang"
                },
                "author": "Jae-Mo Kang"
            },
            {
                "id": "http://arxiv.org/abs/2510.18586v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.18586v2",
                "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications"
                },
                "updated": "2025-10-31T04:17:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    17,
                    5,
                    4,
                    304,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.18586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.18586v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent applications that use external function calls. This workload creates severe performance challenges for the KV Cache: space contention leads to the eviction of critical agents' caches and time underutilization leaves the cache of agents stalled on long-running tool calls idling in GPU memory. We present Tokencake, a KV-Cache-centric serving framework that co-optimizes scheduling and memory management with an agent-aware design. Tokencake's Space Scheduler uses dynamic memory partitioning to shield critical agents from contention, while its Time Scheduler employs a proactive offload and predictive upload mechanism to repurpose GPU memory during function call stalls. Our evaluation on representative multi-agent benchmarks shows that Tokencake can reduce end-to-end latency by over 47.06%, improve effective GPU memory utilization by up to 16.9% compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent applications that use external function calls. This workload creates severe performance challenges for the KV Cache: space contention leads to the eviction of critical agents' caches and time underutilization leaves the cache of agents stalled on long-running tool calls idling in GPU memory. We present Tokencake, a KV-Cache-centric serving framework that co-optimizes scheduling and memory management with an agent-aware design. Tokencake's Space Scheduler uses dynamic memory partitioning to shield critical agents from contention, while its Time Scheduler employs a proactive offload and predictive upload mechanism to repurpose GPU memory during function call stalls. Our evaluation on representative multi-agent benchmarks shows that Tokencake can reduce end-to-end latency by over 47.06%, improve effective GPU memory utilization by up to 16.9% compared to vLLM."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-21T12:39:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo"
            },
            {
                "id": "http://arxiv.org/abs/2506.00413v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.00413v2",
                "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding"
                },
                "updated": "2025-10-30T21:11:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    21,
                    11,
                    33,
                    3,
                    303,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.00413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.00413v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-31T06:10:10Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "arxiv_comment": "10 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover"
            },
            {
                "id": "http://arxiv.org/abs/2510.26944v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.26944v1",
                "title": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache Hierarchies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache Hierarchies"
                },
                "updated": "2025-10-30T18:58:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    18,
                    58,
                    2,
                    3,
                    303,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.26944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.26944v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we introduce Choreographer, a simulation framework that enables a holistic system-level evaluation of fine-grained accelerators designed for latency-sensitive tasks. Unlike existing frameworks, Choreographer captures all hardware and software overheads in core-accelerator and cache-accelerator interactions, integrating a detailed gem5-based hardware stack featuring an AMBA coherent hub interface (CHI) mesh network and a complete Linux-based software stack. To facilitate rapid prototyping, it offers a C++ application programming interface and modular configuration options. Our detailed cache model provides accurate insights into performance variations caused by cache configurations, which are not captured by other frameworks. The framework is demonstrated through two case studies: a data-aware prefetcher for graph analytics workloads, and a quicksort accelerator. Our evaluation shows that the prefetcher achieves speedups between 1.08x and 1.88x by reducing memory access latency, while the quicksort accelerator delivers more than 2x speedup with minimal address translation overhead. These findings underscore the ability of Choreographer to model complex hardware-software interactions and optimize performance in small task offloading scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Choreographer, a simulation framework that enables a holistic system-level evaluation of fine-grained accelerators designed for latency-sensitive tasks. Unlike existing frameworks, Choreographer captures all hardware and software overheads in core-accelerator and cache-accelerator interactions, integrating a detailed gem5-based hardware stack featuring an AMBA coherent hub interface (CHI) mesh network and a complete Linux-based software stack. To facilitate rapid prototyping, it offers a C++ application programming interface and modular configuration options. Our detailed cache model provides accurate insights into performance variations caused by cache configurations, which are not captured by other frameworks. The framework is demonstrated through two case studies: a data-aware prefetcher for graph analytics workloads, and a quicksort accelerator. Our evaluation shows that the prefetcher achieves speedups between 1.08x and 1.88x by reducing memory access latency, while the quicksort accelerator delivers more than 2x speedup with minimal address translation overhead. These findings underscore the ability of Choreographer to model complex hardware-software interactions and optimize performance in small task offloading scenarios."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T18:58:02Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    18,
                    58,
                    2,
                    3,
                    303,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Hoa Nguyen"
                    },
                    {
                        "name": "Pongstorn Maidee"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    },
                    {
                        "name": "Alireza Kaviani"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Kaviani"
                },
                "author": "Alireza Kaviani"
            },
            {
                "id": "http://arxiv.org/abs/2510.26730v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.26730v1",
                "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference"
                },
                "updated": "2025-10-30T17:29:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.26730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.26730v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The expansion of large language models is increasingly limited by the constrained memory capacity of modern GPUs. To mitigate this, Mixture-of-Experts (MoE) architectures activate only a small portion of parameters during inference, significantly lowering both memory demand and computational overhead. However, conventional MoE inference approaches, which select active experts independently at each layer, often introduce considerable latency because of frequent parameter transfers between host and GPU memory. In addition, current cross-layer prediction strategies, which are typically based on fixed steps, lack adaptability across different hardware platforms and workloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE inference that combines adaptive expert prefetching and cache-aware routing. ExpertFlow continuously adjusts its prediction horizon for expert activation by leveraging runtime statistics such as transfer bandwidth, parameter dimensionality, and model feedback signals. Furthermore, it incorporates a hybrid cross-layer prediction scheme that fuses pregating information with intermediate computational states to anticipate future expert needs. By adaptively refining prefetching decisions and aligning them with actual usage behavior, ExpertFlow effectively decreases cache misses and removes latency caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces model stall time to less than 0.1% of the baseline, highlighting its capability to optimize MoE inference under stringent memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models is increasingly limited by the constrained memory capacity of modern GPUs. To mitigate this, Mixture-of-Experts (MoE) architectures activate only a small portion of parameters during inference, significantly lowering both memory demand and computational overhead. However, conventional MoE inference approaches, which select active experts independently at each layer, often introduce considerable latency because of frequent parameter transfers between host and GPU memory. In addition, current cross-layer prediction strategies, which are typically based on fixed steps, lack adaptability across different hardware platforms and workloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE inference that combines adaptive expert prefetching and cache-aware routing. ExpertFlow continuously adjusts its prediction horizon for expert activation by leveraging runtime statistics such as transfer bandwidth, parameter dimensionality, and model feedback signals. Furthermore, it incorporates a hybrid cross-layer prediction scheme that fuses pregating information with intermediate computational states to anticipate future expert needs. By adaptively refining prefetching decisions and aligning them with actual usage behavior, ExpertFlow effectively decreases cache misses and removes latency caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces model stall time to less than 0.1% of the baseline, highlighting its capability to optimize MoE inference under stringent memory constraints."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T17:29:27Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "arxiv_comment": "12 pages, 11 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Runxin Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang"
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.09557v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09557v2",
                "title": "LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication"
                },
                "updated": "2025-11-13T18:59:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    59,
                    38,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09557v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) continue to grow in size, distributed inference has become increasingly important. Model-parallel strategies must now efficiently scale not only across multiple GPUs but also across multiple nodes. In this work, we present a detailed performance study of multi-node distributed inference using LLMs on GPU-based supercomputers. We conduct experiments with several state-of-the-art inference engines alongside YALIS, a research-oriented prototype engine designed for controlled experimentation. We analyze the strong-scaling behavior of different model-parallel schemes and identify key bottlenecks. Since all-reduce operations are a common performance bottleneck, we develop NVRAR, a hierarchical all-reduce algorithm based on recursive doubling with NVSHMEM. NVRAR achieves up to 1.9x-3.6x lower latency than NCCL for message sizes between 128 KB and 2 MB on HPE Slingshot and InfiniBand interconnects. Integrated into YALIS, NVRAR achieves up to a 1.72x reduction in end-to-end batch latency for the Llama 3.1 405B model in multi-node decode-heavy workloads using tensor parallelism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to grow in size, distributed inference has become increasingly important. Model-parallel strategies must now efficiently scale not only across multiple GPUs but also across multiple nodes. In this work, we present a detailed performance study of multi-node distributed inference using LLMs on GPU-based supercomputers. We conduct experiments with several state-of-the-art inference engines alongside YALIS, a research-oriented prototype engine designed for controlled experimentation. We analyze the strong-scaling behavior of different model-parallel schemes and identify key bottlenecks. Since all-reduce operations are a common performance bottleneck, we develop NVRAR, a hierarchical all-reduce algorithm based on recursive doubling with NVSHMEM. NVRAR achieves up to 1.9x-3.6x lower latency than NCCL for message sizes between 128 KB and 2 MB on HPE Slingshot and InfiniBand interconnects. Integrated into YALIS, NVRAR achieves up to a 1.72x reduction in end-to-end batch latency for the Llama 3.1 405B model in multi-node decode-heavy workloads using tensor parallelism."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T18:59:26Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    18,
                    59,
                    26,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "12 Figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Lannie Dalton Hough"
                    },
                    {
                        "name": "Akarsh Srivastava"
                    },
                    {
                        "name": "Harshitha Menon"
                    },
                    {
                        "name": "Charles Fredrick Jekel"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele"
            },
            {
                "id": "http://arxiv.org/abs/2511.10645v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10645v1",
                "title": "ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference"
                },
                "updated": "2025-11-13T18:59:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    59,
                    24,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10645v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:59:24Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    59,
                    24,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yesheng Liang"
                    },
                    {
                        "name": "Haisheng Chen"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Zhijian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijian Liu"
                },
                "author": "Zhijian Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.10643v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10643v1",
                "title": "Black-Box On-Policy Distillation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-Box On-Policy Distillation of Large Language Models"
                },
                "updated": "2025-11-13T18:58:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    58,
                    37,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10643v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:58:37Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    58,
                    37,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tianzhu Ye"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei"
            },
            {
                "id": "http://arxiv.org/abs/2511.10640v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10640v1",
                "title": "Flexible Simulation Based Inference for Galaxy Photometric Fitting with Synthesizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Simulation Based Inference for Galaxy Photometric Fitting with Synthesizer"
                },
                "updated": "2025-11-13T18:56:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    56,
                    48,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10640v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Synference, a new, flexible Python framework for galaxy SED fitting using simulation-based inference (SBI). Synference leverages the Synthesizer package for flexible forward-modelling of galaxy SEDs and integrates the LtU-ILI package to ensure best practices in model training and validation. In this work we demonstrate Synference by training a neural posterior estimator on $10^6$ simulated galaxies, based on a flexible 8-parameter physical model, to infer galaxy properties from 14-band HST and JWST photometry. We validate this model, demonstrating excellent parameter recovery (e.g. R$^2>$0.99 for M$_\\star$) and accurate posterior calibration against nested sampling results. We apply our trained model to 3,088 spectroscopically-confirmed galaxies in the JADES GOODS-South field. The amortized inference is exceptionally fast, having nearly fixed cost per posterior evaluation and processing the entire sample in $\\sim$3 minutes on a single CPU (18 galaxies/CPU/sec), a $\\sim$1700$\\times$ speedup over traditional nested sampling or MCMC techniques. We demonstrate Synference's ability to simultaneously infer photometric redshifts and physical parameters, and highlight its utility for rapid Bayesian model comparison by demonstrating systematic stellar mass differences between two commonly used stellar population synthesis models. Synference is a powerful, scalable tool poised to maximise the scientific return of next-generation galaxy surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Synference, a new, flexible Python framework for galaxy SED fitting using simulation-based inference (SBI). Synference leverages the Synthesizer package for flexible forward-modelling of galaxy SEDs and integrates the LtU-ILI package to ensure best practices in model training and validation. In this work we demonstrate Synference by training a neural posterior estimator on $10^6$ simulated galaxies, based on a flexible 8-parameter physical model, to infer galaxy properties from 14-band HST and JWST photometry. We validate this model, demonstrating excellent parameter recovery (e.g. R$^2>$0.99 for M$_\\star$) and accurate posterior calibration against nested sampling results. We apply our trained model to 3,088 spectroscopically-confirmed galaxies in the JADES GOODS-South field. The amortized inference is exceptionally fast, having nearly fixed cost per posterior evaluation and processing the entire sample in $\\sim$3 minutes on a single CPU (18 galaxies/CPU/sec), a $\\sim$1700$\\times$ speedup over traditional nested sampling or MCMC techniques. We demonstrate Synference's ability to simultaneously infer photometric redshifts and physical parameters, and highlight its utility for rapid Bayesian model comparison by demonstrating systematic stellar mass differences between two commonly used stellar population synthesis models. Synference is a powerful, scalable tool poised to maximise the scientific return of next-generation galaxy surveys."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:56:48Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    56,
                    48,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "23 pages, 12 figures. Submitted to MNRAS. The Synference package is available at https://github.com/synthesizer-project/synference/",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "Thomas Harvey"
                    },
                    {
                        "name": "Christopher C. Lovell"
                    },
                    {
                        "name": "Sophie Newman"
                    },
                    {
                        "name": "Christopher J. Conselice"
                    },
                    {
                        "name": "Duncan Austin"
                    },
                    {
                        "name": "Aswin P. Vijayan"
                    },
                    {
                        "name": "Stephen M. Wilkins"
                    },
                    {
                        "name": "Vadim Rusakov"
                    },
                    {
                        "name": "Qiong Li"
                    },
                    {
                        "name": "Nathan Adams"
                    },
                    {
                        "name": "Kai Magdwick"
                    },
                    {
                        "name": "Matthew Ho"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Ho"
                },
                "author": "Matthew Ho"
            },
            {
                "id": "http://arxiv.org/abs/2309.10378v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2309.10378v3",
                "title": "Group Spike and Slab Variational Bayes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Spike and Slab Variational Bayes"
                },
                "updated": "2025-11-13T18:56:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    56,
                    45,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2309.10378v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2309.10378v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1214/25-BA1542",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "We introduce Group Spike-and-slab Variational Bayes (GSVB), a scalable method for group sparse regression. A fast co-ordinate ascent variational inference (CAVI) algorithm is developed for several common model families including Gaussian, Binomial and Poisson. Theoretical guarantees for our proposed approach are provided by deriving contraction rates for the variational posterior in grouped linear regression. Through extensive numerical studies, we demonstrate that GSVB provides state-of-the-art performance, offering a computationally inexpensive substitute to MCMC, whilst performing comparably or better than existing MAP methods. Additionally, we analyze three real world datasets wherein we highlight the practical utility of our method, demonstrating that GSVB provides parsimonious models with excellent predictive performance, variable selection and uncertainty quantification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Group Spike-and-slab Variational Bayes (GSVB), a scalable method for group sparse regression. A fast co-ordinate ascent variational inference (CAVI) algorithm is developed for several common model families including Gaussian, Binomial and Poisson. Theoretical guarantees for our proposed approach are provided by deriving contraction rates for the variational posterior in grouped linear regression. Through extensive numerical studies, we demonstrate that GSVB provides state-of-the-art performance, offering a computationally inexpensive substitute to MCMC, whilst performing comparably or better than existing MAP methods. Additionally, we analyze three real world datasets wherein we highlight the practical utility of our method, demonstrating that GSVB provides parsimonious models with excellent predictive performance, variable selection and uncertainty quantification."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-09-19T07:26:59Z",
                "published_parsed": [
                    2023,
                    9,
                    19,
                    7,
                    26,
                    59,
                    1,
                    262,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Michael Komodromos"
                    },
                    {
                        "name": "Marina Evangelou"
                    },
                    {
                        "name": "Sarah Filippi"
                    },
                    {
                        "name": "Kolyan Ray"
                    }
                ],
                "author_detail": {
                    "name": "Kolyan Ray"
                },
                "author": "Kolyan Ray",
                "arxiv_doi": "10.1214/25-BA1542"
            },
            {
                "id": "http://arxiv.org/abs/2510.12787v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.12787v3",
                "title": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics"
                },
                "updated": "2025-11-13T18:56:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    56,
                    10,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.12787v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.12787v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Ax-Prover, a multi-agent system for automated theorem proving in Lean that can solve problems across diverse scientific domains and operate either autonomously or collaboratively with human experts. To achieve this, Ax-Prover approaches scientific problem solving through formal proof generation, a process that demands both creative reasoning and strict syntactic rigor. Ax-Prover meets this challenge by equipping Large Language Models (LLMs), which provide knowledge and reasoning, with Lean tools via the Model Context Protocol (MCP), which ensure formal correctness. To evaluate its performance as an autonomous prover, we benchmark our approach against frontier LLMs and specialized prover models on two public math benchmarks and on two Lean benchmarks we introduce in the fields of abstract algebra and quantum theory. On public datasets, Ax-Prover is competitive with state-of-the-art provers, while it largely outperforms them on the new benchmarks. This shows that, unlike specialized systems that struggle to generalize, our tool-based agentic theorem prover approach offers a generalizable methodology for formal verification across diverse scientific domains. Furthermore, we demonstrate Ax-Prover's assistant capabilities in a practical use case, showing how it enabled an expert mathematician to formalize the proof of a complex cryptography theorem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Ax-Prover, a multi-agent system for automated theorem proving in Lean that can solve problems across diverse scientific domains and operate either autonomously or collaboratively with human experts. To achieve this, Ax-Prover approaches scientific problem solving through formal proof generation, a process that demands both creative reasoning and strict syntactic rigor. Ax-Prover meets this challenge by equipping Large Language Models (LLMs), which provide knowledge and reasoning, with Lean tools via the Model Context Protocol (MCP), which ensure formal correctness. To evaluate its performance as an autonomous prover, we benchmark our approach against frontier LLMs and specialized prover models on two public math benchmarks and on two Lean benchmarks we introduce in the fields of abstract algebra and quantum theory. On public datasets, Ax-Prover is competitive with state-of-the-art provers, while it largely outperforms them on the new benchmarks. This shows that, unlike specialized systems that struggle to generalize, our tool-based agentic theorem prover approach offers a generalizable methodology for formal verification across diverse scientific domains. Furthermore, we demonstrate Ax-Prover's assistant capabilities in a practical use case, showing how it enabled an expert mathematician to formalize the proof of a complex cryptography theorem."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-14T17:57:04Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    57,
                    4,
                    1,
                    287,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Benjamin Breen"
                    },
                    {
                        "name": "Marco Del Tredici"
                    },
                    {
                        "name": "Jacob McCarran"
                    },
                    {
                        "name": "Javier Aspuru Mijares"
                    },
                    {
                        "name": "Weichen Winston Yin"
                    },
                    {
                        "name": "Kfir Sulimany"
                    },
                    {
                        "name": "Jacob M. Taylor"
                    },
                    {
                        "name": "Frank H. L. Koppens"
                    },
                    {
                        "name": "Dirk Englund"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Englund"
                },
                "author": "Dirk Englund"
            },
            {
                "id": "http://arxiv.org/abs/2511.10635v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10635v1",
                "title": "Robot Crash Course: Learning Soft and Stylized Falling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot Crash Course: Learning Soft and Stylized Falling"
                },
                "updated": "2025-11-13T18:55:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    55,
                    34,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10635v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite recent advances in robust locomotion, bipedal robots operating in the real world remain at risk of falling. While most research focuses on preventing such events, we instead concentrate on the phenomenon of falling itself. Specifically, we aim to reduce physical damage to the robot while providing users with control over a robot's end pose. To this end, we propose a robot agnostic reward function that balances the achievement of a desired end pose with impact minimization and the protection of critical robot parts during reinforcement learning. To make the policy robust to a broad range of initial falling conditions and to enable the specification of an arbitrary and unseen end pose at inference time, we introduce a simulation-based sampling strategy of initial and end poses. Through simulated and real-world experiments, our work demonstrates that even bipedal robots can perform controlled, soft falls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in robust locomotion, bipedal robots operating in the real world remain at risk of falling. While most research focuses on preventing such events, we instead concentrate on the phenomenon of falling itself. Specifically, we aim to reduce physical damage to the robot while providing users with control over a robot's end pose. To this end, we propose a robot agnostic reward function that balances the achievement of a desired end pose with impact minimization and the protection of critical robot parts during reinforcement learning. To make the policy robust to a broad range of initial falling conditions and to enable the specification of an arbitrary and unseen end pose at inference time, we introduce a simulation-based sampling strategy of initial and end poses. Through simulated and real-world experiments, our work demonstrates that even bipedal robots can perform controlled, soft falls."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:55:34Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    55,
                    34,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Pascal Strauch"
                    },
                    {
                        "name": "David Müller"
                    },
                    {
                        "name": "Sammy Christen"
                    },
                    {
                        "name": "Agon Serifi"
                    },
                    {
                        "name": "Ruben Grandia"
                    },
                    {
                        "name": "Espen Knoop"
                    },
                    {
                        "name": "Moritz Bächer"
                    }
                ],
                "author_detail": {
                    "name": "Moritz Bächer"
                },
                "author": "Moritz Bächer"
            },
            {
                "id": "http://arxiv.org/abs/2511.10634v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10634v1",
                "title": "Baryonic Feedback across Halo Mass: Impact on the Matter Power Spectrum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Baryonic Feedback across Halo Mass: Impact on the Matter Power Spectrum"
                },
                "updated": "2025-11-13T18:55:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    55,
                    17,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10634v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Upcoming weak-lensing surveys will probe the matter distribution at a few percent level on nonlinear scales (k > 1 h/Mpc) where baryonic feedback from galaxy formation modifies the clustering of matter. Using the IllustrisTNG hydrodynamical simulations, we quantify the mass and radial dependence of baryonic suppression of the matter power spectrum by selectively replacing halos in the collisionless run with their full-physics counterparts. We find that group-scale halos with log $M_{200m}/h^{-1} M_{sun}$ in [13, 14] dominate the suppression, contributing a large fraction of the total reduction in power at k ~ 5-30 h/Mpc. The suppression is smaller on either side of this mass bin. Correctly reproducing the full suppression of the power spectrum requires accounting for matter redistribution (while enforcing mass conservation) beyond the virial radius of each halo. Crucially, the same group-scale regime produces the strongest and most detectable deviations in group-galaxy lensing, making stacked group lensing a powerful observational test of feedback models. Our results motivate emulators that jointly predict the matter power spectrum and halo-matter correlations including baryonic effects, enabling unbiased cosmological inference from small scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upcoming weak-lensing surveys will probe the matter distribution at a few percent level on nonlinear scales (k > 1 h/Mpc) where baryonic feedback from galaxy formation modifies the clustering of matter. Using the IllustrisTNG hydrodynamical simulations, we quantify the mass and radial dependence of baryonic suppression of the matter power spectrum by selectively replacing halos in the collisionless run with their full-physics counterparts. We find that group-scale halos with log $M_{200m}/h^{-1} M_{sun}$ in [13, 14] dominate the suppression, contributing a large fraction of the total reduction in power at k ~ 5-30 h/Mpc. The suppression is smaller on either side of this mass bin. Correctly reproducing the full suppression of the power spectrum requires accounting for matter redistribution (while enforcing mass conservation) beyond the virial radius of each halo. Crucially, the same group-scale regime produces the strongest and most detectable deviations in group-galaxy lensing, making stacked group lensing a powerful observational test of feedback models. Our results motivate emulators that jointly predict the matter power spectrum and halo-matter correlations including baryonic effects, enabling unbiased cosmological inference from small scales."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:55:17Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    55,
                    17,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "19 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Kyle Miller"
                    },
                    {
                        "name": "Surhud More"
                    },
                    {
                        "name": "Bhuvnesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvnesh Jain"
                },
                "author": "Bhuvnesh Jain"
            },
            {
                "id": "http://arxiv.org/abs/2511.10628v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10628v1",
                "title": "Instella: Fully Open Language Models with Stellar Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instella: Fully Open Language Models with Stellar Performance"
                },
                "updated": "2025-11-13T18:52:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    52,
                    46,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10628v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:52:46Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    52,
                    46,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Jialian Wu"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Yusheng Su"
                    },
                    {
                        "name": "Prakamya Mishra"
                    },
                    {
                        "name": "Gowtham Ramesh"
                    },
                    {
                        "name": "Sudhanshu Ranjan"
                    },
                    {
                        "name": "Chaitanya Manem"
                    },
                    {
                        "name": "Ximeng Sun"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Pratik Prabhanjan Brahma"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum"
            },
            {
                "id": "http://arxiv.org/abs/2508.01341v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.01341v2",
                "title": "Debiasing Machine Learning Predictions for Causal Inference Without Additional Ground Truth Data: \"One Map, Many Trials\" in Satellite-Driven Poverty Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiasing Machine Learning Predictions for Causal Inference Without Additional Ground Truth Data: \"One Map, Many Trials\" in Satellite-Driven Poverty Analysis"
                },
                "updated": "2025-11-13T18:48:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    48,
                    8,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.01341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.01341v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Machine learning models trained on Earth observation data, such as satellite imagery, have demonstrated significant promise in predicting household-level wealth indices, enabling the creation of high-resolution wealth maps that can be leveraged across multiple causal trials while addressing chronic data scarcity in global development research. However, because standard training objectives prioritize overall predictive accuracy, these predictions often suffer from shrinkage toward the mean, leading to attenuated estimates of causal treatment effects and limiting their utility in policy evaluations. Existing debiasing methods, such as Prediction-Powered Inference (PPI), can handle this attenuation bias but require additional fresh ground-truth data at the downstream stage of causal inference, which restricts their applicability in data-scarce environments. We introduce and evaluate two post-hoc correction methods -- Linear Calibration Correction (LCC) and a Tweedie's correction approach -- that substantially reduce shrinkage-induced prediction bias without relying on newly collected labeled data. LCC applies a simple linear transformation estimated on a held-out calibration split; Tweedie's method locally de-shrink predictions using density score estimates and a noise scale learned upstream. We provide practical diagnostics for when a correction is warranted and discuss practical limitations. Across analytical results, simulations, and experiments with Demographic and Health Surveys (DHS) data, both approaches reduce attenuation; Tweedie's correction yields nearly unbiased treatment-effect estimates, enabling a \"one map, many trials\" paradigm. Although we demonstrate on EO-ML wealth mapping, the methods are not geospatial-specific: they apply to any setting where imputed outcomes are reused downstream (e.g., pollution indices, population density, or LLM-derived indicators).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models trained on Earth observation data, such as satellite imagery, have demonstrated significant promise in predicting household-level wealth indices, enabling the creation of high-resolution wealth maps that can be leveraged across multiple causal trials while addressing chronic data scarcity in global development research. However, because standard training objectives prioritize overall predictive accuracy, these predictions often suffer from shrinkage toward the mean, leading to attenuated estimates of causal treatment effects and limiting their utility in policy evaluations. Existing debiasing methods, such as Prediction-Powered Inference (PPI), can handle this attenuation bias but require additional fresh ground-truth data at the downstream stage of causal inference, which restricts their applicability in data-scarce environments. We introduce and evaluate two post-hoc correction methods -- Linear Calibration Correction (LCC) and a Tweedie's correction approach -- that substantially reduce shrinkage-induced prediction bias without relying on newly collected labeled data. LCC applies a simple linear transformation estimated on a held-out calibration split; Tweedie's method locally de-shrink predictions using density score estimates and a noise scale learned upstream. We provide practical diagnostics for when a correction is warranted and discuss practical limitations. Across analytical results, simulations, and experiments with Demographic and Health Surveys (DHS) data, both approaches reduce attenuation; Tweedie's correction yields nearly unbiased treatment-effect estimates, enabling a \"one map, many trials\" paradigm. Although we demonstrate on EO-ML wealth mapping, the methods are not geospatial-specific: they apply to any setting where imputed outcomes are reused downstream (e.g., pollution indices, population density, or LLM-derived indicators)."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-02T12:26:26Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    12,
                    26,
                    26,
                    5,
                    214,
                    0
                ],
                "arxiv_comment": "To appear in the Proceedings of AAAI 2026",
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Markus Pettersson"
                    },
                    {
                        "name": "Connor T. Jerzak"
                    },
                    {
                        "name": "Adel Daoud"
                    }
                ],
                "author_detail": {
                    "name": "Adel Daoud"
                },
                "author": "Adel Daoud"
            },
            {
                "id": "http://arxiv.org/abs/2511.10621v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10621v1",
                "title": "SSR: Socratic Self-Refine for Large Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSR: Socratic Self-Refine for Large Language Model Reasoning"
                },
                "updated": "2025-11-13T18:47:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    47,
                    7,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10621v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:47:07Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    47,
                    7,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Preprint; work in progress",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Zeyu Leo Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Semih Yavuz"
                    }
                ],
                "author_detail": {
                    "name": "Semih Yavuz"
                },
                "author": "Semih Yavuz"
            },
            {
                "id": "http://arxiv.org/abs/2511.10615v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10615v1",
                "title": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals"
                },
                "updated": "2025-11-13T18:45:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    45,
                    39,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10615v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:45:39Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    45,
                    39,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "8 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shruti Singh Baghel"
                    },
                    {
                        "name": "Yash Pratap Singh Rathore"
                    },
                    {
                        "name": "Sushovan Jena"
                    },
                    {
                        "name": "Anurag Pradhan"
                    },
                    {
                        "name": "Amit Shukla"
                    },
                    {
                        "name": "Arnav Bhavsar"
                    },
                    {
                        "name": "Pawan Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Goyal"
                },
                "author": "Pawan Goyal"
            },
            {
                "id": "http://arxiv.org/abs/2511.10611v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10611v1",
                "title": "Towards an Agentic Workflow for Internet Measurement Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards an Agentic Workflow for Internet Measurement Research"
                },
                "updated": "2025-11-13T18:44:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    44,
                    9,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10611v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Internet measurement research faces an accessibility crisis: complex analyses require custom integration of multiple specialized tools that demands specialized domain expertise. When network disruptions occur, operators need rapid diagnostic workflows spanning infrastructure mapping, routing analysis, and dependency modeling. However, developing these workflows requires specialized knowledge and significant manual effort.\n  We present ArachNet, the first system demonstrating that LLM agents can independently generate measurement workflows that mimics expert reasoning. Our core insight is that measurement expertise follows predictable compositional patterns that can be systematically automated. ArachNet operates through four specialized agents that mirror expert workflow, from problem decomposition to solution implementation. We validate ArachNet with progressively challenging Internet resilience scenarios. The system independently generates workflows that match expert-level reasoning and produce analytical outputs similar to specialist solutions. Generated workflows handle complex multi-framework integration that traditionally requires days of manual coordination. ArachNet lowers barriers to measurement workflow composition by automating the systematic reasoning process that experts use, enabling broader access to sophisticated measurement capabilities while maintaining the technical rigor required for research-quality analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internet measurement research faces an accessibility crisis: complex analyses require custom integration of multiple specialized tools that demands specialized domain expertise. When network disruptions occur, operators need rapid diagnostic workflows spanning infrastructure mapping, routing analysis, and dependency modeling. However, developing these workflows requires specialized knowledge and significant manual effort.\n  We present ArachNet, the first system demonstrating that LLM agents can independently generate measurement workflows that mimics expert reasoning. Our core insight is that measurement expertise follows predictable compositional patterns that can be systematically automated. ArachNet operates through four specialized agents that mirror expert workflow, from problem decomposition to solution implementation. We validate ArachNet with progressively challenging Internet resilience scenarios. The system independently generates workflows that match expert-level reasoning and produce analytical outputs similar to specialist solutions. Generated workflows handle complex multi-framework integration that traditionally requires days of manual coordination. ArachNet lowers barriers to measurement workflow composition by automating the systematic reasoning process that experts use, enabling broader access to sophisticated measurement capabilities while maintaining the technical rigor required for research-quality analysis."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:44:09Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    44,
                    9,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Alagappan Ramanathan"
                    },
                    {
                        "name": "Eunju Kang"
                    },
                    {
                        "name": "Dongsu Han"
                    },
                    {
                        "name": "Sangeetha Abdu Jyothi"
                    }
                ],
                "author_detail": {
                    "name": "Sangeetha Abdu Jyothi"
                },
                "author": "Sangeetha Abdu Jyothi"
            },
            {
                "id": "http://arxiv.org/abs/2511.00730v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00730v3",
                "title": "Teaching LLMs to See and Guide: Context-Aware Real-Time Assistance in Augmented Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching LLMs to See and Guide: Context-Aware Real-Time Assistance in Augmented Reality"
                },
                "updated": "2025-11-13T18:40:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    40,
                    35,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00730v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00730v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The growing adoption of augmented and virtual reality (AR and VR) technologies in industrial training and on-the-job assistance has created new opportunities for intelligent, context-aware support systems. As workers perform complex tasks guided by AR and VR, these devices capture rich streams of multimodal data, including gaze, hand actions, and task progression, that can reveal user intent and task state in real time. Leveraging this information effectively remains a major challenge. In this work, we present a context-aware large language model (LLM) assistant that integrates diverse data modalities, such as hand actions, task steps, and dialogue history, into a unified framework for real-time question answering. To systematically study how context influences performance, we introduce an incremental prompting framework, where each model version receives progressively richer contextual inputs. Using the HoloAssist dataset, which records AR-guided task executions, we evaluate how each modality contributes to the assistant's effectiveness. Our experiments show that incorporating multimodal context significantly improves the accuracy and relevance of responses. These findings highlight the potential of LLM-driven multimodal integration to enable adaptive, intuitive assistance for AR and VR-based industrial training and assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of augmented and virtual reality (AR and VR) technologies in industrial training and on-the-job assistance has created new opportunities for intelligent, context-aware support systems. As workers perform complex tasks guided by AR and VR, these devices capture rich streams of multimodal data, including gaze, hand actions, and task progression, that can reveal user intent and task state in real time. Leveraging this information effectively remains a major challenge. In this work, we present a context-aware large language model (LLM) assistant that integrates diverse data modalities, such as hand actions, task steps, and dialogue history, into a unified framework for real-time question answering. To systematically study how context influences performance, we introduce an incremental prompting framework, where each model version receives progressively richer contextual inputs. Using the HoloAssist dataset, which records AR-guided task executions, we evaluate how each modality contributes to the assistant's effectiveness. Our experiments show that incorporating multimodal context significantly improves the accuracy and relevance of responses. These findings highlight the potential of LLM-driven multimodal integration to enable adaptive, intuitive assistance for AR and VR-based industrial training and assistance."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-01T22:37:26Z",
                "published_parsed": [
                    2025,
                    11,
                    1,
                    22,
                    37,
                    26,
                    5,
                    305,
                    0
                ],
                "arxiv_comment": "This work is intended for submission to the IEEE Transactions on Systems, Man, and Cybernetics: Systems for possible publication",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Mahya Qorbani"
                    },
                    {
                        "name": "Kamran Paynabar"
                    },
                    {
                        "name": "Mohsen Moghaddam"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Moghaddam"
                },
                "author": "Mohsen Moghaddam"
            },
            {
                "id": "http://arxiv.org/abs/2511.10599v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10599v1",
                "title": "The $L_p$-error rate for randomized quasi-Monte Carlo self-normalized importance sampling of unbounded integrands",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $L_p$-error rate for randomized quasi-Monte Carlo self-normalized importance sampling of unbounded integrands"
                },
                "updated": "2025-11-13T18:37:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    37,
                    0,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10599v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Self-normalized importance sampling (SNIS) is a fundamental tool in Bayesian inference when the posterior distribution involves an unknown normalizing constant. Although $L_1$-error (bias) and $L_2$-error (root mean square error) estimates of SNIS are well established for bounded integrands, results for unbounded integrands remain limited, especially under randomized quasi-Monte Carlo (RQMC) sampling. In this work, we derive $L_p$-error rate $(p\\ge1)$ for RQMC-based SNIS (RQMC-SNIS) estimators with unbounded integrands on unbounded domains. A key step in our analysis is to first establish the $L_p$-error rate for plain RQMC integration. Our results allow for a broader class of transport maps used to generate samples from RQMC points. Under mild function boundary growth conditions, we further establish \\(L_p\\)-error rate of order \\(\\mathcal{O}(N^{-β+ ε})\\) for RQMC-SNIS estimators, where $ε>0$ is arbitrarily small, $N$ is the sample size, and \\(β\\in (0,1]\\) depends on the boundary growth rate of the resulting integrand. Numerical experiments validate the theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-normalized importance sampling (SNIS) is a fundamental tool in Bayesian inference when the posterior distribution involves an unknown normalizing constant. Although $L_1$-error (bias) and $L_2$-error (root mean square error) estimates of SNIS are well established for bounded integrands, results for unbounded integrands remain limited, especially under randomized quasi-Monte Carlo (RQMC) sampling. In this work, we derive $L_p$-error rate $(p\\ge1)$ for RQMC-based SNIS (RQMC-SNIS) estimators with unbounded integrands on unbounded domains. A key step in our analysis is to first establish the $L_p$-error rate for plain RQMC integration. Our results allow for a broader class of transport maps used to generate samples from RQMC points. Under mild function boundary growth conditions, we further establish \\(L_p\\)-error rate of order \\(\\mathcal{O}(N^{-β+ ε})\\) for RQMC-SNIS estimators, where $ε>0$ is arbitrarily small, $N$ is the sample size, and \\(β\\in (0,1]\\) depends on the boundary growth rate of the resulting integrand. Numerical experiments validate the theoretical results."
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:37:00Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    37,
                    0,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.NA"
                },
                "authors": [
                    {
                        "name": "Jiarui Du"
                    },
                    {
                        "name": "Zhijian He"
                    }
                ],
                "author_detail": {
                    "name": "Zhijian He"
                },
                "author": "Zhijian He"
            },
            {
                "id": "http://arxiv.org/abs/2511.10587v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10587v1",
                "title": "The relationship between warm and hot gas-phase metallicity in massive elliptical galaxies and the influence of AGN feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The relationship between warm and hot gas-phase metallicity in massive elliptical galaxies and the influence of AGN feedback"
                },
                "updated": "2025-11-13T18:25:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    25,
                    57,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10587v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Warm ionized gas is ubiquitous at the centers of X-ray bright elliptical galaxies. While it is believed to play a key role in the feeding and feedback processes of supermassive black holes, its origins remain under debate. Existing studies have primarily focused on the morphology and kinematics of warm ionized gas. This work aims to provide a new perspective on warm (10,000 K) ionized gas and its connection to X-ray-emitting hot gas (>10^6 K) by measuring and comparing their metallicities. We conducted a joint analysis of 13 massive elliptical galaxies using MUSE/VLT and Chandra observations. Emission-line ratios were measured for the warm ionized gas using MUSE observation, and used to infer the ionization mechanisms and derive metallicities of the warm ionized gas using HII, and LIN(E)R calibrations. We also computed the warm phase metallicity using X-ray/EUV, and pAGB stars models. For two sources at higher redshift, direct Te method was also used to measure warm gas metallicities. Our observations reveal that most sources exhibit composite ionization, with contributions from both star formation and LINER-like emission. A positive linear correlation was found between the gas-phase metallicities of the warm and hot phases, ranging from 0.3 to 1.5 Zsun, and suggest the intimate connection between the two gas phases, likely driven by gas cooling and/or mixing. In some sources the warm gas metallicity shows a central drop. A similar radial trend has been reported for the hot gas metallicity in some galaxy clusters. The ionization mechanisms of cooling flow elliptical galaxies are diverse, suggesting multiple channels for powering the warm ionized gas. The large variation in the warm gas metallicity further suggests that cold gas mass derived under the assumption of solar metallicity for the CO-to-H2 conversion factor needs to be revised by approximately an order of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warm ionized gas is ubiquitous at the centers of X-ray bright elliptical galaxies. While it is believed to play a key role in the feeding and feedback processes of supermassive black holes, its origins remain under debate. Existing studies have primarily focused on the morphology and kinematics of warm ionized gas. This work aims to provide a new perspective on warm (10,000 K) ionized gas and its connection to X-ray-emitting hot gas (>10^6 K) by measuring and comparing their metallicities. We conducted a joint analysis of 13 massive elliptical galaxies using MUSE/VLT and Chandra observations. Emission-line ratios were measured for the warm ionized gas using MUSE observation, and used to infer the ionization mechanisms and derive metallicities of the warm ionized gas using HII, and LIN(E)R calibrations. We also computed the warm phase metallicity using X-ray/EUV, and pAGB stars models. For two sources at higher redshift, direct Te method was also used to measure warm gas metallicities. Our observations reveal that most sources exhibit composite ionization, with contributions from both star formation and LINER-like emission. A positive linear correlation was found between the gas-phase metallicities of the warm and hot phases, ranging from 0.3 to 1.5 Zsun, and suggest the intimate connection between the two gas phases, likely driven by gas cooling and/or mixing. In some sources the warm gas metallicity shows a central drop. A similar radial trend has been reported for the hot gas metallicity in some galaxy clusters. The ionization mechanisms of cooling flow elliptical galaxies are diverse, suggesting multiple channels for powering the warm ionized gas. The large variation in the warm gas metallicity further suggests that cold gas mass derived under the assumption of solar metallicity for the CO-to-H2 conversion factor needs to be revised by approximately an order of magnitude."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:25:57Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    25,
                    57,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "19 pages, 7 figures. Accepted for publication in Astronomy and Astrophysics",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "Valeria Olivares"
                    },
                    {
                        "name": "Yuanyuan Su"
                    },
                    {
                        "name": "Pasquale Temi"
                    },
                    {
                        "name": "Ryan Eskenasy"
                    },
                    {
                        "name": "Helen Russell"
                    },
                    {
                        "name": "Massimo Gaspari"
                    },
                    {
                        "name": "Philippe Salome"
                    },
                    {
                        "name": "Francoise Combes"
                    },
                    {
                        "name": "Ming Sun"
                    },
                    {
                        "name": "Ezequiel Treister"
                    },
                    {
                        "name": "Kevin Fogarty"
                    },
                    {
                        "name": "Ana Jimenez-Gallardo"
                    },
                    {
                        "name": "Patricio Lagos"
                    }
                ],
                "author_detail": {
                    "name": "Patricio Lagos"
                },
                "author": "Patricio Lagos"
            },
            {
                "id": "http://arxiv.org/abs/2511.10560v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10560v1",
                "title": "OmniVGGT: Omni-Modality Driven Visual Geometry Grounded",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniVGGT: Omni-Modality Driven Visual Geometry Grounded"
                },
                "updated": "2025-11-13T17:59:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    59,
                    1,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10560v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T17:59:01Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    59,
                    1,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Project Page: https://livioni.github.io/OmniVGGT-offcial/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Haosong Peng"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yalun Dai"
                    },
                    {
                        "name": "Yushi Lan"
                    },
                    {
                        "name": "Yihang Luo"
                    },
                    {
                        "name": "Tianyu Qi"
                    },
                    {
                        "name": "Zhengshen Zhang"
                    },
                    {
                        "name": "Yufeng Zhan"
                    },
                    {
                        "name": "Junfei Zhang"
                    },
                    {
                        "name": "Wenchao Xu"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.10555v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10555v1",
                "title": "A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space"
                },
                "updated": "2025-11-13T17:56:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    56,
                    10,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10555v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T17:56:10Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    56,
                    10,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "16 pages, 13 figures, 5 tables",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Huijie Liu"
                    },
                    {
                        "name": "Shuhao Cui"
                    },
                    {
                        "name": "Haoxiang Cao"
                    },
                    {
                        "name": "Shuai Ma"
                    },
                    {
                        "name": "Kai Wu"
                    },
                    {
                        "name": "Guoliang Kang"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Kang"
                },
                "author": "Guoliang Kang"
            },
            {
                "id": "http://arxiv.org/abs/2511.10554v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10554v1",
                "title": "GraphFaaS: Serverless GNN Inference for Burst-Resilient, Real-Time Intrusion Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphFaaS: Serverless GNN Inference for Burst-Resilient, Real-Time Intrusion Detection"
                },
                "updated": "2025-11-13T17:55:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    55,
                    6,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10554v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Provenance-based intrusion detection is an increasingly popular application of graphical machine learning in cybersecurity, where system activities are modeled as provenance graphs to capture causality and correlations among potentially malicious actions. Graph Neural Networks (GNNs) have demonstrated strong performance in this setting. However, traditional statically-provisioned GNN inference architectures fall short in meeting two crucial demands of intrusion detection: (1) maintaining consistently low detection latency, and (2) handling highly irregular and bursty workloads. To holistically address these challenges, we present GraphFaaS, a serverless architecture tailored for GNN-based intrusion detection. GraphFaaS leverages the elasticity and agility of serverless computing to dynamically scale the GNN inference pipeline. We parallelize and adapt GNN workflows to a serverless environment, ensuring that the system can respond in real time to fluctuating workloads. By decoupling compute resources from static provisioning, GraphFaaS delivers stable inference latency, which is critical for dependable intrusion detection and timely incident response in cybersecurity operations. Preliminary evaluation shows GraphFaaS reduces average detection latency by 85% and coefficient of variation (CV) by 64% compared to the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provenance-based intrusion detection is an increasingly popular application of graphical machine learning in cybersecurity, where system activities are modeled as provenance graphs to capture causality and correlations among potentially malicious actions. Graph Neural Networks (GNNs) have demonstrated strong performance in this setting. However, traditional statically-provisioned GNN inference architectures fall short in meeting two crucial demands of intrusion detection: (1) maintaining consistently low detection latency, and (2) handling highly irregular and bursty workloads. To holistically address these challenges, we present GraphFaaS, a serverless architecture tailored for GNN-based intrusion detection. GraphFaaS leverages the elasticity and agility of serverless computing to dynamically scale the GNN inference pipeline. We parallelize and adapt GNN workflows to a serverless environment, ensuring that the system can respond in real time to fluctuating workloads. By decoupling compute resources from static provisioning, GraphFaaS delivers stable inference latency, which is critical for dependable intrusion detection and timely incident response in cybersecurity operations. Preliminary evaluation shows GraphFaaS reduces average detection latency by 85% and coefficient of variation (CV) by 64% compared to the baseline."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T17:55:06Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    55,
                    6,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted by ML For Systems workshop at Neural Information Processing Systems (NeurIPS 2025)",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Lingzhi Wang"
                    },
                    {
                        "name": "Vinod Yegneswaran"
                    },
                    {
                        "name": "Xinyi Shi"
                    },
                    {
                        "name": "Ziyu Li"
                    },
                    {
                        "name": "Ashish Gehani"
                    },
                    {
                        "name": "Yan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan Chen"
                },
                "author": "Yan Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.10552v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10552v1",
                "title": "URaG: Unified Retrieval and Generation in Multimodal LLMs for Efficient Long Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "URaG: Unified Retrieval and Generation in Multimodal LLMs for Efficient Long Document Understanding"
                },
                "updated": "2025-11-13T17:54:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    54,
                    9,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10552v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent multimodal large language models (MLLMs) still struggle with long document understanding due to two fundamental challenges: information interference from abundant irrelevant content, and the quadratic computational cost of Transformer-based architectures. Existing approaches primarily fall into two categories: token compression, which sacrifices fine-grained details; and introducing external retrievers, which increase system complexity and prevent end-to-end optimization. To address these issues, we conduct an in-depth analysis and observe that MLLMs exhibit a human-like coarse-to-fine reasoning pattern: early Transformer layers attend broadly across the document, while deeper layers focus on relevant evidence pages. Motivated by this insight, we posit that the inherent evidence localization capabilities of MLLMs can be explicitly leveraged to perform retrieval during the reasoning process, facilitating efficient long document understanding. To this end, we propose URaG, a simple-yet-effective framework that Unifies Retrieval and Generation within a single MLLM. URaG introduces a lightweight cross-modal retrieval module that converts the early Transformer layers into an efficient evidence selector, identifying and preserving the most relevant pages while discarding irrelevant content. This design enables the deeper layers to concentrate computational resources on pertinent information, improving both accuracy and efficiency. Extensive experiments demonstrate that URaG achieves state-of-the-art performance while reducing computational overhead by 44-56%. The code is available at https://github.com/shi-yx/URaG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent multimodal large language models (MLLMs) still struggle with long document understanding due to two fundamental challenges: information interference from abundant irrelevant content, and the quadratic computational cost of Transformer-based architectures. Existing approaches primarily fall into two categories: token compression, which sacrifices fine-grained details; and introducing external retrievers, which increase system complexity and prevent end-to-end optimization. To address these issues, we conduct an in-depth analysis and observe that MLLMs exhibit a human-like coarse-to-fine reasoning pattern: early Transformer layers attend broadly across the document, while deeper layers focus on relevant evidence pages. Motivated by this insight, we posit that the inherent evidence localization capabilities of MLLMs can be explicitly leveraged to perform retrieval during the reasoning process, facilitating efficient long document understanding. To this end, we propose URaG, a simple-yet-effective framework that Unifies Retrieval and Generation within a single MLLM. URaG introduces a lightweight cross-modal retrieval module that converts the early Transformer layers into an efficient evidence selector, identifying and preserving the most relevant pages while discarding irrelevant content. This design enables the deeper layers to concentrate computational resources on pertinent information, improving both accuracy and efficiency. Extensive experiments demonstrate that URaG achieves state-of-the-art performance while reducing computational overhead by 44-56%. The code is available at https://github.com/shi-yx/URaG."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T17:54:09Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    54,
                    9,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026 (Oral)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yongxin Shi"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Zeyu Shan"
                    },
                    {
                        "name": "Dezhi Peng"
                    },
                    {
                        "name": "Zening Lin"
                    },
                    {
                        "name": "Lianwen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Lianwen Jin"
                },
                "author": "Lianwen Jin"
            },
            {
                "id": "http://arxiv.org/abs/2511.10537v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10537v1",
                "title": "Constraining the nature of the most extreme Galactic particle accelerator. H.E.S.S. observations of the microquasar V4641 Sgr",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the nature of the most extreme Galactic particle accelerator. H.E.S.S. observations of the microquasar V4641 Sgr"
                },
                "updated": "2025-11-13T17:36:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    36,
                    47,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10537v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Microquasars have emerged as promising candidates to explain the cosmic-ray flux at petaelectronvolt energies. LHAASO observations revealed V4641~Sgr as the most extreme example so far. Using $\\approx$100~h of H.E.S.S. data, we performed a spectro-morphological study of the gamma-ray emission around V4641~Sgr. We employed HI and dedicated CO observations of the region to infer the target material for cosmic-ray interactions. We detected multi-TeV emission around V4641~Sgr with a high significance. The emission region is elongated. We found a power-law spectrum with an index $\\approx$1.8, and together with results from other gamma-ray instruments, this reveals a spectral energy distribution that peaks at energies of $\\approx$100~TeV for the first time. We found indications (3$σ$) of a two-component morphology, with indistinguishable spectral properties. The position of V4641~Sgr is inconsistent with the best-fit position of the single-component model and with the dip between the two components. We found no significant evidence of an energy-dependent morphology. No dense gas was found at any distance towards V4641~Sgr. The peak of the SED at $\\approx$100~TeV identifies V4641~Sgr as a candidate cosmic-ray accelerator beyond the so-called knee. The absence of dense target gas places stringent energetic constraints on hadronic interpretations, however. The H.E.S.S. measurement requires an unusually hard ($\\approx 1.5$) spectral index for the protons. A leptonic scenario faces fewer obstacles if the particle transport is fast enough to avoid losses and to reproduce the observed energy-independent morphology. The absence of bright \\xray emission across the gamma-ray emission region requires a magnetic field strength $\\lesssim3$~$μ$G, however. Our findings favour a leptonic origin of the gamma-ray emission. This conclusion does not exclude hadron acceleration in the V4641~Sgr system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microquasars have emerged as promising candidates to explain the cosmic-ray flux at petaelectronvolt energies. LHAASO observations revealed V4641~Sgr as the most extreme example so far. Using $\\approx$100~h of H.E.S.S. data, we performed a spectro-morphological study of the gamma-ray emission around V4641~Sgr. We employed HI and dedicated CO observations of the region to infer the target material for cosmic-ray interactions. We detected multi-TeV emission around V4641~Sgr with a high significance. The emission region is elongated. We found a power-law spectrum with an index $\\approx$1.8, and together with results from other gamma-ray instruments, this reveals a spectral energy distribution that peaks at energies of $\\approx$100~TeV for the first time. We found indications (3$σ$) of a two-component morphology, with indistinguishable spectral properties. The position of V4641~Sgr is inconsistent with the best-fit position of the single-component model and with the dip between the two components. We found no significant evidence of an energy-dependent morphology. No dense gas was found at any distance towards V4641~Sgr. The peak of the SED at $\\approx$100~TeV identifies V4641~Sgr as a candidate cosmic-ray accelerator beyond the so-called knee. The absence of dense target gas places stringent energetic constraints on hadronic interpretations, however. The H.E.S.S. measurement requires an unusually hard ($\\approx 1.5$) spectral index for the protons. A leptonic scenario faces fewer obstacles if the particle transport is fast enough to avoid losses and to reproduce the observed energy-independent morphology. The absence of bright \\xray emission across the gamma-ray emission region requires a magnetic field strength $\\lesssim3$~$μ$G, however. Our findings favour a leptonic origin of the gamma-ray emission. This conclusion does not exclude hadron acceleration in the V4641~Sgr system."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T17:36:47Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    36,
                    47,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted for publication in A&A. Machine-readable versions of the maps and spectra are available on Zenodo: https://zenodo.org/records/17570420",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "A. Acharyya"
                    },
                    {
                        "name": "F. Aharonian"
                    },
                    {
                        "name": "H. Ashkar"
                    },
                    {
                        "name": "M. Backes"
                    },
                    {
                        "name": "R. Batzofin"
                    },
                    {
                        "name": "D. Berge"
                    },
                    {
                        "name": "K. Bernlöhr"
                    },
                    {
                        "name": "M. Böttcher"
                    },
                    {
                        "name": "C. Boisson"
                    },
                    {
                        "name": "J. Bolmont"
                    },
                    {
                        "name": "F. Brun"
                    },
                    {
                        "name": "B. Bruno"
                    },
                    {
                        "name": "C. Burger-Scheidlin"
                    },
                    {
                        "name": "T. Bylund"
                    },
                    {
                        "name": "S. Casanova"
                    },
                    {
                        "name": "J. Celic"
                    },
                    {
                        "name": "M. Cerruti"
                    },
                    {
                        "name": "A. Chen"
                    },
                    {
                        "name": "M. Chernyakova"
                    },
                    {
                        "name": "J. O. Chibueze"
                    },
                    {
                        "name": "O. Chibueze"
                    },
                    {
                        "name": "B. Cornejo"
                    },
                    {
                        "name": "G. Cotter"
                    },
                    {
                        "name": "J. de Assis Scarpin"
                    },
                    {
                        "name": "M. de Bony de Lavergne"
                    },
                    {
                        "name": "M. de Naurois"
                    },
                    {
                        "name": "E. de Oña Wilhelmi"
                    },
                    {
                        "name": "A. G. Delgado Giler"
                    },
                    {
                        "name": "J. Devin"
                    },
                    {
                        "name": "A. Djannati-Ataï"
                    },
                    {
                        "name": "A. Dmytriiev"
                    },
                    {
                        "name": "K. Egberts"
                    },
                    {
                        "name": "K. Egg"
                    },
                    {
                        "name": "J. -P. Ernenwein"
                    },
                    {
                        "name": "C. Escañuela Nieves"
                    },
                    {
                        "name": "P. Fauverge"
                    },
                    {
                        "name": "K. Feijen"
                    },
                    {
                        "name": "M. D. Filipovic"
                    },
                    {
                        "name": "G. Fontaine"
                    },
                    {
                        "name": "S. Funk"
                    },
                    {
                        "name": "S. Gabici"
                    },
                    {
                        "name": "Y. A. Gallant"
                    },
                    {
                        "name": "J. F. Glicenstein"
                    },
                    {
                        "name": "J. Glombitza"
                    },
                    {
                        "name": "P. Goswami"
                    },
                    {
                        "name": "M. -H. Grondin"
                    },
                    {
                        "name": "L. Heckmann"
                    },
                    {
                        "name": "B. Heß"
                    },
                    {
                        "name": "J. A. Hinton"
                    },
                    {
                        "name": "W. Hofmann"
                    },
                    {
                        "name": "T. L. Holch"
                    },
                    {
                        "name": "M. Holler"
                    },
                    {
                        "name": "M. Jamrozy"
                    },
                    {
                        "name": "F. Jankowsky"
                    },
                    {
                        "name": "A. Jardin-Blicq"
                    },
                    {
                        "name": "I. Jaroschewski"
                    },
                    {
                        "name": "D. Jimeno"
                    },
                    {
                        "name": "I. Jung-Richardt"
                    },
                    {
                        "name": "K. Katarzyński"
                    },
                    {
                        "name": "D. Kerszberg"
                    },
                    {
                        "name": "B. Khélifi"
                    },
                    {
                        "name": "N. Komin"
                    },
                    {
                        "name": "K. Kosack"
                    },
                    {
                        "name": "D. Kostunin"
                    },
                    {
                        "name": "R. G. Lang"
                    },
                    {
                        "name": "S. Lazarević"
                    },
                    {
                        "name": "A. Lemière"
                    },
                    {
                        "name": "M. Lemoine-Goumard"
                    },
                    {
                        "name": "J. -P. Lenain"
                    },
                    {
                        "name": "P. Liniewicz"
                    },
                    {
                        "name": "A. Luashvili"
                    },
                    {
                        "name": "J. Mackey"
                    },
                    {
                        "name": "D. Malyshev"
                    },
                    {
                        "name": "V. Marandon"
                    },
                    {
                        "name": "M. G. F. Mayer"
                    },
                    {
                        "name": "A. Mehta"
                    },
                    {
                        "name": "A. M. W. Mitchell"
                    },
                    {
                        "name": "R. Moderski"
                    },
                    {
                        "name": "L. Mohrmann"
                    },
                    {
                        "name": "A. Montanari"
                    },
                    {
                        "name": "E. Moulin"
                    },
                    {
                        "name": "J. Niemiec"
                    },
                    {
                        "name": "L. Olivera-Nieto"
                    },
                    {
                        "name": "M. O. Moghadam"
                    },
                    {
                        "name": "S. Panny"
                    },
                    {
                        "name": "R. D. Parsons"
                    },
                    {
                        "name": "U. Pensec"
                    },
                    {
                        "name": "P. Pichard"
                    },
                    {
                        "name": "T. Preis"
                    },
                    {
                        "name": "G. Pühlhofer"
                    },
                    {
                        "name": "M. Punch"
                    },
                    {
                        "name": "A. Quirrenbach"
                    },
                    {
                        "name": "A. Reimer"
                    },
                    {
                        "name": "O. Reimer"
                    },
                    {
                        "name": "I. Reis"
                    },
                    {
                        "name": "Q. Remy"
                    },
                    {
                        "name": "H. X. Ren"
                    },
                    {
                        "name": "B. Reville"
                    },
                    {
                        "name": "F. Rieger"
                    },
                    {
                        "name": "G. Roellinghoff"
                    },
                    {
                        "name": "G. Rowell"
                    },
                    {
                        "name": "B. Rudak"
                    },
                    {
                        "name": "K. Sabri"
                    },
                    {
                        "name": "S. Safi-Harb"
                    },
                    {
                        "name": "V. Sahakian"
                    },
                    {
                        "name": "A. Santangelo"
                    },
                    {
                        "name": "M. Sasaki"
                    },
                    {
                        "name": "F. Schüssler"
                    },
                    {
                        "name": "J. N. S. Shapopi"
                    },
                    {
                        "name": "W. Si Said"
                    },
                    {
                        "name": "H. Sol"
                    },
                    {
                        "name": "Ł. Stawarz"
                    },
                    {
                        "name": "S. Steinmassl"
                    },
                    {
                        "name": "T. Tanaka"
                    },
                    {
                        "name": "A. M. Taylor"
                    },
                    {
                        "name": "G. L. Taylor"
                    },
                    {
                        "name": "R. Terrier"
                    },
                    {
                        "name": "Y. Tian"
                    },
                    {
                        "name": "A. Timmermans"
                    },
                    {
                        "name": "M. Tsirou"
                    },
                    {
                        "name": "N. Tsuji"
                    },
                    {
                        "name": "T. Unbehaun"
                    },
                    {
                        "name": "C. van Eldik"
                    },
                    {
                        "name": "M. Vecchi"
                    },
                    {
                        "name": "C. Venter"
                    },
                    {
                        "name": "J. Vink"
                    },
                    {
                        "name": "V. Voitsekhovskyi"
                    },
                    {
                        "name": "S. J. Wagner"
                    },
                    {
                        "name": "A. Wierzcholska"
                    },
                    {
                        "name": "M. Zacharias"
                    },
                    {
                        "name": "A. A. Zdziarski"
                    },
                    {
                        "name": "A. Zech"
                    },
                    {
                        "name": "W. Zhong"
                    },
                    {
                        "name": "S. Takekawa"
                    }
                ],
                "author_detail": {
                    "name": "S. Takekawa"
                },
                "arxiv_affiliation": "H.E.S.S. Collaboration",
                "author": "S. Takekawa"
            },
            {
                "id": "http://arxiv.org/abs/2511.07503v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07503v2",
                "title": "Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models"
                },
                "updated": "2025-11-13T17:35:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    35,
                    44,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07503v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The increased availability of genetic data has transformed genomics research, but raised many privacy concerns regarding its handling due to its sensitive nature. This work explores the use of language models (LMs) for the generation of synthetic genetic mutation profiles, leveraging differential privacy (DP) for the protection of sensitive genetic data. We empirically evaluate the privacy guarantees of our DP modes by introducing a novel Biologically-Informed Hybrid Membership Inference Attack (biHMIA), which combines traditional black box MIA with contextual genomics metrics for enhanced attack power. Our experiments show that both small and large transformer GPT-like models are viable synthetic variant generators for small-scale genomics, and that our hybrid attack leads, on average, to higher adversarial success compared to traditional metric-based MIAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased availability of genetic data has transformed genomics research, but raised many privacy concerns regarding its handling due to its sensitive nature. This work explores the use of language models (LMs) for the generation of synthetic genetic mutation profiles, leveraging differential privacy (DP) for the protection of sensitive genetic data. We empirically evaluate the privacy guarantees of our DP modes by introducing a novel Biologically-Informed Hybrid Membership Inference Attack (biHMIA), which combines traditional black box MIA with contextual genomics metrics for enhanced attack power. Our experiments show that both small and large transformer GPT-like models are viable synthetic variant generators for small-scale genomics, and that our hybrid attack leads, on average, to higher adversarial success compared to traditional metric-based MIAs."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T17:09:19Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    17,
                    9,
                    19,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Asia Belfiore"
                    },
                    {
                        "name": "Jonathan Passerat-Palmbach"
                    },
                    {
                        "name": "Dmitrii Usynin"
                    }
                ],
                "author_detail": {
                    "name": "Dmitrii Usynin"
                },
                "author": "Dmitrii Usynin"
            },
            {
                "id": "http://arxiv.org/abs/2511.07871v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07871v2",
                "title": "AlignSurvey: A Comprehensive Benchmark for Human Preferences Alignment in Social Surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignSurvey: A Comprehensive Benchmark for Human Preferences Alignment in Social Surveys"
                },
                "updated": "2025-11-13T17:30:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    30,
                    34,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07871v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07871v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding human attitudes, preferences, and behaviors through social surveys is essential for academic research and policymaking. Yet traditional surveys face persistent challenges, including fixed-question formats, high costs, limited adaptability, and difficulties ensuring cross-cultural equivalence. While recent studies explore large language models (LLMs) to simulate survey responses, most are limited to structured questions, overlook the entire survey process, and risks under-representing marginalized groups due to training data biases. We introduce AlignSurvey, the first benchmark that systematically replicates and evaluates the full social survey pipeline using LLMs. It defines four tasks aligned with key survey stages: social role modeling, semi-structured interview modeling, attitude stance modeling and survey response modeling. It also provides task-specific evaluation metrics to assess alignment fidelity, consistency, and fairness at both individual and group levels, with a focus on demographic diversity. To support AlignSurvey, we construct a multi-tiered dataset architecture: (i) the Social Foundation Corpus, a cross-national resource with 44K+ interview dialogues and 400K+ structured survey records; and (ii) a suite of Entire-Pipeline Survey Datasets, including the expert-annotated AlignSurvey-Expert (ASE) and two nationally representative surveys for cross-cultural evaluation. We release the SurveyLM family, obtained through two-stage fine-tuning of open-source LLMs, and offer reference models for evaluating domain-specific alignment. All datasets, models, and tools are available at github and huggingface to support transparent and socially responsible research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding human attitudes, preferences, and behaviors through social surveys is essential for academic research and policymaking. Yet traditional surveys face persistent challenges, including fixed-question formats, high costs, limited adaptability, and difficulties ensuring cross-cultural equivalence. While recent studies explore large language models (LLMs) to simulate survey responses, most are limited to structured questions, overlook the entire survey process, and risks under-representing marginalized groups due to training data biases. We introduce AlignSurvey, the first benchmark that systematically replicates and evaluates the full social survey pipeline using LLMs. It defines four tasks aligned with key survey stages: social role modeling, semi-structured interview modeling, attitude stance modeling and survey response modeling. It also provides task-specific evaluation metrics to assess alignment fidelity, consistency, and fairness at both individual and group levels, with a focus on demographic diversity. To support AlignSurvey, we construct a multi-tiered dataset architecture: (i) the Social Foundation Corpus, a cross-national resource with 44K+ interview dialogues and 400K+ structured survey records; and (ii) a suite of Entire-Pipeline Survey Datasets, including the expert-annotated AlignSurvey-Expert (ASE) and two nationally representative surveys for cross-cultural evaluation. We release the SurveyLM family, obtained through two-stage fine-tuning of open-source LLMs, and offer reference models for evaluating domain-specific alignment. All datasets, models, and tools are available at github and huggingface to support transparent and socially responsible research."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T06:14:21Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    6,
                    14,
                    21,
                    1,
                    315,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chenxi Lin"
                    },
                    {
                        "name": "Weikang Yuan"
                    },
                    {
                        "name": "Zhuoren Jiang"
                    },
                    {
                        "name": "Biao Huang"
                    },
                    {
                        "name": "Ruitao Zhang"
                    },
                    {
                        "name": "Jianan Ge"
                    },
                    {
                        "name": "Yueqian Xu"
                    },
                    {
                        "name": "Jianxing Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jianxing Yu"
                },
                "author": "Jianxing Yu"
            },
            {
                "id": "http://arxiv.org/abs/2511.10519v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10519v1",
                "title": "Say It Differently: Linguistic Styles as Jailbreak Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Say It Differently: Linguistic Styles as Jailbreak Vectors"
                },
                "updated": "2025-11-13T17:24:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    24,
                    38,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10519v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are commonly evaluated for robustness against paraphrased or semantically equivalent jailbreak prompts, yet little attention has been paid to linguistic variation as an attack surface. In this work, we systematically study how linguistic styles such as fear or curiosity can reframe harmful intent and elicit unsafe responses from aligned models. We construct style-augmented jailbreak benchmark by transforming prompts from 3 standard datasets into 11 distinct linguistic styles using handcrafted templates and LLM-based rewrites, while preserving semantic intent. Evaluating 16 open- and close-source instruction-tuned models, we find that stylistic reframing increases jailbreak success rates by up to +57 percentage points. Styles such as fearful, curious and compassionate are most effective and contextualized rewrites outperform templated variants.\n  To mitigate this, we introduce a style neutralization preprocessing step using a secondary LLM to strip manipulative stylistic cues from user inputs, significantly reducing jailbreak success rates. Our findings reveal a systemic and scaling-resistant vulnerability overlooked in current safety pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are commonly evaluated for robustness against paraphrased or semantically equivalent jailbreak prompts, yet little attention has been paid to linguistic variation as an attack surface. In this work, we systematically study how linguistic styles such as fear or curiosity can reframe harmful intent and elicit unsafe responses from aligned models. We construct style-augmented jailbreak benchmark by transforming prompts from 3 standard datasets into 11 distinct linguistic styles using handcrafted templates and LLM-based rewrites, while preserving semantic intent. Evaluating 16 open- and close-source instruction-tuned models, we find that stylistic reframing increases jailbreak success rates by up to +57 percentage points. Styles such as fearful, curious and compassionate are most effective and contextualized rewrites outperform templated variants.\n  To mitigate this, we introduce a style neutralization preprocessing step using a secondary LLM to strip manipulative stylistic cues from user inputs, significantly reducing jailbreak success rates. Our findings reveal a systemic and scaling-resistant vulnerability overlooked in current safety pipelines."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T17:24:38Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    24,
                    38,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Srikant Panda"
                    },
                    {
                        "name": "Avinash Rai"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Rai"
                },
                "author": "Avinash Rai"
            },
            {
                "id": "http://arxiv.org/abs/2511.10518v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10518v1",
                "title": "SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation"
                },
                "updated": "2025-11-13T17:24:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    24,
                    37,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10518v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T17:24:37Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    24,
                    37,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026 (Oral), Project Page: https://github.com/JiuTian-VL/SemanticVLA",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Renshan Zhang"
                    },
                    {
                        "name": "Rui Shao"
                    },
                    {
                        "name": "Zhijian Fang"
                    },
                    {
                        "name": "Kaiwen Zhou"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie"
            },
            {
                "id": "http://arxiv.org/abs/2509.11816v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.11816v2",
                "title": "Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning"
                },
                "updated": "2025-11-13T17:23:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    23,
                    32,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.11816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.11816v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current unlearning and safety training methods consistently fail to remove dangerous knowledge from language models. We identify the root cause - unlearning targets representations which are too general - and develop a highly selective technique that unlearns robustly while preserving general performance.\n  Our method performs PCA on activations and module-output gradients to identify subspaces containing common representations, then collapses these subspaces before computing unlearning updates, a technique we term Collapse of Irrelevant Representations (CIR). This avoids unlearning general knowledge and targets only representations specific to the facts being unlearned.\n  When unlearning bio- and cyber-hazardous facts from Llama-3.1-8B, we achieve over 30x greater reduction in post-attack accuracy than the best baseline (Circuit Breakers), while disrupting general performance 30x less, and using less than 3 GPU-seconds per fact.\n  Thus, by disentangling harmful and benign capabilities at the level of representations, CIR enables robust and non-disruptive unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current unlearning and safety training methods consistently fail to remove dangerous knowledge from language models. We identify the root cause - unlearning targets representations which are too general - and develop a highly selective technique that unlearns robustly while preserving general performance.\n  Our method performs PCA on activations and module-output gradients to identify subspaces containing common representations, then collapses these subspaces before computing unlearning updates, a technique we term Collapse of Irrelevant Representations (CIR). This avoids unlearning general knowledge and targets only representations specific to the facts being unlearned.\n  When unlearning bio- and cyber-hazardous facts from Llama-3.1-8B, we achieve over 30x greater reduction in post-attack accuracy than the best baseline (Circuit Breakers), while disrupting general performance 30x less, and using less than 3 GPU-seconds per fact.\n  Thus, by disentangling harmful and benign capabilities at the level of representations, CIR enables robust and non-disruptive unlearning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-15T11:55:10Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    55,
                    10,
                    0,
                    258,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Filip Sondej"
                    },
                    {
                        "name": "Yushi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yushi Yang"
                },
                "author": "Yushi Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.10516v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10516v1",
                "title": "How Worrying Are Privacy Attacks Against Machine Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Worrying Are Privacy Attacks Against Machine Learning?"
                },
                "updated": "2025-11-13T17:22:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    22,
                    7,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10516v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In several jurisdictions, the regulatory framework on the release and sharing of personal data is being extended to machine learning (ML). The implicit assumption is that disclosing a trained ML model entails a privacy risk for any personal data used in training comparable to directly releasing those data. However, given a trained model, it is necessary to mount a privacy attack to make inferences on the training data. In this concept paper, we examine the main families of privacy attacks against predictive and generative ML, including membership inference attacks (MIAs), property inference attacks, and reconstruction attacks. Our discussion shows that most of these attacks seem less effective in the real world than what a prima face interpretation of the related literature could suggest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In several jurisdictions, the regulatory framework on the release and sharing of personal data is being extended to machine learning (ML). The implicit assumption is that disclosing a trained ML model entails a privacy risk for any personal data used in training comparable to directly releasing those data. However, given a trained model, it is necessary to mount a privacy attack to make inferences on the training data. In this concept paper, we examine the main families of privacy attacks against predictive and generative ML, including membership inference attacks (MIAs), property inference attacks, and reconstruction attacks. Our discussion shows that most of these attacks seem less effective in the real world than what a prima face interpretation of the related literature could suggest."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T17:22:07Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    22,
                    7,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "arxiv_journal_ref": "in Data Privacy Management - DPM 2025, within 30th European Symposium on Research in Computer Security-ESORICS 2025, LNCS, Springer",
                "authors": [
                    {
                        "name": "Josep Domingo-Ferrer"
                    }
                ],
                "author_detail": {
                    "name": "Josep Domingo-Ferrer"
                },
                "author": "Josep Domingo-Ferrer"
            },
            {
                "id": "http://arxiv.org/abs/2511.10507v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10507v1",
                "title": "Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following"
                },
                "updated": "2025-11-13T17:14:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    14,
                    1,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10507v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T17:14:01Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    14,
                    1,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yun He"
                    },
                    {
                        "name": "Wenzhe Li"
                    },
                    {
                        "name": "Hejia Zhang"
                    },
                    {
                        "name": "Songlin Li"
                    },
                    {
                        "name": "Karishma Mandyam"
                    },
                    {
                        "name": "Sopan Khosla"
                    },
                    {
                        "name": "Yuanhao Xiong"
                    },
                    {
                        "name": "Nanshu Wang"
                    },
                    {
                        "name": "Selina Peng"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Shengjie Bi"
                    },
                    {
                        "name": "Shishir G. Patil"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Shengyu Feng"
                    },
                    {
                        "name": "Julian Katz-Samuels"
                    },
                    {
                        "name": "Richard Yuanzhe Pang"
                    },
                    {
                        "name": "Sujan Gonugondla"
                    },
                    {
                        "name": "Hunter Lang"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Yundi Qian"
                    },
                    {
                        "name": "Maryam Fazel-Zarandi"
                    },
                    {
                        "name": "Licheng Yu"
                    },
                    {
                        "name": "Amine Benhalloum"
                    },
                    {
                        "name": "Hany Awadalla"
                    },
                    {
                        "name": "Manaal Faruqui"
                    }
                ],
                "author_detail": {
                    "name": "Manaal Faruqui"
                },
                "author": "Manaal Faruqui"
            },
            {
                "id": "http://arxiv.org/abs/2410.21597v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2410.21597v3",
                "title": "Reducing the Scope of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the Scope of Language Models"
                },
                "updated": "2025-11-13T17:11:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    11,
                    10,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2410.21597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2410.21597v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are deployed in a wide variety of user-facing applications. Typically, these deployments have some specific purpose, like answering questions grounded on documentation or acting as coding assistants, but they require general language understanding. In such deployments, LLMs should respond only to queries that align with the intended purpose and reject all other requests, such as generating poetry or answering questions about physics, a task we refer to as `scoping'. We conduct a comprehensive empirical evaluation of various methods, ranging from prompting, fine-tuning to preference learning and the recently proposed general alignment technique known as Circuit Breakers (CB). Across three families of language models and a broad variety of tasks, we show that it is possible to scope language models. We examine scoping for multiple topics, and fine-grained topics. We ablate diversity of irrelevant queries, layer different techniques, conduct adversarial evaluations and more. Among other results, we find that when diverse examples of irrelevant queries are available, simple supervised fine-tuning produces the best results, but when such diversity is low, Circuit Breakers perform quite well. One can often get the benefits of both methods by layering them in succession. We intend our study to serve as a practitioner's guide to scoping LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are deployed in a wide variety of user-facing applications. Typically, these deployments have some specific purpose, like answering questions grounded on documentation or acting as coding assistants, but they require general language understanding. In such deployments, LLMs should respond only to queries that align with the intended purpose and reject all other requests, such as generating poetry or answering questions about physics, a task we refer to as `scoping'. We conduct a comprehensive empirical evaluation of various methods, ranging from prompting, fine-tuning to preference learning and the recently proposed general alignment technique known as Circuit Breakers (CB). Across three families of language models and a broad variety of tasks, we show that it is possible to scope language models. We examine scoping for multiple topics, and fine-grained topics. We ablate diversity of irrelevant queries, layer different techniques, conduct adversarial evaluations and more. Among other results, we find that when diverse examples of irrelevant queries are available, simple supervised fine-tuning produces the best results, but when such diversity is low, Circuit Breakers perform quite well. One can often get the benefits of both methods by layering them in succession. We intend our study to serve as a practitioner's guide to scoping LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-10-28T23:06:57Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    23,
                    6,
                    57,
                    0,
                    302,
                    0
                ],
                "arxiv_comment": "Appears in AAAI 2026 in the Main Technical Track",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "David Yunis"
                    },
                    {
                        "name": "Siyu Huo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Danish Contractor"
                    }
                ],
                "author_detail": {
                    "name": "Danish Contractor"
                },
                "author": "Danish Contractor"
            },
            {
                "id": "http://arxiv.org/abs/2504.03966v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.03966v2",
                "title": "Bridging LMS and generative AI: dynamic course content integration (DCCI) for enhancing student satisfaction and engagement via the ask ME assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging LMS and generative AI: dynamic course content integration (DCCI) for enhancing student satisfaction and engagement via the ask ME assistant"
                },
                "updated": "2025-11-13T17:07:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    7,
                    1,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.03966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.03966v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1007/s40692-025-00367-w",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Integration of Large Language Models (LLMs) with Learning Management Systems (LMSs) can enhance task automation and accessibility in education. However, hallucination where LLMs generate inaccurate or misleading information remains a challenge. This study introduces the Dynamic Course Content Integration (DCCI) mechanism, which dynamically retrieves course content from Canvas LMS and structures it within an LLM's context window via prompt engineering, enabling the LLM-powered assistant, Ask ME, to deliver context-aware, curriculum-aligned responses while mitigating hallucinations. A mixed-methods pilot study grounded in Self-Determination Theory (autonomy, competence) and the Technology Acceptance Model (perceived usefulness, ease of use) evaluated DCCI's effectiveness with 120 first-year programming students at Eötvös Loránd University. The course focused on foundational programming patterns in C#, including writing program specifications. We analyzed 14,746 logged interactions and a post-course survey completed by 101 students. User satisfaction was measured via a 5-point Likert scale (turn-level ratings), while the survey assessed usability, engagement, and ethical concerns. Results indicated high satisfaction (mean 4.65/5) and strong recognition of Ask ME's ability to provide timely, contextually relevant answers to administrative and course-related queries. 78.06% agreed that Ask ME's Canvas integration reduced platform switching, improving usability, engagement, comprehension, and topic exploration. Many students reported reduced hesitation to ask questions and increased motivation for self-directed learning, though concerns about over-reliance on AI and reduced student-teacher interaction emerged. This study demonstrates that DCCI enhances LLM reliability, student satisfaction, and engagement in AI-driven educational automation, while highlighting the importance of balancing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integration of Large Language Models (LLMs) with Learning Management Systems (LMSs) can enhance task automation and accessibility in education. However, hallucination where LLMs generate inaccurate or misleading information remains a challenge. This study introduces the Dynamic Course Content Integration (DCCI) mechanism, which dynamically retrieves course content from Canvas LMS and structures it within an LLM's context window via prompt engineering, enabling the LLM-powered assistant, Ask ME, to deliver context-aware, curriculum-aligned responses while mitigating hallucinations. A mixed-methods pilot study grounded in Self-Determination Theory (autonomy, competence) and the Technology Acceptance Model (perceived usefulness, ease of use) evaluated DCCI's effectiveness with 120 first-year programming students at Eötvös Loránd University. The course focused on foundational programming patterns in C#, including writing program specifications. We analyzed 14,746 logged interactions and a post-course survey completed by 101 students. User satisfaction was measured via a 5-point Likert scale (turn-level ratings), while the survey assessed usability, engagement, and ethical concerns. Results indicated high satisfaction (mean 4.65/5) and strong recognition of Ask ME's ability to provide timely, contextually relevant answers to administrative and course-related queries. 78.06% agreed that Ask ME's Canvas integration reduced platform switching, improving usability, engagement, comprehension, and topic exploration. Many students reported reduced hesitation to ask questions and increased motivation for self-directed learning, though concerns about over-reliance on AI and reduced student-teacher interaction emerged. This study demonstrates that DCCI enhances LLM reliability, student satisfaction, and engagement in AI-driven educational automation, while highlighting the importance of balancing"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-04T22:17:30Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    22,
                    17,
                    30,
                    4,
                    94,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "arxiv_journal_ref": "J. Comput. Educ. (2025)",
                "authors": [
                    {
                        "name": "Kovan Mzwri"
                    },
                    {
                        "name": "Márta Turcsányi-Szabo"
                    }
                ],
                "author_detail": {
                    "name": "Márta Turcsányi-Szabo"
                },
                "arxiv_affiliation": "Department of Media & Educational Technology, Faculty of Informatics, Eötvös Loránd University, Budapest, Hungary",
                "author": "Márta Turcsányi-Szabo",
                "arxiv_doi": "10.1007/s40692-025-00367-w"
            },
            {
                "id": "http://arxiv.org/abs/2507.20067v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.20067v2",
                "title": "PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training"
                },
                "updated": "2025-11-13T17:05:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    5,
                    21,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.20067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.20067v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inference-time alignment enables large language models (LLMs) to generate outputs aligned with end-user preferences without further training. Recent post-training methods achieve this by using small guidance models to modify token generation during inference. These methods typically optimize a reward function KL-regularized by the original LLM taken as the reference policy. A critical limitation, however, is their dependence on a pre-trained reward model, which requires fitting to human preference feedback--a potentially unstable process. In contrast, we introduce PITA, a novel framework that integrates preference feedback directly into the LLM's token generation, eliminating the need for a reward model. PITA learns a small preference-based guidance policy to modify token probabilities at inference time without LLM fine-tuning, reducing computational cost and bypassing the pre-trained reward model dependency. The problem is framed as identifying an underlying preference distribution, solved through stochastic search and iterative refinement of the preference-based guidance model. We evaluate PITA across diverse tasks, including mathematical reasoning and sentiment classification, demonstrating its effectiveness in aligning LLM outputs with user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time alignment enables large language models (LLMs) to generate outputs aligned with end-user preferences without further training. Recent post-training methods achieve this by using small guidance models to modify token generation during inference. These methods typically optimize a reward function KL-regularized by the original LLM taken as the reference policy. A critical limitation, however, is their dependence on a pre-trained reward model, which requires fitting to human preference feedback--a potentially unstable process. In contrast, we introduce PITA, a novel framework that integrates preference feedback directly into the LLM's token generation, eliminating the need for a reward model. PITA learns a small preference-based guidance policy to modify token probabilities at inference time without LLM fine-tuning, reducing computational cost and bypassing the pre-trained reward model dependency. The problem is framed as identifying an underlying preference distribution, solved through stochastic search and iterative refinement of the preference-based guidance model. We evaluate PITA across diverse tasks, including mathematical reasoning and sentiment classification, demonstrating its effectiveness in aligning LLM outputs with user preferences."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-26T21:46:32Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    21,
                    46,
                    32,
                    5,
                    207,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sarat Chandra Bobbili"
                    },
                    {
                        "name": "Ujwal Dinesha"
                    },
                    {
                        "name": "Dheeraj Narasimha"
                    },
                    {
                        "name": "Srinivas Shakkottai"
                    }
                ],
                "author_detail": {
                    "name": "Srinivas Shakkottai"
                },
                "author": "Srinivas Shakkottai"
            },
            {
                "id": "http://arxiv.org/abs/2511.10492v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10492v1",
                "title": "Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding"
                },
                "updated": "2025-11-13T16:59:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    59,
                    22,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10492v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.\n  Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.\n  Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T16:59:22Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    59,
                    22,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Yunkai Zhang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Feng"
                    },
                    {
                        "name": "Lin"
                    },
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Hanchao Yu"
                    },
                    {
                        "name": "Jason Liu"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Zhuoran Yu"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Diji Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diji Yang"
                },
                "arxiv_affiliation": "Ryan",
                "author": "Diji Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.09540v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09540v2",
                "title": "vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs"
                },
                "updated": "2025-11-13T16:56:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    56,
                    35,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09540v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work aims to continuously expand to encompass more downstream applications, and the corresponding resources are intended to be shared through https://github.com/VinyehShaw/UniEqui.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work aims to continuously expand to encompass more downstream applications, and the corresponding resources are intended to be shared through https://github.com/VinyehShaw/UniEqui."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T18:38:33Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    18,
                    38,
                    33,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "Accepted as an Oral Presentation at AAAI 2026 Main Technical Track (this version is not peer-reviewed; it is the extended version)",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Minye Shao"
                    },
                    {
                        "name": "Sihan Guo"
                    },
                    {
                        "name": "Xinrun Li"
                    },
                    {
                        "name": "Xingyu Miao"
                    },
                    {
                        "name": "Haoran Duan"
                    },
                    {
                        "name": "Yang Long"
                    }
                ],
                "author_detail": {
                    "name": "Yang Long"
                },
                "author": "Yang Long"
            },
            {
                "id": "http://arxiv.org/abs/2511.10488v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10488v1",
                "title": "SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers"
                },
                "updated": "2025-11-13T16:56:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    56,
                    24,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10488v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens. Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation. Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process. SPOT informs token sparsification and facilitates the elimination of such tokens, improving computational efficiency without sacrificing performance. SPOT employs computationally lightweight predictors that can be plugged into various ViT architectures and learn to derive effective input-specific token prioritization across layers. Its versatile design supports a range of performance levels adaptable to varying resource constraints. Empirical evaluations demonstrate significant efficiency gains of up to 40% compared to standard ViTs, while maintaining or even improving accuracy. Code and models are available at https://github.com/odedsc/SPOT .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens. Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation. Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process. SPOT informs token sparsification and facilitates the elimination of such tokens, improving computational efficiency without sacrificing performance. SPOT employs computationally lightweight predictors that can be plugged into various ViT architectures and learn to derive effective input-specific token prioritization across layers. Its versatile design supports a range of performance levels adaptable to varying resource constraints. Empirical evaluations demonstrate significant efficiency gains of up to 40% compared to standard ViTs, while maintaining or even improving accuracy. Code and models are available at https://github.com/odedsc/SPOT ."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T16:56:24Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    56,
                    24,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Project repository: https://github.com/odedsc/SPOT",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Oded Schlesinger"
                    },
                    {
                        "name": "Amirhossein Farzam"
                    },
                    {
                        "name": "J. Matias Di Martino"
                    },
                    {
                        "name": "Guillermo Sapiro"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Sapiro"
                },
                "author": "Guillermo Sapiro"
            },
            {
                "id": "http://arxiv.org/abs/2502.04068v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.04068v3",
                "title": "Detectability of Massive Boson Stars using Gravitational Waves from Fundamental Oscillations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detectability of Massive Boson Stars using Gravitational Waves from Fundamental Oscillations"
                },
                "updated": "2025-11-13T16:48:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    48,
                    45,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.04068v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.04068v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Boson Stars are macroscopic self-gravitating configurations made of complex scalar fields. These exotic compact objects would manifest as dark Boson stars and, in the absence of electromagnetic signatures, could mimic properties of compact stars in the gravitational wave spectrum. In a recent study, using the simplest potential for massive Boson stars, we demonstrated that fundamental non-radial oscillations ($f$-modes) obey scaling relations that allow them to be distinguished from neutron stars and black holes. In this work, we provide analytical fits for these scaling relations, valid for the dark matter parameter space compatible with current astrophysical and cosmological data, that can be directly incorporated into future studies of massive Boson stars in the strong coupling regime, avoiding the need for numerical calculations. We also provide analytical fits for empirical and universal relations for gravitational wave asteroseismology, which can be used to infer microscopic dark matter properties following a successful detection. Further, we investigate the possibility of detection of $f$-modes and the dark matter parameter space that can be probed with current and future gravitational wave detectors across multiple frequency bands. Assuming a burst gravitational wave model and demanding a signal-to-noise ratio of 5, we show that the current and future detectors can, in principle, probe Boson star $f$-modes up to cosmological distances: 1 Mpc with aLIGO, 30 Mpc with Cosmic Explorer and Einstein Telescope, and in the best case scenario, about 300 Mpc with LISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boson Stars are macroscopic self-gravitating configurations made of complex scalar fields. These exotic compact objects would manifest as dark Boson stars and, in the absence of electromagnetic signatures, could mimic properties of compact stars in the gravitational wave spectrum. In a recent study, using the simplest potential for massive Boson stars, we demonstrated that fundamental non-radial oscillations ($f$-modes) obey scaling relations that allow them to be distinguished from neutron stars and black holes. In this work, we provide analytical fits for these scaling relations, valid for the dark matter parameter space compatible with current astrophysical and cosmological data, that can be directly incorporated into future studies of massive Boson stars in the strong coupling regime, avoiding the need for numerical calculations. We also provide analytical fits for empirical and universal relations for gravitational wave asteroseismology, which can be used to infer microscopic dark matter properties following a successful detection. Further, we investigate the possibility of detection of $f$-modes and the dark matter parameter space that can be probed with current and future gravitational wave detectors across multiple frequency bands. Assuming a burst gravitational wave model and demanding a signal-to-noise ratio of 5, we show that the current and future detectors can, in principle, probe Boson star $f$-modes up to cosmological distances: 1 Mpc with aLIGO, 30 Mpc with Cosmic Explorer and Einstein Telescope, and in the best case scenario, about 300 Mpc with LISA."
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-06T13:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    25,
                    9,
                    3,
                    37,
                    0
                ],
                "arxiv_comment": "31 pages, 10 figures, 4 tables. This is the Accepted Manuscript Version of the article accepted for publication in JCAP",
                "arxiv_primary_category": {
                    "term": "gr-qc"
                },
                "authors": [
                    {
                        "name": "Swarnim Shirke"
                    },
                    {
                        "name": "Bikram Keshari Pradhan"
                    },
                    {
                        "name": "Debarati Chatterjee"
                    },
                    {
                        "name": "Laura Sagunski"
                    },
                    {
                        "name": "Jürgen Schaffner-Bielich"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schaffner-Bielich"
                },
                "author": "Jürgen Schaffner-Bielich"
            },
            {
                "id": "http://arxiv.org/abs/2511.08379v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08379v2",
                "title": "SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models"
                },
                "updated": "2025-11-13T16:46:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    46,
                    23,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08379v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Refusal refers to the functional behavior enabling safety-aligned language models to reject harmful or unethical prompts. Following the growing scientific interest in mechanistic interpretability, recent work encoded refusal behavior as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Motivated by these findings, we propose a novel method leveraging Self-Organizing Maps (SOMs) to extract multiple refusal directions. To this end, we first prove that SOMs generalize the prior work's difference-in-means technique. We then train SOMs on harmful prompt representations to identify multiple neurons. By subtracting the centroid of harmless representations from each neuron, we derive a set of multiple directions expressing the refusal concept. We validate our method on an extensive experimental setup, demonstrating that ablating multiple directions from models' internals outperforms not only the single-direction baseline but also specialized jailbreak algorithms, leading to an effective suppression of refusal. Finally, we conclude by analyzing the mechanistic implications of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal refers to the functional behavior enabling safety-aligned language models to reject harmful or unethical prompts. Following the growing scientific interest in mechanistic interpretability, recent work encoded refusal behavior as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Motivated by these findings, we propose a novel method leveraging Self-Organizing Maps (SOMs) to extract multiple refusal directions. To this end, we first prove that SOMs generalize the prior work's difference-in-means technique. We then train SOMs on harmful prompt representations to identify multiple neurons. By subtracting the centroid of harmless representations from each neuron, we derive a set of multiple directions expressing the refusal concept. We validate our method on an extensive experimental setup, demonstrating that ablating multiple directions from models' internals outperforms not only the single-direction baseline but also specialized jailbreak algorithms, leading to an effective suppression of refusal. Finally, we conclude by analyzing the mechanistic implications of our approach."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T16:01:42Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    16,
                    1,
                    42,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "Accepted at AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Giorgio Piras"
                    },
                    {
                        "name": "Raffaele Mura"
                    },
                    {
                        "name": "Fabio Brau"
                    },
                    {
                        "name": "Luca Oneto"
                    },
                    {
                        "name": "Fabio Roli"
                    },
                    {
                        "name": "Battista Biggio"
                    }
                ],
                "author_detail": {
                    "name": "Battista Biggio"
                },
                "author": "Battista Biggio"
            },
            {
                "id": "http://arxiv.org/abs/2511.10480v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10480v1",
                "title": "Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs"
                },
                "updated": "2025-11-13T16:44:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    44,
                    56,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10480v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T16:44:56Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    44,
                    56,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Changhai Man"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Srinivas Sridharan"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna"
            },
            {
                "id": "http://arxiv.org/abs/2510.17064v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.17064v3",
                "title": "A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation"
                },
                "updated": "2025-11-13T16:37:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    37,
                    14,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.17064v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.17064v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Single-cell RNA sequencing has transformed our ability to identify diverse cell types and their transcriptomic signatures. However, annotating these signatures-especially those involving poorly characterized genes-remains a major challenge. Traditional methods, such as Gene Set Enrichment Analysis (GSEA), depend on well-curated annotations and often perform poorly in these contexts. Large Language Models (LLMs) offer a promising alternative but struggle to represent complex biological knowledge within structured ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID: https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that integrates free-text descriptions with ontology labels to enable more accurate and robust gene set annotation. By incorporating retrieval-augmented generation (RAG), we developed a robust agentic workflow that refines predictions using relevant PubMed literature, reducing hallucinations and enhancing interpretability. Using this workflow, we achieved correct annotations for 77% of mouse gene sets among their top predictions. Applying this approach, we annotated 5,322 brain cell clusters from the comprehensive mouse brain cell atlas generated by the BRAIN Initiative Cell Census Network, enabling novel insights into brain cell function by identifying region-specific gene co-expression patterns and inferring functional roles of gene ensembles. BRAINCELL-AID also identifies Basal Ganglia-related cell types with neurologically meaningful descriptions. Hence, we create a valuable resource to support community-driven cell type annotation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-cell RNA sequencing has transformed our ability to identify diverse cell types and their transcriptomic signatures. However, annotating these signatures-especially those involving poorly characterized genes-remains a major challenge. Traditional methods, such as Gene Set Enrichment Analysis (GSEA), depend on well-curated annotations and often perform poorly in these contexts. Large Language Models (LLMs) offer a promising alternative but struggle to represent complex biological knowledge within structured ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID: https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that integrates free-text descriptions with ontology labels to enable more accurate and robust gene set annotation. By incorporating retrieval-augmented generation (RAG), we developed a robust agentic workflow that refines predictions using relevant PubMed literature, reducing hallucinations and enhancing interpretability. Using this workflow, we achieved correct annotations for 77% of mouse gene sets among their top predictions. Applying this approach, we annotated 5,322 brain cell clusters from the comprehensive mouse brain cell atlas generated by the BRAIN Initiative Cell Census Network, enabling novel insights into brain cell function by identifying region-specific gene co-expression patterns and inferring functional roles of gene ensembles. BRAINCELL-AID also identifies Basal Ganglia-related cell types with neurologically meaningful descriptions. Hence, we create a valuable resource to support community-driven cell type annotation."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-20T00:37:55Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    0,
                    37,
                    55,
                    0,
                    293,
                    0
                ],
                "arxiv_comment": "23 pages, 6 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Rongbin Li"
                    },
                    {
                        "name": "Wenbo Chen"
                    },
                    {
                        "name": "Zhao Li"
                    },
                    {
                        "name": "Rodrigo Munoz-Castaneda"
                    },
                    {
                        "name": "Jinbo Li"
                    },
                    {
                        "name": "Neha S. Maurya"
                    },
                    {
                        "name": "Arnav Solanki"
                    },
                    {
                        "name": "Huan He"
                    },
                    {
                        "name": "Hanwen Xing"
                    },
                    {
                        "name": "Meaghan Ramlakhan"
                    },
                    {
                        "name": "Zachary Wise"
                    },
                    {
                        "name": "Nelson Johansen"
                    },
                    {
                        "name": "Zhuhao Wu"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Michael Hawrylycz"
                    },
                    {
                        "name": "W. Jim Zheng"
                    }
                ],
                "author_detail": {
                    "name": "W. Jim Zheng"
                },
                "author": "W. Jim Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2511.10461v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10461v1",
                "title": "OpenSR-SRGAN: A Flexible Super-Resolution Framework for Multispectral Earth Observation Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenSR-SRGAN: A Flexible Super-Resolution Framework for Multispectral Earth Observation Data"
                },
                "updated": "2025-11-13T16:28:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    28,
                    35,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10461v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present OpenSR-SRGAN, an open and modular framework for single-image super-resolution in Earth Observation. The software provides a unified implementation of SRGAN-style models that is easy to configure, extend, and apply to multispectral satellite data such as Sentinel-2. Instead of requiring users to modify model code, OpenSR-SRGAN exposes generators, discriminators, loss functions, and training schedules through concise configuration files, making it straightforward to switch between architectures, scale factors, and band setups. The framework is designed as a practical tool and benchmark implementation rather than a state-of-the-art model. It ships with ready-to-use configurations for common remote sensing scenarios, sensible default settings for adversarial training, and built-in hooks for logging, validation, and large-scene inference. By turning GAN-based super-resolution into a configuration-driven workflow, OpenSR-SRGAN lowers the entry barrier for researchers and practitioners who wish to experiment with SRGANs, compare models in a reproducible way, and deploy super-resolution pipelines across diverse Earth-observation datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OpenSR-SRGAN, an open and modular framework for single-image super-resolution in Earth Observation. The software provides a unified implementation of SRGAN-style models that is easy to configure, extend, and apply to multispectral satellite data such as Sentinel-2. Instead of requiring users to modify model code, OpenSR-SRGAN exposes generators, discriminators, loss functions, and training schedules through concise configuration files, making it straightforward to switch between architectures, scale factors, and band setups. The framework is designed as a practical tool and benchmark implementation rather than a state-of-the-art model. It ships with ready-to-use configurations for common remote sensing scenarios, sensible default settings for adversarial training, and built-in hooks for logging, validation, and large-scene inference. By turning GAN-based super-resolution into a configuration-driven workflow, OpenSR-SRGAN lowers the entry barrier for researchers and practitioners who wish to experiment with SRGANs, compare models in a reproducible way, and deploy super-resolution pipelines across diverse Earth-observation datasets."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T16:28:35Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    28,
                    35,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Simon Donike"
                    },
                    {
                        "name": "Cesar Aybar"
                    },
                    {
                        "name": "Julio Contreras"
                    },
                    {
                        "name": "Luis Gómez-Chova"
                    }
                ],
                "author_detail": {
                    "name": "Luis Gómez-Chova"
                },
                "author": "Luis Gómez-Chova"
            },
            {
                "id": "http://arxiv.org/abs/2511.10459v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10459v1",
                "title": "LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning"
                },
                "updated": "2025-11-13T16:26:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    26,
                    13,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10459v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have been widely evaluated on macro-scale geographic tasks, such as global factual recall, event summarization, and regional reasoning. Yet, their ability to handle hyper-local knowledge remains poorly understood. This gap is increasingly consequential as real-world applications, from civic platforms to community journalism, demand AI systems that can reason about neighborhood-specific dynamics, cultural narratives, and local governance. Existing benchmarks fall short in capturing this complexity, often relying on coarse-grained data or isolated references. We present LocalBench, the first benchmark designed to systematically evaluate LLMs on county-level local knowledge across the United States. Grounded in the Localness Conceptual Framework, LocalBench includes 14,782 validated question-answer pairs across 526 U.S. counties in 49 states, integrating diverse sources such as Census statistics, local subreddit discourse, and regional news. It spans physical, cognitive, and relational dimensions of locality. Using LocalBench, we evaluate 13 state-of-the-art LLMs under both closed-book and web-augmented settings. Our findings reveal critical limitations: even the best-performing models reach only 56.8% accuracy on narrative-style questions and perform below 15.5% on numerical reasoning. Moreover, larger model size and web augmentation do not guarantee better performance, for example, search improves Gemini's accuracy by +13.6%, but reduces GPT-series performance by -11.4%. These results underscore the urgent need for language models that can support equitable, place-aware AI systems: capable of engaging with the diverse, fine-grained realities of local communities across geographic and cultural contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely evaluated on macro-scale geographic tasks, such as global factual recall, event summarization, and regional reasoning. Yet, their ability to handle hyper-local knowledge remains poorly understood. This gap is increasingly consequential as real-world applications, from civic platforms to community journalism, demand AI systems that can reason about neighborhood-specific dynamics, cultural narratives, and local governance. Existing benchmarks fall short in capturing this complexity, often relying on coarse-grained data or isolated references. We present LocalBench, the first benchmark designed to systematically evaluate LLMs on county-level local knowledge across the United States. Grounded in the Localness Conceptual Framework, LocalBench includes 14,782 validated question-answer pairs across 526 U.S. counties in 49 states, integrating diverse sources such as Census statistics, local subreddit discourse, and regional news. It spans physical, cognitive, and relational dimensions of locality. Using LocalBench, we evaluate 13 state-of-the-art LLMs under both closed-book and web-augmented settings. Our findings reveal critical limitations: even the best-performing models reach only 56.8% accuracy on narrative-style questions and perform below 15.5% on numerical reasoning. Moreover, larger model size and web augmentation do not guarantee better performance, for example, search improves Gemini's accuracy by +13.6%, but reduces GPT-series performance by -11.4%. These results underscore the urgent need for language models that can support equitable, place-aware AI systems: capable of engaging with the diverse, fine-grained realities of local communities across geographic and cultural contexts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T16:26:13Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    26,
                    13,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zihan Gao"
                    },
                    {
                        "name": "Yifei Xu"
                    },
                    {
                        "name": "Jacob Thebault-Spieker"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Thebault-Spieker"
                },
                "author": "Jacob Thebault-Spieker"
            },
            {
                "id": "http://arxiv.org/abs/2511.10457v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10457v1",
                "title": "Exploring State Tracking Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring State Tracking Capabilities of Large Language Models"
                },
                "updated": "2025-11-13T16:25:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    25,
                    32,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10457v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in solving complex tasks, including those requiring a certain level of reasoning. In this paper, we focus on state tracking, a problem where models need to keep track of the state governing a number of entities. To isolate the state tracking component from other factors, we propose a benchmark based on three well-defined state tracking tasks and analyse the performance of LLMs in different scenarios. The results indicate that the recent generation of LLMs (specifically, GPT-4 and Llama3) are capable of tracking state, especially when integrated with mechanisms such as Chain of Thought. However, models from the former generation, while understanding the task and being able to solve it at the initial stages, often fail at this task after a certain number of steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in solving complex tasks, including those requiring a certain level of reasoning. In this paper, we focus on state tracking, a problem where models need to keep track of the state governing a number of entities. To isolate the state tracking component from other factors, we propose a benchmark based on three well-defined state tracking tasks and analyse the performance of LLMs in different scenarios. The results indicate that the recent generation of LLMs (specifically, GPT-4 and Llama3) are capable of tracking state, especially when integrated with mechanisms such as Chain of Thought. However, models from the former generation, while understanding the task and being able to solve it at the initial stages, often fail at this task after a certain number of steps."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T16:25:32Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    25,
                    32,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kiamehr Rezaee"
                    },
                    {
                        "name": "Jose Camacho-Collados"
                    },
                    {
                        "name": "Mohammad Taher Pilehvar"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Taher Pilehvar"
                },
                "author": "Mohammad Taher Pilehvar"
            },
            {
                "id": "http://arxiv.org/abs/2511.10455v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10455v1",
                "title": "A scalable and accurate framework for self-calibrating null depth retrieval using neural posterior estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A scalable and accurate framework for self-calibrating null depth retrieval using neural posterior estimation"
                },
                "updated": "2025-11-13T16:20:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    20,
                    46,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10455v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate null depth retrieval is critical in nulling interferometry. However, achieving accurate null depth calibration is challenging due to various noise sources, instrumental imperfections, and the complexity of real observational environments. These challenges necessitate advanced calibration techniques that can efficiently handle such uncertainties while maintaining a high accuracy. This paper aims to incorporate machine-learning techniques with a Bayesian inference to improve the accuracy and efficiency of null depth retrieval in nulling interferometry. Specifically, it explores the use of neural posterior estimation (NPE) to develop models that overcome the computational limitations of conventional methods, such as numerical self-calibration (NSC), providing a more robust solution for accurate null depth calibration. An NPE-based model was developed, with a simulator that incorporates real data to better represent specific conditions. The model was tested on both synthetic and observational data from the LBTI nuller for evaluation. The NPE model successfully demonstrated improved efficiency, achieving results comparable to current methods in use. It achieved a null depth retrieval accuracy down to a few $10^{-4}$ on real observational data, matching the performance of conventional approaches while offering significant computational advantages, reducing the data retrieval time to one-quarter of the time required by self-calibration methods. The NPE model presents a practical and scalable solution for null depth calibration in nulling interferometry, offering substantial improvements in efficiency over existing methods with a better precision and application to other interferometric techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate null depth retrieval is critical in nulling interferometry. However, achieving accurate null depth calibration is challenging due to various noise sources, instrumental imperfections, and the complexity of real observational environments. These challenges necessitate advanced calibration techniques that can efficiently handle such uncertainties while maintaining a high accuracy. This paper aims to incorporate machine-learning techniques with a Bayesian inference to improve the accuracy and efficiency of null depth retrieval in nulling interferometry. Specifically, it explores the use of neural posterior estimation (NPE) to develop models that overcome the computational limitations of conventional methods, such as numerical self-calibration (NSC), providing a more robust solution for accurate null depth calibration. An NPE-based model was developed, with a simulator that incorporates real data to better represent specific conditions. The model was tested on both synthetic and observational data from the LBTI nuller for evaluation. The NPE model successfully demonstrated improved efficiency, achieving results comparable to current methods in use. It achieved a null depth retrieval accuracy down to a few $10^{-4}$ on real observational data, matching the performance of conventional approaches while offering significant computational advantages, reducing the data retrieval time to one-quarter of the time required by self-calibration methods. The NPE model presents a practical and scalable solution for null depth calibration in nulling interferometry, offering substantial improvements in efficiency over existing methods with a better precision and application to other interferometric techniques."
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T16:20:46Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    20,
                    46,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "9 pages, 7 Figures. accepted for publication in A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM"
                },
                "authors": [
                    {
                        "name": "Baoyi Zeng"
                    },
                    {
                        "name": "Marc-Antoine Martinod"
                    },
                    {
                        "name": "Denis Defrère"
                    }
                ],
                "author_detail": {
                    "name": "Denis Defrère"
                },
                "author": "Denis Defrère"
            },
            {
                "id": "http://arxiv.org/abs/2511.10452v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10452v1",
                "title": "Learning parameter-dependent shear viscosity from data, with application to sea and land ice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning parameter-dependent shear viscosity from data, with application to sea and land ice"
                },
                "updated": "2025-11-13T16:14:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    14,
                    48,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10452v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Complex physical systems which exhibit fluid-like behavior are often modeled as non-Newtonian fluids. A crucial element of a non-Newtonian model is the rheology, which relates inner stresses with strain-rates. We propose a framework for inferring rheological models from data that represents the fluid's effective viscosity with a neural network. By writing the rheological law in terms of tensor invariants and tailoring the network's properties, the inferred model satisfies key physical and mathematical properties, such as isotropic frame-indifference and existence of a convex potential of dissipation. Within this framework, we propose two approaches to learning a fluid's rheology: 1) a standard regression that fits the rheological model to stress data and 2) a PDE-constrained optimization method that infers rheological models from velocity data. For the latter approach, we combine finite element and machine learning libraries. We demonstrate the accuracy and robustness of our method on land and sea ice rheologies which also depend on external parameters. For land ice, we infer the temperature-dependent Glen's law and, for sea ice, the concentration-dependent shear component of the viscous-plastic model. For these two models, we explore the effects of large data errors. Finally, we infer an unknown concentration-dependent model that reproduces Lagrangian ice floe simulation data. Our method discovers a rheology that generalizes well outside of the training dataset and exhibits both shear-thickening and thinning behaviors depending on the concentrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex physical systems which exhibit fluid-like behavior are often modeled as non-Newtonian fluids. A crucial element of a non-Newtonian model is the rheology, which relates inner stresses with strain-rates. We propose a framework for inferring rheological models from data that represents the fluid's effective viscosity with a neural network. By writing the rheological law in terms of tensor invariants and tailoring the network's properties, the inferred model satisfies key physical and mathematical properties, such as isotropic frame-indifference and existence of a convex potential of dissipation. Within this framework, we propose two approaches to learning a fluid's rheology: 1) a standard regression that fits the rheological model to stress data and 2) a PDE-constrained optimization method that infers rheological models from velocity data. For the latter approach, we combine finite element and machine learning libraries. We demonstrate the accuracy and robustness of our method on land and sea ice rheologies which also depend on external parameters. For land ice, we infer the temperature-dependent Glen's law and, for sea ice, the concentration-dependent shear component of the viscous-plastic model. For these two models, we explore the effects of large data errors. Finally, we infer an unknown concentration-dependent model that reproduces Lagrangian ice floe simulation data. Our method discovers a rheology that generalizes well outside of the training dataset and exhibits both shear-thickening and thinning behaviors depending on the concentrations."
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T16:14:48Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    14,
                    48,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.NA"
                },
                "authors": [
                    {
                        "name": "Gonzalo G. de Diego"
                    },
                    {
                        "name": "Georg Stadler"
                    }
                ],
                "author_detail": {
                    "name": "Georg Stadler"
                },
                "author": "Georg Stadler"
            },
            {
                "id": "http://arxiv.org/abs/2509.06890v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.06890v2",
                "title": "Intraoperative 2D/3D Registration via Spherical Similarity Learning and Inference-Time Differentiable Levenberg-Marquardt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intraoperative 2D/3D Registration via Spherical Similarity Learning and Inference-Time Differentiable Levenberg-Marquardt Optimization"
                },
                "updated": "2025-11-13T16:14:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    14,
                    4,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.06890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.06890v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Intraoperative 2D/3D registration aligns preoperative 3D volumes with real-time 2D radiographs, enabling accurate localization of instruments and implants. A recent fully differentiable similarity learning framework approximates geodesic distances on SE(3), expanding the capture range of registration and mitigating the effects of substantial disturbances, but existing Euclidean approximations distort manifold structure and slow convergence. To address these limitations, we explore similarity learning in non-Euclidean spherical feature spaces to better capture and fit complex manifold structure. We extract feature embeddings using a CNN-Transformer encoder, project them into spherical space, and approximate their geodesic distances with Riemannian distances in the bi-invariant SO(4) space. This enables a more expressive and geometrically consistent deep similarity metric, enhancing the ability to distinguish subtle pose differences. During inference, we replace gradient descent with fully differentiable Levenberg-Marquardt optimization to accelerate convergence. Experiments on real and synthetic datasets show superior accuracy in both patient-specific and patient-agnostic scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intraoperative 2D/3D registration aligns preoperative 3D volumes with real-time 2D radiographs, enabling accurate localization of instruments and implants. A recent fully differentiable similarity learning framework approximates geodesic distances on SE(3), expanding the capture range of registration and mitigating the effects of substantial disturbances, but existing Euclidean approximations distort manifold structure and slow convergence. To address these limitations, we explore similarity learning in non-Euclidean spherical feature spaces to better capture and fit complex manifold structure. We extract feature embeddings using a CNN-Transformer encoder, project them into spherical space, and approximate their geodesic distances with Riemannian distances in the bi-invariant SO(4) space. This enables a more expressive and geometrically consistent deep similarity metric, enhancing the ability to distinguish subtle pose differences. During inference, we replace gradient descent with fully differentiable Levenberg-Marquardt optimization to accelerate convergence. Experiments on real and synthetic datasets show superior accuracy in both patient-specific and patient-agnostic scenarios."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-08T17:10:43Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    10,
                    43,
                    0,
                    251,
                    0
                ],
                "arxiv_comment": "WACV 2026 Accepted",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Minheng Chen"
                    },
                    {
                        "name": "Youyong Kong"
                    }
                ],
                "author_detail": {
                    "name": "Youyong Kong"
                },
                "author": "Youyong Kong"
            },
            {
                "id": "http://arxiv.org/abs/2511.07127v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07127v3",
                "title": "REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features in Clinical Prognostic Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features in Clinical Prognostic Tasks"
                },
                "updated": "2025-11-13T16:00:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    0,
                    18,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07127v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07127v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) and causal learning each hold strong potential for clinical decision making (CDM). However, their synergy remains poorly understood, largely due to the lack of systematic benchmarks evaluating their integration in clinical risk prediction. In real-world healthcare, identifying features with causal influence on outcomes is crucial for actionable and trustworthy predictions. While recent work highlights LLMs' emerging causal reasoning abilities, there lacks comprehensive benchmarks to assess their causal learning and performance informed by causal features in clinical risk prediction. To address this, we introduce REACT-LLM, a benchmark designed to evaluate whether combining LLMs with causal features can enhance clinical prognostic performance and potentially outperform traditional machine learning (ML) methods. Unlike existing LLM-clinical benchmarks that often focus on a limited set of outcomes, REACT-LLM evaluates 7 clinical outcomes across 2 real-world datasets, comparing 15 prominent LLMs, 6 traditional ML models, and 3 causal discovery (CD) algorithms. Our findings indicate that while LLMs perform reasonably in clinical prognostics, they have not yet outperformed traditional ML models. Integrating causal features derived from CD algorithms into LLMs offers limited performance gains, primarily due to the strict assumptions of many CD methods, which are often violated in complex clinical data. While the direct integration yields limited improvement, our benchmark reveals a more promising synergy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and causal learning each hold strong potential for clinical decision making (CDM). However, their synergy remains poorly understood, largely due to the lack of systematic benchmarks evaluating their integration in clinical risk prediction. In real-world healthcare, identifying features with causal influence on outcomes is crucial for actionable and trustworthy predictions. While recent work highlights LLMs' emerging causal reasoning abilities, there lacks comprehensive benchmarks to assess their causal learning and performance informed by causal features in clinical risk prediction. To address this, we introduce REACT-LLM, a benchmark designed to evaluate whether combining LLMs with causal features can enhance clinical prognostic performance and potentially outperform traditional machine learning (ML) methods. Unlike existing LLM-clinical benchmarks that often focus on a limited set of outcomes, REACT-LLM evaluates 7 clinical outcomes across 2 real-world datasets, comparing 15 prominent LLMs, 6 traditional ML models, and 3 causal discovery (CD) algorithms. Our findings indicate that while LLMs perform reasonably in clinical prognostics, they have not yet outperformed traditional ML models. Integrating causal features derived from CD algorithms into LLMs offers limited performance gains, primarily due to the strict assumptions of many CD methods, which are often violated in complex clinical data. While the direct integration yields limited improvement, our benchmark reveals a more promising synergy."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T14:12:35Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    14,
                    12,
                    35,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Linna Wang"
                    },
                    {
                        "name": "Zhixuan You"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Jiunan Wen"
                    },
                    {
                        "name": "Ji Shi"
                    },
                    {
                        "name": "Yimin Chen"
                    },
                    {
                        "name": "Yusen Wang"
                    },
                    {
                        "name": "Fanqi Ding"
                    },
                    {
                        "name": "Ziliang Feng"
                    },
                    {
                        "name": "Li Lu"
                    }
                ],
                "author_detail": {
                    "name": "Li Lu"
                },
                "author": "Li Lu"
            },
            {
                "id": "http://arxiv.org/abs/2510.03553v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.03553v2",
                "title": "CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making"
                },
                "updated": "2025-11-13T15:56:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    56,
                    59,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.03553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.03553v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although large language models (LLMs) are increasingly implicated in interpersonal and societal decision-making, their ability to navigate explicit conflicts between legitimately different cultural value systems remains largely unexamined. Existing benchmarks predominantly target cultural knowledge (CulturalBench), value prediction (WorldValuesBench), or single-axis bias diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple culturally grounded values directly clash. We address this gap with CCD-Bench, a benchmark that assesses LLM decision-making under cross-cultural value conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains, each paired with ten anonymized response options corresponding to the ten GLOBE cultural clusters. These dilemmas are presented using a stratified Latin square to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe (12.4 percent), while options for Eastern Europe and the Middle East and North Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of rationales reference multiple GLOBE dimensions, this pluralism is superficial: models recombine Future Orientation and Performance Orientation, and rarely ground choices in Assertiveness or Gender Egalitarianism (both under 3 percent). Ordering effects are negligible (Cramer's V less than 0.10), and symmetrized KL divergence shows clustering by developer lineage rather than geography. These patterns suggest that current alignment pipelines promote a consensus-oriented worldview that underserves scenarios demanding power negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts evaluation beyond isolated bias detection toward pluralistic decision making and highlights the need for alignment strategies that substantively engage diverse worldviews.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) are increasingly implicated in interpersonal and societal decision-making, their ability to navigate explicit conflicts between legitimately different cultural value systems remains largely unexamined. Existing benchmarks predominantly target cultural knowledge (CulturalBench), value prediction (WorldValuesBench), or single-axis bias diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple culturally grounded values directly clash. We address this gap with CCD-Bench, a benchmark that assesses LLM decision-making under cross-cultural value conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains, each paired with ten anonymized response options corresponding to the ten GLOBE cultural clusters. These dilemmas are presented using a stratified Latin square to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe (12.4 percent), while options for Eastern Europe and the Middle East and North Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of rationales reference multiple GLOBE dimensions, this pluralism is superficial: models recombine Future Orientation and Performance Orientation, and rarely ground choices in Assertiveness or Gender Egalitarianism (both under 3 percent). Ordering effects are negligible (Cramer's V less than 0.10), and symmetrized KL divergence shows clustering by developer lineage rather than geography. These patterns suggest that current alignment pipelines promote a consensus-oriented worldview that underserves scenarios demanding power negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts evaluation beyond isolated bias detection toward pluralistic decision making and highlights the need for alignment strategies that substantively engage diverse worldviews."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-03T22:55:37Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    22,
                    55,
                    37,
                    4,
                    276,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hasibur Rahman"
                    },
                    {
                        "name": "Hanan Salam"
                    }
                ],
                "author_detail": {
                    "name": "Hanan Salam"
                },
                "author": "Hanan Salam"
            },
            {
                "id": "http://arxiv.org/abs/2511.04427v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04427v2",
                "title": "Does AI-Assisted Coding Deliver? A Difference-in-Differences Study of Cursor's Impact on Software Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does AI-Assisted Coding Deliver? A Difference-in-Differences Study of Cursor's Impact on Software Projects"
                },
                "updated": "2025-11-13T15:51:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    51,
                    45,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04427v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated the promise to revolutionize the field of software engineering. Among other things, LLM agents are rapidly gaining momentum in their application to software development, with practitioners claiming a multifold productivity increase after adoption. Yet, empirical evidence is lacking around these claims. In this paper, we estimate the causal effect of adopting a widely popular LLM agent assistant, namely Cursor, on development velocity and software quality. The estimation is enabled by a state-of-the-art difference-in-differences design comparing Cursor-adopting GitHub projects with a matched control group of similar GitHub projects that do not use Cursor. We find that the adoption of Cursor leads to a significant, large, but transient increase in project-level development velocity, along with a significant and persistent increase in static analysis warnings and code complexity. Further panel generalized method of moments estimation reveals that the increase in static analysis warnings and code complexity acts as a major factor causing long-term velocity slowdown. Our study carries implications for software engineering practitioners, LLM agent assistant designers, and researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the promise to revolutionize the field of software engineering. Among other things, LLM agents are rapidly gaining momentum in their application to software development, with practitioners claiming a multifold productivity increase after adoption. Yet, empirical evidence is lacking around these claims. In this paper, we estimate the causal effect of adopting a widely popular LLM agent assistant, namely Cursor, on development velocity and software quality. The estimation is enabled by a state-of-the-art difference-in-differences design comparing Cursor-adopting GitHub projects with a matched control group of similar GitHub projects that do not use Cursor. We find that the adoption of Cursor leads to a significant, large, but transient increase in project-level development velocity, along with a significant and persistent increase in static analysis warnings and code complexity. Further panel generalized method of moments estimation reveals that the increase in static analysis warnings and code complexity acts as a major factor causing long-term velocity slowdown. Our study carries implications for software engineering practitioners, LLM agent assistant designers, and researchers."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T15:00:51Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    0,
                    51,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Courtney Miller"
                    },
                    {
                        "name": "Shyam Agarwal"
                    },
                    {
                        "name": "Christian Kästner"
                    },
                    {
                        "name": "Bogdan Vasilescu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Vasilescu"
                },
                "author": "Bogdan Vasilescu"
            },
            {
                "id": "http://arxiv.org/abs/2508.02635v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.02635v2",
                "title": "Test Set Quality in Multilingual LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Set Quality in Multilingual LLM Evaluation"
                },
                "updated": "2025-11-13T15:40:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    40,
                    24,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.02635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.02635v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Several multilingual benchmark datasets have been developed in a semi-automatic manner in the recent past to measure progress and understand the state-of-the-art in the multilingual capabilities of Large Language Models. However, there is not a lot of attention paid to the quality of the datasets themselves, despite the existence of previous work in identifying errors in even fully human-annotated test sets. In this paper, we manually analyze recent multilingual evaluation sets in two languages - French and Telugu, identifying several errors in the process. We compare the performance difference across several LLMs with the original and revised versions of the datasets and identify large differences (almost 10% in some cases) in both languages). Based on these results, we argue that test sets should not be considered immutable and should be revisited, checked for correctness, and potentially versioned. We end with some recommendations for both the dataset creators as well as consumers on addressing the dataset quality issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several multilingual benchmark datasets have been developed in a semi-automatic manner in the recent past to measure progress and understand the state-of-the-art in the multilingual capabilities of Large Language Models. However, there is not a lot of attention paid to the quality of the datasets themselves, despite the existence of previous work in identifying errors in even fully human-annotated test sets. In this paper, we manually analyze recent multilingual evaluation sets in two languages - French and Telugu, identifying several errors in the process. We compare the performance difference across several LLMs with the original and revised versions of the datasets and identify large differences (almost 10% in some cases) in both languages). Based on these results, we argue that test sets should not be considered immutable and should be revisited, checked for correctness, and potentially versioned. We end with some recommendations for both the dataset creators as well as consumers on addressing the dataset quality issues."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-04T17:22:08Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    22,
                    8,
                    0,
                    216,
                    0
                ],
                "arxiv_comment": "to appear in the proceedings of Eval4NLP workshop at AACL 2025. Camera ready version",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chalamalasetti Kranti"
                    },
                    {
                        "name": "Gabriel Bernier-Colborne"
                    },
                    {
                        "name": "Yvan Gauthier"
                    },
                    {
                        "name": "Sowmya Vajjala"
                    }
                ],
                "author_detail": {
                    "name": "Sowmya Vajjala"
                },
                "author": "Sowmya Vajjala"
            },
            {
                "id": "http://arxiv.org/abs/2511.10418v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10418v1",
                "title": "CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models"
                },
                "updated": "2025-11-13T15:40:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    40,
                    17,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10418v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:40:17Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    40,
                    17,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Yaqiao Zhu"
                    },
                    {
                        "name": "Hongkai Wen"
                    },
                    {
                        "name": "Mark Birkin"
                    },
                    {
                        "name": "Man Luo"
                    }
                ],
                "author_detail": {
                    "name": "Man Luo"
                },
                "author": "Man Luo"
            },
            {
                "id": "http://arxiv.org/abs/2511.10416v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10416v1",
                "title": "Generalizing Analogical Inference from Boolean to Continuous Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing Analogical Inference from Boolean to Continuous Domains"
                },
                "updated": "2025-11-13T15:37:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    37,
                    18,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10416v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Analogical reasoning is a powerful inductive mechanism, widely used in human cognition and increasingly applied in artificial intelligence. Formal frameworks for analogical inference have been developed for Boolean domains, where inference is provably sound for affine functions and approximately correct for functions close to affine. These results have informed the design of analogy-based classifiers. However, they do not extend to regression tasks or continuous domains. In this paper, we revisit analogical inference from a foundational perspective. We first present a counterexample showing that existing generalization bounds fail even in the Boolean setting. We then introduce a unified framework for analogical reasoning in real-valued domains based on parameterized analogies defined via generalized means. This model subsumes both Boolean classification and regression, and supports analogical inference over continuous functions. We characterize the class of analogy-preserving functions in this setting and derive both worst-case and average-case error bounds under smoothness assumptions. Our results offer a general theory of analogical inference across discrete and continuous domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analogical reasoning is a powerful inductive mechanism, widely used in human cognition and increasingly applied in artificial intelligence. Formal frameworks for analogical inference have been developed for Boolean domains, where inference is provably sound for affine functions and approximately correct for functions close to affine. These results have informed the design of analogy-based classifiers. However, they do not extend to regression tasks or continuous domains. In this paper, we revisit analogical inference from a foundational perspective. We first present a counterexample showing that existing generalization bounds fail even in the Boolean setting. We then introduce a unified framework for analogical reasoning in real-valued domains based on parameterized analogies defined via generalized means. This model subsumes both Boolean classification and regression, and supports analogical inference over continuous functions. We characterize the class of analogy-preserving functions in this setting and derive both worst-case and average-case error bounds under smoothness assumptions. Our results offer a general theory of analogical inference across discrete and continuous domains."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:37:18Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    37,
                    18,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "11 pages, to appear in AAAI 2026, extended version",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Francisco Cunha"
                    },
                    {
                        "name": "Yves Lepage"
                    },
                    {
                        "name": "Zied Bouraoui"
                    },
                    {
                        "name": "Miguel Couceiro"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Couceiro"
                },
                "author": "Miguel Couceiro"
            },
            {
                "id": "http://arxiv.org/abs/2412.12837v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.12837v4",
                "title": "Exposing the Vulnerability of Decentralized Learning to Membership Inference Attacks Through the Lens of Graph Mixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing the Vulnerability of Decentralized Learning to Membership Inference Attacks Through the Lens of Graph Mixing"
                },
                "updated": "2025-11-13T15:25:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    25,
                    12,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.12837v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.12837v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The primary promise of decentralized learning is to allow users to engage in the training of machine learning models in a collaborative manner while keeping their data on their premises and without relying on any central entity. However, this paradigm necessitates the exchange of model parameters or gradients between peers. Such exchanges can be exploited to infer sensitive information about training data, which is achieved through privacy attacks (e.g., Membership Inference Attacks -- MIA). In order to devise effective defense mechanisms, it is important to understand the factors that increase/reduce the vulnerability of a given decentralized learning architecture to MIA. In this study, we extensively explore the vulnerability to MIA of various decentralized learning architectures by varying the graph structure (e.g., number of neighbors), the graph dynamics, and the aggregation strategy, across diverse datasets and data distributions. Our key finding, which to the best of our knowledge we are the first to report, is that the vulnerability to MIA is heavily correlated to (i) the local model mixing strategy performed by each node upon reception of models from neighboring nodes and (ii) the global mixing properties of the communication graph. We illustrate these results experimentally using four datasets and by theoretically analyzing the mixing properties of various decentralized architectures. We also empirically show that enhancing mixing properties is highly beneficial when combined with other privacy-preserving techniques such as Differential Privacy. Our paper draws a set of lessons learned for devising decentralized learning systems that reduce by design the vulnerability to MIA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The primary promise of decentralized learning is to allow users to engage in the training of machine learning models in a collaborative manner while keeping their data on their premises and without relying on any central entity. However, this paradigm necessitates the exchange of model parameters or gradients between peers. Such exchanges can be exploited to infer sensitive information about training data, which is achieved through privacy attacks (e.g., Membership Inference Attacks -- MIA). In order to devise effective defense mechanisms, it is important to understand the factors that increase/reduce the vulnerability of a given decentralized learning architecture to MIA. In this study, we extensively explore the vulnerability to MIA of various decentralized learning architectures by varying the graph structure (e.g., number of neighbors), the graph dynamics, and the aggregation strategy, across diverse datasets and data distributions. Our key finding, which to the best of our knowledge we are the first to report, is that the vulnerability to MIA is heavily correlated to (i) the local model mixing strategy performed by each node upon reception of models from neighboring nodes and (ii) the global mixing properties of the communication graph. We illustrate these results experimentally using four datasets and by theoretically analyzing the mixing properties of various decentralized architectures. We also empirically show that enhancing mixing properties is highly beneficial when combined with other privacy-preserving techniques such as Differential Privacy. Our paper draws a set of lessons learned for devising decentralized learning systems that reduce by design the vulnerability to MIA."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-17T12:02:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    2,
                    47,
                    1,
                    352,
                    0
                ],
                "arxiv_comment": "Accepted at Middleware'25, 13 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ousmane Touat"
                    },
                    {
                        "name": "Jezekael Brunon"
                    },
                    {
                        "name": "Yacine Belal"
                    },
                    {
                        "name": "Julien Nicolas"
                    },
                    {
                        "name": "César Sabater"
                    },
                    {
                        "name": "Mohamed Maouche"
                    },
                    {
                        "name": "Sonia Ben Mokhtar"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Ben Mokhtar"
                },
                "author": "Sonia Ben Mokhtar"
            },
            {
                "id": "http://arxiv.org/abs/2511.10400v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10400v1",
                "title": "Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance"
                },
                "updated": "2025-11-13T15:20:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    20,
                    12,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10400v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ensuring the reliability of agent architectures and effectively identifying problematic agents when failures occur are crucial challenges in multi-agent systems (MAS). Advances in large language models (LLMs) have established LLM-based agents as a major branch of MAS, enabling major breakthroughs in complex problem solving and world modeling. However, the reliability implications of this shift remain largely unexplored. i.e., whether substituting traditional agents with LLM-based agents can effectively enhance the reliability of MAS. In this work, we investigate and quantify the reliability of LLM-based agents from the perspective of Byzantine fault tolerance. We observe that LLM-based agents demonstrate stronger skepticism when processing erroneous message flows, a characteristic that enables them to outperform traditional agents across different topological structures. Motivated by the results of the pilot experiment, we design CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism to enhance the stability of MAS with different topologies. It capitalizes on the intrinsic reflective and discriminative capabilities of LLMs by employing a probe-based, weighted information flow transmission method to improve the reliability of LLM-based agents. Extensive experiments demonstrate that CP-WBFT achieves superior performance across diverse network topologies under extreme Byzantine conditions (85.7\\% fault rate). Notably, our approach surpasses traditional methods by attaining remarkable accuracy on various topologies and maintaining strong reliability in both mathematical reasoning and safety assessment tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the reliability of agent architectures and effectively identifying problematic agents when failures occur are crucial challenges in multi-agent systems (MAS). Advances in large language models (LLMs) have established LLM-based agents as a major branch of MAS, enabling major breakthroughs in complex problem solving and world modeling. However, the reliability implications of this shift remain largely unexplored. i.e., whether substituting traditional agents with LLM-based agents can effectively enhance the reliability of MAS. In this work, we investigate and quantify the reliability of LLM-based agents from the perspective of Byzantine fault tolerance. We observe that LLM-based agents demonstrate stronger skepticism when processing erroneous message flows, a characteristic that enables them to outperform traditional agents across different topological structures. Motivated by the results of the pilot experiment, we design CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism to enhance the stability of MAS with different topologies. It capitalizes on the intrinsic reflective and discriminative capabilities of LLMs by employing a probe-based, weighted information flow transmission method to improve the reliability of LLM-based agents. Extensive experiments demonstrate that CP-WBFT achieves superior performance across diverse network topologies under extreme Byzantine conditions (85.7\\% fault rate). Notably, our approach surpasses traditional methods by attaining remarkable accuracy on various topologies and maintaining strong reliability in both mathematical reasoning and safety assessment tasks."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:20:12Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    20,
                    12,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Lifan Zheng"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Qinghong Yin"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Xinyi Zeng"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian"
            },
            {
                "id": "http://arxiv.org/abs/2511.06625v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06625v2",
                "title": "Explainable Cross-Disease Reasoning for Cardiovascular Risk Assessment from LDCT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Cross-Disease Reasoning for Cardiovascular Risk Assessment from LDCT"
                },
                "updated": "2025-11-13T15:16:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    16,
                    3,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06625v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Low-dose chest computed tomography (LDCT) inherently captures both pulmonary and cardiac structures, offering a unique opportunity for joint assessment of lung and cardiovascular health. However, most existing approaches treat these domains as independent tasks, overlooking their physiological interplay and shared imaging biomarkers. We propose an Explainable Cross-Disease Reasoning Framework that enables interpretable cardiopulmonary risk assessment from a single LDCT scan. The framework introduces an agentic reasoning process that emulates clinical diagnostic thinking-first perceiving pulmonary findings, then reasoning through established medical knowledge, and finally deriving a cardiovascular judgment with explanatory rationale. It integrates three synergistic components: a pulmonary perception module that summarizes lung abnormalities, a knowledge-guided reasoning module that infers their cardiovascular implications, and a cardiac representation module that encodes structural biomarkers. Their outputs are fused to produce a holistic cardiovascular risk prediction that is both accurate and physiologically grounded. Experiments on the NLST cohort demonstrate that the proposed framework achieves state-of-the-art performance for CVD screening and mortality prediction, outperforming single-disease and purely image-based baselines. Beyond quantitative gains, the framework provides human-verifiable reasoning that aligns with cardiological understanding, revealing coherent links between pulmonary abnormalities and cardiac stress mechanisms. Overall, this work establishes a unified and explainable paradigm for cardiovascular analysis from LDCT, bridging the gap between image-based prediction and mechanism-based medical interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-dose chest computed tomography (LDCT) inherently captures both pulmonary and cardiac structures, offering a unique opportunity for joint assessment of lung and cardiovascular health. However, most existing approaches treat these domains as independent tasks, overlooking their physiological interplay and shared imaging biomarkers. We propose an Explainable Cross-Disease Reasoning Framework that enables interpretable cardiopulmonary risk assessment from a single LDCT scan. The framework introduces an agentic reasoning process that emulates clinical diagnostic thinking-first perceiving pulmonary findings, then reasoning through established medical knowledge, and finally deriving a cardiovascular judgment with explanatory rationale. It integrates three synergistic components: a pulmonary perception module that summarizes lung abnormalities, a knowledge-guided reasoning module that infers their cardiovascular implications, and a cardiac representation module that encodes structural biomarkers. Their outputs are fused to produce a holistic cardiovascular risk prediction that is both accurate and physiologically grounded. Experiments on the NLST cohort demonstrate that the proposed framework achieves state-of-the-art performance for CVD screening and mortality prediction, outperforming single-disease and purely image-based baselines. Beyond quantitative gains, the framework provides human-verifiable reasoning that aligns with cardiological understanding, revealing coherent links between pulmonary abnormalities and cardiac stress mechanisms. Overall, this work establishes a unified and explainable paradigm for cardiovascular analysis from LDCT, bridging the gap between image-based prediction and mechanism-based medical interpretation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T02:04:46Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    2,
                    4,
                    46,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Jiashuo Zhang"
                    },
                    {
                        "name": "Mojtaba Safari"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Liang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Liang Zhao"
                },
                "author": "Liang Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2511.10395v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10395v1",
                "title": "AgentEvolver: Towards Efficient Self-Evolving Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentEvolver: Towards Efficient Self-Evolving Agent System"
                },
                "updated": "2025-11-13T15:14:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    14,
                    47,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10395v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:14:47Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    14,
                    47,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yunpeng Zhai"
                    },
                    {
                        "name": "Shuchang Tao"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Anni Zou"
                    },
                    {
                        "name": "Ziqian Chen"
                    },
                    {
                        "name": "Qingxu Fu"
                    },
                    {
                        "name": "Shinji Mai"
                    },
                    {
                        "name": "Li Yu"
                    },
                    {
                        "name": "Jiaji Deng"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2511.10394v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10394v1",
                "title": "LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components"
                },
                "updated": "2025-11-13T15:14:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    14,
                    34,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10394v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The health condition of wind turbine (WT) components is crucial for ensuring stable and reliable operation. However, existing fault detection methods are largely limited to visual recognition, producing structured outputs that lack semantic interpretability and fail to support maintenance decision-making. To address these limitations, this study proposes an integrated framework that combines YOLOMS with a large language model (LLM) for intelligent fault analysis and diagnosis. Specifically, YOLOMS employs multi-scale detection and sliding-window cropping to enhance fault feature extraction, while a lightweight key-value (KV) mapping module bridges the gap between visual outputs and textual inputs. This module converts YOLOMS detection results into structured textual representations enriched with both qualitative and quantitative attributes. A domain-tuned LLM then performs semantic reasoning to generate interpretable fault analyses and maintenance recommendations. Experiments on real-world datasets demonstrate that the proposed framework achieves a fault detection accuracy of 90.6\\% and generates maintenance reports with an average accuracy of 89\\%, thereby improving the interpretability of diagnostic results and providing practical decision support for the operation and maintenance of wind turbines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The health condition of wind turbine (WT) components is crucial for ensuring stable and reliable operation. However, existing fault detection methods are largely limited to visual recognition, producing structured outputs that lack semantic interpretability and fail to support maintenance decision-making. To address these limitations, this study proposes an integrated framework that combines YOLOMS with a large language model (LLM) for intelligent fault analysis and diagnosis. Specifically, YOLOMS employs multi-scale detection and sliding-window cropping to enhance fault feature extraction, while a lightweight key-value (KV) mapping module bridges the gap between visual outputs and textual inputs. This module converts YOLOMS detection results into structured textual representations enriched with both qualitative and quantitative attributes. A domain-tuned LLM then performs semantic reasoning to generate interpretable fault analyses and maintenance recommendations. Experiments on real-world datasets demonstrate that the proposed framework achieves a fault detection accuracy of 90.6\\% and generates maintenance reports with an average accuracy of 89\\%, thereby improving the interpretability of diagnostic results and providing practical decision support for the operation and maintenance of wind turbines."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:14:34Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    14,
                    34,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Journal resubmission",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yaru Li"
                    },
                    {
                        "name": "Yanxue Wang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Xinming Li"
                    },
                    {
                        "name": "Jianbo Feng"
                    }
                ],
                "author_detail": {
                    "name": "Jianbo Feng"
                },
                "author": "Jianbo Feng"
            },
            {
                "id": "http://arxiv.org/abs/2511.10387v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10387v1",
                "title": "Physics informed Transformer-VAE for biophysical parameter estimation: PROSAIL model inversion in Sentinel-2 imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics informed Transformer-VAE for biophysical parameter estimation: PROSAIL model inversion in Sentinel-2 imagery"
                },
                "updated": "2025-11-13T15:08:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    8,
                    33,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10387v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate retrieval of vegetation biophysical variables from satellite imagery is crucial for ecosystem monitoring and agricultural management. In this work, we propose a physics-informed Transformer-VAE architecture to invert the PROSAIL radiative transfer model for simultaneous estimation of key canopy parameters from Sentinel-2 data. Unlike previous hybrid approaches that require real satellite images for self-supevised training. Our model is trained exclusively on simulated data, yet achieves performance on par with state-of-the-art methods that utilize real imagery. The Transformer-VAE incorporates the PROSAIL model as a differentiable physical decoder, ensuring that inferred latent variables correspond to physically plausible leaf and canopy properties. We demonstrate retrieval of leaf area index (LAI) and canopy chlorophyll content (CCC) on real-world field datasets (FRM4Veg and BelSAR) with accuracy comparable to models trained with real Sentinel-2 data. Our method requires no in-situ labels or calibration on real images, offering a cost-effective and self-supervised solution for global vegetation monitoring. The proposed approach illustrates how integrating physical models with advanced deep networks can improve the inversion of RTMs, opening new prospects for large-scale, physically-constrained remote sensing of vegetation traits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate retrieval of vegetation biophysical variables from satellite imagery is crucial for ecosystem monitoring and agricultural management. In this work, we propose a physics-informed Transformer-VAE architecture to invert the PROSAIL radiative transfer model for simultaneous estimation of key canopy parameters from Sentinel-2 data. Unlike previous hybrid approaches that require real satellite images for self-supevised training. Our model is trained exclusively on simulated data, yet achieves performance on par with state-of-the-art methods that utilize real imagery. The Transformer-VAE incorporates the PROSAIL model as a differentiable physical decoder, ensuring that inferred latent variables correspond to physically plausible leaf and canopy properties. We demonstrate retrieval of leaf area index (LAI) and canopy chlorophyll content (CCC) on real-world field datasets (FRM4Veg and BelSAR) with accuracy comparable to models trained with real Sentinel-2 data. Our method requires no in-situ labels or calibration on real images, offering a cost-effective and self-supervised solution for global vegetation monitoring. The proposed approach illustrates how integrating physical models with advanced deep networks can improve the inversion of RTMs, opening new prospects for large-scale, physically-constrained remote sensing of vegetation traits."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:08:33Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    8,
                    33,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "10 pages, 6 figures, uses fancyhdr.sty",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Prince Mensah"
                    },
                    {
                        "name": "Pelumi Victor Aderinto"
                    },
                    {
                        "name": "Ibrahim Salihu Yusuf"
                    },
                    {
                        "name": "Arnu Pretorius"
                    }
                ],
                "author_detail": {
                    "name": "Arnu Pretorius"
                },
                "author": "Arnu Pretorius"
            },
            {
                "id": "http://arxiv.org/abs/2510.22350v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.22350v2",
                "title": "Tensor gravity gradiometry with a single-axis atom gradiometer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor gravity gradiometry with a single-axis atom gradiometer"
                },
                "updated": "2025-11-13T15:07:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    7,
                    32,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.22350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.22350v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1103/vqjd-8qwh",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "We propose a method for using a single-axis atom interferometric gravity gradiometer to measure off-diagonal elements of the gravity gradient tensor. By tilting the gradiometer, the measured gradient becomes a linear combination of different components of the gravity gradient tensor, and through multiple measurements at different tilts the separate tensor components can be inferred. We present a theoretical and numerical investigation of this technique, both for terrestrial surveys where the tilt is statically set by the user and for surveys where a strapdown sensor is dynamically tilted by the motion of the platform. We show that the gradiometer's sensitivity to the vertical gravity gradient is only slightly reduced by this method while allowing for more gradiometer information to be obtained. Major sources of error and loss of sensitivity on dynamic platforms are shown to be mitigated using an optical-gimbal technique employing commercially-available fibre-optic gyroscopes and tip-tilt mirrors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method for using a single-axis atom interferometric gravity gradiometer to measure off-diagonal elements of the gravity gradient tensor. By tilting the gradiometer, the measured gradient becomes a linear combination of different components of the gravity gradient tensor, and through multiple measurements at different tilts the separate tensor components can be inferred. We present a theoretical and numerical investigation of this technique, both for terrestrial surveys where the tilt is statically set by the user and for surveys where a strapdown sensor is dynamically tilted by the motion of the platform. We show that the gradiometer's sensitivity to the vertical gravity gradient is only slightly reduced by this method while allowing for more gradiometer information to be obtained. Major sources of error and loss of sensitivity on dynamic platforms are shown to be mitigated using an optical-gimbal technique employing commercially-available fibre-optic gyroscopes and tip-tilt mirrors."
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-25T16:21:08Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    16,
                    21,
                    8,
                    5,
                    298,
                    0
                ],
                "arxiv_comment": "10 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph"
                },
                "arxiv_journal_ref": "Physical Review A 112, 053314 (2025)",
                "authors": [
                    {
                        "name": "Ryan J. Thomas"
                    },
                    {
                        "name": "Samuel Legge"
                    },
                    {
                        "name": "John D. Close"
                    }
                ],
                "author_detail": {
                    "name": "John D. Close"
                },
                "author": "John D. Close",
                "arxiv_doi": "10.1103/vqjd-8qwh"
            },
            {
                "id": "http://arxiv.org/abs/2408.11062v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.11062v2",
                "title": "Multi-Turn Interactions for Text-to-SQL with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Turn Interactions for Text-to-SQL with Large Language Models"
                },
                "updated": "2025-11-13T15:01:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    1,
                    48,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.11062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.11062v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3746252.3761052",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This study explores text-to-SQL parsing by leveraging the powerful reasoning capabilities of large language models (LLMs). Despite recent advancements, existing LLM-based methods are still inefficient and struggle to handle cases with wide tables effectively. Furthermore, current interaction-based approaches either lack a step-by-step, interpretable SQL generation process or fail to provide a universally applicable interaction design. To address these challenges, we introduce Interactive-T2S, a framework that generates SQL queries through direct interactions with databases. This framework includes four general tools that facilitate proactive and efficient information retrieval by the LLM. Additionally, we have developed detailed exemplars to demonstrate the step-wise reasoning processes within our framework. Our approach achieves advanced performance on the Spider and BIRD datasets as well as their variants. Notably, we obtain state-of-the-art results on the BIRD leaderboard under the setting without oracle knowledge, demonstrating the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores text-to-SQL parsing by leveraging the powerful reasoning capabilities of large language models (LLMs). Despite recent advancements, existing LLM-based methods are still inefficient and struggle to handle cases with wide tables effectively. Furthermore, current interaction-based approaches either lack a step-by-step, interpretable SQL generation process or fail to provide a universally applicable interaction design. To address these challenges, we introduce Interactive-T2S, a framework that generates SQL queries through direct interactions with databases. This framework includes four general tools that facilitate proactive and efficient information retrieval by the LLM. Additionally, we have developed detailed exemplars to demonstrate the step-wise reasoning processes within our framework. Our approach achieves advanced performance on the Spider and BIRD datasets as well as their variants. Notably, we obtain state-of-the-art results on the BIRD leaderboard under the setting without oracle knowledge, demonstrating the effectiveness of our method."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-09T07:43:21Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    7,
                    43,
                    21,
                    4,
                    222,
                    0
                ],
                "arxiv_comment": "This work has been accepted to CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Guanming Xiong"
                    },
                    {
                        "name": "Junwei Bao"
                    },
                    {
                        "name": "Hongfei Jiang"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Wen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Zhao"
                },
                "author": "Wen Zhao",
                "arxiv_doi": "10.1145/3746252.3761052"
            },
            {
                "id": "http://arxiv.org/abs/2511.10384v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10384v1",
                "title": "Simulating Misinformation Propagation in Social Networks using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Misinformation Propagation in Social Networks using Large Language Models"
                },
                "updated": "2025-11-13T15:01:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    1,
                    19,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10384v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. News articles are propagated across networks of persona-conditioned LLM nodes, each rewriting received content. A question--answering-based auditor then measures factual fidelity at every step, offering interpretable, claim-level tracking of misinformation drift. We formalize a misinformation index and a misinformation propagation rate to quantify factual degradation across homogeneous and heterogeneous branches of up to 30 sequential rewrites. Experiments with 21 personas across 10 domains reveal that identity- and ideology-based personas act as misinformation accelerators, especially in politics, marketing, and technology. By contrast, expert-driven personas preserve factual stability. Controlled-random branch simulations further show that once early distortions emerge, heterogeneous persona interactions rapidly escalate misinformation to propaganda-level distortion. Our taxonomy of misinformation severity -- spanning factual errors, lies, and propaganda -- connects observed drift to established theories in misinformation studies. These findings demonstrate the dual role of LLMs as both proxies for human-like biases and as auditors capable of tracing information fidelity. The proposed framework provides an interpretable, empirically grounded approach for studying, simulating, and mitigating misinformation diffusion in digital ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. News articles are propagated across networks of persona-conditioned LLM nodes, each rewriting received content. A question--answering-based auditor then measures factual fidelity at every step, offering interpretable, claim-level tracking of misinformation drift. We formalize a misinformation index and a misinformation propagation rate to quantify factual degradation across homogeneous and heterogeneous branches of up to 30 sequential rewrites. Experiments with 21 personas across 10 domains reveal that identity- and ideology-based personas act as misinformation accelerators, especially in politics, marketing, and technology. By contrast, expert-driven personas preserve factual stability. Controlled-random branch simulations further show that once early distortions emerge, heterogeneous persona interactions rapidly escalate misinformation to propaganda-level distortion. Our taxonomy of misinformation severity -- spanning factual errors, lies, and propaganda -- connects observed drift to established theories in misinformation studies. These findings demonstrate the dual role of LLMs as both proxies for human-like biases and as auditors capable of tracing information fidelity. The proposed framework provides an interpretable, empirically grounded approach for studying, simulating, and mitigating misinformation diffusion in digital ecosystems."
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:01:19Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    1,
                    19,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted to CIKM 2025 Workshop LASS",
                "arxiv_primary_category": {
                    "term": "cs.SI"
                },
                "authors": [
                    {
                        "name": "Raj Gaurav Maurya"
                    },
                    {
                        "name": "Vaibhav Shukla"
                    },
                    {
                        "name": "Raj Abhijit Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat"
            },
            {
                "id": "http://arxiv.org/abs/2504.09971v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.09971v4",
                "title": "Proofs of Useful Work from Arbitrary Matrix Multiplication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proofs of Useful Work from Arbitrary Matrix Multiplication"
                },
                "updated": "2025-11-13T14:58:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    58,
                    36,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.09971v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.09971v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We revisit the longstanding open problem of implementing Nakamoto's proof-of-work (PoW) consensus based on a real-world computational task $T(x)$ (as opposed to artificial random hashing), in a truly permissionless setting where the miner itself chooses the input $x$. The challenge in designing such a Proof-of-Useful-Work (PoUW) protocol, is using the native computation of $T(x)$ to produce a PoW certificate with prescribed hardness and with negligible computational overhead over the worst-case complexity of $T(\\cdot)$ -- This ensures malicious miners cannot ``game the system\" by fooling the verifier to accept with higher probability compared to honest miners (while using similar computational resources). Indeed, obtaining a PoUW with $O(1)$-factor overhead is trivial for any task $T$, but also useless.\n  Our main result is a PoUW for the task of Matrix Multiplication $MatMul(A,B)$ of arbitrary matrices with $1+o(1)$ multiplicative overhead compared to naive $MatMul$ (even in the presence of Fast Matrix Multiplication-style algorithms, which are currently impractical). We conjecture that our protocol has optimal security in the sense that a malicious prover cannot obtain any significant advantage over an honest prover. This conjecture is based on reducing hardness of our protocol to the task of solving a batch of low-rank random linear equations which is of independent interest.\n  Since $MatMul$s are the bottleneck of AI compute as well as countless industry-scale applications, this primitive suggests a concrete design of a new L1 base-layer protocol, which nearly eliminates the energy-waste of Bitcoin mining -- allowing GPU consumers to reduce their AI training and inference costs by ``re-using\" it for blockchain consensus, in exchange for block rewards (2-for-1). This blockchain is currently under construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We revisit the longstanding open problem of implementing Nakamoto's proof-of-work (PoW) consensus based on a real-world computational task $T(x)$ (as opposed to artificial random hashing), in a truly permissionless setting where the miner itself chooses the input $x$. The challenge in designing such a Proof-of-Useful-Work (PoUW) protocol, is using the native computation of $T(x)$ to produce a PoW certificate with prescribed hardness and with negligible computational overhead over the worst-case complexity of $T(\\cdot)$ -- This ensures malicious miners cannot ``game the system\" by fooling the verifier to accept with higher probability compared to honest miners (while using similar computational resources). Indeed, obtaining a PoUW with $O(1)$-factor overhead is trivial for any task $T$, but also useless.\n  Our main result is a PoUW for the task of Matrix Multiplication $MatMul(A,B)$ of arbitrary matrices with $1+o(1)$ multiplicative overhead compared to naive $MatMul$ (even in the presence of Fast Matrix Multiplication-style algorithms, which are currently impractical). We conjecture that our protocol has optimal security in the sense that a malicious prover cannot obtain any significant advantage over an honest prover. This conjecture is based on reducing hardness of our protocol to the task of solving a batch of low-rank random linear equations which is of independent interest.\n  Since $MatMul$s are the bottleneck of AI compute as well as countless industry-scale applications, this primitive suggests a concrete design of a new L1 base-layer protocol, which nearly eliminates the energy-waste of Bitcoin mining -- allowing GPU consumers to reduce their AI training and inference costs by ``re-using\" it for blockchain consensus, in exchange for block rewards (2-for-1). This blockchain is currently under construction."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-14T08:22:40Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    22,
                    40,
                    0,
                    104,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Ilan Komargodski"
                    },
                    {
                        "name": "Omri Weinstein"
                    }
                ],
                "author_detail": {
                    "name": "Omri Weinstein"
                },
                "author": "Omri Weinstein"
            },
            {
                "id": "http://arxiv.org/abs/2511.10381v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10381v1",
                "title": "Position: On the Methodological Pitfalls of Evaluating Base LLMs for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: On the Methodological Pitfalls of Evaluating Base LLMs for Reasoning"
                },
                "updated": "2025-11-13T14:55:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    55,
                    51,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10381v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing work investigates the reasoning capabilities of large language models (LLMs) to uncover their limitations, human-like biases and underlying processes. Such studies include evaluations of base LLMs (pre-trained on unlabeled corpora only) for this purpose. Our position paper argues that evaluating base LLMs' reasoning capabilities raises inherent methodological concerns that are overlooked in such existing studies. We highlight the fundamental mismatch between base LLMs' pretraining objective and normative qualities, such as correctness, by which reasoning is assessed. In particular, we show how base LLMs generate logically valid or invalid conclusions as coincidental byproducts of conforming to purely linguistic patterns of statistical plausibility. This fundamental mismatch challenges the assumptions that (a) base LLMs' outputs can be assessed as their bona fide attempts at correct answers or conclusions; and (b) conclusions about base LLMs' reasoning can generalize to post-trained LLMs optimized for successful instruction-following. We call for a critical re-examination of existing work that relies implicitly on these assumptions, and for future work to account for these methodological pitfalls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing work investigates the reasoning capabilities of large language models (LLMs) to uncover their limitations, human-like biases and underlying processes. Such studies include evaluations of base LLMs (pre-trained on unlabeled corpora only) for this purpose. Our position paper argues that evaluating base LLMs' reasoning capabilities raises inherent methodological concerns that are overlooked in such existing studies. We highlight the fundamental mismatch between base LLMs' pretraining objective and normative qualities, such as correctness, by which reasoning is assessed. In particular, we show how base LLMs generate logically valid or invalid conclusions as coincidental byproducts of conforming to purely linguistic patterns of statistical plausibility. This fundamental mismatch challenges the assumptions that (a) base LLMs' outputs can be assessed as their bona fide attempts at correct answers or conclusions; and (b) conclusions about base LLMs' reasoning can generalize to post-trained LLMs optimized for successful instruction-following. We call for a critical re-examination of existing work that relies implicitly on these assumptions, and for future work to account for these methodological pitfalls."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:55:51Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    55,
                    51,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Preprint",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jason Chan"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Robert Gaizauskas"
                    }
                ],
                "author_detail": {
                    "name": "Robert Gaizauskas"
                },
                "author": "Robert Gaizauskas"
            },
            {
                "id": "http://arxiv.org/abs/2511.10375v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10375v1",
                "title": "TruthfulRAG: Resolving Factual-level Conflicts in Retrieval-Augmented Generation with Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TruthfulRAG: Resolving Factual-level Conflicts in Retrieval-Augmented Generation with Knowledge Graphs"
                },
                "updated": "2025-11-13T14:49:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    49,
                    44,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10375v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for enhancing the capabilities of Large Language Models (LLMs) by integrating retrieval-based methods with generative models. As external knowledge repositories continue to expand and the parametric knowledge within models becomes outdated, a critical challenge for RAG systems is resolving conflicts between retrieved external information and LLMs' internal knowledge, which can significantly compromise the accuracy and reliability of generated content. However, existing approaches to conflict resolution typically operate at the token or semantic level, often leading to fragmented and partial understanding of factual discrepancies between LLMs' knowledge and context, particularly in knowledge-intensive tasks. To address this limitation, we propose TruthfulRAG, the first framework that leverages Knowledge Graphs (KGs) to resolve factual-level knowledge conflicts in RAG systems. Specifically, TruthfulRAG constructs KGs by systematically extracting triples from retrieved content, utilizes query-based graph retrieval to identify relevant knowledge, and employs entropy-based filtering mechanisms to precisely locate conflicting elements and mitigate factual inconsistencies, thereby enabling LLMs to generate faithful and accurate responses. Extensive experiments reveal that TruthfulRAG outperforms existing methods, effectively alleviating knowledge conflicts and improving the robustness and trustworthiness of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for enhancing the capabilities of Large Language Models (LLMs) by integrating retrieval-based methods with generative models. As external knowledge repositories continue to expand and the parametric knowledge within models becomes outdated, a critical challenge for RAG systems is resolving conflicts between retrieved external information and LLMs' internal knowledge, which can significantly compromise the accuracy and reliability of generated content. However, existing approaches to conflict resolution typically operate at the token or semantic level, often leading to fragmented and partial understanding of factual discrepancies between LLMs' knowledge and context, particularly in knowledge-intensive tasks. To address this limitation, we propose TruthfulRAG, the first framework that leverages Knowledge Graphs (KGs) to resolve factual-level knowledge conflicts in RAG systems. Specifically, TruthfulRAG constructs KGs by systematically extracting triples from retrieved content, utilizes query-based graph retrieval to identify relevant knowledge, and employs entropy-based filtering mechanisms to precisely locate conflicting elements and mitigate factual inconsistencies, thereby enabling LLMs to generate faithful and accurate responses. Extensive experiments reveal that TruthfulRAG outperforms existing methods, effectively alleviating knowledge conflicts and improving the robustness and trustworthiness of RAG systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:49:44Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    49,
                    44,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "12 pages, 3 figures, accepted at AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shuyi Liu"
                    },
                    {
                        "name": "Yuming Shang"
                    },
                    {
                        "name": "Xi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xi Zhang"
                },
                "author": "Xi Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2504.21087v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.21087v2",
                "title": "Fast and accurate parameter estimation of high-redshift sources with the Einstein Telescope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and accurate parameter estimation of high-redshift sources with the Einstein Telescope"
                },
                "updated": "2025-11-13T14:38:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    38,
                    30,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.21087v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.21087v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1103/wf1k-p5cl",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The Einstein Telescope (ET), along with other third-generation gravitational wave (GW) detectors, will be a key instrument for detecting GWs in the coming decades. However, analyzing the data and estimating source parameters will be challenging, especially given the large number of expected detections-on the order of $10^5$ per year-which makes current methods based on stochastic sampling impractical. In this work, we use Dingo-IS to perform neural posterior estimation (NPE) of high-redshift events detectable with ET in its triangular configuration. NPE is a likelihood-free inference technique that leverages normalizing flows to approximate posterior distributions. After training, inference is fast, requiring only a few minutes per source, and accurate, as corrected through importance sampling and validated against standard Bayesian inference methods. To confirm previous findings on the ability to estimate parameters for high-redshift sources with ET, we compare NPE results with predictions from the Fisher information matrix (FIM) approximation. We find that NPE correctly recovers the eight degenerate sky modes induced by the triangular detector geometry, missed by the FIM analysis, resulting in an underestimation of sky localization uncertainties for most sources. FIM also overestimates the uncertainty in luminosity distance by a factor of $\\sim 3$ on average when the injected luminosity distance is $d^{\\mathrm{inj}}_{\\mathrm{L}} > 10^5~$Mpc, further confirming that ET will be particularly well suited for studying the early Universe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Einstein Telescope (ET), along with other third-generation gravitational wave (GW) detectors, will be a key instrument for detecting GWs in the coming decades. However, analyzing the data and estimating source parameters will be challenging, especially given the large number of expected detections-on the order of $10^5$ per year-which makes current methods based on stochastic sampling impractical. In this work, we use Dingo-IS to perform neural posterior estimation (NPE) of high-redshift events detectable with ET in its triangular configuration. NPE is a likelihood-free inference technique that leverages normalizing flows to approximate posterior distributions. After training, inference is fast, requiring only a few minutes per source, and accurate, as corrected through importance sampling and validated against standard Bayesian inference methods. To confirm previous findings on the ability to estimate parameters for high-redshift sources with ET, we compare NPE results with predictions from the Fisher information matrix (FIM) approximation. We find that NPE correctly recovers the eight degenerate sky modes induced by the triangular detector geometry, missed by the FIM analysis, resulting in an underestimation of sky localization uncertainties for most sources. FIM also overestimates the uncertainty in luminosity distance by a factor of $\\sim 3$ on average when the injected luminosity distance is $d^{\\mathrm{inj}}_{\\mathrm{L}} > 10^5~$Mpc, further confirming that ET will be particularly well suited for studying the early Universe."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-29T18:00:16Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    18,
                    0,
                    16,
                    1,
                    119,
                    0
                ],
                "arxiv_comment": "24 pages, 16 figures, 4 tables. Published in Physical Review D",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "arxiv_journal_ref": "Phys. Rev. D 112 (2025) 103015",
                "authors": [
                    {
                        "name": "Filippo Santoliquido"
                    },
                    {
                        "name": "Jacopo Tissino"
                    },
                    {
                        "name": "Ulyana Dupletsa"
                    },
                    {
                        "name": "Marica Branchesi"
                    },
                    {
                        "name": "Jan Harms"
                    },
                    {
                        "name": "Manuel Arca Sedda"
                    },
                    {
                        "name": "Maximilian Dax"
                    },
                    {
                        "name": "Annalena Kofler"
                    },
                    {
                        "name": "Stephen R. Green"
                    },
                    {
                        "name": "Nihar Gupte"
                    },
                    {
                        "name": "Isobel M. Romero-Shaw"
                    },
                    {
                        "name": "Emanuele Berti"
                    }
                ],
                "author_detail": {
                    "name": "Emanuele Berti"
                },
                "author": "Emanuele Berti",
                "arxiv_doi": "10.1103/wf1k-p5cl"
            },
            {
                "id": "http://arxiv.org/abs/2511.10356v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10356v1",
                "title": "SITA: A Framework for Structure-to-Instance Theorem Autoformalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SITA: A Framework for Structure-to-Instance Theorem Autoformalization"
                },
                "updated": "2025-11-13T14:33:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    33,
                    51,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10356v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While large language models (LLMs) have shown progress in mathematical reasoning, they still face challenges in formalizing theorems that arise from instantiating abstract structures in concrete settings. With the goal of auto-formalizing mathematical results at the research level, we develop a framework for structure-to-instance theorem autoformalization (SITA), which systematically bridges the gap between abstract mathematical theories and their concrete applications in Lean proof assistant. Formalized abstract structures are treated as modular templates that contain definitions, assumptions, operations, and theorems. These templates serve as reusable guides for the formalization of concrete instances. Given a specific instantiation, we generate corresponding Lean definitions and instance declarations, integrate them using Lean's typeclass mechanism, and construct verified theorems by checking structural assumptions. We incorporate LLM-based generation with feedback-guided refinement to ensure both automation and formal correctness. Experiments on a dataset of optimization problems demonstrate that SITA effectively formalizes diverse instances grounded in abstract structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown progress in mathematical reasoning, they still face challenges in formalizing theorems that arise from instantiating abstract structures in concrete settings. With the goal of auto-formalizing mathematical results at the research level, we develop a framework for structure-to-instance theorem autoformalization (SITA), which systematically bridges the gap between abstract mathematical theories and their concrete applications in Lean proof assistant. Formalized abstract structures are treated as modular templates that contain definitions, assumptions, operations, and theorems. These templates serve as reusable guides for the formalization of concrete instances. Given a specific instantiation, we generate corresponding Lean definitions and instance declarations, integrate them using Lean's typeclass mechanism, and construct verified theorems by checking structural assumptions. We incorporate LLM-based generation with feedback-guided refinement to ensure both automation and formal correctness. Experiments on a dataset of optimization problems demonstrate that SITA effectively formalizes diverse instances grounded in abstract structures."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:33:51Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    33,
                    51,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Chenyi Li"
                    },
                    {
                        "name": "Wanli Ma"
                    },
                    {
                        "name": "Zichen Wang"
                    },
                    {
                        "name": "Zaiwen Wen"
                    }
                ],
                "author_detail": {
                    "name": "Zaiwen Wen"
                },
                "author": "Zaiwen Wen"
            },
            {
                "id": "http://arxiv.org/abs/2511.10354v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10354v1",
                "title": "Knowledge Graphs Generation from Cultural Heritage Texts: Combining LLMs and Ontological Engineering for Scholarly Debates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graphs Generation from Cultural Heritage Texts: Combining LLMs and Ontological Engineering for Scholarly Debates"
                },
                "updated": "2025-11-13T14:29:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    29,
                    51,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10354v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cultural Heritage texts contain rich knowledge that is difficult to query systematically due to the challenges of converting unstructured discourse into structured Knowledge Graphs (KGs). This paper introduces ATR4CH (Adaptive Text-to-RDF for Cultural Heritage), a systematic five-step methodology for Large Language Model-based Knowledge Extraction from Cultural Heritage documents. We validate the methodology through a case study on authenticity assessment debates. Methodology - ATR4CH combines annotation models, ontological frameworks, and LLM-based extraction through iterative development: foundational analysis, annotation schema development, pipeline architecture, integration refinement, and comprehensive evaluation. We demonstrate the approach using Wikipedia articles about disputed items (documents, artifacts...), implementing a sequential pipeline with three LLMs (Claude Sonnet 3.7, Llama 3.3 70B, GPT-4o-mini). Findings - The methodology successfully extracts complex Cultural Heritage knowledge: 0.96-0.99 F1 for metadata extraction, 0.7-0.8 F1 for entity recognition, 0.65-0.75 F1 for hypothesis extraction, 0.95-0.97 for evidence extraction, and 0.62 G-EVAL for discourse representation. Smaller models performed competitively, enabling cost-effective deployment. Originality - This is the first systematic methodology for coordinating LLM-based extraction with Cultural Heritage ontologies. ATR4CH provides a replicable framework adaptable across CH domains and institutional resources. Research Limitations - The produced KG is limited to Wikipedia articles. While the results are encouraging, human oversight is necessary during post-processing. Practical Implications - ATR4CH enables Cultural Heritage institutions to systematically convert textual knowledge into queryable KGs, supporting automated metadata enrichment and knowledge discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural Heritage texts contain rich knowledge that is difficult to query systematically due to the challenges of converting unstructured discourse into structured Knowledge Graphs (KGs). This paper introduces ATR4CH (Adaptive Text-to-RDF for Cultural Heritage), a systematic five-step methodology for Large Language Model-based Knowledge Extraction from Cultural Heritage documents. We validate the methodology through a case study on authenticity assessment debates. Methodology - ATR4CH combines annotation models, ontological frameworks, and LLM-based extraction through iterative development: foundational analysis, annotation schema development, pipeline architecture, integration refinement, and comprehensive evaluation. We demonstrate the approach using Wikipedia articles about disputed items (documents, artifacts...), implementing a sequential pipeline with three LLMs (Claude Sonnet 3.7, Llama 3.3 70B, GPT-4o-mini). Findings - The methodology successfully extracts complex Cultural Heritage knowledge: 0.96-0.99 F1 for metadata extraction, 0.7-0.8 F1 for entity recognition, 0.65-0.75 F1 for hypothesis extraction, 0.95-0.97 for evidence extraction, and 0.62 G-EVAL for discourse representation. Smaller models performed competitively, enabling cost-effective deployment. Originality - This is the first systematic methodology for coordinating LLM-based extraction with Cultural Heritage ontologies. ATR4CH provides a replicable framework adaptable across CH domains and institutional resources. Research Limitations - The produced KG is limited to Wikipedia articles. While the results are encouraging, human oversight is necessary during post-processing. Practical Implications - ATR4CH enables Cultural Heritage institutions to systematically convert textual knowledge into queryable KGs, supporting automated metadata enrichment and knowledge discovery."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:29:51Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    29,
                    51,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "46 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Andrea Schimmenti"
                    },
                    {
                        "name": "Valentina Pasqual"
                    },
                    {
                        "name": "Fabio Vitali"
                    },
                    {
                        "name": "Marieke van Erp"
                    }
                ],
                "author_detail": {
                    "name": "Marieke van Erp"
                },
                "author": "Marieke van Erp"
            },
            {
                "id": "http://arxiv.org/abs/2511.09231v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09231v2",
                "title": "Leveraging Large Language Models for Use Case Model Generation from Software Requirements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Use Case Model Generation from Software Requirements"
                },
                "updated": "2025-11-13T14:27:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    27,
                    15,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09231v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T11:47:33Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    11,
                    47,
                    33,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "Accepted at the Intelligent Software Engineering Workshop (ISE 2025) at ASE 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Tobias Eisenreich"
                    },
                    {
                        "name": "Nicholas Friedlaender"
                    },
                    {
                        "name": "Stefan Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wagner"
                },
                "author": "Stefan Wagner"
            },
            {
                "id": "http://arxiv.org/abs/2511.05534v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05534v2",
                "title": "FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference"
                },
                "updated": "2025-11-13T14:25:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    25,
                    8,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05534v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05534v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-29T13:20:16Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    20,
                    16,
                    2,
                    302,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kunxi Li"
                    },
                    {
                        "name": "Yufan Xiong"
                    },
                    {
                        "name": "Zhonghua Jiang"
                    },
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Zhaode Wang"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.08525v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08525v2",
                "title": "Investigating CoT Monitorability in Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating CoT Monitorability in Large Reasoning Models"
                },
                "updated": "2025-11-13T14:15:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    15,
                    59,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08525v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models' long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models' misbehavior through their CoT and provide structured judgments along with supporting evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models' long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models' misbehavior through their CoT and provide structured judgments along with supporting evidence."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T18:06:34Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    18,
                    6,
                    34,
                    1,
                    315,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Junchao Wu"
                    },
                    {
                        "name": "Xilin Gong"
                    },
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Derek Wong"
                    },
                    {
                        "name": "Ninhao Liu"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.10343v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10343v1",
                "title": "Omnidirectional type inference for ML: principality any way",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omnidirectional type inference for ML: principality any way"
                },
                "updated": "2025-11-13T14:15:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    15,
                    36,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10343v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Damas-Hindley-Milner (ML) type system owes its success to principality, the property that every well-typed expression has a unique most general type. This makes inference predictable and efficient. Unfortunately, many extensions of ML (GADTs, higher-rank polymorphism, and static overloading) endanger princpality by introducing _fragile_ constructs that resist principal inference. Existing approaches recover principality through directional inference algorithms, which propagate _known_ type information in a fixed (or static) order (e.g. as in bidirectional typing) to disambiguate such constructs. However, the rigidity of a static inference order often causes otherwise well-typed programs to be rejected.\n  We propose _omnidirectional_ type inference, where type information flows in a dynamic order. Typing constraints may be solved in any order, suspending when progress requires known type information and resuming once it becomes available, using _suspended match constraints_. This approach is straightforward for simply typed systems, but extending it to ML is challenging due to let-generalization. Existing ML inference algorithms type let-bindings (let x = e1 in e2) in a fixed order: type e1, generalize its type, and then type e2. To overcome this, we introduce _incremental instantiation_, allowing partially solved type schemes containing suspended constraints to be instantiated, with a mechanism to incrementally update instances as the scheme is refined.\n  Omnidirectionality provides a general framework for restoring principality in the presence of fragile features. We demonstrate its versatility on two fundamentally different features of OCaml: static overloading of record labels and datatype constructors and semi-explicit first-class polymorphism. In both cases, we obtain a principal type inference algorithm that is more expressive than OCaml's current typechecker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Damas-Hindley-Milner (ML) type system owes its success to principality, the property that every well-typed expression has a unique most general type. This makes inference predictable and efficient. Unfortunately, many extensions of ML (GADTs, higher-rank polymorphism, and static overloading) endanger princpality by introducing _fragile_ constructs that resist principal inference. Existing approaches recover principality through directional inference algorithms, which propagate _known_ type information in a fixed (or static) order (e.g. as in bidirectional typing) to disambiguate such constructs. However, the rigidity of a static inference order often causes otherwise well-typed programs to be rejected.\n  We propose _omnidirectional_ type inference, where type information flows in a dynamic order. Typing constraints may be solved in any order, suspending when progress requires known type information and resuming once it becomes available, using _suspended match constraints_. This approach is straightforward for simply typed systems, but extending it to ML is challenging due to let-generalization. Existing ML inference algorithms type let-bindings (let x = e1 in e2) in a fixed order: type e1, generalize its type, and then type e2. To overcome this, we introduce _incremental instantiation_, allowing partially solved type schemes containing suspended constraints to be instantiated, with a mechanism to incrementally update instances as the scheme is refined.\n  Omnidirectionality provides a general framework for restoring principality in the presence of fragile features. We demonstrate its versatility on two fundamentally different features of OCaml: static overloading of record labels and datatype constructors and semi-explicit first-class polymorphism. In both cases, we obtain a principal type inference algorithm that is more expressive than OCaml's current typechecker."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:15:36Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    15,
                    36,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "39 pages + appendices",
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Alistair O'Brien"
                    },
                    {
                        "name": "Didier Rémy"
                    },
                    {
                        "name": "Gabriel Scherer"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Scherer"
                },
                "author": "Gabriel Scherer"
            },
            {
                "id": "http://arxiv.org/abs/2511.10338v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10338v1",
                "title": "BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages"
                },
                "updated": "2025-11-13T14:12:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    12,
                    44,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10338v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In the context of pretraining of Large Language Models (LLMs), synthetic data has emerged as an alternative for generating high-quality pretraining data at scale. This is particularly beneficial in low-resource language settings where the benefits of recent LLMs have been unevenly distributed across languages. In this work, we present a systematic study on the generation and evaluation of synthetic multilingual pretraining data for Indic languages, where we construct a large-scale synthetic dataset BhashaKritika, comprising 540B tokens using 5 different techniques for 10 languages. We explore the impact of grounding generation in documents, personas, and topics. We analyze how language choice, both in the prompt instructions and document grounding, affects data quality, and we compare translations of English content with native generation in Indic languages. To support scalable and language-sensitive evaluation, we introduce a modular quality evaluation pipeline that integrates script and language detection, metadata consistency checks, n-gram repetition analysis, and perplexity-based filtering using KenLM models. Our framework enables robust quality control across diverse scripts and linguistic contexts. Empirical results through model runs reveal key trade-offs in generation strategies and highlight best practices for constructing effective multilingual corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of pretraining of Large Language Models (LLMs), synthetic data has emerged as an alternative for generating high-quality pretraining data at scale. This is particularly beneficial in low-resource language settings where the benefits of recent LLMs have been unevenly distributed across languages. In this work, we present a systematic study on the generation and evaluation of synthetic multilingual pretraining data for Indic languages, where we construct a large-scale synthetic dataset BhashaKritika, comprising 540B tokens using 5 different techniques for 10 languages. We explore the impact of grounding generation in documents, personas, and topics. We analyze how language choice, both in the prompt instructions and document grounding, affects data quality, and we compare translations of English content with native generation in Indic languages. To support scalable and language-sensitive evaluation, we introduce a modular quality evaluation pipeline that integrates script and language detection, metadata consistency checks, n-gram repetition analysis, and perplexity-based filtering using KenLM models. Our framework enables robust quality control across diverse scripts and linguistic contexts. Empirical results through model runs reveal key trade-offs in generation strategies and highlight best practices for constructing effective multilingual corpora."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:12:44Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    12,
                    44,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Guduru Manoj"
                    },
                    {
                        "name": "Neel Prabhanjan Rachamalla"
                    },
                    {
                        "name": "Ashish Kulkarni"
                    },
                    {
                        "name": "Gautam Rajeev"
                    },
                    {
                        "name": "Jay Piplodiya"
                    },
                    {
                        "name": "Arul Menezes"
                    },
                    {
                        "name": "Shaharukh Khan"
                    },
                    {
                        "name": "Souvik Rana"
                    },
                    {
                        "name": "Manya Sah"
                    },
                    {
                        "name": "Chandra Khatri"
                    },
                    {
                        "name": "Shubham Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Shubham Agarwal"
                },
                "author": "Shubham Agarwal"
            },
            {
                "id": "http://arxiv.org/abs/2511.10333v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10333v1",
                "title": "EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training"
                },
                "updated": "2025-11-13T14:05:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    5,
                    34,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10333v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:05:34Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    5,
                    34,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Qingao Yi"
                    },
                    {
                        "name": "Jiaang Duan"
                    },
                    {
                        "name": "Hanwen Hu"
                    },
                    {
                        "name": "Qin Hua"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Shiyou Qian"
                    },
                    {
                        "name": "Dingyu Yang"
                    },
                    {
                        "name": "Jian Cao"
                    },
                    {
                        "name": "Jinghua Tang"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Chenzhi Liao"
                    },
                    {
                        "name": "Kangjin Wang"
                    },
                    {
                        "name": "Liping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liping Zhang"
                },
                "author": "Liping Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2508.02573v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.02573v2",
                "title": "Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs"
                },
                "updated": "2025-11-13T14:05:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    5,
                    2,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.02573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.02573v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted phenomenon involving distinct underlying mechanisms. We introduce a novel method to analyze the different forms of memorization described by the existing taxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the attention weights of the LLM and evaluate the alignment between this taxonomy and the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect distinct mechanisms within the attention blocks. We propose a new taxonomy that maximizes alignment with the attention weights, consisting of three categories: memorized samples that are guessed using language modeling abilities, memorized samples that are recalled due to high duplication in the training set, and non-memorized samples. Our results reveal that few-shot verbatim memorization does not correspond to a distinct attention mechanism. We also show that a significant proportion of extractable samples are in fact guessed by the model and should therefore be studied separately. Finally, we develop a custom visual interpretability technique to localize the regions of the attention weights involved in each form of memorization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted phenomenon involving distinct underlying mechanisms. We introduce a novel method to analyze the different forms of memorization described by the existing taxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the attention weights of the LLM and evaluate the alignment between this taxonomy and the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect distinct mechanisms within the attention blocks. We propose a new taxonomy that maximizes alignment with the attention weights, consisting of three categories: memorized samples that are guessed using language modeling abilities, memorized samples that are recalled due to high duplication in the training set, and non-memorized samples. Our results reveal that few-shot verbatim memorization does not correspond to a distinct attention mechanism. We also show that a significant proportion of extractable samples are in fact guessed by the model and should therefore be studied separately. Finally, we develop a custom visual interpretability technique to localize the regions of the attention weights involved in each form of memorization."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-04T16:27:56Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    27,
                    56,
                    0,
                    216,
                    0
                ],
                "arxiv_comment": "This paper has been accepted for publication at AAAI-26",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jérémie Dentan"
                    },
                    {
                        "name": "Davide Buscaldi"
                    },
                    {
                        "name": "Sonia Vanier"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Vanier"
                },
                "author": "Sonia Vanier"
            },
            {
                "id": "http://arxiv.org/abs/2511.10325v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10325v1",
                "title": "TMDC: A Two-Stage Modality Denoising and Complementation Framework for Multimodal Sentiment Analysis with Missing and Noisy Modalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TMDC: A Two-Stage Modality Denoising and Complementation Framework for Multimodal Sentiment Analysis with Missing and Noisy Modalities"
                },
                "updated": "2025-11-13T13:59:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    59,
                    57,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10325v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal Sentiment Analysis (MSA) aims to infer human sentiment by integrating information from multiple modalities such as text, audio, and video. In real-world scenarios, however, the presence of missing modalities and noisy signals significantly hinders the robustness and accuracy of existing models. While prior works have made progress on these issues, they are typically addressed in isolation, limiting overall effectiveness in practical settings. To jointly mitigate the challenges posed by missing and noisy modalities, we propose a framework called Two-stage Modality Denoising and Complementation (TMDC). TMDC comprises two sequential training stages. In the Intra-Modality Denoising Stage, denoised modality-specific and modality-shared representations are extracted from complete data using dedicated denoising modules, reducing the impact of noise and enhancing representational robustness. In the Inter-Modality Complementation Stage, these representations are leveraged to compensate for missing modalities, thereby enriching the available information and further improving robustness. Extensive evaluations on MOSI, MOSEI, and IEMOCAP demonstrate that TMDC consistently achieves superior performance compared to existing methods, establishing new state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Sentiment Analysis (MSA) aims to infer human sentiment by integrating information from multiple modalities such as text, audio, and video. In real-world scenarios, however, the presence of missing modalities and noisy signals significantly hinders the robustness and accuracy of existing models. While prior works have made progress on these issues, they are typically addressed in isolation, limiting overall effectiveness in practical settings. To jointly mitigate the challenges posed by missing and noisy modalities, we propose a framework called Two-stage Modality Denoising and Complementation (TMDC). TMDC comprises two sequential training stages. In the Intra-Modality Denoising Stage, denoised modality-specific and modality-shared representations are extracted from complete data using dedicated denoising modules, reducing the impact of noise and enhancing representational robustness. In the Inter-Modality Complementation Stage, these representations are leveraged to compensate for missing modalities, thereby enriching the available information and further improving robustness. Extensive evaluations on MOSI, MOSEI, and IEMOCAP demonstrate that TMDC consistently achieves superior performance compared to existing methods, establishing new state-of-the-art results."
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:59:57Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    59,
                    57,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.MM"
                },
                "authors": [
                    {
                        "name": "Yan Zhuang"
                    },
                    {
                        "name": "Minhao Liu"
                    },
                    {
                        "name": "Yanru Zhang"
                    },
                    {
                        "name": "Jiawen Deng"
                    },
                    {
                        "name": "Fuji Ren"
                    }
                ],
                "author_detail": {
                    "name": "Fuji Ren"
                },
                "author": "Fuji Ren"
            },
            {
                "id": "http://arxiv.org/abs/2511.08132v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08132v2",
                "title": "National Institute on Aging PREPARE Challenge: Early Detection of Cognitive Impairment Using Speech -- The SpeechCARE Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "National Institute on Aging PREPARE Challenge: Early Detection of Cognitive Impairment Using Speech -- The SpeechCARE Solution"
                },
                "updated": "2025-11-13T13:58:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    58,
                    9,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08132v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Alzheimer's disease and related dementias (ADRD) affect one in five adults over 60, yet more than half of individuals with cognitive decline remain undiagnosed. Speech-based assessments show promise for early detection, as phonetic motor planning deficits alter acoustic features (e.g., pitch, tone), while memory and language impairments lead to syntactic and semantic errors. However, conventional speech-processing pipelines with hand-crafted features or general-purpose audio classifiers often exhibit limited performance and generalizability. To address these limitations, we introduce SpeechCARE, a multimodal speech processing pipeline that leverages pretrained, multilingual acoustic and linguistic transformer models to capture subtle speech-related cues associated with cognitive impairment. Inspired by the Mixture of Experts (MoE) paradigm, SpeechCARE employs a dynamic fusion architecture that weights transformer-based acoustic, linguistic, and demographic inputs, allowing integration of additional modalities (e.g., social factors, imaging) and enhancing robustness across diverse tasks. Its robust preprocessing includes automatic transcription, large language model (LLM)-based anomaly detection, and task identification. A SHAP-based explainability module and LLM reasoning highlight each modality's contribution to decision-making. SpeechCARE achieved AUC = 0.88 and F1 = 0.72 for classifying cognitively healthy, MCI, and AD individuals, with AUC = 0.90 and F1 = 0.62 for MCI detection. Bias analysis showed minimal disparities, except for adults over 80. Mitigation techniques included oversampling and weighted loss. Future work includes deployment in real-world care settings (e.g., VNS Health, Columbia ADRC) and EHR-integrated explainability for underrepresented populations in New York City.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alzheimer's disease and related dementias (ADRD) affect one in five adults over 60, yet more than half of individuals with cognitive decline remain undiagnosed. Speech-based assessments show promise for early detection, as phonetic motor planning deficits alter acoustic features (e.g., pitch, tone), while memory and language impairments lead to syntactic and semantic errors. However, conventional speech-processing pipelines with hand-crafted features or general-purpose audio classifiers often exhibit limited performance and generalizability. To address these limitations, we introduce SpeechCARE, a multimodal speech processing pipeline that leverages pretrained, multilingual acoustic and linguistic transformer models to capture subtle speech-related cues associated with cognitive impairment. Inspired by the Mixture of Experts (MoE) paradigm, SpeechCARE employs a dynamic fusion architecture that weights transformer-based acoustic, linguistic, and demographic inputs, allowing integration of additional modalities (e.g., social factors, imaging) and enhancing robustness across diverse tasks. Its robust preprocessing includes automatic transcription, large language model (LLM)-based anomaly detection, and task identification. A SHAP-based explainability module and LLM reasoning highlight each modality's contribution to decision-making. SpeechCARE achieved AUC = 0.88 and F1 = 0.72 for classifying cognitively healthy, MCI, and AD individuals, with AUC = 0.90 and F1 = 0.62 for MCI detection. Bias analysis showed minimal disparities, except for adults over 80. Mitigation techniques included oversampling and weighted loss. Future work includes deployment in real-world care settings (e.g., VNS Health, Columbia ADRC) and EHR-integrated explainability for underrepresented populations in New York City."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T11:39:20Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    11,
                    39,
                    20,
                    1,
                    315,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Maryam Zolnoori"
                    },
                    {
                        "name": "Hossein Azadmaleki"
                    },
                    {
                        "name": "Yasaman Haghbin"
                    },
                    {
                        "name": "Ali Zolnour"
                    },
                    {
                        "name": "Mohammad Javad Momeni Nezhad"
                    },
                    {
                        "name": "Sina Rashidi"
                    },
                    {
                        "name": "Mehdi Naserian"
                    },
                    {
                        "name": "Elyas Esmaeili"
                    },
                    {
                        "name": "Sepehr Karimi Arpanahi"
                    }
                ],
                "author_detail": {
                    "name": "Sepehr Karimi Arpanahi"
                },
                "author": "Sepehr Karimi Arpanahi"
            },
            {
                "id": "http://arxiv.org/abs/2509.23213v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.23213v2",
                "title": "One-Shot Multi-Label Causal Discovery in High-Dimensional Event Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Shot Multi-Label Causal Discovery in High-Dimensional Event Sequences"
                },
                "updated": "2025-11-13T13:51:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    51,
                    1,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.23213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.23213v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding causality in event sequences with thousands of sparse event types is critical in domains such as healthcare, cybersecurity, or vehicle diagnostics, yet current methods fail to scale. We present OSCAR, a one-shot causal autoregressive method that infers per-sequence Markov Boundaries using two pretrained Transformers as density estimators. This enables efficient, parallel causal discovery without costly global CI testing. On a real-world automotive dataset with 29,100 events and 474 labels, OSCAR recovers interpretable causal structures in minutes, while classical methods fail to scale, enabling practical scientific diagnostics at production scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding causality in event sequences with thousands of sparse event types is critical in domains such as healthcare, cybersecurity, or vehicle diagnostics, yet current methods fail to scale. We present OSCAR, a one-shot causal autoregressive method that infers per-sequence Markov Boundaries using two pretrained Transformers as density estimators. This enables efficient, parallel causal discovery without costly global CI testing. On a real-world automotive dataset with 29,100 events and 474 labels, OSCAR recovers interpretable causal structures in minutes, while classical methods fail to scale, enabling practical scientific diagnostics at production scale."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-27T09:49:26Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    9,
                    49,
                    26,
                    5,
                    270,
                    0
                ],
                "arxiv_comment": "Accepted at NeuRIPS2025 Workshop CauScien: Discovering Causality in Science. arXiv admin note: substantial text overlap with arXiv:2509.19112",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Hugo Math"
                    },
                    {
                        "name": "Robin Schön"
                    },
                    {
                        "name": "Rainer Lienhart"
                    }
                ],
                "author_detail": {
                    "name": "Rainer Lienhart"
                },
                "author": "Rainer Lienhart"
            },
            {
                "id": "http://arxiv.org/abs/2505.17519v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.17519v2",
                "title": "Chain-of-Lure: A Universal Jailbreak Attack Framework using Unconstrained Synthetic Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Lure: A Universal Jailbreak Attack Framework using Unconstrained Synthetic Narratives"
                },
                "updated": "2025-11-13T13:50:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    50,
                    58,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.17519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.17519v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In the era of rapid generative AI development, interactions with large language models (LLMs) pose increasing risks of misuse. Prior research has primarily focused on attacks using template-based prompts and optimization-oriented methods, while overlooking the fact that LLMs possess strong unconstrained deceptive capabilities to attack other LLMs. This paper introduces a novel jailbreaking method inspired by the Chain-of-Thought mechanism. The attacker employs mission transfer to conceal harmful user intent within dialogue and generates a progressive chain of lure questions without relying on predefined templates, enabling successful jailbreaks. To further improve the attack's strength, we incorporate a helper LLM model that performs randomized narrative optimization over multi-turn interactions, enhancing the attack performance while preserving alignment with the original intent. We also propose a toxicity-based framework using third-party LLMs to evaluate harmful content and its alignment with malicious intent. Extensive experiments demonstrate that our method consistently achieves high attack success rates and elevated toxicity scores across diverse types of LLMs under black-box API settings. These findings reveal the intrinsic potential of LLMs to perform unrestricted attacks in the absence of robust alignment constraints. Our approach offers data-driven insights to inform the design of future alignment mechanisms. Finally, we propose two concrete defense strategies to support the development of safer generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of rapid generative AI development, interactions with large language models (LLMs) pose increasing risks of misuse. Prior research has primarily focused on attacks using template-based prompts and optimization-oriented methods, while overlooking the fact that LLMs possess strong unconstrained deceptive capabilities to attack other LLMs. This paper introduces a novel jailbreaking method inspired by the Chain-of-Thought mechanism. The attacker employs mission transfer to conceal harmful user intent within dialogue and generates a progressive chain of lure questions without relying on predefined templates, enabling successful jailbreaks. To further improve the attack's strength, we incorporate a helper LLM model that performs randomized narrative optimization over multi-turn interactions, enhancing the attack performance while preserving alignment with the original intent. We also propose a toxicity-based framework using third-party LLMs to evaluate harmful content and its alignment with malicious intent. Extensive experiments demonstrate that our method consistently achieves high attack success rates and elevated toxicity scores across diverse types of LLMs under black-box API settings. These findings reveal the intrinsic potential of LLMs to perform unrestricted attacks in the absence of robust alignment constraints. Our approach offers data-driven insights to inform the design of future alignment mechanisms. Finally, we propose two concrete defense strategies to support the development of safer generative models."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-23T06:19:05Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    6,
                    19,
                    5,
                    4,
                    143,
                    0
                ],
                "arxiv_comment": "23 pages, 3 main figures",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Wenhan Chang"
                    },
                    {
                        "name": "Tianqing Zhu"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Shuangyong Song"
                    },
                    {
                        "name": "Ping Xiong"
                    },
                    {
                        "name": "Wanlei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wanlei Zhou"
                },
                "author": "Wanlei Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2509.19112v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.19112v2",
                "title": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation"
                },
                "updated": "2025-11-13T13:47:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    47,
                    9,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.19112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.19112v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding causality in event sequences where outcome labels such as diseases or system failures arise from preceding events like symptoms or error codes is critical. Yet remains an unsolved challenge across domains like healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label causal discovery method for sparse, high-dimensional event sequences comprising of thousands of unique event types. Using two pretrained causal Transformers as domain-specific foundation models for event sequences. CARGO infers in parallel, per sequence one-shot causal graphs and aggregates them using an adaptive frequency fusion to reconstruct the global Markov boundaries of labels. This two-stage approach enables efficient probabilistic reasoning at scale while bypassing the intractable cost of full-dataset conditional independence testing. Our results on a challenging real-world automotive fault prediction dataset with over 29,100 unique event types and 474 imbalanced labels demonstrate CARGO's ability to perform structured reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding causality in event sequences where outcome labels such as diseases or system failures arise from preceding events like symptoms or error codes is critical. Yet remains an unsolved challenge across domains like healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label causal discovery method for sparse, high-dimensional event sequences comprising of thousands of unique event types. Using two pretrained causal Transformers as domain-specific foundation models for event sequences. CARGO infers in parallel, per sequence one-shot causal graphs and aggregates them using an adaptive frequency fusion to reconstruct the global Markov boundaries of labels. This two-stage approach enables efficient probabilistic reasoning at scale while bypassing the intractable cost of full-dataset conditional independence testing. Our results on a challenging real-world automotive fault prediction dataset with over 29,100 unique event types and 474 imbalanced labels demonstrate CARGO's ability to perform structured reasoning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-23T14:58:50Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    58,
                    50,
                    1,
                    266,
                    0
                ],
                "arxiv_comment": "Accepted at NeuRIPS2025 Workshop on Structured Probabilistic Inference and Generative Modeling",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Hugo Math"
                    },
                    {
                        "name": "Rainer Lienhart"
                    }
                ],
                "author_detail": {
                    "name": "Rainer Lienhart"
                },
                "author": "Rainer Lienhart"
            },
            {
                "id": "http://arxiv.org/abs/2511.10303v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10303v1",
                "title": "Rectify Evaluation Preference: Improving LLMs' Critique on Math Reasoning via Perplexity-aware Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rectify Evaluation Preference: Improving LLMs' Critique on Math Reasoning via Perplexity-aware Reinforcement Learning"
                },
                "updated": "2025-11-13T13:37:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    37,
                    45,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10303v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "To improve Multi-step Mathematical Reasoning (MsMR) of Large Language Models (LLMs), it is crucial to obtain scalable supervision from the corpus by automatically critiquing mistakes in the reasoning process of MsMR and rendering a final verdict of the problem-solution. Most existing methods rely on crafting high-quality supervised fine-tuning demonstrations for critiquing capability enhancement and pay little attention to delving into the underlying reason for the poor critiquing performance of LLMs. In this paper, we orthogonally quantify and investigate the potential reason -- imbalanced evaluation preference, and conduct a statistical preference analysis. Motivated by the analysis of the reason, a novel perplexity-aware reinforcement learning algorithm is proposed to rectify the evaluation preference, elevating the critiquing capability. Specifically, to probe into LLMs' critiquing characteristics, a One-to-many Problem-Solution (OPS) benchmark is meticulously constructed to quantify the behavior difference of LLMs when evaluating the problem solutions generated by itself and others. Then, to investigate the behavior difference in depth, we conduct a statistical preference analysis oriented on perplexity and find an intriguing phenomenon -- ``LLMs incline to judge solutions with lower perplexity as correct'', which is dubbed as \\textit{imbalanced evaluation preference}. To rectify this preference, we regard perplexity as the baton in the algorithm of Group Relative Policy Optimization, supporting the LLMs to explore trajectories that judge lower perplexity as wrong and higher perplexity as correct. Extensive experimental results on our built OPS and existing available critic benchmarks demonstrate the validity of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve Multi-step Mathematical Reasoning (MsMR) of Large Language Models (LLMs), it is crucial to obtain scalable supervision from the corpus by automatically critiquing mistakes in the reasoning process of MsMR and rendering a final verdict of the problem-solution. Most existing methods rely on crafting high-quality supervised fine-tuning demonstrations for critiquing capability enhancement and pay little attention to delving into the underlying reason for the poor critiquing performance of LLMs. In this paper, we orthogonally quantify and investigate the potential reason -- imbalanced evaluation preference, and conduct a statistical preference analysis. Motivated by the analysis of the reason, a novel perplexity-aware reinforcement learning algorithm is proposed to rectify the evaluation preference, elevating the critiquing capability. Specifically, to probe into LLMs' critiquing characteristics, a One-to-many Problem-Solution (OPS) benchmark is meticulously constructed to quantify the behavior difference of LLMs when evaluating the problem solutions generated by itself and others. Then, to investigate the behavior difference in depth, we conduct a statistical preference analysis oriented on perplexity and find an intriguing phenomenon -- ``LLMs incline to judge solutions with lower perplexity as correct'', which is dubbed as \\textit{imbalanced evaluation preference}. To rectify this preference, we regard perplexity as the baton in the algorithm of Group Relative Policy Optimization, supporting the LLMs to explore trajectories that judge lower perplexity as wrong and higher perplexity as correct. Extensive experimental results on our built OPS and existing available critic benchmarks demonstrate the validity of our method."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:37:45Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    37,
                    45,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Changyuan Tian"
                    },
                    {
                        "name": "Zhicong Lu"
                    },
                    {
                        "name": "Shuang Qian"
                    },
                    {
                        "name": "Nayu Liu"
                    },
                    {
                        "name": "Peiguang Li"
                    },
                    {
                        "name": "Li Jin"
                    },
                    {
                        "name": "Leiyi Hu"
                    },
                    {
                        "name": "Zhizhao Zeng"
                    },
                    {
                        "name": "Sirui Wang"
                    },
                    {
                        "name": "Ke Zeng"
                    },
                    {
                        "name": "Zhi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Guo"
                },
                "author": "Zhi Guo"
            },
            {
                "id": "http://arxiv.org/abs/2511.10301v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10301v1",
                "title": "Rethinking Visual Information Processing in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Visual Information Processing in Multimodal LLMs"
                },
                "updated": "2025-11-13T13:36:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    36,
                    30,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10301v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities. We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder. To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations. Through extensive controlled experiments on a wide range of LLMs, we demonstrate that LLaViT significantly outperforms the baseline LLaVA method on a multitude of benchmarks, even surpassing models with double its parameter count, establishing a more effective approach to vision-language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities. We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder. To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations. Through extensive controlled experiments on a wide range of LLMs, we demonstrate that LLaViT significantly outperforms the baseline LLaVA method on a multitude of benchmarks, even surpassing models with double its parameter count, establishing a more effective approach to vision-language modeling."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:36:30Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    36,
                    30,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Dongwan Kim"
                    },
                    {
                        "name": "Viresh Ranjan"
                    },
                    {
                        "name": "Takashi Nagata"
                    },
                    {
                        "name": "Arnab Dhua"
                    },
                    {
                        "name": "Amit Kumar K C"
                    }
                ],
                "author_detail": {
                    "name": "Amit Kumar K C"
                },
                "author": "Amit Kumar K C"
            },
            {
                "id": "http://arxiv.org/abs/2508.11034v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.11034v2",
                "title": "The Impact of Large Language Models (LLMs) on Code Review Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Large Language Models (LLMs) on Code Review Process"
                },
                "updated": "2025-11-13T13:33:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    33,
                    44,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.11034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.11034v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have recently gained prominence in the field of software development, significantly boosting productivity and simplifying teamwork. Although prior studies have examined task-specific applications, the phase-specific effects of LLM assistance on the efficiency of code review processes remain underexplored. This research investigates the effect of GPT on GitHub pull request (PR) workflows, with a focus on reducing resolution time, optimizing phase-specific performance, and assisting developers. We curated a dataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted PRs using a semi-automated heuristic approach that combines keyword-based detection, regular expression filtering, and manual verification until achieving 95% labeling accuracy. We then applied statistical modeling, including multiple linear regression and Mann-Whitney U test, to evaluate differences between GPT-assisted and non-assisted PRs, both at the overall resolution level and across distinct review phases. Our research has revealed that early adoption of GPT can substantially boost the effectiveness of the PR process, leading to considerable time savings at various stages. Our findings suggest that GPT-assisted PRs reduced median resolution time by more than 60% (9 hours compared to 23 hours for non-assisted PRs). We discovered that utilizing GPT can reduce the review time by 33% and the waiting time before acceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we discovered that developers predominantly use GPT for code optimization (60%), bug fixing (26%), and documentation updates (12%). This research sheds light on the impact of the GPT model on the code review process, offering actionable insights for software teams seeking to enhance workflows and promote seamless collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently gained prominence in the field of software development, significantly boosting productivity and simplifying teamwork. Although prior studies have examined task-specific applications, the phase-specific effects of LLM assistance on the efficiency of code review processes remain underexplored. This research investigates the effect of GPT on GitHub pull request (PR) workflows, with a focus on reducing resolution time, optimizing phase-specific performance, and assisting developers. We curated a dataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted PRs using a semi-automated heuristic approach that combines keyword-based detection, regular expression filtering, and manual verification until achieving 95% labeling accuracy. We then applied statistical modeling, including multiple linear regression and Mann-Whitney U test, to evaluate differences between GPT-assisted and non-assisted PRs, both at the overall resolution level and across distinct review phases. Our research has revealed that early adoption of GPT can substantially boost the effectiveness of the PR process, leading to considerable time savings at various stages. Our findings suggest that GPT-assisted PRs reduced median resolution time by more than 60% (9 hours compared to 23 hours for non-assisted PRs). We discovered that utilizing GPT can reduce the review time by 33% and the waiting time before acceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we discovered that developers predominantly use GPT for code optimization (60%), bug fixing (26%), and documentation updates (12%). This research sheds light on the impact of the GPT model on the code review process, offering actionable insights for software teams seeking to enhance workflows and promote seamless collaboration."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-14T19:39:01Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    19,
                    39,
                    1,
                    3,
                    226,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Antonio Collante"
                    },
                    {
                        "name": "Samuel Abedu"
                    },
                    {
                        "name": "SayedHassan Khatoonabadi"
                    },
                    {
                        "name": "Ahmad Abdellatif"
                    },
                    {
                        "name": "Ebube Alor"
                    },
                    {
                        "name": "Emad Shihab"
                    }
                ],
                "author_detail": {
                    "name": "Emad Shihab"
                },
                "author": "Emad Shihab"
            },
            {
                "id": "http://arxiv.org/abs/2511.10292v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10292v1",
                "title": "Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models"
                },
                "updated": "2025-11-13T13:29:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    29,
                    38,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10292v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Vision-Language Models (LVLMs) often suffer from object hallucination, generating text inconsistent with visual inputs, which can critically undermine their reliability. Existing inference-time interventions to mitigate this issue present a challenging trade-off: while methods that steer internal states or adjust output logits can be effective, they often incur substantial computational overhead, typically requiring extra forward passes. This efficiency bottleneck can limit their practicality for real-world, latency-sensitive deployments. In this work, we aim to address this trade-off with Residual-Update Directed DEcoding Regulation (RUDDER), a low-overhead framework that steers LVLMs towards visually-grounded generation. RUDDER is built on two key innovations: (1) Contextual Activation Residual Direction (CARD) vector, a per-sample visual evidence vector extracted from the residual update of a self-attention layer during a single, standard forward pass. (2) A Bayesian-inspired adaptive gate that performs token-wise injection, applying a corrective signal whose strength is conditioned on the model's deviation from the visual context. Extensive experiments on key hallucination benchmarks, including POPE and CHAIR, indicate that RUDDER achieves performance comparable to state-of-the-art methods while introducing negligible computational latency, validating RUDDER as a pragmatic and effective approach for improving LVLMs' reliability without a significant compromise on efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) often suffer from object hallucination, generating text inconsistent with visual inputs, which can critically undermine their reliability. Existing inference-time interventions to mitigate this issue present a challenging trade-off: while methods that steer internal states or adjust output logits can be effective, they often incur substantial computational overhead, typically requiring extra forward passes. This efficiency bottleneck can limit their practicality for real-world, latency-sensitive deployments. In this work, we aim to address this trade-off with Residual-Update Directed DEcoding Regulation (RUDDER), a low-overhead framework that steers LVLMs towards visually-grounded generation. RUDDER is built on two key innovations: (1) Contextual Activation Residual Direction (CARD) vector, a per-sample visual evidence vector extracted from the residual update of a self-attention layer during a single, standard forward pass. (2) A Bayesian-inspired adaptive gate that performs token-wise injection, applying a corrective signal whose strength is conditioned on the model's deviation from the visual context. Extensive experiments on key hallucination benchmarks, including POPE and CHAIR, indicate that RUDDER achieves performance comparable to state-of-the-art methods while introducing negligible computational latency, validating RUDDER as a pragmatic and effective approach for improving LVLMs' reliability without a significant compromise on efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:29:38Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    29,
                    38,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Under review",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhengtao Zou"
                    },
                    {
                        "name": "Ya Gao"
                    },
                    {
                        "name": "Jiarui Guan"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Pekka Marttinen"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Marttinen"
                },
                "author": "Pekka Marttinen"
            },
            {
                "id": "http://arxiv.org/abs/2508.05909v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.05909v2",
                "title": "Beyond Perplexity: Let the Reader Select Retrieval Summaries via Spectrum Projection Score",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Perplexity: Let the Reader Select Retrieval Summaries via Spectrum Projection Score"
                },
                "updated": "2025-11-13T13:28:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    28,
                    8,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.05909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.05909v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have shown improved generation performance through retrieval-augmented generation (RAG) following the retriever-reader paradigm, which supplements model inputs with externally retrieved knowledge. However, prior work often evaluates RAG holistically, assessing the retriever and reader jointly, making it difficult to isolate the true contribution of retrieval, particularly given the prompt sensitivity of LLMs used as readers. We move beyond perplexity and introduce Spectrum Projection Score (SPS), a lightweight and supervision-free metric that allows the reader to gauge the semantic alignment of a retrieved summary with its hidden representation by comparing the area formed by generated tokens from the summary, and the principal directions of subspace in the reader and to measure the relevance. Building on SPS we present xCompress, an inference-time controller framework that dynamically samples, ranks, and compresses retrieval summary candidates. Extensive experiments on five QA benchmarks with four open-sourced LLMs show that SPS not only enhances performance across a range of tasks but also provides a principled perspective on the interaction between retrieval and generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown improved generation performance through retrieval-augmented generation (RAG) following the retriever-reader paradigm, which supplements model inputs with externally retrieved knowledge. However, prior work often evaluates RAG holistically, assessing the retriever and reader jointly, making it difficult to isolate the true contribution of retrieval, particularly given the prompt sensitivity of LLMs used as readers. We move beyond perplexity and introduce Spectrum Projection Score (SPS), a lightweight and supervision-free metric that allows the reader to gauge the semantic alignment of a retrieved summary with its hidden representation by comparing the area formed by generated tokens from the summary, and the principal directions of subspace in the reader and to measure the relevance. Building on SPS we present xCompress, an inference-time controller framework that dynamically samples, ranks, and compresses retrieval summary candidates. Extensive experiments on five QA benchmarks with four open-sourced LLMs show that SPS not only enhances performance across a range of tasks but also provides a principled perspective on the interaction between retrieval and generation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-08T00:13:48Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    0,
                    13,
                    48,
                    4,
                    220,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026 Oral. Project link: https://zhanghao-aaai2026-sps.github.io/AAAI2026-SPS/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhanghao Hu"
                    },
                    {
                        "name": "Qinglin Zhu"
                    },
                    {
                        "name": "Siya Qi"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Lin Gui"
                    }
                ],
                "author_detail": {
                    "name": "Lin Gui"
                },
                "author": "Lin Gui"
            },
            {
                "id": "http://arxiv.org/abs/2511.10291v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10291v1",
                "title": "Causal Model-Based Reinforcement Learning for Sample-Efficient IoT Channel Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Model-Based Reinforcement Learning for Sample-Efficient IoT Channel Access"
                },
                "updated": "2025-11-13T13:26:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    26,
                    33,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10291v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite the advantages of multi-agent reinforcement learning (MARL) for wireless use case such as medium access control (MAC), their real-world deployment in Internet of Things (IoT) is hindered by their sample inefficiency. To alleviate this challenge, one can leverage model-based reinforcement learning (MBRL) solutions, however, conventional MBRL approaches rely on black-box models that are not interpretable and cannot reason. In contrast, in this paper, a novel causal model-based MARL framework is developed by leveraging tools from causal learn- ing. In particular, the proposed model can explicitly represent causal dependencies between network variables using structural causal models (SCMs) and attention-based inference networks. Interpretable causal models are then developed to capture how MAC control messages influence observations, how transmission actions determine outcomes, and how channel observations affect rewards. Data augmentation techniques are then used to generate synthetic rollouts using the learned causal model for policy optimization via proximal policy optimization (PPO). Analytical results demonstrate exponential sample complexity gains of causal MBRL over black-box approaches. Extensive simulations demonstrate that, on average, the proposed approach can reduce environment interactions by 58%, and yield faster convergence compared to model-free baselines. The proposed approach inherently is also shown to provide interpretable scheduling decisions via attention-based causal attribution, revealing which network conditions drive the policy. The resulting combination of sample efficiency and interpretability establishes causal MBRL as a practical approach for resource-constrained wireless systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the advantages of multi-agent reinforcement learning (MARL) for wireless use case such as medium access control (MAC), their real-world deployment in Internet of Things (IoT) is hindered by their sample inefficiency. To alleviate this challenge, one can leverage model-based reinforcement learning (MBRL) solutions, however, conventional MBRL approaches rely on black-box models that are not interpretable and cannot reason. In contrast, in this paper, a novel causal model-based MARL framework is developed by leveraging tools from causal learn- ing. In particular, the proposed model can explicitly represent causal dependencies between network variables using structural causal models (SCMs) and attention-based inference networks. Interpretable causal models are then developed to capture how MAC control messages influence observations, how transmission actions determine outcomes, and how channel observations affect rewards. Data augmentation techniques are then used to generate synthetic rollouts using the learned causal model for policy optimization via proximal policy optimization (PPO). Analytical results demonstrate exponential sample complexity gains of causal MBRL over black-box approaches. Extensive simulations demonstrate that, on average, the proposed approach can reduce environment interactions by 58%, and yield faster convergence compared to model-free baselines. The proposed approach inherently is also shown to provide interpretable scheduling decisions via attention-based causal attribution, revealing which network conditions drive the policy. The resulting combination of sample efficiency and interpretability establishes causal MBRL as a practical approach for resource-constrained wireless systems."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:26:33Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    26,
                    33,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Aswin Arun"
                    },
                    {
                        "name": "Christo Kurisummoottil Thomas"
                    },
                    {
                        "name": "Rimalpudi Sarvendranath"
                    },
                    {
                        "name": "Walid Saad"
                    }
                ],
                "author_detail": {
                    "name": "Walid Saad"
                },
                "author": "Walid Saad"
            },
            {
                "id": "http://arxiv.org/abs/2503.02947v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.02947v2",
                "title": "Evidence for a hot galactic halo around the Andromeda Galaxy using fast radio bursts along two sightlines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence for a hot galactic halo around the Andromeda Galaxy using fast radio bursts along two sightlines"
                },
                "updated": "2025-11-13T13:20:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    20,
                    24,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.02947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.02947v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.3847/1538-4357/ae1014",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Fast Radio Bursts (FRBs) are millisecond-duration radio transients that serve as unique probes of ionized extragalactic matter. We report the discovery and localization of two FRBs piercing the Andromeda galaxy (M31) with the realfast transient-detection system at the Very Large Array. These unique sightlines enable constraints on M31's electron density distribution. We localized FRB 20230930A to a host galaxy at redshift $z=0.0925$ and FRB 20230506C to a host galaxy at redshift $z=0.3896$. After accounting for the dispersion contributions from the Milky Way, the host galaxies, and the intergalactic medium, we estimate M31's contribution to be $26-239$ $\\rm pc~cm^{-3}$ toward FRB 20230930A and $51-366$ $\\rm pc~cm^{-3}$ toward FRB 20230506C, within the 90% credible interval (CI). By modeling the M31 disk's contribution, we isolate the halo component and find that M31's halo contributes $7-169$ $\\rm pc~cm^{-3}$ along FRB 20230930A (90% CI). The inferred values of $\\rm DM_{M31,halo}$ from the FRBs are consistent with predictions from a modified Navarro-Frenk-White (mNFW) profile at the corresponding impact parameter. The cool and warm phase gas is unlikely to account for the $\\rm DM_{M31,halo}$ unless the ionization fraction is as high as 90%. While limited to two sightlines, these results offer tentative evidence for the existence of a hot halo surrounding M31. We also discuss the potential contribution of other foreground structures, particularly in explaining the DM excess observed in FRB 20230506C. This work demonstrates how FRBs can be used to probe the circumgalactic medium of intervening galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Radio Bursts (FRBs) are millisecond-duration radio transients that serve as unique probes of ionized extragalactic matter. We report the discovery and localization of two FRBs piercing the Andromeda galaxy (M31) with the realfast transient-detection system at the Very Large Array. These unique sightlines enable constraints on M31's electron density distribution. We localized FRB 20230930A to a host galaxy at redshift $z=0.0925$ and FRB 20230506C to a host galaxy at redshift $z=0.3896$. After accounting for the dispersion contributions from the Milky Way, the host galaxies, and the intergalactic medium, we estimate M31's contribution to be $26-239$ $\\rm pc~cm^{-3}$ toward FRB 20230930A and $51-366$ $\\rm pc~cm^{-3}$ toward FRB 20230506C, within the 90% credible interval (CI). By modeling the M31 disk's contribution, we isolate the halo component and find that M31's halo contributes $7-169$ $\\rm pc~cm^{-3}$ along FRB 20230930A (90% CI). The inferred values of $\\rm DM_{M31,halo}$ from the FRBs are consistent with predictions from a modified Navarro-Frenk-White (mNFW) profile at the corresponding impact parameter. The cool and warm phase gas is unlikely to account for the $\\rm DM_{M31,halo}$ unless the ionization fraction is as high as 90%. While limited to two sightlines, these results offer tentative evidence for the existence of a hot halo surrounding M31. We also discuss the potential contribution of other foreground structures, particularly in explaining the DM excess observed in FRB 20230506C. This work demonstrates how FRBs can be used to probe the circumgalactic medium of intervening galaxies."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-04T19:08:11Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    8,
                    11,
                    1,
                    63,
                    0
                ],
                "arxiv_comment": "Published in ApJ",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "arxiv_journal_ref": "The Astrophysical Journal, 993, 221, 2025",
                "authors": [
                    {
                        "name": "Reshma Anna-Thomas"
                    },
                    {
                        "name": "Casey J. Law"
                    },
                    {
                        "name": "Eric W. Koch"
                    },
                    {
                        "name": "Alexa C. Gordon"
                    },
                    {
                        "name": "Kritti Sharma"
                    },
                    {
                        "name": "Benjamin F. Williams"
                    },
                    {
                        "name": "Nickolas M. Pingel"
                    },
                    {
                        "name": "Sarah Burke-Spolaor"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Jordan Stanley"
                    },
                    {
                        "name": "Calvin Dear"
                    },
                    {
                        "name": "Frank Verdi"
                    },
                    {
                        "name": "J. Xavier Prochaska"
                    },
                    {
                        "name": "Geoffrey C. Bower"
                    },
                    {
                        "name": "Laura Chomiuk"
                    },
                    {
                        "name": "Liam Connor"
                    },
                    {
                        "name": "Paul B. Demorest"
                    },
                    {
                        "name": "Wen-Fai Fong"
                    },
                    {
                        "name": "Anya Nugent"
                    },
                    {
                        "name": "Fabian Walter"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Walter"
                },
                "author": "Fabian Walter",
                "arxiv_doi": "10.3847/1538-4357/ae1014"
            },
            {
                "id": "http://arxiv.org/abs/2511.10284v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10284v1",
                "title": "Beyond Verification: Abductive Explanations for Post-AI Assessment of Privacy Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Verification: Abductive Explanations for Post-AI Assessment of Privacy Leakage"
                },
                "updated": "2025-11-13T13:14:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    14,
                    24,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10284v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Privacy leakage in AI-based decision processes poses significant risks, particularly when sensitive information can be inferred. We propose a formal framework to audit privacy leakage using abductive explanations, which identifies minimal sufficient evidence justifying model decisions and determines whether sensitive information disclosed. Our framework formalizes both individual and system-level leakage, introducing the notion of Potentially Applicable Explanations (PAE) to identify individuals whose outcomes can shield those with sensitive features. This approach provides rigorous privacy guarantees while producing human understandable explanations, a key requirement for auditing tools. Experimental evaluation on the German Credit Dataset illustrates how the importance of sensitive literal in the model decision process affects privacy leakage. Despite computational challenges and simplifying assumptions, our results demonstrate that abductive reasoning enables interpretable privacy auditing, offering a practical pathway to reconcile transparency, model interpretability, and privacy preserving in AI decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy leakage in AI-based decision processes poses significant risks, particularly when sensitive information can be inferred. We propose a formal framework to audit privacy leakage using abductive explanations, which identifies minimal sufficient evidence justifying model decisions and determines whether sensitive information disclosed. Our framework formalizes both individual and system-level leakage, introducing the notion of Potentially Applicable Explanations (PAE) to identify individuals whose outcomes can shield those with sensitive features. This approach provides rigorous privacy guarantees while producing human understandable explanations, a key requirement for auditing tools. Experimental evaluation on the German Credit Dataset illustrates how the importance of sensitive literal in the model decision process affects privacy leakage. Despite computational challenges and simplifying assumptions, our results demonstrate that abductive reasoning enables interpretable privacy auditing, offering a practical pathway to reconcile transparency, model interpretability, and privacy preserving in AI decision-making."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:14:24Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    14,
                    24,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted at the Workshop on Post-AI Formal Methods at AAAI-26",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Belona Sonna"
                    },
                    {
                        "name": "Alban Grastien"
                    },
                    {
                        "name": "Claire Benn"
                    }
                ],
                "author_detail": {
                    "name": "Claire Benn"
                },
                "author": "Claire Benn"
            },
            {
                "id": "http://arxiv.org/abs/2511.10283v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10283v1",
                "title": "Behavior Modeling for Training-free Building of Private Domain Multi Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavior Modeling for Training-free Building of Private Domain Multi Agent System"
                },
                "updated": "2025-11-13T13:14:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    14,
                    6,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10283v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rise of agentic systems that combine orchestration, tool use, and conversational capabilities, has been more visible by the recent advent of large language models (LLMs). While open-domain frameworks exist, applying them in private domains remains difficult due to heterogeneous tool formats, domain-specific jargon, restricted accessibility of APIs, and complex governance. Conventional solutions, such as fine-tuning on synthetic dialogue data, are burdensome and brittle under domain shifts, and risk degrading general performance. In this light, we introduce a framework for private-domain multi-agent conversational systems that avoids training and data generation by adopting behavior modeling and documentation. Our design simply assumes an orchestrator, a tool-calling agent, and a general chat agent, with tool integration defined through structured specifications and domain-informed instructions. This approach enables scalable adaptation to private tools and evolving contexts without continual retraining. The framework supports practical use cases, including lightweight deployment of multi-agent systems, leveraging API specifications as retrieval resources, and generating synthetic dialogue for evaluation -- providing a sustainable method for aligning agent behavior with domain expertise in private conversational ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of agentic systems that combine orchestration, tool use, and conversational capabilities, has been more visible by the recent advent of large language models (LLMs). While open-domain frameworks exist, applying them in private domains remains difficult due to heterogeneous tool formats, domain-specific jargon, restricted accessibility of APIs, and complex governance. Conventional solutions, such as fine-tuning on synthetic dialogue data, are burdensome and brittle under domain shifts, and risk degrading general performance. In this light, we introduce a framework for private-domain multi-agent conversational systems that avoids training and data generation by adopting behavior modeling and documentation. Our design simply assumes an orchestrator, a tool-calling agent, and a general chat agent, with tool integration defined through structured specifications and domain-informed instructions. This approach enables scalable adaptation to private tools and evolving contexts without continual retraining. The framework supports practical use cases, including lightweight deployment of multi-agent systems, leveraging API specifications as retrieval resources, and generating synthetic dialogue for evaluation -- providing a sustainable method for aligning agent behavior with domain expertise in private conversational ecosystems."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:14:06Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    14,
                    6,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "10 pages, 1 figure, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Won Ik Cho"
                    },
                    {
                        "name": "Woonghee Han"
                    },
                    {
                        "name": "Kyung Seo Ki"
                    },
                    {
                        "name": "Young Min Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young Min Kim"
                },
                "author": "Young Min Kim"
            },
            {
                "id": "http://arxiv.org/abs/2511.10281v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10281v1",
                "title": "FactGuard: Event-Centric and Commonsense-Guided Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FactGuard: Event-Centric and Commonsense-Guided Fake News Detection"
                },
                "updated": "2025-11-13T13:11:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    11,
                    42,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10281v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fake news detection methods based on writing style have achieved remarkable progress. However, as adversaries increasingly imitate the style of authentic news, the effectiveness of such approaches is gradually diminishing. Recent research has explored incorporating large language models (LLMs) to enhance fake news detection. Yet, despite their transformative potential, LLMs remain an untapped goldmine for fake news detection, with their real-world adoption hampered by shallow functionality exploration, ambiguous usability, and prohibitive inference costs. In this paper, we propose a novel fake news detection framework, dubbed FactGuard, that leverages LLMs to extract event-centric content, thereby reducing the impact of writing style on detection performance. Furthermore, our approach introduces a dynamic usability mechanism that identifies contradictions and ambiguous cases in factual reasoning, adaptively incorporating LLM advice to improve decision reliability. To ensure efficiency and practical deployment, we employ knowledge distillation to derive FactGuard-D, enabling the framework to operate effectively in cold-start and resource-constrained scenarios. Comprehensive experiments on two benchmark datasets demonstrate that our approach consistently outperforms existing methods in both robustness and accuracy, effectively addressing the challenges of style sensitivity and LLM usability in fake news detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fake news detection methods based on writing style have achieved remarkable progress. However, as adversaries increasingly imitate the style of authentic news, the effectiveness of such approaches is gradually diminishing. Recent research has explored incorporating large language models (LLMs) to enhance fake news detection. Yet, despite their transformative potential, LLMs remain an untapped goldmine for fake news detection, with their real-world adoption hampered by shallow functionality exploration, ambiguous usability, and prohibitive inference costs. In this paper, we propose a novel fake news detection framework, dubbed FactGuard, that leverages LLMs to extract event-centric content, thereby reducing the impact of writing style on detection performance. Furthermore, our approach introduces a dynamic usability mechanism that identifies contradictions and ambiguous cases in factual reasoning, adaptively incorporating LLM advice to improve decision reliability. To ensure efficiency and practical deployment, we employ knowledge distillation to derive FactGuard-D, enabling the framework to operate effectively in cold-start and resource-constrained scenarios. Comprehensive experiments on two benchmark datasets demonstrate that our approach consistently outperforms existing methods in both robustness and accuracy, effectively addressing the challenges of style sensitivity and LLM usability in fake news detection."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:11:42Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    11,
                    42,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jing He"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Yuanhui Xiao"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Shaowen Yao"
                    },
                    {
                        "name": "Renyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Renyang Liu"
                },
                "author": "Renyang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.10279v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10279v1",
                "title": "PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning"
                },
                "updated": "2025-11-13T13:06:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    6,
                    12,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10279v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite significant progress, Vision-Language Models (VLMs) still struggle with complex visual reasoning, where multi-step dependencies cause early errors to cascade through the reasoning chain. Existing post-training paradigms are limited: Supervised Fine-Tuning (SFT) relies on costly step-level annotations, while Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO provide only sparse, outcome-level feedback, hindering stable optimization. We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations. To overcome the cold-start problem, PROPA interleaves GRPO updates with SFT, enabling the model to learn from both successful and failed reasoning trajectories. A Process Reward Model (PRM) is further trained to guide inference-time search, aligning the test-time search with the training signal. Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines. It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks. The code isavailable at: https://github.com/YanbeiJiang/PROPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress, Vision-Language Models (VLMs) still struggle with complex visual reasoning, where multi-step dependencies cause early errors to cascade through the reasoning chain. Existing post-training paradigms are limited: Supervised Fine-Tuning (SFT) relies on costly step-level annotations, while Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO provide only sparse, outcome-level feedback, hindering stable optimization. We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations. To overcome the cold-start problem, PROPA interleaves GRPO updates with SFT, enabling the model to learn from both successful and failed reasoning trajectories. A Process Reward Model (PRM) is further trained to guide inference-time search, aligning the test-time search with the training signal. Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines. It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks. The code isavailable at: https://github.com/YanbeiJiang/PROPA."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:06:12Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    6,
                    12,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yanbei Jiang"
                    },
                    {
                        "name": "Chao Lei"
                    },
                    {
                        "name": "Yihao Ding"
                    },
                    {
                        "name": "Krista Ehinger"
                    },
                    {
                        "name": "Jey Han Lau"
                    }
                ],
                "author_detail": {
                    "name": "Jey Han Lau"
                },
                "author": "Jey Han Lau"
            },
            {
                "id": "http://arxiv.org/abs/2511.10277v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10277v1",
                "title": "Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware"
                },
                "updated": "2025-11-13T13:03:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    3,
                    37,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10277v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet their applicability to dialogue systems in computer games remains limited. This limitation arises from their substantial hardware requirements, latency constraints, and the necessity to maintain clearly defined knowledge boundaries within a game setting. In this paper, we propose a modular NPC dialogue system that leverages Small Language Models (SLMs), fine-tuned to encode specific NPC personas and integrated with runtime-swappable memory modules. These memory modules preserve character-specific conversational context and world knowledge, enabling expressive interactions and long-term memory without retraining or model reloading during gameplay. We comprehensively evaluate our system using three open-source SLMs: DistilGPT-2, TinyLlama-1.1B-Chat, and Mistral-7B-Instruct, trained on synthetic persona-aligned data and benchmarked on consumer-grade hardware. While our approach is motivated by applications in gaming, its modular design and persona-driven memory architecture hold significant potential for broader adoption in domains requiring expressive, scalable, and memory-rich conversational agents, such as virtual assistants, customer support bots, or interactive educational systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet their applicability to dialogue systems in computer games remains limited. This limitation arises from their substantial hardware requirements, latency constraints, and the necessity to maintain clearly defined knowledge boundaries within a game setting. In this paper, we propose a modular NPC dialogue system that leverages Small Language Models (SLMs), fine-tuned to encode specific NPC personas and integrated with runtime-swappable memory modules. These memory modules preserve character-specific conversational context and world knowledge, enabling expressive interactions and long-term memory without retraining or model reloading during gameplay. We comprehensively evaluate our system using three open-source SLMs: DistilGPT-2, TinyLlama-1.1B-Chat, and Mistral-7B-Instruct, trained on synthetic persona-aligned data and benchmarked on consumer-grade hardware. While our approach is motivated by applications in gaming, its modular design and persona-driven memory architecture hold significant potential for broader adoption in domains requiring expressive, scalable, and memory-rich conversational agents, such as virtual assistants, customer support bots, or interactive educational systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:03:37Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    3,
                    37,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Martin Braas"
                    },
                    {
                        "name": "Lukas Esterle"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Esterle"
                },
                "author": "Lukas Esterle"
            },
            {
                "id": "http://arxiv.org/abs/2511.10271v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10271v1",
                "title": "Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics"
                },
                "updated": "2025-11-13T12:56:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    56,
                    7,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10271v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:56:07Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    56,
                    7,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Daniel Ståhl"
                    },
                    {
                        "name": "Kristian Sandahl"
                    },
                    {
                        "name": "Christoph Kessler"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Kessler"
                },
                "author": "Christoph Kessler"
            },
            {
                "id": "http://arxiv.org/abs/2508.05615v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.05615v2",
                "title": "Test-Time Reinforcement Learning for GUI Grounding via Region Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Reinforcement Learning for GUI Grounding via Region Consistency"
                },
                "updated": "2025-11-13T12:54:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    54,
                    36,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.05615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.05615v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Graphical User Interface (GUI) grounding, the task of mapping natural language instructions to precise screen coordinates, is fundamental to autonomous GUI agents. While existing methods achieve strong performance through extensive supervised training or reinforcement learning with labeled rewards, they remain constrained by the cost and availability of pixel-level annotations. We observe that when models generate multiple predictions for the same GUI element, the spatial overlap patterns reveal implicit confidence signals that can guide more accurate localization. Leveraging this insight, we propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions where models show highest agreement. Without any training, GUI-RC improves accuracy by 2-3% across various architectures on ScreenSpot benchmarks. We further introduce GUI-RCPO (Region Consistency Policy Optimization), transforming these consistency patterns into rewards for test-time reinforcement learning. By computing how well each prediction aligns with the collective consensus, GUI-RCPO enables models to iteratively refine their outputs on unlabeled data during inference. Extensive experiments demonstrate the generality of our approach: using only 1,272 unlabeled data, GUI-RCPO achieves 3-6% accuracy improvements across various architectures on ScreenSpot benchmarks. Our approach reveals the untapped potential of test-time scaling and test-time reinforcement learning for GUI grounding, offering a promising path toward more data-efficient GUI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical User Interface (GUI) grounding, the task of mapping natural language instructions to precise screen coordinates, is fundamental to autonomous GUI agents. While existing methods achieve strong performance through extensive supervised training or reinforcement learning with labeled rewards, they remain constrained by the cost and availability of pixel-level annotations. We observe that when models generate multiple predictions for the same GUI element, the spatial overlap patterns reveal implicit confidence signals that can guide more accurate localization. Leveraging this insight, we propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions where models show highest agreement. Without any training, GUI-RC improves accuracy by 2-3% across various architectures on ScreenSpot benchmarks. We further introduce GUI-RCPO (Region Consistency Policy Optimization), transforming these consistency patterns into rewards for test-time reinforcement learning. By computing how well each prediction aligns with the collective consensus, GUI-RCPO enables models to iteratively refine their outputs on unlabeled data during inference. Extensive experiments demonstrate the generality of our approach: using only 1,272 unlabeled data, GUI-RCPO achieves 3-6% accuracy improvements across various architectures on ScreenSpot benchmarks. Our approach reveals the untapped potential of test-time scaling and test-time reinforcement learning for GUI grounding, offering a promising path toward more data-efficient GUI agents."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-07T17:54:27Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    54,
                    27,
                    3,
                    219,
                    0
                ],
                "arxiv_comment": "[Accepted by AAAI2026] Project Page: https://zju-real.github.io/gui-rcpo Code: https://github.com/zju-real/gui-rcpo",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yong Du"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Fei Tang"
                    },
                    {
                        "name": "Zhengxi Lu"
                    },
                    {
                        "name": "Chang Zong"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Shengpei Jiang"
                    },
                    {
                        "name": "Yongliang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yongliang Shen"
                },
                "author": "Yongliang Shen"
            },
            {
                "id": "http://arxiv.org/abs/2511.10269v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10269v1",
                "title": "Fundamentals of interior modelling and challenges in the interpretation of observed rocky exoplanets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of interior modelling and challenges in the interpretation of observed rocky exoplanets"
                },
                "updated": "2025-11-13T12:53:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    53,
                    41,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10269v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Most our knowledge about rocky exoplanets is based on their measure of mass and radius. These two parameters are routinely measured and are used to categorise different populations of observed exoplanets. They are also tightly linked to the planet's properties, in particular those of the interior. As such they offer the unique opportunity to interpret the observations and potentially infer the planet's chemistry and structure. Required for the interpretation are models of planetary interiors, calculated a priori, constrained using other available data, and based on the physiochemical properties of mineralogical phases. This article offers an overview of the current knowledge about exoplanet interiors, the fundamental aspects and tools for interior modelling and how to improve the contraints on the models, along with a discussion on the sources of uncertainty. The origin and fate of volatiles, and their role in planetary evolution is discussed. The chemistry and structure of planetary interiors have a pivotal role in the thermal evolution of planets and the development of large scale properties that might become observables with future space missions and ground-based surveys. As such, having reliable and well constrained interior models is of the utmost importance for the advancement of the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most our knowledge about rocky exoplanets is based on their measure of mass and radius. These two parameters are routinely measured and are used to categorise different populations of observed exoplanets. They are also tightly linked to the planet's properties, in particular those of the interior. As such they offer the unique opportunity to interpret the observations and potentially infer the planet's chemistry and structure. Required for the interpretation are models of planetary interiors, calculated a priori, constrained using other available data, and based on the physiochemical properties of mineralogical phases. This article offers an overview of the current knowledge about exoplanet interiors, the fundamental aspects and tools for interior modelling and how to improve the contraints on the models, along with a discussion on the sources of uncertainty. The origin and fate of volatiles, and their role in planetary evolution is discussed. The chemistry and structure of planetary interiors have a pivotal role in the thermal evolution of planets and the development of large scale properties that might become observables with future space missions and ground-based surveys. As such, having reliable and well constrained interior models is of the utmost importance for the advancement of the field."
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:53:41Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    53,
                    41,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "58 pages, 9 figures, 1 table. Accepted for publication in Space Science Reviews",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP"
                },
                "authors": [
                    {
                        "name": "Philipp Baumeister"
                    },
                    {
                        "name": "Francesca Miozzi"
                    },
                    {
                        "name": "Claire Marie Guimond"
                    },
                    {
                        "name": "Marie-Luise Steinmeyer"
                    },
                    {
                        "name": "Caroline Dorn"
                    },
                    {
                        "name": "Shun-Ichiro Karato"
                    },
                    {
                        "name": "Emeline Bolmont"
                    },
                    {
                        "name": "Alexandre Revol"
                    },
                    {
                        "name": "Alexander Thamm"
                    },
                    {
                        "name": "Lena Noack"
                    }
                ],
                "author_detail": {
                    "name": "Lena Noack"
                },
                "author": "Lena Noack"
            },
            {
                "id": "http://arxiv.org/abs/2211.01921v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2211.01921v5",
                "title": "Asymptotic Theory of Principal Component Analysis for High-Dimensional Time Series Data under a Factor Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotic Theory of Principal Component Analysis for High-Dimensional Time Series Data under a Factor Structure"
                },
                "updated": "2025-11-13T12:51:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    51,
                    55,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2211.01921v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2211.01921v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We review Principal Components (PC) estimation of a large approximate factor model for a panel of n stationary time series and we provide new derivations of the asymptotic properties of the estimators, which are derived under a minimal set of assumptions requiring only the existence of 4th order moments. To this end, we also review various alternative sets of primitive sufficient conditions for mean-squared consistency of the sample covariance matrix. Finally, we discuss in detail the issue of identification of the loadings and factors as well as its implications for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We review Principal Components (PC) estimation of a large approximate factor model for a panel of n stationary time series and we provide new derivations of the asymptotic properties of the estimators, which are derived under a minimal set of assumptions requiring only the existence of 4th order moments. To this end, we also review various alternative sets of primitive sufficient conditions for mean-squared consistency of the sample covariance matrix. Finally, we discuss in detail the issue of identification of the loadings and factors as well as its implications for inference."
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2022-11-03T16:01:49Z",
                "published_parsed": [
                    2022,
                    11,
                    3,
                    16,
                    1,
                    49,
                    3,
                    307,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM"
                },
                "authors": [
                    {
                        "name": "Matteo Barigozzi"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Barigozzi"
                },
                "author": "Matteo Barigozzi"
            },
            {
                "id": "http://arxiv.org/abs/2511.10262v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10262v1",
                "title": "MTR-DuplexBench: Towards a Comprehensive Evaluation of Multi-Round Conversations for Full-Duplex Speech Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTR-DuplexBench: Towards a Comprehensive Evaluation of Multi-Round Conversations for Full-Duplex Speech Language Models"
                },
                "updated": "2025-11-13T12:50:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    50,
                    4,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10262v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Full-Duplex Speech Language Models (FD-SLMs) enable real-time, overlapping conversational interactions, offering a more dynamic user experience compared to traditional half-duplex models. However, existing benchmarks primarily focus on evaluating single-round interactions and conversational features, neglecting the complexities of multi-round communication and critical capabilities such as instruction following and safety. Evaluating FD-SLMs in multi-round settings poses significant challenges, including blurred turn boundaries in communication and context inconsistency during model inference. To address these gaps, we introduce MTR-DuplexBench, a novel benchmark that segments continuous full-duplex dialogues into discrete turns, enabling comprehensive, turn-by-turn evaluation of FD-SLMs across dialogue quality, conversational dynamics, instruction following, and safety. Experimental results reveal that current FD-SLMs face difficulties in maintaining consistent performance across multiple rounds and evaluation dimensions, highlighting the necessity and effectiveness of our proposed benchmark. The benchmark and code will be available in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-Duplex Speech Language Models (FD-SLMs) enable real-time, overlapping conversational interactions, offering a more dynamic user experience compared to traditional half-duplex models. However, existing benchmarks primarily focus on evaluating single-round interactions and conversational features, neglecting the complexities of multi-round communication and critical capabilities such as instruction following and safety. Evaluating FD-SLMs in multi-round settings poses significant challenges, including blurred turn boundaries in communication and context inconsistency during model inference. To address these gaps, we introduce MTR-DuplexBench, a novel benchmark that segments continuous full-duplex dialogues into discrete turns, enabling comprehensive, turn-by-turn evaluation of FD-SLMs across dialogue quality, conversational dynamics, instruction following, and safety. Experimental results reveal that current FD-SLMs face difficulties in maintaining consistent performance across multiple rounds and evaluation dimensions, highlighting the necessity and effectiveness of our proposed benchmark. The benchmark and code will be available in the future."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:50:04Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    50,
                    4,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Work in progress",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "He Zhang"
                    },
                    {
                        "name": "Wenqian Cui"
                    },
                    {
                        "name": "Haoning Xu"
                    },
                    {
                        "name": "Xiaohui Li"
                    },
                    {
                        "name": "Lei Zhu"
                    },
                    {
                        "name": "Shaohua Ma"
                    },
                    {
                        "name": "Irwin King"
                    }
                ],
                "author_detail": {
                    "name": "Irwin King"
                },
                "author": "Irwin King"
            },
            {
                "id": "http://arxiv.org/abs/2506.19794v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.19794v5",
                "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study"
                },
                "updated": "2025-11-13T12:47:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    47,
                    18,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.19794v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.19794v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-24T17:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    4,
                    23,
                    1,
                    175,
                    0
                ],
                "arxiv_comment": "AAAI 2026 (oral)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Yi Zhong"
                    },
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Ziheng Zhang"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.07250v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07250v2",
                "title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs"
                },
                "updated": "2025-11-13T12:42:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    42,
                    38,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07250v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T16:02:33Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    16,
                    2,
                    33,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "arxiv_journal_ref": "The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)",
                "authors": [
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Haochen Wang"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Gavin Chang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Shihao Li"
                    },
                    {
                        "name": "Yanghai Wang"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Steven Huang"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Liu"
                },
                "author": "Jiaheng Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.10240v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10240v1",
                "title": "ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs"
                },
                "updated": "2025-11-13T12:14:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    14,
                    36,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10240v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:14:36Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    14,
                    36,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Minbae Park"
                    },
                    {
                        "name": "Hyemin Yang"
                    },
                    {
                        "name": "Jeonghyun Kim"
                    },
                    {
                        "name": "Kunsoo Park"
                    },
                    {
                        "name": "Hyunjoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunjoon Kim"
                },
                "author": "Hyunjoon Kim"
            },
            {
                "id": "http://arxiv.org/abs/2511.10234v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10234v1",
                "title": "Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners"
                },
                "updated": "2025-11-13T12:06:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    6,
                    12,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10234v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While promising, graph reasoners based on Large Language Models (LLMs) lack built-in invariance to symmetries in graph representations. Operating on sequential graph serializations, LLMs can produce different outputs under node reindexing, edge reordering, or formatting changes, raising robustness concerns. We systematically analyze these effects, studying how fine-tuning impacts encoding sensitivity as well generalization on unseen tasks. We propose a principled decomposition of graph serializations into node labeling, edge encoding, and syntax, and evaluate LLM robustness to variations of each of these factors on a comprehensive benchmarking suite. We also contribute a novel set of spectral tasks to further assess generalization abilities of fine-tuned reasoners. Results show that larger (non-fine-tuned) models are more robust. Fine-tuning reduces sensitivity to node relabeling but may increase it to variations in structure and format, while it does not consistently improve performance on unseen tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While promising, graph reasoners based on Large Language Models (LLMs) lack built-in invariance to symmetries in graph representations. Operating on sequential graph serializations, LLMs can produce different outputs under node reindexing, edge reordering, or formatting changes, raising robustness concerns. We systematically analyze these effects, studying how fine-tuning impacts encoding sensitivity as well generalization on unseen tasks. We propose a principled decomposition of graph serializations into node labeling, edge encoding, and syntax, and evaluate LLM robustness to variations of each of these factors on a comprehensive benchmarking suite. We also contribute a novel set of spectral tasks to further assess generalization abilities of fine-tuned reasoners. Results show that larger (non-fine-tuned) models are more robust. Fine-tuning reduces sensitivity to node relabeling but may increase it to variations in structure and format, while it does not consistently improve performance on unseen tasks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:06:12Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    6,
                    12,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "AAAI 2026 Workshop on Graphs and more Complex Structures For Learning and Reasoning (GCLR)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Daniel Herbst"
                    },
                    {
                        "name": "Lea Karbeska"
                    },
                    {
                        "name": "Divyanshu Kumar"
                    },
                    {
                        "name": "Akanksha Ahuja"
                    },
                    {
                        "name": "Fatemeh Gholamzadeh Nasrabadi"
                    },
                    {
                        "name": "Fabrizio Frasca"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Frasca"
                },
                "author": "Fabrizio Frasca"
            },
            {
                "id": "http://arxiv.org/abs/2511.10233v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10233v1",
                "title": "Bridging Synthetic and Real Routing Problems via LLM-Guided Instance Generation and Progressive Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Synthetic and Real Routing Problems via LLM-Guided Instance Generation and Progressive Adaptation"
                },
                "updated": "2025-11-13T12:06:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    6,
                    11,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10233v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in Neural Combinatorial Optimization (NCO) methods have significantly improved the capability of neural solvers to handle synthetic routing instances. Nonetheless, existing neural solvers typically struggle to generalize effectively from synthetic, uniformly-distributed training data to real-world VRP scenarios, including widely recognized benchmark instances from TSPLib and CVRPLib. To bridge this generalization gap, we present Evolutionary Realistic Instance Synthesis (EvoReal), which leverages an evolutionary module guided by large language models (LLMs) to generate synthetic instances characterized by diverse and realistic structural patterns. Specifically, the evolutionary module produces synthetic instances whose structural attributes statistically mimics those observed in authentic real-world instances. Subsequently, pre-trained NCO models are progressively refined, firstly aligning them with these structurally enriched synthetic distributions and then further adapting them through direct fine-tuning on actual benchmark instances. Extensive experimental evaluations demonstrate that EvoReal markedly improves the generalization capabilities of state-of-the-art neural solvers, yielding a notable reduced performance gap compared to the optimal solutions on the TSPLib (1.05%) and CVRPLib (2.71%) benchmarks across a broad spectrum of problem scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Neural Combinatorial Optimization (NCO) methods have significantly improved the capability of neural solvers to handle synthetic routing instances. Nonetheless, existing neural solvers typically struggle to generalize effectively from synthetic, uniformly-distributed training data to real-world VRP scenarios, including widely recognized benchmark instances from TSPLib and CVRPLib. To bridge this generalization gap, we present Evolutionary Realistic Instance Synthesis (EvoReal), which leverages an evolutionary module guided by large language models (LLMs) to generate synthetic instances characterized by diverse and realistic structural patterns. Specifically, the evolutionary module produces synthetic instances whose structural attributes statistically mimics those observed in authentic real-world instances. Subsequently, pre-trained NCO models are progressively refined, firstly aligning them with these structurally enriched synthetic distributions and then further adapting them through direct fine-tuning on actual benchmark instances. Extensive experimental evaluations demonstrate that EvoReal markedly improves the generalization capabilities of state-of-the-art neural solvers, yielding a notable reduced performance gap compared to the optimal solutions on the TSPLib (1.05%) and CVRPLib (2.71%) benchmarks across a broad spectrum of problem scales."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:06:11Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    6,
                    11,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "21 pages; To be published in AAAI-26",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jianghan Zhu"
                    },
                    {
                        "name": "Yaoxin Wu"
                    },
                    {
                        "name": "Zhuoyi Lin"
                    },
                    {
                        "name": "Zhengyuan Zhang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zhiguang Cao"
                    },
                    {
                        "name": "Senthilnath Jayavelu"
                    },
                    {
                        "name": "Xiaoli Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoli Li"
                },
                "author": "Xiaoli Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.10229v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10229v1",
                "title": "LangGPS: Language Separability Guided Data Pre-Selection for Joint Multilingual Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LangGPS: Language Separability Guided Data Pre-Selection for Joint Multilingual Instruction Tuning"
                },
                "updated": "2025-11-13T12:02:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    2,
                    32,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10229v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Joint multilingual instruction tuning is a widely adopted approach to improve the multilingual instruction-following ability and downstream performance of large language models (LLMs), but the resulting multilingual capability remains highly sensitive to the composition and selection of the training data. Existing selection methods, often based on features like text quality, diversity, or task relevance, typically overlook the intrinsic linguistic structure of multilingual data. In this paper, we propose LangGPS, a lightweight two-stage pre-selection framework guided by language separability which quantifies how well samples in different languages can be distinguished in the model's representation space. LangGPS first filters training data based on separability scores and then refines the subset using existing selection methods. Extensive experiments across six benchmarks and 22 languages demonstrate that applying LangGPS on top of existing selection methods improves their effectiveness and generalizability in multilingual training, especially for understanding tasks and low-resource languages. Further analysis reveals that highly separable samples facilitate the formation of clearer language boundaries and support faster adaptation, while low-separability samples tend to function as bridges for cross-lingual alignment. Besides, we also find that language separability can serve as an effective signal for multilingual curriculum learning, where interleaving samples with diverse separability levels yields stable and generalizable gains. Together, we hope our work offers a new perspective on data utility in multilingual contexts and support the development of more linguistically informed LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint multilingual instruction tuning is a widely adopted approach to improve the multilingual instruction-following ability and downstream performance of large language models (LLMs), but the resulting multilingual capability remains highly sensitive to the composition and selection of the training data. Existing selection methods, often based on features like text quality, diversity, or task relevance, typically overlook the intrinsic linguistic structure of multilingual data. In this paper, we propose LangGPS, a lightweight two-stage pre-selection framework guided by language separability which quantifies how well samples in different languages can be distinguished in the model's representation space. LangGPS first filters training data based on separability scores and then refines the subset using existing selection methods. Extensive experiments across six benchmarks and 22 languages demonstrate that applying LangGPS on top of existing selection methods improves their effectiveness and generalizability in multilingual training, especially for understanding tasks and low-resource languages. Further analysis reveals that highly separable samples facilitate the formation of clearer language boundaries and support faster adaptation, while low-separability samples tend to function as bridges for cross-lingual alignment. Besides, we also find that language separability can serve as an effective signal for multilingual curriculum learning, where interleaving samples with diverse separability levels yields stable and generalizable gains. Together, we hope our work offers a new perspective on data utility in multilingual contexts and support the development of more linguistically informed LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:02:32Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    2,
                    32,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "AAAI2026 Main Track Accepted",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yangfan Ye"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Weitao Ma"
                    },
                    {
                        "name": "Qichen Hong"
                    },
                    {
                        "name": "Yunfei Lu"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin"
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.09557v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09557v2",
                "title": "LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication"
                },
                "updated": "2025-11-13T18:59:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    59,
                    38,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09557v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) continue to grow in size, distributed inference has become increasingly important. Model-parallel strategies must now efficiently scale not only across multiple GPUs but also across multiple nodes. In this work, we present a detailed performance study of multi-node distributed inference using LLMs on GPU-based supercomputers. We conduct experiments with several state-of-the-art inference engines alongside YALIS, a research-oriented prototype engine designed for controlled experimentation. We analyze the strong-scaling behavior of different model-parallel schemes and identify key bottlenecks. Since all-reduce operations are a common performance bottleneck, we develop NVRAR, a hierarchical all-reduce algorithm based on recursive doubling with NVSHMEM. NVRAR achieves up to 1.9x-3.6x lower latency than NCCL for message sizes between 128 KB and 2 MB on HPE Slingshot and InfiniBand interconnects. Integrated into YALIS, NVRAR achieves up to a 1.72x reduction in end-to-end batch latency for the Llama 3.1 405B model in multi-node decode-heavy workloads using tensor parallelism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to grow in size, distributed inference has become increasingly important. Model-parallel strategies must now efficiently scale not only across multiple GPUs but also across multiple nodes. In this work, we present a detailed performance study of multi-node distributed inference using LLMs on GPU-based supercomputers. We conduct experiments with several state-of-the-art inference engines alongside YALIS, a research-oriented prototype engine designed for controlled experimentation. We analyze the strong-scaling behavior of different model-parallel schemes and identify key bottlenecks. Since all-reduce operations are a common performance bottleneck, we develop NVRAR, a hierarchical all-reduce algorithm based on recursive doubling with NVSHMEM. NVRAR achieves up to 1.9x-3.6x lower latency than NCCL for message sizes between 128 KB and 2 MB on HPE Slingshot and InfiniBand interconnects. Integrated into YALIS, NVRAR achieves up to a 1.72x reduction in end-to-end batch latency for the Llama 3.1 405B model in multi-node decode-heavy workloads using tensor parallelism."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T18:59:26Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    18,
                    59,
                    26,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "12 Figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Lannie Dalton Hough"
                    },
                    {
                        "name": "Akarsh Srivastava"
                    },
                    {
                        "name": "Harshitha Menon"
                    },
                    {
                        "name": "Charles Fredrick Jekel"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele"
            },
            {
                "id": "http://arxiv.org/abs/2511.10645v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10645v1",
                "title": "ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference"
                },
                "updated": "2025-11-13T18:59:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    59,
                    24,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10645v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:59:24Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    59,
                    24,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yesheng Liang"
                    },
                    {
                        "name": "Haisheng Chen"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Zhijian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijian Liu"
                },
                "author": "Zhijian Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.10643v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10643v1",
                "title": "Black-Box On-Policy Distillation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-Box On-Policy Distillation of Large Language Models"
                },
                "updated": "2025-11-13T18:58:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    58,
                    37,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10643v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:58:37Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    58,
                    37,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tianzhu Ye"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei"
            },
            {
                "id": "http://arxiv.org/abs/2510.12787v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.12787v3",
                "title": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics"
                },
                "updated": "2025-11-13T18:56:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    56,
                    10,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.12787v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.12787v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Ax-Prover, a multi-agent system for automated theorem proving in Lean that can solve problems across diverse scientific domains and operate either autonomously or collaboratively with human experts. To achieve this, Ax-Prover approaches scientific problem solving through formal proof generation, a process that demands both creative reasoning and strict syntactic rigor. Ax-Prover meets this challenge by equipping Large Language Models (LLMs), which provide knowledge and reasoning, with Lean tools via the Model Context Protocol (MCP), which ensure formal correctness. To evaluate its performance as an autonomous prover, we benchmark our approach against frontier LLMs and specialized prover models on two public math benchmarks and on two Lean benchmarks we introduce in the fields of abstract algebra and quantum theory. On public datasets, Ax-Prover is competitive with state-of-the-art provers, while it largely outperforms them on the new benchmarks. This shows that, unlike specialized systems that struggle to generalize, our tool-based agentic theorem prover approach offers a generalizable methodology for formal verification across diverse scientific domains. Furthermore, we demonstrate Ax-Prover's assistant capabilities in a practical use case, showing how it enabled an expert mathematician to formalize the proof of a complex cryptography theorem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Ax-Prover, a multi-agent system for automated theorem proving in Lean that can solve problems across diverse scientific domains and operate either autonomously or collaboratively with human experts. To achieve this, Ax-Prover approaches scientific problem solving through formal proof generation, a process that demands both creative reasoning and strict syntactic rigor. Ax-Prover meets this challenge by equipping Large Language Models (LLMs), which provide knowledge and reasoning, with Lean tools via the Model Context Protocol (MCP), which ensure formal correctness. To evaluate its performance as an autonomous prover, we benchmark our approach against frontier LLMs and specialized prover models on two public math benchmarks and on two Lean benchmarks we introduce in the fields of abstract algebra and quantum theory. On public datasets, Ax-Prover is competitive with state-of-the-art provers, while it largely outperforms them on the new benchmarks. This shows that, unlike specialized systems that struggle to generalize, our tool-based agentic theorem prover approach offers a generalizable methodology for formal verification across diverse scientific domains. Furthermore, we demonstrate Ax-Prover's assistant capabilities in a practical use case, showing how it enabled an expert mathematician to formalize the proof of a complex cryptography theorem."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-14T17:57:04Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    57,
                    4,
                    1,
                    287,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Benjamin Breen"
                    },
                    {
                        "name": "Marco Del Tredici"
                    },
                    {
                        "name": "Jacob McCarran"
                    },
                    {
                        "name": "Javier Aspuru Mijares"
                    },
                    {
                        "name": "Weichen Winston Yin"
                    },
                    {
                        "name": "Kfir Sulimany"
                    },
                    {
                        "name": "Jacob M. Taylor"
                    },
                    {
                        "name": "Frank H. L. Koppens"
                    },
                    {
                        "name": "Dirk Englund"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Englund"
                },
                "author": "Dirk Englund"
            },
            {
                "id": "http://arxiv.org/abs/2511.10628v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10628v1",
                "title": "Instella: Fully Open Language Models with Stellar Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instella: Fully Open Language Models with Stellar Performance"
                },
                "updated": "2025-11-13T18:52:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    52,
                    46,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10628v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:52:46Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    52,
                    46,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Jialian Wu"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Yusheng Su"
                    },
                    {
                        "name": "Prakamya Mishra"
                    },
                    {
                        "name": "Gowtham Ramesh"
                    },
                    {
                        "name": "Sudhanshu Ranjan"
                    },
                    {
                        "name": "Chaitanya Manem"
                    },
                    {
                        "name": "Ximeng Sun"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Pratik Prabhanjan Brahma"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum"
            },
            {
                "id": "http://arxiv.org/abs/2508.01341v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.01341v2",
                "title": "Debiasing Machine Learning Predictions for Causal Inference Without Additional Ground Truth Data: \"One Map, Many Trials\" in Satellite-Driven Poverty Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiasing Machine Learning Predictions for Causal Inference Without Additional Ground Truth Data: \"One Map, Many Trials\" in Satellite-Driven Poverty Analysis"
                },
                "updated": "2025-11-13T18:48:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    48,
                    8,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.01341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.01341v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Machine learning models trained on Earth observation data, such as satellite imagery, have demonstrated significant promise in predicting household-level wealth indices, enabling the creation of high-resolution wealth maps that can be leveraged across multiple causal trials while addressing chronic data scarcity in global development research. However, because standard training objectives prioritize overall predictive accuracy, these predictions often suffer from shrinkage toward the mean, leading to attenuated estimates of causal treatment effects and limiting their utility in policy evaluations. Existing debiasing methods, such as Prediction-Powered Inference (PPI), can handle this attenuation bias but require additional fresh ground-truth data at the downstream stage of causal inference, which restricts their applicability in data-scarce environments. We introduce and evaluate two post-hoc correction methods -- Linear Calibration Correction (LCC) and a Tweedie's correction approach -- that substantially reduce shrinkage-induced prediction bias without relying on newly collected labeled data. LCC applies a simple linear transformation estimated on a held-out calibration split; Tweedie's method locally de-shrink predictions using density score estimates and a noise scale learned upstream. We provide practical diagnostics for when a correction is warranted and discuss practical limitations. Across analytical results, simulations, and experiments with Demographic and Health Surveys (DHS) data, both approaches reduce attenuation; Tweedie's correction yields nearly unbiased treatment-effect estimates, enabling a \"one map, many trials\" paradigm. Although we demonstrate on EO-ML wealth mapping, the methods are not geospatial-specific: they apply to any setting where imputed outcomes are reused downstream (e.g., pollution indices, population density, or LLM-derived indicators).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models trained on Earth observation data, such as satellite imagery, have demonstrated significant promise in predicting household-level wealth indices, enabling the creation of high-resolution wealth maps that can be leveraged across multiple causal trials while addressing chronic data scarcity in global development research. However, because standard training objectives prioritize overall predictive accuracy, these predictions often suffer from shrinkage toward the mean, leading to attenuated estimates of causal treatment effects and limiting their utility in policy evaluations. Existing debiasing methods, such as Prediction-Powered Inference (PPI), can handle this attenuation bias but require additional fresh ground-truth data at the downstream stage of causal inference, which restricts their applicability in data-scarce environments. We introduce and evaluate two post-hoc correction methods -- Linear Calibration Correction (LCC) and a Tweedie's correction approach -- that substantially reduce shrinkage-induced prediction bias without relying on newly collected labeled data. LCC applies a simple linear transformation estimated on a held-out calibration split; Tweedie's method locally de-shrink predictions using density score estimates and a noise scale learned upstream. We provide practical diagnostics for when a correction is warranted and discuss practical limitations. Across analytical results, simulations, and experiments with Demographic and Health Surveys (DHS) data, both approaches reduce attenuation; Tweedie's correction yields nearly unbiased treatment-effect estimates, enabling a \"one map, many trials\" paradigm. Although we demonstrate on EO-ML wealth mapping, the methods are not geospatial-specific: they apply to any setting where imputed outcomes are reused downstream (e.g., pollution indices, population density, or LLM-derived indicators)."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-02T12:26:26Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    12,
                    26,
                    26,
                    5,
                    214,
                    0
                ],
                "arxiv_comment": "To appear in the Proceedings of AAAI 2026",
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Markus Pettersson"
                    },
                    {
                        "name": "Connor T. Jerzak"
                    },
                    {
                        "name": "Adel Daoud"
                    }
                ],
                "author_detail": {
                    "name": "Adel Daoud"
                },
                "author": "Adel Daoud"
            },
            {
                "id": "http://arxiv.org/abs/2511.10621v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10621v1",
                "title": "SSR: Socratic Self-Refine for Large Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSR: Socratic Self-Refine for Large Language Model Reasoning"
                },
                "updated": "2025-11-13T18:47:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    47,
                    7,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10621v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:47:07Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    47,
                    7,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Preprint; work in progress",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Zeyu Leo Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Semih Yavuz"
                    }
                ],
                "author_detail": {
                    "name": "Semih Yavuz"
                },
                "author": "Semih Yavuz"
            },
            {
                "id": "http://arxiv.org/abs/2511.10615v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10615v1",
                "title": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals"
                },
                "updated": "2025-11-13T18:45:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    45,
                    39,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10615v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:45:39Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    45,
                    39,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "8 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shruti Singh Baghel"
                    },
                    {
                        "name": "Yash Pratap Singh Rathore"
                    },
                    {
                        "name": "Sushovan Jena"
                    },
                    {
                        "name": "Anurag Pradhan"
                    },
                    {
                        "name": "Amit Shukla"
                    },
                    {
                        "name": "Arnav Bhavsar"
                    },
                    {
                        "name": "Pawan Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Goyal"
                },
                "author": "Pawan Goyal"
            },
            {
                "id": "http://arxiv.org/abs/2511.10611v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10611v1",
                "title": "Towards an Agentic Workflow for Internet Measurement Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards an Agentic Workflow for Internet Measurement Research"
                },
                "updated": "2025-11-13T18:44:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    44,
                    9,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10611v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Internet measurement research faces an accessibility crisis: complex analyses require custom integration of multiple specialized tools that demands specialized domain expertise. When network disruptions occur, operators need rapid diagnostic workflows spanning infrastructure mapping, routing analysis, and dependency modeling. However, developing these workflows requires specialized knowledge and significant manual effort.\n  We present ArachNet, the first system demonstrating that LLM agents can independently generate measurement workflows that mimics expert reasoning. Our core insight is that measurement expertise follows predictable compositional patterns that can be systematically automated. ArachNet operates through four specialized agents that mirror expert workflow, from problem decomposition to solution implementation. We validate ArachNet with progressively challenging Internet resilience scenarios. The system independently generates workflows that match expert-level reasoning and produce analytical outputs similar to specialist solutions. Generated workflows handle complex multi-framework integration that traditionally requires days of manual coordination. ArachNet lowers barriers to measurement workflow composition by automating the systematic reasoning process that experts use, enabling broader access to sophisticated measurement capabilities while maintaining the technical rigor required for research-quality analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internet measurement research faces an accessibility crisis: complex analyses require custom integration of multiple specialized tools that demands specialized domain expertise. When network disruptions occur, operators need rapid diagnostic workflows spanning infrastructure mapping, routing analysis, and dependency modeling. However, developing these workflows requires specialized knowledge and significant manual effort.\n  We present ArachNet, the first system demonstrating that LLM agents can independently generate measurement workflows that mimics expert reasoning. Our core insight is that measurement expertise follows predictable compositional patterns that can be systematically automated. ArachNet operates through four specialized agents that mirror expert workflow, from problem decomposition to solution implementation. We validate ArachNet with progressively challenging Internet resilience scenarios. The system independently generates workflows that match expert-level reasoning and produce analytical outputs similar to specialist solutions. Generated workflows handle complex multi-framework integration that traditionally requires days of manual coordination. ArachNet lowers barriers to measurement workflow composition by automating the systematic reasoning process that experts use, enabling broader access to sophisticated measurement capabilities while maintaining the technical rigor required for research-quality analysis."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:44:09Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    44,
                    9,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Alagappan Ramanathan"
                    },
                    {
                        "name": "Eunju Kang"
                    },
                    {
                        "name": "Dongsu Han"
                    },
                    {
                        "name": "Sangeetha Abdu Jyothi"
                    }
                ],
                "author_detail": {
                    "name": "Sangeetha Abdu Jyothi"
                },
                "author": "Sangeetha Abdu Jyothi"
            },
            {
                "id": "http://arxiv.org/abs/2511.00730v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00730v3",
                "title": "Teaching LLMs to See and Guide: Context-Aware Real-Time Assistance in Augmented Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching LLMs to See and Guide: Context-Aware Real-Time Assistance in Augmented Reality"
                },
                "updated": "2025-11-13T18:40:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    40,
                    35,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00730v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00730v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The growing adoption of augmented and virtual reality (AR and VR) technologies in industrial training and on-the-job assistance has created new opportunities for intelligent, context-aware support systems. As workers perform complex tasks guided by AR and VR, these devices capture rich streams of multimodal data, including gaze, hand actions, and task progression, that can reveal user intent and task state in real time. Leveraging this information effectively remains a major challenge. In this work, we present a context-aware large language model (LLM) assistant that integrates diverse data modalities, such as hand actions, task steps, and dialogue history, into a unified framework for real-time question answering. To systematically study how context influences performance, we introduce an incremental prompting framework, where each model version receives progressively richer contextual inputs. Using the HoloAssist dataset, which records AR-guided task executions, we evaluate how each modality contributes to the assistant's effectiveness. Our experiments show that incorporating multimodal context significantly improves the accuracy and relevance of responses. These findings highlight the potential of LLM-driven multimodal integration to enable adaptive, intuitive assistance for AR and VR-based industrial training and assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of augmented and virtual reality (AR and VR) technologies in industrial training and on-the-job assistance has created new opportunities for intelligent, context-aware support systems. As workers perform complex tasks guided by AR and VR, these devices capture rich streams of multimodal data, including gaze, hand actions, and task progression, that can reveal user intent and task state in real time. Leveraging this information effectively remains a major challenge. In this work, we present a context-aware large language model (LLM) assistant that integrates diverse data modalities, such as hand actions, task steps, and dialogue history, into a unified framework for real-time question answering. To systematically study how context influences performance, we introduce an incremental prompting framework, where each model version receives progressively richer contextual inputs. Using the HoloAssist dataset, which records AR-guided task executions, we evaluate how each modality contributes to the assistant's effectiveness. Our experiments show that incorporating multimodal context significantly improves the accuracy and relevance of responses. These findings highlight the potential of LLM-driven multimodal integration to enable adaptive, intuitive assistance for AR and VR-based industrial training and assistance."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-01T22:37:26Z",
                "published_parsed": [
                    2025,
                    11,
                    1,
                    22,
                    37,
                    26,
                    5,
                    305,
                    0
                ],
                "arxiv_comment": "This work is intended for submission to the IEEE Transactions on Systems, Man, and Cybernetics: Systems for possible publication",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Mahya Qorbani"
                    },
                    {
                        "name": "Kamran Paynabar"
                    },
                    {
                        "name": "Mohsen Moghaddam"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Moghaddam"
                },
                "author": "Mohsen Moghaddam"
            },
            {
                "id": "http://arxiv.org/abs/2511.10580v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10580v1",
                "title": "From Fold to Function: Dynamic Modeling and Simulation-Driven Design of Origami Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Fold to Function: Dynamic Modeling and Simulation-Driven Design of Origami Mechanisms"
                },
                "updated": "2025-11-13T18:19:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    19,
                    41,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10580v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Origami-inspired mechanisms can transform flat sheets into functional three-dimensional dynamic structures that are lightweight, compact, and capable of complex motion. These properties make origami increasingly valuable in robotic and deployable systems. However, accurately simulating their folding behavior and interactions with the environment remains challenging. To address this, we present a design framework for origami mechanism simulation that utilizes MuJoCo's deformable-body capabilities. In our approach, origami sheets are represented as graphs of interconnected deformable elements with user-specified constraints such as creases and actuation, defined through an intuitive graphical user interface (GUI). This framework allows users to generate physically consistent simulations that capture both the geometric structure of origami mechanisms and their interactions with external objects and surfaces. We demonstrate our method's utility through a case study on an origami catapult, where design parameters are optimized in simulation using the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and validated experimentally on physical prototypes. The optimized structure achieves improved throwing performance, illustrating how our system enables rapid, simulation-driven origami design, optimization, and analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Origami-inspired mechanisms can transform flat sheets into functional three-dimensional dynamic structures that are lightweight, compact, and capable of complex motion. These properties make origami increasingly valuable in robotic and deployable systems. However, accurately simulating their folding behavior and interactions with the environment remains challenging. To address this, we present a design framework for origami mechanism simulation that utilizes MuJoCo's deformable-body capabilities. In our approach, origami sheets are represented as graphs of interconnected deformable elements with user-specified constraints such as creases and actuation, defined through an intuitive graphical user interface (GUI). This framework allows users to generate physically consistent simulations that capture both the geometric structure of origami mechanisms and their interactions with external objects and surfaces. We demonstrate our method's utility through a case study on an origami catapult, where design parameters are optimized in simulation using the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and validated experimentally on physical prototypes. The optimized structure achieves improved throwing performance, illustrating how our system enables rapid, simulation-driven origami design, optimization, and analysis."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:19:41Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    19,
                    41,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "8 Pages, 9 Figures, Submitted to IEEE RoboSoft",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Tianhui Han"
                    },
                    {
                        "name": "Shashwat Singh"
                    },
                    {
                        "name": "Sarvesh Patil"
                    },
                    {
                        "name": "Zeynep Temel"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Temel"
                },
                "author": "Zeynep Temel"
            },
            {
                "id": "http://arxiv.org/abs/2511.10563v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10563v1",
                "title": "Beamspace Equalization for mmWave Massive MIMO: Algorithms and VLSI Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beamspace Equalization for mmWave Massive MIMO: Algorithms and VLSI Implementations"
                },
                "updated": "2025-11-13T18:02:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    2,
                    33,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10563v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Massive multiuser multiple-input multiple-output (MIMO) and millimeter-wave (mmWave) communication are key physical layer technologies in future wireless systems. Their deployment, however, is expected to incur excessive baseband processing hardware cost and power consumption. Beamspace processing leverages the channel sparsity at mmWave frequencies to reduce baseband processing complexity. In this paper, we review existing beamspace data detection algorithms and propose new algorithms as well as corresponding VLSI architectures that reduce data detection power. We present VLSI implementation results for the proposed architectures in a 22nm FDSOI process. Our results demonstrate that a fully-parallelized implementation of the proposed complex sparsity-adaptive equalizer (CSPADE) achieves up to 54% power savings compared to antenna-domain equalization. Furthermore, our fully-parallelized designs achieve the highest reported throughput among existing massive MIMO data detectors, while achieving better energy and area efficiency. We also present a sequential multiply-accumulate (MAC)-based architecture for CSPADE, which enables even higher power savings, i.e., up to 66%, compared to a MAC-based antenna-domain equalizer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive multiuser multiple-input multiple-output (MIMO) and millimeter-wave (mmWave) communication are key physical layer technologies in future wireless systems. Their deployment, however, is expected to incur excessive baseband processing hardware cost and power consumption. Beamspace processing leverages the channel sparsity at mmWave frequencies to reduce baseband processing complexity. In this paper, we review existing beamspace data detection algorithms and propose new algorithms as well as corresponding VLSI architectures that reduce data detection power. We present VLSI implementation results for the proposed architectures in a 22nm FDSOI process. Our results demonstrate that a fully-parallelized implementation of the proposed complex sparsity-adaptive equalizer (CSPADE) achieves up to 54% power savings compared to antenna-domain equalization. Furthermore, our fully-parallelized designs achieve the highest reported throughput among existing massive MIMO data detectors, while achieving better energy and area efficiency. We also present a sequential multiply-accumulate (MAC)-based architecture for CSPADE, which enables even higher power savings, i.e., up to 66%, compared to a MAC-based antenna-domain equalizer."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T18:02:33Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    18,
                    2,
                    33,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "14 pages",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Seyed Hadi Mirfarshbafan"
                    },
                    {
                        "name": "Christoph Studer"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Studer"
                },
                "author": "Christoph Studer"
            },
            {
                "id": "http://arxiv.org/abs/2511.10552v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10552v1",
                "title": "URaG: Unified Retrieval and Generation in Multimodal LLMs for Efficient Long Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "URaG: Unified Retrieval and Generation in Multimodal LLMs for Efficient Long Document Understanding"
                },
                "updated": "2025-11-13T17:54:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    54,
                    9,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10552v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent multimodal large language models (MLLMs) still struggle with long document understanding due to two fundamental challenges: information interference from abundant irrelevant content, and the quadratic computational cost of Transformer-based architectures. Existing approaches primarily fall into two categories: token compression, which sacrifices fine-grained details; and introducing external retrievers, which increase system complexity and prevent end-to-end optimization. To address these issues, we conduct an in-depth analysis and observe that MLLMs exhibit a human-like coarse-to-fine reasoning pattern: early Transformer layers attend broadly across the document, while deeper layers focus on relevant evidence pages. Motivated by this insight, we posit that the inherent evidence localization capabilities of MLLMs can be explicitly leveraged to perform retrieval during the reasoning process, facilitating efficient long document understanding. To this end, we propose URaG, a simple-yet-effective framework that Unifies Retrieval and Generation within a single MLLM. URaG introduces a lightweight cross-modal retrieval module that converts the early Transformer layers into an efficient evidence selector, identifying and preserving the most relevant pages while discarding irrelevant content. This design enables the deeper layers to concentrate computational resources on pertinent information, improving both accuracy and efficiency. Extensive experiments demonstrate that URaG achieves state-of-the-art performance while reducing computational overhead by 44-56%. The code is available at https://github.com/shi-yx/URaG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent multimodal large language models (MLLMs) still struggle with long document understanding due to two fundamental challenges: information interference from abundant irrelevant content, and the quadratic computational cost of Transformer-based architectures. Existing approaches primarily fall into two categories: token compression, which sacrifices fine-grained details; and introducing external retrievers, which increase system complexity and prevent end-to-end optimization. To address these issues, we conduct an in-depth analysis and observe that MLLMs exhibit a human-like coarse-to-fine reasoning pattern: early Transformer layers attend broadly across the document, while deeper layers focus on relevant evidence pages. Motivated by this insight, we posit that the inherent evidence localization capabilities of MLLMs can be explicitly leveraged to perform retrieval during the reasoning process, facilitating efficient long document understanding. To this end, we propose URaG, a simple-yet-effective framework that Unifies Retrieval and Generation within a single MLLM. URaG introduces a lightweight cross-modal retrieval module that converts the early Transformer layers into an efficient evidence selector, identifying and preserving the most relevant pages while discarding irrelevant content. This design enables the deeper layers to concentrate computational resources on pertinent information, improving both accuracy and efficiency. Extensive experiments demonstrate that URaG achieves state-of-the-art performance while reducing computational overhead by 44-56%. The code is available at https://github.com/shi-yx/URaG."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T17:54:09Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    54,
                    9,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026 (Oral)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yongxin Shi"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Zeyu Shan"
                    },
                    {
                        "name": "Dezhi Peng"
                    },
                    {
                        "name": "Zening Lin"
                    },
                    {
                        "name": "Lianwen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Lianwen Jin"
                },
                "author": "Lianwen Jin"
            },
            {
                "id": "http://arxiv.org/abs/2511.07871v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07871v2",
                "title": "AlignSurvey: A Comprehensive Benchmark for Human Preferences Alignment in Social Surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignSurvey: A Comprehensive Benchmark for Human Preferences Alignment in Social Surveys"
                },
                "updated": "2025-11-13T17:30:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    30,
                    34,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07871v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07871v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding human attitudes, preferences, and behaviors through social surveys is essential for academic research and policymaking. Yet traditional surveys face persistent challenges, including fixed-question formats, high costs, limited adaptability, and difficulties ensuring cross-cultural equivalence. While recent studies explore large language models (LLMs) to simulate survey responses, most are limited to structured questions, overlook the entire survey process, and risks under-representing marginalized groups due to training data biases. We introduce AlignSurvey, the first benchmark that systematically replicates and evaluates the full social survey pipeline using LLMs. It defines four tasks aligned with key survey stages: social role modeling, semi-structured interview modeling, attitude stance modeling and survey response modeling. It also provides task-specific evaluation metrics to assess alignment fidelity, consistency, and fairness at both individual and group levels, with a focus on demographic diversity. To support AlignSurvey, we construct a multi-tiered dataset architecture: (i) the Social Foundation Corpus, a cross-national resource with 44K+ interview dialogues and 400K+ structured survey records; and (ii) a suite of Entire-Pipeline Survey Datasets, including the expert-annotated AlignSurvey-Expert (ASE) and two nationally representative surveys for cross-cultural evaluation. We release the SurveyLM family, obtained through two-stage fine-tuning of open-source LLMs, and offer reference models for evaluating domain-specific alignment. All datasets, models, and tools are available at github and huggingface to support transparent and socially responsible research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding human attitudes, preferences, and behaviors through social surveys is essential for academic research and policymaking. Yet traditional surveys face persistent challenges, including fixed-question formats, high costs, limited adaptability, and difficulties ensuring cross-cultural equivalence. While recent studies explore large language models (LLMs) to simulate survey responses, most are limited to structured questions, overlook the entire survey process, and risks under-representing marginalized groups due to training data biases. We introduce AlignSurvey, the first benchmark that systematically replicates and evaluates the full social survey pipeline using LLMs. It defines four tasks aligned with key survey stages: social role modeling, semi-structured interview modeling, attitude stance modeling and survey response modeling. It also provides task-specific evaluation metrics to assess alignment fidelity, consistency, and fairness at both individual and group levels, with a focus on demographic diversity. To support AlignSurvey, we construct a multi-tiered dataset architecture: (i) the Social Foundation Corpus, a cross-national resource with 44K+ interview dialogues and 400K+ structured survey records; and (ii) a suite of Entire-Pipeline Survey Datasets, including the expert-annotated AlignSurvey-Expert (ASE) and two nationally representative surveys for cross-cultural evaluation. We release the SurveyLM family, obtained through two-stage fine-tuning of open-source LLMs, and offer reference models for evaluating domain-specific alignment. All datasets, models, and tools are available at github and huggingface to support transparent and socially responsible research."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T06:14:21Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    6,
                    14,
                    21,
                    1,
                    315,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chenxi Lin"
                    },
                    {
                        "name": "Weikang Yuan"
                    },
                    {
                        "name": "Zhuoren Jiang"
                    },
                    {
                        "name": "Biao Huang"
                    },
                    {
                        "name": "Ruitao Zhang"
                    },
                    {
                        "name": "Jianan Ge"
                    },
                    {
                        "name": "Yueqian Xu"
                    },
                    {
                        "name": "Jianxing Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jianxing Yu"
                },
                "author": "Jianxing Yu"
            },
            {
                "id": "http://arxiv.org/abs/2511.10526v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10526v1",
                "title": "Evaluation of Grid-based Uncertainty Propagation for Collaborative Self-Calibration in Indoor Positioning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Grid-based Uncertainty Propagation for Collaborative Self-Calibration in Indoor Positioning Systems"
                },
                "updated": "2025-11-13T17:26:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    26,
                    51,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10526v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Radio-based localization systems conventionally require stationary reference points (e.g. anchors) with precisely surveyed positions, making deployment time-consuming and costly. This paper presents an empirical evaluation of collaborative self-calibration for Ultra-Wideband (UWB) networks, extending a discrete Bayesian approach based on grid-based uncertainty propagation. The enhanced algorithm reduces measurement availability requirements while maintaining positioning accuracy through probabilistic state estimation. We validate the approach using real-world data from controlled indoor UWB network experiments with 12 nodes in a static environment. Experimental evaluation demonstrates 0.28~m mean ranging error under line-of-sight conditions and 1.11~m overall ranging error across mixed propagation scenarios, achieving sub-meter positioning accuracy. Results demonstrate the algorithm's robustness to measurement noise and partial connectivity scenarios typical in industrial deployments. The findings contribute to automated UWB network initialization for indoor positioning applications, reducing infrastructure dependency compared to manual anchor calibration procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio-based localization systems conventionally require stationary reference points (e.g. anchors) with precisely surveyed positions, making deployment time-consuming and costly. This paper presents an empirical evaluation of collaborative self-calibration for Ultra-Wideband (UWB) networks, extending a discrete Bayesian approach based on grid-based uncertainty propagation. The enhanced algorithm reduces measurement availability requirements while maintaining positioning accuracy through probabilistic state estimation. We validate the approach using real-world data from controlled indoor UWB network experiments with 12 nodes in a static environment. Experimental evaluation demonstrates 0.28~m mean ranging error under line-of-sight conditions and 1.11~m overall ranging error across mixed propagation scenarios, achieving sub-meter positioning accuracy. Results demonstrate the algorithm's robustness to measurement noise and partial connectivity scenarios typical in industrial deployments. The findings contribute to automated UWB network initialization for indoor positioning applications, reducing infrastructure dependency compared to manual anchor calibration procedures."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T17:26:51Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    26,
                    51,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Andrea Jung"
                    },
                    {
                        "name": "Paul Schwarzbach"
                    },
                    {
                        "name": "Oliver Michler"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Michler"
                },
                "author": "Oliver Michler"
            },
            {
                "id": "http://arxiv.org/abs/2511.10519v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10519v1",
                "title": "Say It Differently: Linguistic Styles as Jailbreak Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Say It Differently: Linguistic Styles as Jailbreak Vectors"
                },
                "updated": "2025-11-13T17:24:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    24,
                    38,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10519v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are commonly evaluated for robustness against paraphrased or semantically equivalent jailbreak prompts, yet little attention has been paid to linguistic variation as an attack surface. In this work, we systematically study how linguistic styles such as fear or curiosity can reframe harmful intent and elicit unsafe responses from aligned models. We construct style-augmented jailbreak benchmark by transforming prompts from 3 standard datasets into 11 distinct linguistic styles using handcrafted templates and LLM-based rewrites, while preserving semantic intent. Evaluating 16 open- and close-source instruction-tuned models, we find that stylistic reframing increases jailbreak success rates by up to +57 percentage points. Styles such as fearful, curious and compassionate are most effective and contextualized rewrites outperform templated variants.\n  To mitigate this, we introduce a style neutralization preprocessing step using a secondary LLM to strip manipulative stylistic cues from user inputs, significantly reducing jailbreak success rates. Our findings reveal a systemic and scaling-resistant vulnerability overlooked in current safety pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are commonly evaluated for robustness against paraphrased or semantically equivalent jailbreak prompts, yet little attention has been paid to linguistic variation as an attack surface. In this work, we systematically study how linguistic styles such as fear or curiosity can reframe harmful intent and elicit unsafe responses from aligned models. We construct style-augmented jailbreak benchmark by transforming prompts from 3 standard datasets into 11 distinct linguistic styles using handcrafted templates and LLM-based rewrites, while preserving semantic intent. Evaluating 16 open- and close-source instruction-tuned models, we find that stylistic reframing increases jailbreak success rates by up to +57 percentage points. Styles such as fearful, curious and compassionate are most effective and contextualized rewrites outperform templated variants.\n  To mitigate this, we introduce a style neutralization preprocessing step using a secondary LLM to strip manipulative stylistic cues from user inputs, significantly reducing jailbreak success rates. Our findings reveal a systemic and scaling-resistant vulnerability overlooked in current safety pipelines."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T17:24:38Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    24,
                    38,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Srikant Panda"
                    },
                    {
                        "name": "Avinash Rai"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Rai"
                },
                "author": "Avinash Rai"
            },
            {
                "id": "http://arxiv.org/abs/2511.10518v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10518v1",
                "title": "SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation"
                },
                "updated": "2025-11-13T17:24:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    24,
                    37,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10518v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T17:24:37Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    24,
                    37,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026 (Oral), Project Page: https://github.com/JiuTian-VL/SemanticVLA",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Renshan Zhang"
                    },
                    {
                        "name": "Rui Shao"
                    },
                    {
                        "name": "Zhijian Fang"
                    },
                    {
                        "name": "Kaiwen Zhou"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie"
            },
            {
                "id": "http://arxiv.org/abs/2509.11816v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.11816v2",
                "title": "Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning"
                },
                "updated": "2025-11-13T17:23:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    23,
                    32,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.11816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.11816v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current unlearning and safety training methods consistently fail to remove dangerous knowledge from language models. We identify the root cause - unlearning targets representations which are too general - and develop a highly selective technique that unlearns robustly while preserving general performance.\n  Our method performs PCA on activations and module-output gradients to identify subspaces containing common representations, then collapses these subspaces before computing unlearning updates, a technique we term Collapse of Irrelevant Representations (CIR). This avoids unlearning general knowledge and targets only representations specific to the facts being unlearned.\n  When unlearning bio- and cyber-hazardous facts from Llama-3.1-8B, we achieve over 30x greater reduction in post-attack accuracy than the best baseline (Circuit Breakers), while disrupting general performance 30x less, and using less than 3 GPU-seconds per fact.\n  Thus, by disentangling harmful and benign capabilities at the level of representations, CIR enables robust and non-disruptive unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current unlearning and safety training methods consistently fail to remove dangerous knowledge from language models. We identify the root cause - unlearning targets representations which are too general - and develop a highly selective technique that unlearns robustly while preserving general performance.\n  Our method performs PCA on activations and module-output gradients to identify subspaces containing common representations, then collapses these subspaces before computing unlearning updates, a technique we term Collapse of Irrelevant Representations (CIR). This avoids unlearning general knowledge and targets only representations specific to the facts being unlearned.\n  When unlearning bio- and cyber-hazardous facts from Llama-3.1-8B, we achieve over 30x greater reduction in post-attack accuracy than the best baseline (Circuit Breakers), while disrupting general performance 30x less, and using less than 3 GPU-seconds per fact.\n  Thus, by disentangling harmful and benign capabilities at the level of representations, CIR enables robust and non-disruptive unlearning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-15T11:55:10Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    55,
                    10,
                    0,
                    258,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Filip Sondej"
                    },
                    {
                        "name": "Yushi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yushi Yang"
                },
                "author": "Yushi Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.10507v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10507v1",
                "title": "Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following"
                },
                "updated": "2025-11-13T17:14:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    14,
                    1,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10507v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T17:14:01Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    14,
                    1,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yun He"
                    },
                    {
                        "name": "Wenzhe Li"
                    },
                    {
                        "name": "Hejia Zhang"
                    },
                    {
                        "name": "Songlin Li"
                    },
                    {
                        "name": "Karishma Mandyam"
                    },
                    {
                        "name": "Sopan Khosla"
                    },
                    {
                        "name": "Yuanhao Xiong"
                    },
                    {
                        "name": "Nanshu Wang"
                    },
                    {
                        "name": "Selina Peng"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Shengjie Bi"
                    },
                    {
                        "name": "Shishir G. Patil"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Shengyu Feng"
                    },
                    {
                        "name": "Julian Katz-Samuels"
                    },
                    {
                        "name": "Richard Yuanzhe Pang"
                    },
                    {
                        "name": "Sujan Gonugondla"
                    },
                    {
                        "name": "Hunter Lang"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Yundi Qian"
                    },
                    {
                        "name": "Maryam Fazel-Zarandi"
                    },
                    {
                        "name": "Licheng Yu"
                    },
                    {
                        "name": "Amine Benhalloum"
                    },
                    {
                        "name": "Hany Awadalla"
                    },
                    {
                        "name": "Manaal Faruqui"
                    }
                ],
                "author_detail": {
                    "name": "Manaal Faruqui"
                },
                "author": "Manaal Faruqui"
            },
            {
                "id": "http://arxiv.org/abs/2410.21597v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2410.21597v3",
                "title": "Reducing the Scope of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the Scope of Language Models"
                },
                "updated": "2025-11-13T17:11:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    11,
                    10,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2410.21597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2410.21597v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are deployed in a wide variety of user-facing applications. Typically, these deployments have some specific purpose, like answering questions grounded on documentation or acting as coding assistants, but they require general language understanding. In such deployments, LLMs should respond only to queries that align with the intended purpose and reject all other requests, such as generating poetry or answering questions about physics, a task we refer to as `scoping'. We conduct a comprehensive empirical evaluation of various methods, ranging from prompting, fine-tuning to preference learning and the recently proposed general alignment technique known as Circuit Breakers (CB). Across three families of language models and a broad variety of tasks, we show that it is possible to scope language models. We examine scoping for multiple topics, and fine-grained topics. We ablate diversity of irrelevant queries, layer different techniques, conduct adversarial evaluations and more. Among other results, we find that when diverse examples of irrelevant queries are available, simple supervised fine-tuning produces the best results, but when such diversity is low, Circuit Breakers perform quite well. One can often get the benefits of both methods by layering them in succession. We intend our study to serve as a practitioner's guide to scoping LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are deployed in a wide variety of user-facing applications. Typically, these deployments have some specific purpose, like answering questions grounded on documentation or acting as coding assistants, but they require general language understanding. In such deployments, LLMs should respond only to queries that align with the intended purpose and reject all other requests, such as generating poetry or answering questions about physics, a task we refer to as `scoping'. We conduct a comprehensive empirical evaluation of various methods, ranging from prompting, fine-tuning to preference learning and the recently proposed general alignment technique known as Circuit Breakers (CB). Across three families of language models and a broad variety of tasks, we show that it is possible to scope language models. We examine scoping for multiple topics, and fine-grained topics. We ablate diversity of irrelevant queries, layer different techniques, conduct adversarial evaluations and more. Among other results, we find that when diverse examples of irrelevant queries are available, simple supervised fine-tuning produces the best results, but when such diversity is low, Circuit Breakers perform quite well. One can often get the benefits of both methods by layering them in succession. We intend our study to serve as a practitioner's guide to scoping LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-10-28T23:06:57Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    23,
                    6,
                    57,
                    0,
                    302,
                    0
                ],
                "arxiv_comment": "Appears in AAAI 2026 in the Main Technical Track",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "David Yunis"
                    },
                    {
                        "name": "Siyu Huo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Danish Contractor"
                    }
                ],
                "author_detail": {
                    "name": "Danish Contractor"
                },
                "author": "Danish Contractor"
            },
            {
                "id": "http://arxiv.org/abs/2504.03966v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.03966v2",
                "title": "Bridging LMS and generative AI: dynamic course content integration (DCCI) for enhancing student satisfaction and engagement via the ask ME assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging LMS and generative AI: dynamic course content integration (DCCI) for enhancing student satisfaction and engagement via the ask ME assistant"
                },
                "updated": "2025-11-13T17:07:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    7,
                    1,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.03966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.03966v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1007/s40692-025-00367-w",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Integration of Large Language Models (LLMs) with Learning Management Systems (LMSs) can enhance task automation and accessibility in education. However, hallucination where LLMs generate inaccurate or misleading information remains a challenge. This study introduces the Dynamic Course Content Integration (DCCI) mechanism, which dynamically retrieves course content from Canvas LMS and structures it within an LLM's context window via prompt engineering, enabling the LLM-powered assistant, Ask ME, to deliver context-aware, curriculum-aligned responses while mitigating hallucinations. A mixed-methods pilot study grounded in Self-Determination Theory (autonomy, competence) and the Technology Acceptance Model (perceived usefulness, ease of use) evaluated DCCI's effectiveness with 120 first-year programming students at Eötvös Loránd University. The course focused on foundational programming patterns in C#, including writing program specifications. We analyzed 14,746 logged interactions and a post-course survey completed by 101 students. User satisfaction was measured via a 5-point Likert scale (turn-level ratings), while the survey assessed usability, engagement, and ethical concerns. Results indicated high satisfaction (mean 4.65/5) and strong recognition of Ask ME's ability to provide timely, contextually relevant answers to administrative and course-related queries. 78.06% agreed that Ask ME's Canvas integration reduced platform switching, improving usability, engagement, comprehension, and topic exploration. Many students reported reduced hesitation to ask questions and increased motivation for self-directed learning, though concerns about over-reliance on AI and reduced student-teacher interaction emerged. This study demonstrates that DCCI enhances LLM reliability, student satisfaction, and engagement in AI-driven educational automation, while highlighting the importance of balancing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integration of Large Language Models (LLMs) with Learning Management Systems (LMSs) can enhance task automation and accessibility in education. However, hallucination where LLMs generate inaccurate or misleading information remains a challenge. This study introduces the Dynamic Course Content Integration (DCCI) mechanism, which dynamically retrieves course content from Canvas LMS and structures it within an LLM's context window via prompt engineering, enabling the LLM-powered assistant, Ask ME, to deliver context-aware, curriculum-aligned responses while mitigating hallucinations. A mixed-methods pilot study grounded in Self-Determination Theory (autonomy, competence) and the Technology Acceptance Model (perceived usefulness, ease of use) evaluated DCCI's effectiveness with 120 first-year programming students at Eötvös Loránd University. The course focused on foundational programming patterns in C#, including writing program specifications. We analyzed 14,746 logged interactions and a post-course survey completed by 101 students. User satisfaction was measured via a 5-point Likert scale (turn-level ratings), while the survey assessed usability, engagement, and ethical concerns. Results indicated high satisfaction (mean 4.65/5) and strong recognition of Ask ME's ability to provide timely, contextually relevant answers to administrative and course-related queries. 78.06% agreed that Ask ME's Canvas integration reduced platform switching, improving usability, engagement, comprehension, and topic exploration. Many students reported reduced hesitation to ask questions and increased motivation for self-directed learning, though concerns about over-reliance on AI and reduced student-teacher interaction emerged. This study demonstrates that DCCI enhances LLM reliability, student satisfaction, and engagement in AI-driven educational automation, while highlighting the importance of balancing"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-04T22:17:30Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    22,
                    17,
                    30,
                    4,
                    94,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "arxiv_journal_ref": "J. Comput. Educ. (2025)",
                "authors": [
                    {
                        "name": "Kovan Mzwri"
                    },
                    {
                        "name": "Márta Turcsányi-Szabo"
                    }
                ],
                "author_detail": {
                    "name": "Márta Turcsányi-Szabo"
                },
                "arxiv_affiliation": "Department of Media & Educational Technology, Faculty of Informatics, Eötvös Loránd University, Budapest, Hungary",
                "author": "Márta Turcsányi-Szabo",
                "arxiv_doi": "10.1007/s40692-025-00367-w"
            },
            {
                "id": "http://arxiv.org/abs/2507.20067v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.20067v2",
                "title": "PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training"
                },
                "updated": "2025-11-13T17:05:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    17,
                    5,
                    21,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.20067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.20067v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inference-time alignment enables large language models (LLMs) to generate outputs aligned with end-user preferences without further training. Recent post-training methods achieve this by using small guidance models to modify token generation during inference. These methods typically optimize a reward function KL-regularized by the original LLM taken as the reference policy. A critical limitation, however, is their dependence on a pre-trained reward model, which requires fitting to human preference feedback--a potentially unstable process. In contrast, we introduce PITA, a novel framework that integrates preference feedback directly into the LLM's token generation, eliminating the need for a reward model. PITA learns a small preference-based guidance policy to modify token probabilities at inference time without LLM fine-tuning, reducing computational cost and bypassing the pre-trained reward model dependency. The problem is framed as identifying an underlying preference distribution, solved through stochastic search and iterative refinement of the preference-based guidance model. We evaluate PITA across diverse tasks, including mathematical reasoning and sentiment classification, demonstrating its effectiveness in aligning LLM outputs with user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time alignment enables large language models (LLMs) to generate outputs aligned with end-user preferences without further training. Recent post-training methods achieve this by using small guidance models to modify token generation during inference. These methods typically optimize a reward function KL-regularized by the original LLM taken as the reference policy. A critical limitation, however, is their dependence on a pre-trained reward model, which requires fitting to human preference feedback--a potentially unstable process. In contrast, we introduce PITA, a novel framework that integrates preference feedback directly into the LLM's token generation, eliminating the need for a reward model. PITA learns a small preference-based guidance policy to modify token probabilities at inference time without LLM fine-tuning, reducing computational cost and bypassing the pre-trained reward model dependency. The problem is framed as identifying an underlying preference distribution, solved through stochastic search and iterative refinement of the preference-based guidance model. We evaluate PITA across diverse tasks, including mathematical reasoning and sentiment classification, demonstrating its effectiveness in aligning LLM outputs with user preferences."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-26T21:46:32Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    21,
                    46,
                    32,
                    5,
                    207,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Sarat Chandra Bobbili"
                    },
                    {
                        "name": "Ujwal Dinesha"
                    },
                    {
                        "name": "Dheeraj Narasimha"
                    },
                    {
                        "name": "Srinivas Shakkottai"
                    }
                ],
                "author_detail": {
                    "name": "Srinivas Shakkottai"
                },
                "author": "Srinivas Shakkottai"
            },
            {
                "id": "http://arxiv.org/abs/2511.10492v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10492v1",
                "title": "Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding"
                },
                "updated": "2025-11-13T16:59:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    59,
                    22,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10492v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.\n  Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.\n  Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T16:59:22Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    59,
                    22,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Yunkai Zhang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Feng"
                    },
                    {
                        "name": "Lin"
                    },
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Hanchao Yu"
                    },
                    {
                        "name": "Jason Liu"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Zhuoran Yu"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Diji Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diji Yang"
                },
                "arxiv_affiliation": "Ryan",
                "author": "Diji Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.09540v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09540v2",
                "title": "vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs"
                },
                "updated": "2025-11-13T16:56:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    56,
                    35,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09540v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work aims to continuously expand to encompass more downstream applications, and the corresponding resources are intended to be shared through https://github.com/VinyehShaw/UniEqui.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work aims to continuously expand to encompass more downstream applications, and the corresponding resources are intended to be shared through https://github.com/VinyehShaw/UniEqui."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T18:38:33Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    18,
                    38,
                    33,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "Accepted as an Oral Presentation at AAAI 2026 Main Technical Track (this version is not peer-reviewed; it is the extended version)",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Minye Shao"
                    },
                    {
                        "name": "Sihan Guo"
                    },
                    {
                        "name": "Xinrun Li"
                    },
                    {
                        "name": "Xingyu Miao"
                    },
                    {
                        "name": "Haoran Duan"
                    },
                    {
                        "name": "Yang Long"
                    }
                ],
                "author_detail": {
                    "name": "Yang Long"
                },
                "author": "Yang Long"
            },
            {
                "id": "http://arxiv.org/abs/2511.08379v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08379v2",
                "title": "SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models"
                },
                "updated": "2025-11-13T16:46:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    46,
                    23,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08379v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Refusal refers to the functional behavior enabling safety-aligned language models to reject harmful or unethical prompts. Following the growing scientific interest in mechanistic interpretability, recent work encoded refusal behavior as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Motivated by these findings, we propose a novel method leveraging Self-Organizing Maps (SOMs) to extract multiple refusal directions. To this end, we first prove that SOMs generalize the prior work's difference-in-means technique. We then train SOMs on harmful prompt representations to identify multiple neurons. By subtracting the centroid of harmless representations from each neuron, we derive a set of multiple directions expressing the refusal concept. We validate our method on an extensive experimental setup, demonstrating that ablating multiple directions from models' internals outperforms not only the single-direction baseline but also specialized jailbreak algorithms, leading to an effective suppression of refusal. Finally, we conclude by analyzing the mechanistic implications of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal refers to the functional behavior enabling safety-aligned language models to reject harmful or unethical prompts. Following the growing scientific interest in mechanistic interpretability, recent work encoded refusal behavior as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Motivated by these findings, we propose a novel method leveraging Self-Organizing Maps (SOMs) to extract multiple refusal directions. To this end, we first prove that SOMs generalize the prior work's difference-in-means technique. We then train SOMs on harmful prompt representations to identify multiple neurons. By subtracting the centroid of harmless representations from each neuron, we derive a set of multiple directions expressing the refusal concept. We validate our method on an extensive experimental setup, demonstrating that ablating multiple directions from models' internals outperforms not only the single-direction baseline but also specialized jailbreak algorithms, leading to an effective suppression of refusal. Finally, we conclude by analyzing the mechanistic implications of our approach."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T16:01:42Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    16,
                    1,
                    42,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "Accepted at AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Giorgio Piras"
                    },
                    {
                        "name": "Raffaele Mura"
                    },
                    {
                        "name": "Fabio Brau"
                    },
                    {
                        "name": "Luca Oneto"
                    },
                    {
                        "name": "Fabio Roli"
                    },
                    {
                        "name": "Battista Biggio"
                    }
                ],
                "author_detail": {
                    "name": "Battista Biggio"
                },
                "author": "Battista Biggio"
            },
            {
                "id": "http://arxiv.org/abs/2511.10480v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10480v1",
                "title": "Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs"
                },
                "updated": "2025-11-13T16:44:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    44,
                    56,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10480v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T16:44:56Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    44,
                    56,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Changhai Man"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Srinivas Sridharan"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna"
            },
            {
                "id": "http://arxiv.org/abs/2511.10479v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10479v1",
                "title": "Quantum Design Automation: Foundations, Challenges, and the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Design Automation: Foundations, Challenges, and the Road Ahead"
                },
                "updated": "2025-11-13T16:44:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    44,
                    36,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10479v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Quantum computing is transitioning from laboratory research to industrial deployment, yet significant challenges persist: system scalability and performance, fabrication yields, and the advancement of algorithms and applications. We emphasize that in building quantum computers -- spanning quantum chips, system integration, instruction sets, algorithms, and middleware such as quantum error correction schemes -- design is everywhere. In this paper, we advocate for a holistic design perspective in quantum computing, a perspective we argue is pivotal to unlocking innovative co-design opportunities and addressing the aforementioned key challenges. To equip readers with sufficient background for exploring co-optimization opportunities, we detail how interconnected computational methods and tools collaborate to enable end-to-end quantum computer design. This coverage encompasses critical stages -- such as chip layout design automation, high-fidelity system-level simulation, Hamiltonian derivation for quantum system modeling, control pulse simulation, decoherence analysis, and physical verification and testing -- followed by quantum instruction set design. We then proceed to quantum system and software development, including quantum circuit synthesis, quantum error correction and fault tolerance, and logic verification and testing. Through these discussions, we illustrate with concrete examples -- including co-optimizing quantum instruction sets with algorithmic considerations, customizing error correction circuits to hardware-specific constraints, and streamlining quantum chip design through tailored code design, among others. We hope that the detailed end-to-end design workflow as well as these examples will foster dialogue between the hardware and software communities, ultimately facilitating the translation of meaningful research findings into future quantum hardware implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computing is transitioning from laboratory research to industrial deployment, yet significant challenges persist: system scalability and performance, fabrication yields, and the advancement of algorithms and applications. We emphasize that in building quantum computers -- spanning quantum chips, system integration, instruction sets, algorithms, and middleware such as quantum error correction schemes -- design is everywhere. In this paper, we advocate for a holistic design perspective in quantum computing, a perspective we argue is pivotal to unlocking innovative co-design opportunities and addressing the aforementioned key challenges. To equip readers with sufficient background for exploring co-optimization opportunities, we detail how interconnected computational methods and tools collaborate to enable end-to-end quantum computer design. This coverage encompasses critical stages -- such as chip layout design automation, high-fidelity system-level simulation, Hamiltonian derivation for quantum system modeling, control pulse simulation, decoherence analysis, and physical verification and testing -- followed by quantum instruction set design. We then proceed to quantum system and software development, including quantum circuit synthesis, quantum error correction and fault tolerance, and logic verification and testing. Through these discussions, we illustrate with concrete examples -- including co-optimizing quantum instruction sets with algorithmic considerations, customizing error correction circuits to hardware-specific constraints, and streamlining quantum chip design through tailored code design, among others. We hope that the detailed end-to-end design workflow as well as these examples will foster dialogue between the hardware and software communities, ultimately facilitating the translation of meaningful research findings into future quantum hardware implementations."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T16:44:36Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    44,
                    36,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "143 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Feng Wu"
                    },
                    {
                        "name": "Jingzhe Guo"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Linghang Kong"
                    },
                    {
                        "name": "Fang Zhang"
                    },
                    {
                        "name": "Ziang Wang"
                    },
                    {
                        "name": "Aochu Dai"
                    },
                    {
                        "name": "Ziyuan Wang"
                    },
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Hao Deng"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Zhengfeng Ji"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Hui-Hai Zhao"
                    },
                    {
                        "name": "Jianxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Chen"
                },
                "author": "Jianxin Chen"
            },
            {
                "id": "http://arxiv.org/abs/2510.17064v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.17064v3",
                "title": "A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation"
                },
                "updated": "2025-11-13T16:37:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    37,
                    14,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.17064v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.17064v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Single-cell RNA sequencing has transformed our ability to identify diverse cell types and their transcriptomic signatures. However, annotating these signatures-especially those involving poorly characterized genes-remains a major challenge. Traditional methods, such as Gene Set Enrichment Analysis (GSEA), depend on well-curated annotations and often perform poorly in these contexts. Large Language Models (LLMs) offer a promising alternative but struggle to represent complex biological knowledge within structured ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID: https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that integrates free-text descriptions with ontology labels to enable more accurate and robust gene set annotation. By incorporating retrieval-augmented generation (RAG), we developed a robust agentic workflow that refines predictions using relevant PubMed literature, reducing hallucinations and enhancing interpretability. Using this workflow, we achieved correct annotations for 77% of mouse gene sets among their top predictions. Applying this approach, we annotated 5,322 brain cell clusters from the comprehensive mouse brain cell atlas generated by the BRAIN Initiative Cell Census Network, enabling novel insights into brain cell function by identifying region-specific gene co-expression patterns and inferring functional roles of gene ensembles. BRAINCELL-AID also identifies Basal Ganglia-related cell types with neurologically meaningful descriptions. Hence, we create a valuable resource to support community-driven cell type annotation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-cell RNA sequencing has transformed our ability to identify diverse cell types and their transcriptomic signatures. However, annotating these signatures-especially those involving poorly characterized genes-remains a major challenge. Traditional methods, such as Gene Set Enrichment Analysis (GSEA), depend on well-curated annotations and often perform poorly in these contexts. Large Language Models (LLMs) offer a promising alternative but struggle to represent complex biological knowledge within structured ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID: https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that integrates free-text descriptions with ontology labels to enable more accurate and robust gene set annotation. By incorporating retrieval-augmented generation (RAG), we developed a robust agentic workflow that refines predictions using relevant PubMed literature, reducing hallucinations and enhancing interpretability. Using this workflow, we achieved correct annotations for 77% of mouse gene sets among their top predictions. Applying this approach, we annotated 5,322 brain cell clusters from the comprehensive mouse brain cell atlas generated by the BRAIN Initiative Cell Census Network, enabling novel insights into brain cell function by identifying region-specific gene co-expression patterns and inferring functional roles of gene ensembles. BRAINCELL-AID also identifies Basal Ganglia-related cell types with neurologically meaningful descriptions. Hence, we create a valuable resource to support community-driven cell type annotation."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-20T00:37:55Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    0,
                    37,
                    55,
                    0,
                    293,
                    0
                ],
                "arxiv_comment": "23 pages, 6 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Rongbin Li"
                    },
                    {
                        "name": "Wenbo Chen"
                    },
                    {
                        "name": "Zhao Li"
                    },
                    {
                        "name": "Rodrigo Munoz-Castaneda"
                    },
                    {
                        "name": "Jinbo Li"
                    },
                    {
                        "name": "Neha S. Maurya"
                    },
                    {
                        "name": "Arnav Solanki"
                    },
                    {
                        "name": "Huan He"
                    },
                    {
                        "name": "Hanwen Xing"
                    },
                    {
                        "name": "Meaghan Ramlakhan"
                    },
                    {
                        "name": "Zachary Wise"
                    },
                    {
                        "name": "Nelson Johansen"
                    },
                    {
                        "name": "Zhuhao Wu"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Michael Hawrylycz"
                    },
                    {
                        "name": "W. Jim Zheng"
                    }
                ],
                "author_detail": {
                    "name": "W. Jim Zheng"
                },
                "author": "W. Jim Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2511.10459v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10459v1",
                "title": "LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning"
                },
                "updated": "2025-11-13T16:26:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    26,
                    13,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10459v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have been widely evaluated on macro-scale geographic tasks, such as global factual recall, event summarization, and regional reasoning. Yet, their ability to handle hyper-local knowledge remains poorly understood. This gap is increasingly consequential as real-world applications, from civic platforms to community journalism, demand AI systems that can reason about neighborhood-specific dynamics, cultural narratives, and local governance. Existing benchmarks fall short in capturing this complexity, often relying on coarse-grained data or isolated references. We present LocalBench, the first benchmark designed to systematically evaluate LLMs on county-level local knowledge across the United States. Grounded in the Localness Conceptual Framework, LocalBench includes 14,782 validated question-answer pairs across 526 U.S. counties in 49 states, integrating diverse sources such as Census statistics, local subreddit discourse, and regional news. It spans physical, cognitive, and relational dimensions of locality. Using LocalBench, we evaluate 13 state-of-the-art LLMs under both closed-book and web-augmented settings. Our findings reveal critical limitations: even the best-performing models reach only 56.8% accuracy on narrative-style questions and perform below 15.5% on numerical reasoning. Moreover, larger model size and web augmentation do not guarantee better performance, for example, search improves Gemini's accuracy by +13.6%, but reduces GPT-series performance by -11.4%. These results underscore the urgent need for language models that can support equitable, place-aware AI systems: capable of engaging with the diverse, fine-grained realities of local communities across geographic and cultural contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely evaluated on macro-scale geographic tasks, such as global factual recall, event summarization, and regional reasoning. Yet, their ability to handle hyper-local knowledge remains poorly understood. This gap is increasingly consequential as real-world applications, from civic platforms to community journalism, demand AI systems that can reason about neighborhood-specific dynamics, cultural narratives, and local governance. Existing benchmarks fall short in capturing this complexity, often relying on coarse-grained data or isolated references. We present LocalBench, the first benchmark designed to systematically evaluate LLMs on county-level local knowledge across the United States. Grounded in the Localness Conceptual Framework, LocalBench includes 14,782 validated question-answer pairs across 526 U.S. counties in 49 states, integrating diverse sources such as Census statistics, local subreddit discourse, and regional news. It spans physical, cognitive, and relational dimensions of locality. Using LocalBench, we evaluate 13 state-of-the-art LLMs under both closed-book and web-augmented settings. Our findings reveal critical limitations: even the best-performing models reach only 56.8% accuracy on narrative-style questions and perform below 15.5% on numerical reasoning. Moreover, larger model size and web augmentation do not guarantee better performance, for example, search improves Gemini's accuracy by +13.6%, but reduces GPT-series performance by -11.4%. These results underscore the urgent need for language models that can support equitable, place-aware AI systems: capable of engaging with the diverse, fine-grained realities of local communities across geographic and cultural contexts."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T16:26:13Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    26,
                    13,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zihan Gao"
                    },
                    {
                        "name": "Yifei Xu"
                    },
                    {
                        "name": "Jacob Thebault-Spieker"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Thebault-Spieker"
                },
                "author": "Jacob Thebault-Spieker"
            },
            {
                "id": "http://arxiv.org/abs/2511.10457v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10457v1",
                "title": "Exploring State Tracking Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring State Tracking Capabilities of Large Language Models"
                },
                "updated": "2025-11-13T16:25:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    25,
                    32,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10457v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in solving complex tasks, including those requiring a certain level of reasoning. In this paper, we focus on state tracking, a problem where models need to keep track of the state governing a number of entities. To isolate the state tracking component from other factors, we propose a benchmark based on three well-defined state tracking tasks and analyse the performance of LLMs in different scenarios. The results indicate that the recent generation of LLMs (specifically, GPT-4 and Llama3) are capable of tracking state, especially when integrated with mechanisms such as Chain of Thought. However, models from the former generation, while understanding the task and being able to solve it at the initial stages, often fail at this task after a certain number of steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in solving complex tasks, including those requiring a certain level of reasoning. In this paper, we focus on state tracking, a problem where models need to keep track of the state governing a number of entities. To isolate the state tracking component from other factors, we propose a benchmark based on three well-defined state tracking tasks and analyse the performance of LLMs in different scenarios. The results indicate that the recent generation of LLMs (specifically, GPT-4 and Llama3) are capable of tracking state, especially when integrated with mechanisms such as Chain of Thought. However, models from the former generation, while understanding the task and being able to solve it at the initial stages, often fail at this task after a certain number of steps."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T16:25:32Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    25,
                    32,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kiamehr Rezaee"
                    },
                    {
                        "name": "Jose Camacho-Collados"
                    },
                    {
                        "name": "Mohammad Taher Pilehvar"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Taher Pilehvar"
                },
                "author": "Mohammad Taher Pilehvar"
            },
            {
                "id": "http://arxiv.org/abs/2506.06094v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.06094v4",
                "title": "Onboard Mission Replanning for Adaptive Cooperative Multi-Robot Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Onboard Mission Replanning for Adaptive Cooperative Multi-Robot Systems"
                },
                "updated": "2025-11-13T16:15:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    15,
                    6,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.06094v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.06094v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/LRA.2025.3627093",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Cooperative autonomous robotic systems have significant potential for executing complex multi-task missions across space, air, ground, and maritime domains. But they commonly operate in remote, dynamic and hazardous environments, requiring rapid in-mission adaptation without reliance on fragile or slow communication links to centralised compute. Fast, on-board replanning algorithms are therefore needed to enhance resilience. Reinforcement Learning shows strong promise for efficiently solving mission planning tasks when formulated as Travelling Salesperson Problems (TSPs), but existing methods: 1) are unsuitable for replanning, where agents do not start at a single location; 2) do not allow cooperation between agents; 3) are unable to model tasks with variable durations; or 4) lack practical considerations for on-board deployment. Here we define the Cooperative Mission Replanning Problem as a novel variant of multiple TSP with adaptations to overcome these issues, and develop a new encoder/decoder-based model using Graph Attention Networks and Attention Models to solve it effectively and efficiently. Using a simple example of cooperative drones, we show our replanner consistently (90% of the time) maintains performance within 10% of the state-of-the-art LKH3 heuristic solver, whilst running 85-370 times faster on a Raspberry Pi. This work paves the way for increased resilience in autonomous multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative autonomous robotic systems have significant potential for executing complex multi-task missions across space, air, ground, and maritime domains. But they commonly operate in remote, dynamic and hazardous environments, requiring rapid in-mission adaptation without reliance on fragile or slow communication links to centralised compute. Fast, on-board replanning algorithms are therefore needed to enhance resilience. Reinforcement Learning shows strong promise for efficiently solving mission planning tasks when formulated as Travelling Salesperson Problems (TSPs), but existing methods: 1) are unsuitable for replanning, where agents do not start at a single location; 2) do not allow cooperation between agents; 3) are unable to model tasks with variable durations; or 4) lack practical considerations for on-board deployment. Here we define the Cooperative Mission Replanning Problem as a novel variant of multiple TSP with adaptations to overcome these issues, and develop a new encoder/decoder-based model using Graph Attention Networks and Attention Models to solve it effectively and efficiently. Using a simple example of cooperative drones, we show our replanner consistently (90% of the time) maintains performance within 10% of the state-of-the-art LKH3 heuristic solver, whilst running 85-370 times faster on a Raspberry Pi. This work paves the way for increased resilience in autonomous multi-agent systems."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-06T13:54:19Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    13,
                    54,
                    19,
                    4,
                    157,
                    0
                ],
                "arxiv_comment": "9 pages, 5 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "arxiv_journal_ref": "IEEE Robotics and Automation Letters (Volume: 10, Issue: 12, December 2025) Pages: 13225 - 13232",
                "authors": [
                    {
                        "name": "Elim Kwan"
                    },
                    {
                        "name": "Rehman Qureshi"
                    },
                    {
                        "name": "Liam Fletcher"
                    },
                    {
                        "name": "Colin Laganier"
                    },
                    {
                        "name": "Victoria Nockles"
                    },
                    {
                        "name": "Richard Walters"
                    }
                ],
                "author_detail": {
                    "name": "Richard Walters"
                },
                "author": "Richard Walters",
                "arxiv_doi": "10.1109/LRA.2025.3627093"
            },
            {
                "id": "http://arxiv.org/abs/2511.07127v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07127v3",
                "title": "REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features in Clinical Prognostic Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features in Clinical Prognostic Tasks"
                },
                "updated": "2025-11-13T16:00:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    16,
                    0,
                    18,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07127v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07127v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) and causal learning each hold strong potential for clinical decision making (CDM). However, their synergy remains poorly understood, largely due to the lack of systematic benchmarks evaluating their integration in clinical risk prediction. In real-world healthcare, identifying features with causal influence on outcomes is crucial for actionable and trustworthy predictions. While recent work highlights LLMs' emerging causal reasoning abilities, there lacks comprehensive benchmarks to assess their causal learning and performance informed by causal features in clinical risk prediction. To address this, we introduce REACT-LLM, a benchmark designed to evaluate whether combining LLMs with causal features can enhance clinical prognostic performance and potentially outperform traditional machine learning (ML) methods. Unlike existing LLM-clinical benchmarks that often focus on a limited set of outcomes, REACT-LLM evaluates 7 clinical outcomes across 2 real-world datasets, comparing 15 prominent LLMs, 6 traditional ML models, and 3 causal discovery (CD) algorithms. Our findings indicate that while LLMs perform reasonably in clinical prognostics, they have not yet outperformed traditional ML models. Integrating causal features derived from CD algorithms into LLMs offers limited performance gains, primarily due to the strict assumptions of many CD methods, which are often violated in complex clinical data. While the direct integration yields limited improvement, our benchmark reveals a more promising synergy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and causal learning each hold strong potential for clinical decision making (CDM). However, their synergy remains poorly understood, largely due to the lack of systematic benchmarks evaluating their integration in clinical risk prediction. In real-world healthcare, identifying features with causal influence on outcomes is crucial for actionable and trustworthy predictions. While recent work highlights LLMs' emerging causal reasoning abilities, there lacks comprehensive benchmarks to assess their causal learning and performance informed by causal features in clinical risk prediction. To address this, we introduce REACT-LLM, a benchmark designed to evaluate whether combining LLMs with causal features can enhance clinical prognostic performance and potentially outperform traditional machine learning (ML) methods. Unlike existing LLM-clinical benchmarks that often focus on a limited set of outcomes, REACT-LLM evaluates 7 clinical outcomes across 2 real-world datasets, comparing 15 prominent LLMs, 6 traditional ML models, and 3 causal discovery (CD) algorithms. Our findings indicate that while LLMs perform reasonably in clinical prognostics, they have not yet outperformed traditional ML models. Integrating causal features derived from CD algorithms into LLMs offers limited performance gains, primarily due to the strict assumptions of many CD methods, which are often violated in complex clinical data. While the direct integration yields limited improvement, our benchmark reveals a more promising synergy."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T14:12:35Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    14,
                    12,
                    35,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Linna Wang"
                    },
                    {
                        "name": "Zhixuan You"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Jiunan Wen"
                    },
                    {
                        "name": "Ji Shi"
                    },
                    {
                        "name": "Yimin Chen"
                    },
                    {
                        "name": "Yusen Wang"
                    },
                    {
                        "name": "Fanqi Ding"
                    },
                    {
                        "name": "Ziliang Feng"
                    },
                    {
                        "name": "Li Lu"
                    }
                ],
                "author_detail": {
                    "name": "Li Lu"
                },
                "author": "Li Lu"
            },
            {
                "id": "http://arxiv.org/abs/2510.03553v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.03553v2",
                "title": "CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making"
                },
                "updated": "2025-11-13T15:56:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    56,
                    59,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.03553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.03553v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although large language models (LLMs) are increasingly implicated in interpersonal and societal decision-making, their ability to navigate explicit conflicts between legitimately different cultural value systems remains largely unexamined. Existing benchmarks predominantly target cultural knowledge (CulturalBench), value prediction (WorldValuesBench), or single-axis bias diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple culturally grounded values directly clash. We address this gap with CCD-Bench, a benchmark that assesses LLM decision-making under cross-cultural value conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains, each paired with ten anonymized response options corresponding to the ten GLOBE cultural clusters. These dilemmas are presented using a stratified Latin square to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe (12.4 percent), while options for Eastern Europe and the Middle East and North Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of rationales reference multiple GLOBE dimensions, this pluralism is superficial: models recombine Future Orientation and Performance Orientation, and rarely ground choices in Assertiveness or Gender Egalitarianism (both under 3 percent). Ordering effects are negligible (Cramer's V less than 0.10), and symmetrized KL divergence shows clustering by developer lineage rather than geography. These patterns suggest that current alignment pipelines promote a consensus-oriented worldview that underserves scenarios demanding power negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts evaluation beyond isolated bias detection toward pluralistic decision making and highlights the need for alignment strategies that substantively engage diverse worldviews.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) are increasingly implicated in interpersonal and societal decision-making, their ability to navigate explicit conflicts between legitimately different cultural value systems remains largely unexamined. Existing benchmarks predominantly target cultural knowledge (CulturalBench), value prediction (WorldValuesBench), or single-axis bias diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple culturally grounded values directly clash. We address this gap with CCD-Bench, a benchmark that assesses LLM decision-making under cross-cultural value conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains, each paired with ten anonymized response options corresponding to the ten GLOBE cultural clusters. These dilemmas are presented using a stratified Latin square to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe (12.4 percent), while options for Eastern Europe and the Middle East and North Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of rationales reference multiple GLOBE dimensions, this pluralism is superficial: models recombine Future Orientation and Performance Orientation, and rarely ground choices in Assertiveness or Gender Egalitarianism (both under 3 percent). Ordering effects are negligible (Cramer's V less than 0.10), and symmetrized KL divergence shows clustering by developer lineage rather than geography. These patterns suggest that current alignment pipelines promote a consensus-oriented worldview that underserves scenarios demanding power negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts evaluation beyond isolated bias detection toward pluralistic decision making and highlights the need for alignment strategies that substantively engage diverse worldviews."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-03T22:55:37Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    22,
                    55,
                    37,
                    4,
                    276,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hasibur Rahman"
                    },
                    {
                        "name": "Hanan Salam"
                    }
                ],
                "author_detail": {
                    "name": "Hanan Salam"
                },
                "author": "Hanan Salam"
            },
            {
                "id": "http://arxiv.org/abs/2511.04427v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04427v2",
                "title": "Does AI-Assisted Coding Deliver? A Difference-in-Differences Study of Cursor's Impact on Software Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does AI-Assisted Coding Deliver? A Difference-in-Differences Study of Cursor's Impact on Software Projects"
                },
                "updated": "2025-11-13T15:51:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    51,
                    45,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04427v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated the promise to revolutionize the field of software engineering. Among other things, LLM agents are rapidly gaining momentum in their application to software development, with practitioners claiming a multifold productivity increase after adoption. Yet, empirical evidence is lacking around these claims. In this paper, we estimate the causal effect of adopting a widely popular LLM agent assistant, namely Cursor, on development velocity and software quality. The estimation is enabled by a state-of-the-art difference-in-differences design comparing Cursor-adopting GitHub projects with a matched control group of similar GitHub projects that do not use Cursor. We find that the adoption of Cursor leads to a significant, large, but transient increase in project-level development velocity, along with a significant and persistent increase in static analysis warnings and code complexity. Further panel generalized method of moments estimation reveals that the increase in static analysis warnings and code complexity acts as a major factor causing long-term velocity slowdown. Our study carries implications for software engineering practitioners, LLM agent assistant designers, and researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the promise to revolutionize the field of software engineering. Among other things, LLM agents are rapidly gaining momentum in their application to software development, with practitioners claiming a multifold productivity increase after adoption. Yet, empirical evidence is lacking around these claims. In this paper, we estimate the causal effect of adopting a widely popular LLM agent assistant, namely Cursor, on development velocity and software quality. The estimation is enabled by a state-of-the-art difference-in-differences design comparing Cursor-adopting GitHub projects with a matched control group of similar GitHub projects that do not use Cursor. We find that the adoption of Cursor leads to a significant, large, but transient increase in project-level development velocity, along with a significant and persistent increase in static analysis warnings and code complexity. Further panel generalized method of moments estimation reveals that the increase in static analysis warnings and code complexity acts as a major factor causing long-term velocity slowdown. Our study carries implications for software engineering practitioners, LLM agent assistant designers, and researchers."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T15:00:51Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    0,
                    51,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Courtney Miller"
                    },
                    {
                        "name": "Shyam Agarwal"
                    },
                    {
                        "name": "Christian Kästner"
                    },
                    {
                        "name": "Bogdan Vasilescu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Vasilescu"
                },
                "author": "Bogdan Vasilescu"
            },
            {
                "id": "http://arxiv.org/abs/2508.02635v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.02635v2",
                "title": "Test Set Quality in Multilingual LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Set Quality in Multilingual LLM Evaluation"
                },
                "updated": "2025-11-13T15:40:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    40,
                    24,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.02635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.02635v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Several multilingual benchmark datasets have been developed in a semi-automatic manner in the recent past to measure progress and understand the state-of-the-art in the multilingual capabilities of Large Language Models. However, there is not a lot of attention paid to the quality of the datasets themselves, despite the existence of previous work in identifying errors in even fully human-annotated test sets. In this paper, we manually analyze recent multilingual evaluation sets in two languages - French and Telugu, identifying several errors in the process. We compare the performance difference across several LLMs with the original and revised versions of the datasets and identify large differences (almost 10% in some cases) in both languages). Based on these results, we argue that test sets should not be considered immutable and should be revisited, checked for correctness, and potentially versioned. We end with some recommendations for both the dataset creators as well as consumers on addressing the dataset quality issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several multilingual benchmark datasets have been developed in a semi-automatic manner in the recent past to measure progress and understand the state-of-the-art in the multilingual capabilities of Large Language Models. However, there is not a lot of attention paid to the quality of the datasets themselves, despite the existence of previous work in identifying errors in even fully human-annotated test sets. In this paper, we manually analyze recent multilingual evaluation sets in two languages - French and Telugu, identifying several errors in the process. We compare the performance difference across several LLMs with the original and revised versions of the datasets and identify large differences (almost 10% in some cases) in both languages). Based on these results, we argue that test sets should not be considered immutable and should be revisited, checked for correctness, and potentially versioned. We end with some recommendations for both the dataset creators as well as consumers on addressing the dataset quality issues."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-04T17:22:08Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    22,
                    8,
                    0,
                    216,
                    0
                ],
                "arxiv_comment": "to appear in the proceedings of Eval4NLP workshop at AACL 2025. Camera ready version",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chalamalasetti Kranti"
                    },
                    {
                        "name": "Gabriel Bernier-Colborne"
                    },
                    {
                        "name": "Yvan Gauthier"
                    },
                    {
                        "name": "Sowmya Vajjala"
                    }
                ],
                "author_detail": {
                    "name": "Sowmya Vajjala"
                },
                "author": "Sowmya Vajjala"
            },
            {
                "id": "http://arxiv.org/abs/2511.10418v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10418v1",
                "title": "CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models"
                },
                "updated": "2025-11-13T15:40:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    40,
                    17,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10418v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:40:17Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    40,
                    17,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Yaqiao Zhu"
                    },
                    {
                        "name": "Hongkai Wen"
                    },
                    {
                        "name": "Mark Birkin"
                    },
                    {
                        "name": "Man Luo"
                    }
                ],
                "author_detail": {
                    "name": "Man Luo"
                },
                "author": "Man Luo"
            },
            {
                "id": "http://arxiv.org/abs/2511.10400v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10400v1",
                "title": "Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance"
                },
                "updated": "2025-11-13T15:20:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    20,
                    12,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10400v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ensuring the reliability of agent architectures and effectively identifying problematic agents when failures occur are crucial challenges in multi-agent systems (MAS). Advances in large language models (LLMs) have established LLM-based agents as a major branch of MAS, enabling major breakthroughs in complex problem solving and world modeling. However, the reliability implications of this shift remain largely unexplored. i.e., whether substituting traditional agents with LLM-based agents can effectively enhance the reliability of MAS. In this work, we investigate and quantify the reliability of LLM-based agents from the perspective of Byzantine fault tolerance. We observe that LLM-based agents demonstrate stronger skepticism when processing erroneous message flows, a characteristic that enables them to outperform traditional agents across different topological structures. Motivated by the results of the pilot experiment, we design CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism to enhance the stability of MAS with different topologies. It capitalizes on the intrinsic reflective and discriminative capabilities of LLMs by employing a probe-based, weighted information flow transmission method to improve the reliability of LLM-based agents. Extensive experiments demonstrate that CP-WBFT achieves superior performance across diverse network topologies under extreme Byzantine conditions (85.7\\% fault rate). Notably, our approach surpasses traditional methods by attaining remarkable accuracy on various topologies and maintaining strong reliability in both mathematical reasoning and safety assessment tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the reliability of agent architectures and effectively identifying problematic agents when failures occur are crucial challenges in multi-agent systems (MAS). Advances in large language models (LLMs) have established LLM-based agents as a major branch of MAS, enabling major breakthroughs in complex problem solving and world modeling. However, the reliability implications of this shift remain largely unexplored. i.e., whether substituting traditional agents with LLM-based agents can effectively enhance the reliability of MAS. In this work, we investigate and quantify the reliability of LLM-based agents from the perspective of Byzantine fault tolerance. We observe that LLM-based agents demonstrate stronger skepticism when processing erroneous message flows, a characteristic that enables them to outperform traditional agents across different topological structures. Motivated by the results of the pilot experiment, we design CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism to enhance the stability of MAS with different topologies. It capitalizes on the intrinsic reflective and discriminative capabilities of LLMs by employing a probe-based, weighted information flow transmission method to improve the reliability of LLM-based agents. Extensive experiments demonstrate that CP-WBFT achieves superior performance across diverse network topologies under extreme Byzantine conditions (85.7\\% fault rate). Notably, our approach surpasses traditional methods by attaining remarkable accuracy on various topologies and maintaining strong reliability in both mathematical reasoning and safety assessment tasks."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:20:12Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    20,
                    12,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Lifan Zheng"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Qinghong Yin"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Xinyi Zeng"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian"
            },
            {
                "id": "http://arxiv.org/abs/2511.10395v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10395v1",
                "title": "AgentEvolver: Towards Efficient Self-Evolving Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentEvolver: Towards Efficient Self-Evolving Agent System"
                },
                "updated": "2025-11-13T15:14:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    14,
                    47,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10395v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:14:47Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    14,
                    47,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yunpeng Zhai"
                    },
                    {
                        "name": "Shuchang Tao"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Anni Zou"
                    },
                    {
                        "name": "Ziqian Chen"
                    },
                    {
                        "name": "Qingxu Fu"
                    },
                    {
                        "name": "Shinji Mai"
                    },
                    {
                        "name": "Li Yu"
                    },
                    {
                        "name": "Jiaji Deng"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2511.10394v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10394v1",
                "title": "LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components"
                },
                "updated": "2025-11-13T15:14:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    14,
                    34,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10394v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The health condition of wind turbine (WT) components is crucial for ensuring stable and reliable operation. However, existing fault detection methods are largely limited to visual recognition, producing structured outputs that lack semantic interpretability and fail to support maintenance decision-making. To address these limitations, this study proposes an integrated framework that combines YOLOMS with a large language model (LLM) for intelligent fault analysis and diagnosis. Specifically, YOLOMS employs multi-scale detection and sliding-window cropping to enhance fault feature extraction, while a lightweight key-value (KV) mapping module bridges the gap between visual outputs and textual inputs. This module converts YOLOMS detection results into structured textual representations enriched with both qualitative and quantitative attributes. A domain-tuned LLM then performs semantic reasoning to generate interpretable fault analyses and maintenance recommendations. Experiments on real-world datasets demonstrate that the proposed framework achieves a fault detection accuracy of 90.6\\% and generates maintenance reports with an average accuracy of 89\\%, thereby improving the interpretability of diagnostic results and providing practical decision support for the operation and maintenance of wind turbines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The health condition of wind turbine (WT) components is crucial for ensuring stable and reliable operation. However, existing fault detection methods are largely limited to visual recognition, producing structured outputs that lack semantic interpretability and fail to support maintenance decision-making. To address these limitations, this study proposes an integrated framework that combines YOLOMS with a large language model (LLM) for intelligent fault analysis and diagnosis. Specifically, YOLOMS employs multi-scale detection and sliding-window cropping to enhance fault feature extraction, while a lightweight key-value (KV) mapping module bridges the gap between visual outputs and textual inputs. This module converts YOLOMS detection results into structured textual representations enriched with both qualitative and quantitative attributes. A domain-tuned LLM then performs semantic reasoning to generate interpretable fault analyses and maintenance recommendations. Experiments on real-world datasets demonstrate that the proposed framework achieves a fault detection accuracy of 90.6\\% and generates maintenance reports with an average accuracy of 89\\%, thereby improving the interpretability of diagnostic results and providing practical decision support for the operation and maintenance of wind turbines."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:14:34Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    14,
                    34,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Journal resubmission",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yaru Li"
                    },
                    {
                        "name": "Yanxue Wang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Xinming Li"
                    },
                    {
                        "name": "Jianbo Feng"
                    }
                ],
                "author_detail": {
                    "name": "Jianbo Feng"
                },
                "author": "Jianbo Feng"
            },
            {
                "id": "http://arxiv.org/abs/2408.11062v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.11062v2",
                "title": "Multi-Turn Interactions for Text-to-SQL with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Turn Interactions for Text-to-SQL with Large Language Models"
                },
                "updated": "2025-11-13T15:01:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    1,
                    48,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.11062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.11062v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3746252.3761052",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This study explores text-to-SQL parsing by leveraging the powerful reasoning capabilities of large language models (LLMs). Despite recent advancements, existing LLM-based methods are still inefficient and struggle to handle cases with wide tables effectively. Furthermore, current interaction-based approaches either lack a step-by-step, interpretable SQL generation process or fail to provide a universally applicable interaction design. To address these challenges, we introduce Interactive-T2S, a framework that generates SQL queries through direct interactions with databases. This framework includes four general tools that facilitate proactive and efficient information retrieval by the LLM. Additionally, we have developed detailed exemplars to demonstrate the step-wise reasoning processes within our framework. Our approach achieves advanced performance on the Spider and BIRD datasets as well as their variants. Notably, we obtain state-of-the-art results on the BIRD leaderboard under the setting without oracle knowledge, demonstrating the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores text-to-SQL parsing by leveraging the powerful reasoning capabilities of large language models (LLMs). Despite recent advancements, existing LLM-based methods are still inefficient and struggle to handle cases with wide tables effectively. Furthermore, current interaction-based approaches either lack a step-by-step, interpretable SQL generation process or fail to provide a universally applicable interaction design. To address these challenges, we introduce Interactive-T2S, a framework that generates SQL queries through direct interactions with databases. This framework includes four general tools that facilitate proactive and efficient information retrieval by the LLM. Additionally, we have developed detailed exemplars to demonstrate the step-wise reasoning processes within our framework. Our approach achieves advanced performance on the Spider and BIRD datasets as well as their variants. Notably, we obtain state-of-the-art results on the BIRD leaderboard under the setting without oracle knowledge, demonstrating the effectiveness of our method."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-09T07:43:21Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    7,
                    43,
                    21,
                    4,
                    222,
                    0
                ],
                "arxiv_comment": "This work has been accepted to CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Guanming Xiong"
                    },
                    {
                        "name": "Junwei Bao"
                    },
                    {
                        "name": "Hongfei Jiang"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Wen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Zhao"
                },
                "author": "Wen Zhao",
                "arxiv_doi": "10.1145/3746252.3761052"
            },
            {
                "id": "http://arxiv.org/abs/2511.10384v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10384v1",
                "title": "Simulating Misinformation Propagation in Social Networks using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Misinformation Propagation in Social Networks using Large Language Models"
                },
                "updated": "2025-11-13T15:01:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    1,
                    19,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10384v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. News articles are propagated across networks of persona-conditioned LLM nodes, each rewriting received content. A question--answering-based auditor then measures factual fidelity at every step, offering interpretable, claim-level tracking of misinformation drift. We formalize a misinformation index and a misinformation propagation rate to quantify factual degradation across homogeneous and heterogeneous branches of up to 30 sequential rewrites. Experiments with 21 personas across 10 domains reveal that identity- and ideology-based personas act as misinformation accelerators, especially in politics, marketing, and technology. By contrast, expert-driven personas preserve factual stability. Controlled-random branch simulations further show that once early distortions emerge, heterogeneous persona interactions rapidly escalate misinformation to propaganda-level distortion. Our taxonomy of misinformation severity -- spanning factual errors, lies, and propaganda -- connects observed drift to established theories in misinformation studies. These findings demonstrate the dual role of LLMs as both proxies for human-like biases and as auditors capable of tracing information fidelity. The proposed framework provides an interpretable, empirically grounded approach for studying, simulating, and mitigating misinformation diffusion in digital ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. News articles are propagated across networks of persona-conditioned LLM nodes, each rewriting received content. A question--answering-based auditor then measures factual fidelity at every step, offering interpretable, claim-level tracking of misinformation drift. We formalize a misinformation index and a misinformation propagation rate to quantify factual degradation across homogeneous and heterogeneous branches of up to 30 sequential rewrites. Experiments with 21 personas across 10 domains reveal that identity- and ideology-based personas act as misinformation accelerators, especially in politics, marketing, and technology. By contrast, expert-driven personas preserve factual stability. Controlled-random branch simulations further show that once early distortions emerge, heterogeneous persona interactions rapidly escalate misinformation to propaganda-level distortion. Our taxonomy of misinformation severity -- spanning factual errors, lies, and propaganda -- connects observed drift to established theories in misinformation studies. These findings demonstrate the dual role of LLMs as both proxies for human-like biases and as auditors capable of tracing information fidelity. The proposed framework provides an interpretable, empirically grounded approach for studying, simulating, and mitigating misinformation diffusion in digital ecosystems."
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:01:19Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    1,
                    19,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted to CIKM 2025 Workshop LASS",
                "arxiv_primary_category": {
                    "term": "cs.SI"
                },
                "authors": [
                    {
                        "name": "Raj Gaurav Maurya"
                    },
                    {
                        "name": "Vaibhav Shukla"
                    },
                    {
                        "name": "Raj Abhijit Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat"
            },
            {
                "id": "http://arxiv.org/abs/2511.10381v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10381v1",
                "title": "Position: On the Methodological Pitfalls of Evaluating Base LLMs for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: On the Methodological Pitfalls of Evaluating Base LLMs for Reasoning"
                },
                "updated": "2025-11-13T14:55:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    55,
                    51,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10381v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing work investigates the reasoning capabilities of large language models (LLMs) to uncover their limitations, human-like biases and underlying processes. Such studies include evaluations of base LLMs (pre-trained on unlabeled corpora only) for this purpose. Our position paper argues that evaluating base LLMs' reasoning capabilities raises inherent methodological concerns that are overlooked in such existing studies. We highlight the fundamental mismatch between base LLMs' pretraining objective and normative qualities, such as correctness, by which reasoning is assessed. In particular, we show how base LLMs generate logically valid or invalid conclusions as coincidental byproducts of conforming to purely linguistic patterns of statistical plausibility. This fundamental mismatch challenges the assumptions that (a) base LLMs' outputs can be assessed as their bona fide attempts at correct answers or conclusions; and (b) conclusions about base LLMs' reasoning can generalize to post-trained LLMs optimized for successful instruction-following. We call for a critical re-examination of existing work that relies implicitly on these assumptions, and for future work to account for these methodological pitfalls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing work investigates the reasoning capabilities of large language models (LLMs) to uncover their limitations, human-like biases and underlying processes. Such studies include evaluations of base LLMs (pre-trained on unlabeled corpora only) for this purpose. Our position paper argues that evaluating base LLMs' reasoning capabilities raises inherent methodological concerns that are overlooked in such existing studies. We highlight the fundamental mismatch between base LLMs' pretraining objective and normative qualities, such as correctness, by which reasoning is assessed. In particular, we show how base LLMs generate logically valid or invalid conclusions as coincidental byproducts of conforming to purely linguistic patterns of statistical plausibility. This fundamental mismatch challenges the assumptions that (a) base LLMs' outputs can be assessed as their bona fide attempts at correct answers or conclusions; and (b) conclusions about base LLMs' reasoning can generalize to post-trained LLMs optimized for successful instruction-following. We call for a critical re-examination of existing work that relies implicitly on these assumptions, and for future work to account for these methodological pitfalls."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:55:51Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    55,
                    51,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Preprint",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jason Chan"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Robert Gaizauskas"
                    }
                ],
                "author_detail": {
                    "name": "Robert Gaizauskas"
                },
                "author": "Robert Gaizauskas"
            },
            {
                "id": "http://arxiv.org/abs/2511.10376v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10376v1",
                "title": "MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation"
                },
                "updated": "2025-11-13T14:51:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    51,
                    21,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10376v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relational edges with dynamically assigned images. Built on M3DSG, we propose MSGNav, a zero-shot navigation system that includes a Key Subgraph Selection module for efficient reasoning, an Adaptive Vocabulary Update module for open vocabulary support, and a Closed-Loop Reasoning module for accurate exploration reasoning. Additionally, we further identify the last-mile problem in zero-shot navigation - determining the feasible target location with a suitable final viewpoint, and propose a Visibility-based Viewpoint Decision module to explicitly resolve it. Comprehensive experimental results demonstrate that MSGNav achieves state-of-the-art performance on GOAT-Bench and HM3D-OVON datasets. The open-source code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relational edges with dynamically assigned images. Built on M3DSG, we propose MSGNav, a zero-shot navigation system that includes a Key Subgraph Selection module for efficient reasoning, an Adaptive Vocabulary Update module for open vocabulary support, and a Closed-Loop Reasoning module for accurate exploration reasoning. Additionally, we further identify the last-mile problem in zero-shot navigation - determining the feasible target location with a suitable final viewpoint, and propose a Visibility-based Viewpoint Decision module to explicitly resolve it. Comprehensive experimental results demonstrate that MSGNav achieves state-of-the-art performance on GOAT-Bench and HM3D-OVON datasets. The open-source code will be publicly available."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:51:21Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    51,
                    21,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "10 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xun Huang"
                    },
                    {
                        "name": "Shijia Zhao"
                    },
                    {
                        "name": "Yunxiang Wang"
                    },
                    {
                        "name": "Xin Lu"
                    },
                    {
                        "name": "Wanfa Zhang"
                    },
                    {
                        "name": "Rongsheng Qu"
                    },
                    {
                        "name": "Weixin Li"
                    },
                    {
                        "name": "Yunhong Wang"
                    },
                    {
                        "name": "Chenglu Wen"
                    }
                ],
                "author_detail": {
                    "name": "Chenglu Wen"
                },
                "author": "Chenglu Wen"
            },
            {
                "id": "http://arxiv.org/abs/2511.10375v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10375v1",
                "title": "TruthfulRAG: Resolving Factual-level Conflicts in Retrieval-Augmented Generation with Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TruthfulRAG: Resolving Factual-level Conflicts in Retrieval-Augmented Generation with Knowledge Graphs"
                },
                "updated": "2025-11-13T14:49:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    49,
                    44,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10375v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for enhancing the capabilities of Large Language Models (LLMs) by integrating retrieval-based methods with generative models. As external knowledge repositories continue to expand and the parametric knowledge within models becomes outdated, a critical challenge for RAG systems is resolving conflicts between retrieved external information and LLMs' internal knowledge, which can significantly compromise the accuracy and reliability of generated content. However, existing approaches to conflict resolution typically operate at the token or semantic level, often leading to fragmented and partial understanding of factual discrepancies between LLMs' knowledge and context, particularly in knowledge-intensive tasks. To address this limitation, we propose TruthfulRAG, the first framework that leverages Knowledge Graphs (KGs) to resolve factual-level knowledge conflicts in RAG systems. Specifically, TruthfulRAG constructs KGs by systematically extracting triples from retrieved content, utilizes query-based graph retrieval to identify relevant knowledge, and employs entropy-based filtering mechanisms to precisely locate conflicting elements and mitigate factual inconsistencies, thereby enabling LLMs to generate faithful and accurate responses. Extensive experiments reveal that TruthfulRAG outperforms existing methods, effectively alleviating knowledge conflicts and improving the robustness and trustworthiness of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for enhancing the capabilities of Large Language Models (LLMs) by integrating retrieval-based methods with generative models. As external knowledge repositories continue to expand and the parametric knowledge within models becomes outdated, a critical challenge for RAG systems is resolving conflicts between retrieved external information and LLMs' internal knowledge, which can significantly compromise the accuracy and reliability of generated content. However, existing approaches to conflict resolution typically operate at the token or semantic level, often leading to fragmented and partial understanding of factual discrepancies between LLMs' knowledge and context, particularly in knowledge-intensive tasks. To address this limitation, we propose TruthfulRAG, the first framework that leverages Knowledge Graphs (KGs) to resolve factual-level knowledge conflicts in RAG systems. Specifically, TruthfulRAG constructs KGs by systematically extracting triples from retrieved content, utilizes query-based graph retrieval to identify relevant knowledge, and employs entropy-based filtering mechanisms to precisely locate conflicting elements and mitigate factual inconsistencies, thereby enabling LLMs to generate faithful and accurate responses. Extensive experiments reveal that TruthfulRAG outperforms existing methods, effectively alleviating knowledge conflicts and improving the robustness and trustworthiness of RAG systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:49:44Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    49,
                    44,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "12 pages, 3 figures, accepted at AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shuyi Liu"
                    },
                    {
                        "name": "Yuming Shang"
                    },
                    {
                        "name": "Xi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xi Zhang"
                },
                "author": "Xi Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.10370v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10370v1",
                "title": "SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation"
                },
                "updated": "2025-11-13T14:48:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    48,
                    55,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10370v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Geospatial foundation models for Earth observation often fail to perform reliably in environments underrepresented during pretraining. We introduce SHRUG-FM, a framework for reliability-aware prediction that integrates three complementary signals: out-of-distribution (OOD) detection in the input space, OOD detection in the embedding space and task-specific predictive uncertainty. Applied to burn scar segmentation, SHRUG-FM shows that OOD scores correlate with lower performance in specific environmental conditions, while uncertainty-based flags help discard many poorly performing predictions. Linking these flags to land cover attributes from HydroATLAS shows that failures are not random but concentrated in certain geographies, such as low-elevation zones and large river areas, likely due to underrepresentation in pretraining data. SHRUG-FM provides a pathway toward safer and more interpretable deployment of GFMs in climate-sensitive applications, helping bridge the gap between benchmark performance and real-world reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geospatial foundation models for Earth observation often fail to perform reliably in environments underrepresented during pretraining. We introduce SHRUG-FM, a framework for reliability-aware prediction that integrates three complementary signals: out-of-distribution (OOD) detection in the input space, OOD detection in the embedding space and task-specific predictive uncertainty. Applied to burn scar segmentation, SHRUG-FM shows that OOD scores correlate with lower performance in specific environmental conditions, while uncertainty-based flags help discard many poorly performing predictions. Linking these flags to land cover attributes from HydroATLAS shows that failures are not random but concentrated in certain geographies, such as low-elevation zones and large river areas, likely due to underrepresentation in pretraining data. SHRUG-FM provides a pathway toward safer and more interpretable deployment of GFMs in climate-sensitive applications, helping bridge the gap between benchmark performance and real-world reliability."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:48:55Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    48,
                    55,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Kai-Hendrik Cohrs"
                    },
                    {
                        "name": "Zuzanna Osika"
                    },
                    {
                        "name": "Maria Gonzalez-Calabuig"
                    },
                    {
                        "name": "Vishal Nedungadi"
                    },
                    {
                        "name": "Ruben Cartuyvels"
                    },
                    {
                        "name": "Steffen Knoblauch"
                    },
                    {
                        "name": "Joppe Massant"
                    },
                    {
                        "name": "Shruti Nath"
                    },
                    {
                        "name": "Patrick Ebel"
                    },
                    {
                        "name": "Vasileios Sitokonstantinou"
                    }
                ],
                "author_detail": {
                    "name": "Vasileios Sitokonstantinou"
                },
                "author": "Vasileios Sitokonstantinou"
            },
            {
                "id": "http://arxiv.org/abs/2511.10356v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10356v1",
                "title": "SITA: A Framework for Structure-to-Instance Theorem Autoformalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SITA: A Framework for Structure-to-Instance Theorem Autoformalization"
                },
                "updated": "2025-11-13T14:33:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    33,
                    51,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10356v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While large language models (LLMs) have shown progress in mathematical reasoning, they still face challenges in formalizing theorems that arise from instantiating abstract structures in concrete settings. With the goal of auto-formalizing mathematical results at the research level, we develop a framework for structure-to-instance theorem autoformalization (SITA), which systematically bridges the gap between abstract mathematical theories and their concrete applications in Lean proof assistant. Formalized abstract structures are treated as modular templates that contain definitions, assumptions, operations, and theorems. These templates serve as reusable guides for the formalization of concrete instances. Given a specific instantiation, we generate corresponding Lean definitions and instance declarations, integrate them using Lean's typeclass mechanism, and construct verified theorems by checking structural assumptions. We incorporate LLM-based generation with feedback-guided refinement to ensure both automation and formal correctness. Experiments on a dataset of optimization problems demonstrate that SITA effectively formalizes diverse instances grounded in abstract structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown progress in mathematical reasoning, they still face challenges in formalizing theorems that arise from instantiating abstract structures in concrete settings. With the goal of auto-formalizing mathematical results at the research level, we develop a framework for structure-to-instance theorem autoformalization (SITA), which systematically bridges the gap between abstract mathematical theories and their concrete applications in Lean proof assistant. Formalized abstract structures are treated as modular templates that contain definitions, assumptions, operations, and theorems. These templates serve as reusable guides for the formalization of concrete instances. Given a specific instantiation, we generate corresponding Lean definitions and instance declarations, integrate them using Lean's typeclass mechanism, and construct verified theorems by checking structural assumptions. We incorporate LLM-based generation with feedback-guided refinement to ensure both automation and formal correctness. Experiments on a dataset of optimization problems demonstrate that SITA effectively formalizes diverse instances grounded in abstract structures."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:33:51Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    33,
                    51,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Chenyi Li"
                    },
                    {
                        "name": "Wanli Ma"
                    },
                    {
                        "name": "Zichen Wang"
                    },
                    {
                        "name": "Zaiwen Wen"
                    }
                ],
                "author_detail": {
                    "name": "Zaiwen Wen"
                },
                "author": "Zaiwen Wen"
            },
            {
                "id": "http://arxiv.org/abs/2511.10354v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10354v1",
                "title": "Knowledge Graphs Generation from Cultural Heritage Texts: Combining LLMs and Ontological Engineering for Scholarly Debates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graphs Generation from Cultural Heritage Texts: Combining LLMs and Ontological Engineering for Scholarly Debates"
                },
                "updated": "2025-11-13T14:29:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    29,
                    51,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10354v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cultural Heritage texts contain rich knowledge that is difficult to query systematically due to the challenges of converting unstructured discourse into structured Knowledge Graphs (KGs). This paper introduces ATR4CH (Adaptive Text-to-RDF for Cultural Heritage), a systematic five-step methodology for Large Language Model-based Knowledge Extraction from Cultural Heritage documents. We validate the methodology through a case study on authenticity assessment debates. Methodology - ATR4CH combines annotation models, ontological frameworks, and LLM-based extraction through iterative development: foundational analysis, annotation schema development, pipeline architecture, integration refinement, and comprehensive evaluation. We demonstrate the approach using Wikipedia articles about disputed items (documents, artifacts...), implementing a sequential pipeline with three LLMs (Claude Sonnet 3.7, Llama 3.3 70B, GPT-4o-mini). Findings - The methodology successfully extracts complex Cultural Heritage knowledge: 0.96-0.99 F1 for metadata extraction, 0.7-0.8 F1 for entity recognition, 0.65-0.75 F1 for hypothesis extraction, 0.95-0.97 for evidence extraction, and 0.62 G-EVAL for discourse representation. Smaller models performed competitively, enabling cost-effective deployment. Originality - This is the first systematic methodology for coordinating LLM-based extraction with Cultural Heritage ontologies. ATR4CH provides a replicable framework adaptable across CH domains and institutional resources. Research Limitations - The produced KG is limited to Wikipedia articles. While the results are encouraging, human oversight is necessary during post-processing. Practical Implications - ATR4CH enables Cultural Heritage institutions to systematically convert textual knowledge into queryable KGs, supporting automated metadata enrichment and knowledge discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural Heritage texts contain rich knowledge that is difficult to query systematically due to the challenges of converting unstructured discourse into structured Knowledge Graphs (KGs). This paper introduces ATR4CH (Adaptive Text-to-RDF for Cultural Heritage), a systematic five-step methodology for Large Language Model-based Knowledge Extraction from Cultural Heritage documents. We validate the methodology through a case study on authenticity assessment debates. Methodology - ATR4CH combines annotation models, ontological frameworks, and LLM-based extraction through iterative development: foundational analysis, annotation schema development, pipeline architecture, integration refinement, and comprehensive evaluation. We demonstrate the approach using Wikipedia articles about disputed items (documents, artifacts...), implementing a sequential pipeline with three LLMs (Claude Sonnet 3.7, Llama 3.3 70B, GPT-4o-mini). Findings - The methodology successfully extracts complex Cultural Heritage knowledge: 0.96-0.99 F1 for metadata extraction, 0.7-0.8 F1 for entity recognition, 0.65-0.75 F1 for hypothesis extraction, 0.95-0.97 for evidence extraction, and 0.62 G-EVAL for discourse representation. Smaller models performed competitively, enabling cost-effective deployment. Originality - This is the first systematic methodology for coordinating LLM-based extraction with Cultural Heritage ontologies. ATR4CH provides a replicable framework adaptable across CH domains and institutional resources. Research Limitations - The produced KG is limited to Wikipedia articles. While the results are encouraging, human oversight is necessary during post-processing. Practical Implications - ATR4CH enables Cultural Heritage institutions to systematically convert textual knowledge into queryable KGs, supporting automated metadata enrichment and knowledge discovery."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:29:51Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    29,
                    51,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "46 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Andrea Schimmenti"
                    },
                    {
                        "name": "Valentina Pasqual"
                    },
                    {
                        "name": "Fabio Vitali"
                    },
                    {
                        "name": "Marieke van Erp"
                    }
                ],
                "author_detail": {
                    "name": "Marieke van Erp"
                },
                "author": "Marieke van Erp"
            },
            {
                "id": "http://arxiv.org/abs/2511.09231v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09231v2",
                "title": "Leveraging Large Language Models for Use Case Model Generation from Software Requirements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Use Case Model Generation from Software Requirements"
                },
                "updated": "2025-11-13T14:27:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    27,
                    15,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09231v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T11:47:33Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    11,
                    47,
                    33,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "Accepted at the Intelligent Software Engineering Workshop (ISE 2025) at ASE 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Tobias Eisenreich"
                    },
                    {
                        "name": "Nicholas Friedlaender"
                    },
                    {
                        "name": "Stefan Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wagner"
                },
                "author": "Stefan Wagner"
            },
            {
                "id": "http://arxiv.org/abs/2511.08525v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08525v2",
                "title": "Investigating CoT Monitorability in Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating CoT Monitorability in Large Reasoning Models"
                },
                "updated": "2025-11-13T14:15:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    15,
                    59,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08525v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models' long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models' misbehavior through their CoT and provide structured judgments along with supporting evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models' long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models' misbehavior through their CoT and provide structured judgments along with supporting evidence."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T18:06:34Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    18,
                    6,
                    34,
                    1,
                    315,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Junchao Wu"
                    },
                    {
                        "name": "Xilin Gong"
                    },
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Derek Wong"
                    },
                    {
                        "name": "Ninhao Liu"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.10338v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10338v1",
                "title": "BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages"
                },
                "updated": "2025-11-13T14:12:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    12,
                    44,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10338v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In the context of pretraining of Large Language Models (LLMs), synthetic data has emerged as an alternative for generating high-quality pretraining data at scale. This is particularly beneficial in low-resource language settings where the benefits of recent LLMs have been unevenly distributed across languages. In this work, we present a systematic study on the generation and evaluation of synthetic multilingual pretraining data for Indic languages, where we construct a large-scale synthetic dataset BhashaKritika, comprising 540B tokens using 5 different techniques for 10 languages. We explore the impact of grounding generation in documents, personas, and topics. We analyze how language choice, both in the prompt instructions and document grounding, affects data quality, and we compare translations of English content with native generation in Indic languages. To support scalable and language-sensitive evaluation, we introduce a modular quality evaluation pipeline that integrates script and language detection, metadata consistency checks, n-gram repetition analysis, and perplexity-based filtering using KenLM models. Our framework enables robust quality control across diverse scripts and linguistic contexts. Empirical results through model runs reveal key trade-offs in generation strategies and highlight best practices for constructing effective multilingual corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of pretraining of Large Language Models (LLMs), synthetic data has emerged as an alternative for generating high-quality pretraining data at scale. This is particularly beneficial in low-resource language settings where the benefits of recent LLMs have been unevenly distributed across languages. In this work, we present a systematic study on the generation and evaluation of synthetic multilingual pretraining data for Indic languages, where we construct a large-scale synthetic dataset BhashaKritika, comprising 540B tokens using 5 different techniques for 10 languages. We explore the impact of grounding generation in documents, personas, and topics. We analyze how language choice, both in the prompt instructions and document grounding, affects data quality, and we compare translations of English content with native generation in Indic languages. To support scalable and language-sensitive evaluation, we introduce a modular quality evaluation pipeline that integrates script and language detection, metadata consistency checks, n-gram repetition analysis, and perplexity-based filtering using KenLM models. Our framework enables robust quality control across diverse scripts and linguistic contexts. Empirical results through model runs reveal key trade-offs in generation strategies and highlight best practices for constructing effective multilingual corpora."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:12:44Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    12,
                    44,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Guduru Manoj"
                    },
                    {
                        "name": "Neel Prabhanjan Rachamalla"
                    },
                    {
                        "name": "Ashish Kulkarni"
                    },
                    {
                        "name": "Gautam Rajeev"
                    },
                    {
                        "name": "Jay Piplodiya"
                    },
                    {
                        "name": "Arul Menezes"
                    },
                    {
                        "name": "Shaharukh Khan"
                    },
                    {
                        "name": "Souvik Rana"
                    },
                    {
                        "name": "Manya Sah"
                    },
                    {
                        "name": "Chandra Khatri"
                    },
                    {
                        "name": "Shubham Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Shubham Agarwal"
                },
                "author": "Shubham Agarwal"
            },
            {
                "id": "http://arxiv.org/abs/2511.10333v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10333v1",
                "title": "EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training"
                },
                "updated": "2025-11-13T14:05:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    5,
                    34,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10333v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T14:05:34Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    5,
                    34,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Qingao Yi"
                    },
                    {
                        "name": "Jiaang Duan"
                    },
                    {
                        "name": "Hanwen Hu"
                    },
                    {
                        "name": "Qin Hua"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Shiyou Qian"
                    },
                    {
                        "name": "Dingyu Yang"
                    },
                    {
                        "name": "Jian Cao"
                    },
                    {
                        "name": "Jinghua Tang"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Chenzhi Liao"
                    },
                    {
                        "name": "Kangjin Wang"
                    },
                    {
                        "name": "Liping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liping Zhang"
                },
                "author": "Liping Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2508.02573v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.02573v2",
                "title": "Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs"
                },
                "updated": "2025-11-13T14:05:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    5,
                    2,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.02573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.02573v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted phenomenon involving distinct underlying mechanisms. We introduce a novel method to analyze the different forms of memorization described by the existing taxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the attention weights of the LLM and evaluate the alignment between this taxonomy and the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect distinct mechanisms within the attention blocks. We propose a new taxonomy that maximizes alignment with the attention weights, consisting of three categories: memorized samples that are guessed using language modeling abilities, memorized samples that are recalled due to high duplication in the training set, and non-memorized samples. Our results reveal that few-shot verbatim memorization does not correspond to a distinct attention mechanism. We also show that a significant proportion of extractable samples are in fact guessed by the model and should therefore be studied separately. Finally, we develop a custom visual interpretability technique to localize the regions of the attention weights involved in each form of memorization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted phenomenon involving distinct underlying mechanisms. We introduce a novel method to analyze the different forms of memorization described by the existing taxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the attention weights of the LLM and evaluate the alignment between this taxonomy and the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect distinct mechanisms within the attention blocks. We propose a new taxonomy that maximizes alignment with the attention weights, consisting of three categories: memorized samples that are guessed using language modeling abilities, memorized samples that are recalled due to high duplication in the training set, and non-memorized samples. Our results reveal that few-shot verbatim memorization does not correspond to a distinct attention mechanism. We also show that a significant proportion of extractable samples are in fact guessed by the model and should therefore be studied separately. Finally, we develop a custom visual interpretability technique to localize the regions of the attention weights involved in each form of memorization."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-04T16:27:56Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    27,
                    56,
                    0,
                    216,
                    0
                ],
                "arxiv_comment": "This paper has been accepted for publication at AAAI-26",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jérémie Dentan"
                    },
                    {
                        "name": "Davide Buscaldi"
                    },
                    {
                        "name": "Sonia Vanier"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Vanier"
                },
                "author": "Sonia Vanier"
            },
            {
                "id": "http://arxiv.org/abs/2511.08132v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08132v2",
                "title": "National Institute on Aging PREPARE Challenge: Early Detection of Cognitive Impairment Using Speech -- The SpeechCARE Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "National Institute on Aging PREPARE Challenge: Early Detection of Cognitive Impairment Using Speech -- The SpeechCARE Solution"
                },
                "updated": "2025-11-13T13:58:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    58,
                    9,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08132v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Alzheimer's disease and related dementias (ADRD) affect one in five adults over 60, yet more than half of individuals with cognitive decline remain undiagnosed. Speech-based assessments show promise for early detection, as phonetic motor planning deficits alter acoustic features (e.g., pitch, tone), while memory and language impairments lead to syntactic and semantic errors. However, conventional speech-processing pipelines with hand-crafted features or general-purpose audio classifiers often exhibit limited performance and generalizability. To address these limitations, we introduce SpeechCARE, a multimodal speech processing pipeline that leverages pretrained, multilingual acoustic and linguistic transformer models to capture subtle speech-related cues associated with cognitive impairment. Inspired by the Mixture of Experts (MoE) paradigm, SpeechCARE employs a dynamic fusion architecture that weights transformer-based acoustic, linguistic, and demographic inputs, allowing integration of additional modalities (e.g., social factors, imaging) and enhancing robustness across diverse tasks. Its robust preprocessing includes automatic transcription, large language model (LLM)-based anomaly detection, and task identification. A SHAP-based explainability module and LLM reasoning highlight each modality's contribution to decision-making. SpeechCARE achieved AUC = 0.88 and F1 = 0.72 for classifying cognitively healthy, MCI, and AD individuals, with AUC = 0.90 and F1 = 0.62 for MCI detection. Bias analysis showed minimal disparities, except for adults over 80. Mitigation techniques included oversampling and weighted loss. Future work includes deployment in real-world care settings (e.g., VNS Health, Columbia ADRC) and EHR-integrated explainability for underrepresented populations in New York City.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alzheimer's disease and related dementias (ADRD) affect one in five adults over 60, yet more than half of individuals with cognitive decline remain undiagnosed. Speech-based assessments show promise for early detection, as phonetic motor planning deficits alter acoustic features (e.g., pitch, tone), while memory and language impairments lead to syntactic and semantic errors. However, conventional speech-processing pipelines with hand-crafted features or general-purpose audio classifiers often exhibit limited performance and generalizability. To address these limitations, we introduce SpeechCARE, a multimodal speech processing pipeline that leverages pretrained, multilingual acoustic and linguistic transformer models to capture subtle speech-related cues associated with cognitive impairment. Inspired by the Mixture of Experts (MoE) paradigm, SpeechCARE employs a dynamic fusion architecture that weights transformer-based acoustic, linguistic, and demographic inputs, allowing integration of additional modalities (e.g., social factors, imaging) and enhancing robustness across diverse tasks. Its robust preprocessing includes automatic transcription, large language model (LLM)-based anomaly detection, and task identification. A SHAP-based explainability module and LLM reasoning highlight each modality's contribution to decision-making. SpeechCARE achieved AUC = 0.88 and F1 = 0.72 for classifying cognitively healthy, MCI, and AD individuals, with AUC = 0.90 and F1 = 0.62 for MCI detection. Bias analysis showed minimal disparities, except for adults over 80. Mitigation techniques included oversampling and weighted loss. Future work includes deployment in real-world care settings (e.g., VNS Health, Columbia ADRC) and EHR-integrated explainability for underrepresented populations in New York City."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T11:39:20Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    11,
                    39,
                    20,
                    1,
                    315,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Maryam Zolnoori"
                    },
                    {
                        "name": "Hossein Azadmaleki"
                    },
                    {
                        "name": "Yasaman Haghbin"
                    },
                    {
                        "name": "Ali Zolnour"
                    },
                    {
                        "name": "Mohammad Javad Momeni Nezhad"
                    },
                    {
                        "name": "Sina Rashidi"
                    },
                    {
                        "name": "Mehdi Naserian"
                    },
                    {
                        "name": "Elyas Esmaeili"
                    },
                    {
                        "name": "Sepehr Karimi Arpanahi"
                    }
                ],
                "author_detail": {
                    "name": "Sepehr Karimi Arpanahi"
                },
                "author": "Sepehr Karimi Arpanahi"
            },
            {
                "id": "http://arxiv.org/abs/2505.17519v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.17519v2",
                "title": "Chain-of-Lure: A Universal Jailbreak Attack Framework using Unconstrained Synthetic Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Lure: A Universal Jailbreak Attack Framework using Unconstrained Synthetic Narratives"
                },
                "updated": "2025-11-13T13:50:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    50,
                    58,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.17519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.17519v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In the era of rapid generative AI development, interactions with large language models (LLMs) pose increasing risks of misuse. Prior research has primarily focused on attacks using template-based prompts and optimization-oriented methods, while overlooking the fact that LLMs possess strong unconstrained deceptive capabilities to attack other LLMs. This paper introduces a novel jailbreaking method inspired by the Chain-of-Thought mechanism. The attacker employs mission transfer to conceal harmful user intent within dialogue and generates a progressive chain of lure questions without relying on predefined templates, enabling successful jailbreaks. To further improve the attack's strength, we incorporate a helper LLM model that performs randomized narrative optimization over multi-turn interactions, enhancing the attack performance while preserving alignment with the original intent. We also propose a toxicity-based framework using third-party LLMs to evaluate harmful content and its alignment with malicious intent. Extensive experiments demonstrate that our method consistently achieves high attack success rates and elevated toxicity scores across diverse types of LLMs under black-box API settings. These findings reveal the intrinsic potential of LLMs to perform unrestricted attacks in the absence of robust alignment constraints. Our approach offers data-driven insights to inform the design of future alignment mechanisms. Finally, we propose two concrete defense strategies to support the development of safer generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of rapid generative AI development, interactions with large language models (LLMs) pose increasing risks of misuse. Prior research has primarily focused on attacks using template-based prompts and optimization-oriented methods, while overlooking the fact that LLMs possess strong unconstrained deceptive capabilities to attack other LLMs. This paper introduces a novel jailbreaking method inspired by the Chain-of-Thought mechanism. The attacker employs mission transfer to conceal harmful user intent within dialogue and generates a progressive chain of lure questions without relying on predefined templates, enabling successful jailbreaks. To further improve the attack's strength, we incorporate a helper LLM model that performs randomized narrative optimization over multi-turn interactions, enhancing the attack performance while preserving alignment with the original intent. We also propose a toxicity-based framework using third-party LLMs to evaluate harmful content and its alignment with malicious intent. Extensive experiments demonstrate that our method consistently achieves high attack success rates and elevated toxicity scores across diverse types of LLMs under black-box API settings. These findings reveal the intrinsic potential of LLMs to perform unrestricted attacks in the absence of robust alignment constraints. Our approach offers data-driven insights to inform the design of future alignment mechanisms. Finally, we propose two concrete defense strategies to support the development of safer generative models."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-23T06:19:05Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    6,
                    19,
                    5,
                    4,
                    143,
                    0
                ],
                "arxiv_comment": "23 pages, 3 main figures",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Wenhan Chang"
                    },
                    {
                        "name": "Tianqing Zhu"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Shuangyong Song"
                    },
                    {
                        "name": "Ping Xiong"
                    },
                    {
                        "name": "Wanlei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wanlei Zhou"
                },
                "author": "Wanlei Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2511.10310v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10310v1",
                "title": "Reconfigurable Airspace: Synergizing Movable Antenna and Intelligent Surface for Low-Altitude ISAC Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Airspace: Synergizing Movable Antenna and Intelligent Surface for Low-Altitude ISAC Networks"
                },
                "updated": "2025-11-13T13:47:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    47,
                    24,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10310v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Low-altitude unmanned aerial vehicle (UAV) networks are integral to future 6G integrated sensing and communication (ISAC) systems. However, their deployment is hindered by challenges stemming from high mobility of UAVs, complex propagation environments, and the inherent trade-offs between coexisting sensing and communication functions. This article proposes a novel framework that leverages movable antennas (MAs) and intelligent reflecting surfaces (IRSs) as dual enablers to overcome these limitations. MAs, through active transceiver reconfiguration, and IRSs, via passive channel reconstruction, can work in synergy to significantly enhance system performance. Our analysis first elaborates on the fundamental gains offered by MAs and IRSs, and provides simulation results that validate the immense potential of the MA-IRS-enabled ISAC architecture. Two core UAV deployment scenarios are then investigated: (i) UAVs as ISAC users, where we focus on achieving high-precision tracking and aerial safety, and (ii) UAVs as aerial network nodes, where we address robust design and complex coupled resource optimization. Finally, key technical challenges and research opportunities are identified and analyzed for each scenario, charting a clear course for the future design of advanced low-altitude ISAC networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-altitude unmanned aerial vehicle (UAV) networks are integral to future 6G integrated sensing and communication (ISAC) systems. However, their deployment is hindered by challenges stemming from high mobility of UAVs, complex propagation environments, and the inherent trade-offs between coexisting sensing and communication functions. This article proposes a novel framework that leverages movable antennas (MAs) and intelligent reflecting surfaces (IRSs) as dual enablers to overcome these limitations. MAs, through active transceiver reconfiguration, and IRSs, via passive channel reconstruction, can work in synergy to significantly enhance system performance. Our analysis first elaborates on the fundamental gains offered by MAs and IRSs, and provides simulation results that validate the immense potential of the MA-IRS-enabled ISAC architecture. Two core UAV deployment scenarios are then investigated: (i) UAVs as ISAC users, where we focus on achieving high-precision tracking and aerial safety, and (ii) UAVs as aerial network nodes, where we address robust design and complex coupled resource optimization. Finally, key technical challenges and research opportunities are identified and analyzed for each scenario, charting a clear course for the future design of advanced low-altitude ISAC networks."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:47:24Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    47,
                    24,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Honghao Wang"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Yifan Jiang"
                    },
                    {
                        "name": "Ziyuan Zheng"
                    },
                    {
                        "name": "Ziheng Zhang"
                    },
                    {
                        "name": "Yanze Zhu"
                    },
                    {
                        "name": "Ying Gao"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Guanghai Liu"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Jamalipour"
                },
                "author": "Abbas Jamalipour"
            },
            {
                "id": "http://arxiv.org/abs/2511.10303v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10303v1",
                "title": "Rectify Evaluation Preference: Improving LLMs' Critique on Math Reasoning via Perplexity-aware Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rectify Evaluation Preference: Improving LLMs' Critique on Math Reasoning via Perplexity-aware Reinforcement Learning"
                },
                "updated": "2025-11-13T13:37:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    37,
                    45,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10303v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "To improve Multi-step Mathematical Reasoning (MsMR) of Large Language Models (LLMs), it is crucial to obtain scalable supervision from the corpus by automatically critiquing mistakes in the reasoning process of MsMR and rendering a final verdict of the problem-solution. Most existing methods rely on crafting high-quality supervised fine-tuning demonstrations for critiquing capability enhancement and pay little attention to delving into the underlying reason for the poor critiquing performance of LLMs. In this paper, we orthogonally quantify and investigate the potential reason -- imbalanced evaluation preference, and conduct a statistical preference analysis. Motivated by the analysis of the reason, a novel perplexity-aware reinforcement learning algorithm is proposed to rectify the evaluation preference, elevating the critiquing capability. Specifically, to probe into LLMs' critiquing characteristics, a One-to-many Problem-Solution (OPS) benchmark is meticulously constructed to quantify the behavior difference of LLMs when evaluating the problem solutions generated by itself and others. Then, to investigate the behavior difference in depth, we conduct a statistical preference analysis oriented on perplexity and find an intriguing phenomenon -- ``LLMs incline to judge solutions with lower perplexity as correct'', which is dubbed as \\textit{imbalanced evaluation preference}. To rectify this preference, we regard perplexity as the baton in the algorithm of Group Relative Policy Optimization, supporting the LLMs to explore trajectories that judge lower perplexity as wrong and higher perplexity as correct. Extensive experimental results on our built OPS and existing available critic benchmarks demonstrate the validity of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve Multi-step Mathematical Reasoning (MsMR) of Large Language Models (LLMs), it is crucial to obtain scalable supervision from the corpus by automatically critiquing mistakes in the reasoning process of MsMR and rendering a final verdict of the problem-solution. Most existing methods rely on crafting high-quality supervised fine-tuning demonstrations for critiquing capability enhancement and pay little attention to delving into the underlying reason for the poor critiquing performance of LLMs. In this paper, we orthogonally quantify and investigate the potential reason -- imbalanced evaluation preference, and conduct a statistical preference analysis. Motivated by the analysis of the reason, a novel perplexity-aware reinforcement learning algorithm is proposed to rectify the evaluation preference, elevating the critiquing capability. Specifically, to probe into LLMs' critiquing characteristics, a One-to-many Problem-Solution (OPS) benchmark is meticulously constructed to quantify the behavior difference of LLMs when evaluating the problem solutions generated by itself and others. Then, to investigate the behavior difference in depth, we conduct a statistical preference analysis oriented on perplexity and find an intriguing phenomenon -- ``LLMs incline to judge solutions with lower perplexity as correct'', which is dubbed as \\textit{imbalanced evaluation preference}. To rectify this preference, we regard perplexity as the baton in the algorithm of Group Relative Policy Optimization, supporting the LLMs to explore trajectories that judge lower perplexity as wrong and higher perplexity as correct. Extensive experimental results on our built OPS and existing available critic benchmarks demonstrate the validity of our method."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:37:45Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    37,
                    45,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Changyuan Tian"
                    },
                    {
                        "name": "Zhicong Lu"
                    },
                    {
                        "name": "Shuang Qian"
                    },
                    {
                        "name": "Nayu Liu"
                    },
                    {
                        "name": "Peiguang Li"
                    },
                    {
                        "name": "Li Jin"
                    },
                    {
                        "name": "Leiyi Hu"
                    },
                    {
                        "name": "Zhizhao Zeng"
                    },
                    {
                        "name": "Sirui Wang"
                    },
                    {
                        "name": "Ke Zeng"
                    },
                    {
                        "name": "Zhi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Guo"
                },
                "author": "Zhi Guo"
            },
            {
                "id": "http://arxiv.org/abs/2511.10301v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10301v1",
                "title": "Rethinking Visual Information Processing in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Visual Information Processing in Multimodal LLMs"
                },
                "updated": "2025-11-13T13:36:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    36,
                    30,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10301v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities. We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder. To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations. Through extensive controlled experiments on a wide range of LLMs, we demonstrate that LLaViT significantly outperforms the baseline LLaVA method on a multitude of benchmarks, even surpassing models with double its parameter count, establishing a more effective approach to vision-language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities. We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder. To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations. Through extensive controlled experiments on a wide range of LLMs, we demonstrate that LLaViT significantly outperforms the baseline LLaVA method on a multitude of benchmarks, even surpassing models with double its parameter count, establishing a more effective approach to vision-language modeling."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:36:30Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    36,
                    30,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Dongwan Kim"
                    },
                    {
                        "name": "Viresh Ranjan"
                    },
                    {
                        "name": "Takashi Nagata"
                    },
                    {
                        "name": "Arnab Dhua"
                    },
                    {
                        "name": "Amit Kumar K C"
                    }
                ],
                "author_detail": {
                    "name": "Amit Kumar K C"
                },
                "author": "Amit Kumar K C"
            },
            {
                "id": "http://arxiv.org/abs/2511.10297v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10297v1",
                "title": "Local Hybrid Retrieval-Augmented Document QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Hybrid Retrieval-Augmented Document QA"
                },
                "updated": "2025-11-13T13:34:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    34,
                    32,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10297v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Organizations handling sensitive documents face a critical dilemma: adopt cloud-based AI systems that offer powerful question-answering capabilities but compromise data privacy, or maintain local processing that ensures security but delivers poor accuracy. We present a question-answering system that resolves this trade-off by combining semantic understanding with keyword precision, operating entirely on local infrastructure without internet access. Our approach demonstrates that organizations can achieve competitive accuracy on complex queries across legal, scientific, and conversational documents while keeping all data on their machines. By balancing two complementary retrieval strategies and using consumer-grade hardware acceleration, the system delivers reliable answers with minimal errors, letting banks, hospitals, and law firms adopt conversational document AI without transmitting proprietary information to external providers. This work establishes that privacy and performance need not be mutually exclusive in enterprise AI deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organizations handling sensitive documents face a critical dilemma: adopt cloud-based AI systems that offer powerful question-answering capabilities but compromise data privacy, or maintain local processing that ensures security but delivers poor accuracy. We present a question-answering system that resolves this trade-off by combining semantic understanding with keyword precision, operating entirely on local infrastructure without internet access. Our approach demonstrates that organizations can achieve competitive accuracy on complex queries across legal, scientific, and conversational documents while keeping all data on their machines. By balancing two complementary retrieval strategies and using consumer-grade hardware acceleration, the system delivers reliable answers with minimal errors, letting banks, hospitals, and law firms adopt conversational document AI without transmitting proprietary information to external providers. This work establishes that privacy and performance need not be mutually exclusive in enterprise AI deployment."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:34:32Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    34,
                    32,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "10 pages, 5 figures, 3 tables; conference-style (ACL format); fully local RAG system",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Paolo Astrino"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Astrino"
                },
                "author": "Paolo Astrino"
            },
            {
                "id": "http://arxiv.org/abs/2508.11034v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.11034v2",
                "title": "The Impact of Large Language Models (LLMs) on Code Review Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Large Language Models (LLMs) on Code Review Process"
                },
                "updated": "2025-11-13T13:33:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    33,
                    44,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.11034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.11034v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have recently gained prominence in the field of software development, significantly boosting productivity and simplifying teamwork. Although prior studies have examined task-specific applications, the phase-specific effects of LLM assistance on the efficiency of code review processes remain underexplored. This research investigates the effect of GPT on GitHub pull request (PR) workflows, with a focus on reducing resolution time, optimizing phase-specific performance, and assisting developers. We curated a dataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted PRs using a semi-automated heuristic approach that combines keyword-based detection, regular expression filtering, and manual verification until achieving 95% labeling accuracy. We then applied statistical modeling, including multiple linear regression and Mann-Whitney U test, to evaluate differences between GPT-assisted and non-assisted PRs, both at the overall resolution level and across distinct review phases. Our research has revealed that early adoption of GPT can substantially boost the effectiveness of the PR process, leading to considerable time savings at various stages. Our findings suggest that GPT-assisted PRs reduced median resolution time by more than 60% (9 hours compared to 23 hours for non-assisted PRs). We discovered that utilizing GPT can reduce the review time by 33% and the waiting time before acceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we discovered that developers predominantly use GPT for code optimization (60%), bug fixing (26%), and documentation updates (12%). This research sheds light on the impact of the GPT model on the code review process, offering actionable insights for software teams seeking to enhance workflows and promote seamless collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently gained prominence in the field of software development, significantly boosting productivity and simplifying teamwork. Although prior studies have examined task-specific applications, the phase-specific effects of LLM assistance on the efficiency of code review processes remain underexplored. This research investigates the effect of GPT on GitHub pull request (PR) workflows, with a focus on reducing resolution time, optimizing phase-specific performance, and assisting developers. We curated a dataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted PRs using a semi-automated heuristic approach that combines keyword-based detection, regular expression filtering, and manual verification until achieving 95% labeling accuracy. We then applied statistical modeling, including multiple linear regression and Mann-Whitney U test, to evaluate differences between GPT-assisted and non-assisted PRs, both at the overall resolution level and across distinct review phases. Our research has revealed that early adoption of GPT can substantially boost the effectiveness of the PR process, leading to considerable time savings at various stages. Our findings suggest that GPT-assisted PRs reduced median resolution time by more than 60% (9 hours compared to 23 hours for non-assisted PRs). We discovered that utilizing GPT can reduce the review time by 33% and the waiting time before acceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we discovered that developers predominantly use GPT for code optimization (60%), bug fixing (26%), and documentation updates (12%). This research sheds light on the impact of the GPT model on the code review process, offering actionable insights for software teams seeking to enhance workflows and promote seamless collaboration."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-14T19:39:01Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    19,
                    39,
                    1,
                    3,
                    226,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Antonio Collante"
                    },
                    {
                        "name": "Samuel Abedu"
                    },
                    {
                        "name": "SayedHassan Khatoonabadi"
                    },
                    {
                        "name": "Ahmad Abdellatif"
                    },
                    {
                        "name": "Ebube Alor"
                    },
                    {
                        "name": "Emad Shihab"
                    }
                ],
                "author_detail": {
                    "name": "Emad Shihab"
                },
                "author": "Emad Shihab"
            },
            {
                "id": "http://arxiv.org/abs/2511.10292v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10292v1",
                "title": "Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models"
                },
                "updated": "2025-11-13T13:29:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    29,
                    38,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10292v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Vision-Language Models (LVLMs) often suffer from object hallucination, generating text inconsistent with visual inputs, which can critically undermine their reliability. Existing inference-time interventions to mitigate this issue present a challenging trade-off: while methods that steer internal states or adjust output logits can be effective, they often incur substantial computational overhead, typically requiring extra forward passes. This efficiency bottleneck can limit their practicality for real-world, latency-sensitive deployments. In this work, we aim to address this trade-off with Residual-Update Directed DEcoding Regulation (RUDDER), a low-overhead framework that steers LVLMs towards visually-grounded generation. RUDDER is built on two key innovations: (1) Contextual Activation Residual Direction (CARD) vector, a per-sample visual evidence vector extracted from the residual update of a self-attention layer during a single, standard forward pass. (2) A Bayesian-inspired adaptive gate that performs token-wise injection, applying a corrective signal whose strength is conditioned on the model's deviation from the visual context. Extensive experiments on key hallucination benchmarks, including POPE and CHAIR, indicate that RUDDER achieves performance comparable to state-of-the-art methods while introducing negligible computational latency, validating RUDDER as a pragmatic and effective approach for improving LVLMs' reliability without a significant compromise on efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) often suffer from object hallucination, generating text inconsistent with visual inputs, which can critically undermine their reliability. Existing inference-time interventions to mitigate this issue present a challenging trade-off: while methods that steer internal states or adjust output logits can be effective, they often incur substantial computational overhead, typically requiring extra forward passes. This efficiency bottleneck can limit their practicality for real-world, latency-sensitive deployments. In this work, we aim to address this trade-off with Residual-Update Directed DEcoding Regulation (RUDDER), a low-overhead framework that steers LVLMs towards visually-grounded generation. RUDDER is built on two key innovations: (1) Contextual Activation Residual Direction (CARD) vector, a per-sample visual evidence vector extracted from the residual update of a self-attention layer during a single, standard forward pass. (2) A Bayesian-inspired adaptive gate that performs token-wise injection, applying a corrective signal whose strength is conditioned on the model's deviation from the visual context. Extensive experiments on key hallucination benchmarks, including POPE and CHAIR, indicate that RUDDER achieves performance comparable to state-of-the-art methods while introducing negligible computational latency, validating RUDDER as a pragmatic and effective approach for improving LVLMs' reliability without a significant compromise on efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:29:38Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    29,
                    38,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Under review",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhengtao Zou"
                    },
                    {
                        "name": "Ya Gao"
                    },
                    {
                        "name": "Jiarui Guan"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Pekka Marttinen"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Marttinen"
                },
                "author": "Pekka Marttinen"
            },
            {
                "id": "http://arxiv.org/abs/2508.05909v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.05909v2",
                "title": "Beyond Perplexity: Let the Reader Select Retrieval Summaries via Spectrum Projection Score",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Perplexity: Let the Reader Select Retrieval Summaries via Spectrum Projection Score"
                },
                "updated": "2025-11-13T13:28:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    28,
                    8,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.05909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.05909v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have shown improved generation performance through retrieval-augmented generation (RAG) following the retriever-reader paradigm, which supplements model inputs with externally retrieved knowledge. However, prior work often evaluates RAG holistically, assessing the retriever and reader jointly, making it difficult to isolate the true contribution of retrieval, particularly given the prompt sensitivity of LLMs used as readers. We move beyond perplexity and introduce Spectrum Projection Score (SPS), a lightweight and supervision-free metric that allows the reader to gauge the semantic alignment of a retrieved summary with its hidden representation by comparing the area formed by generated tokens from the summary, and the principal directions of subspace in the reader and to measure the relevance. Building on SPS we present xCompress, an inference-time controller framework that dynamically samples, ranks, and compresses retrieval summary candidates. Extensive experiments on five QA benchmarks with four open-sourced LLMs show that SPS not only enhances performance across a range of tasks but also provides a principled perspective on the interaction between retrieval and generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown improved generation performance through retrieval-augmented generation (RAG) following the retriever-reader paradigm, which supplements model inputs with externally retrieved knowledge. However, prior work often evaluates RAG holistically, assessing the retriever and reader jointly, making it difficult to isolate the true contribution of retrieval, particularly given the prompt sensitivity of LLMs used as readers. We move beyond perplexity and introduce Spectrum Projection Score (SPS), a lightweight and supervision-free metric that allows the reader to gauge the semantic alignment of a retrieved summary with its hidden representation by comparing the area formed by generated tokens from the summary, and the principal directions of subspace in the reader and to measure the relevance. Building on SPS we present xCompress, an inference-time controller framework that dynamically samples, ranks, and compresses retrieval summary candidates. Extensive experiments on five QA benchmarks with four open-sourced LLMs show that SPS not only enhances performance across a range of tasks but also provides a principled perspective on the interaction between retrieval and generation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-08T00:13:48Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    0,
                    13,
                    48,
                    4,
                    220,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026 Oral. Project link: https://zhanghao-aaai2026-sps.github.io/AAAI2026-SPS/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhanghao Hu"
                    },
                    {
                        "name": "Qinglin Zhu"
                    },
                    {
                        "name": "Siya Qi"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Lin Gui"
                    }
                ],
                "author_detail": {
                    "name": "Lin Gui"
                },
                "author": "Lin Gui"
            },
            {
                "id": "http://arxiv.org/abs/2511.10291v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10291v1",
                "title": "Causal Model-Based Reinforcement Learning for Sample-Efficient IoT Channel Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Model-Based Reinforcement Learning for Sample-Efficient IoT Channel Access"
                },
                "updated": "2025-11-13T13:26:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    26,
                    33,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10291v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite the advantages of multi-agent reinforcement learning (MARL) for wireless use case such as medium access control (MAC), their real-world deployment in Internet of Things (IoT) is hindered by their sample inefficiency. To alleviate this challenge, one can leverage model-based reinforcement learning (MBRL) solutions, however, conventional MBRL approaches rely on black-box models that are not interpretable and cannot reason. In contrast, in this paper, a novel causal model-based MARL framework is developed by leveraging tools from causal learn- ing. In particular, the proposed model can explicitly represent causal dependencies between network variables using structural causal models (SCMs) and attention-based inference networks. Interpretable causal models are then developed to capture how MAC control messages influence observations, how transmission actions determine outcomes, and how channel observations affect rewards. Data augmentation techniques are then used to generate synthetic rollouts using the learned causal model for policy optimization via proximal policy optimization (PPO). Analytical results demonstrate exponential sample complexity gains of causal MBRL over black-box approaches. Extensive simulations demonstrate that, on average, the proposed approach can reduce environment interactions by 58%, and yield faster convergence compared to model-free baselines. The proposed approach inherently is also shown to provide interpretable scheduling decisions via attention-based causal attribution, revealing which network conditions drive the policy. The resulting combination of sample efficiency and interpretability establishes causal MBRL as a practical approach for resource-constrained wireless systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the advantages of multi-agent reinforcement learning (MARL) for wireless use case such as medium access control (MAC), their real-world deployment in Internet of Things (IoT) is hindered by their sample inefficiency. To alleviate this challenge, one can leverage model-based reinforcement learning (MBRL) solutions, however, conventional MBRL approaches rely on black-box models that are not interpretable and cannot reason. In contrast, in this paper, a novel causal model-based MARL framework is developed by leveraging tools from causal learn- ing. In particular, the proposed model can explicitly represent causal dependencies between network variables using structural causal models (SCMs) and attention-based inference networks. Interpretable causal models are then developed to capture how MAC control messages influence observations, how transmission actions determine outcomes, and how channel observations affect rewards. Data augmentation techniques are then used to generate synthetic rollouts using the learned causal model for policy optimization via proximal policy optimization (PPO). Analytical results demonstrate exponential sample complexity gains of causal MBRL over black-box approaches. Extensive simulations demonstrate that, on average, the proposed approach can reduce environment interactions by 58%, and yield faster convergence compared to model-free baselines. The proposed approach inherently is also shown to provide interpretable scheduling decisions via attention-based causal attribution, revealing which network conditions drive the policy. The resulting combination of sample efficiency and interpretability establishes causal MBRL as a practical approach for resource-constrained wireless systems."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:26:33Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    26,
                    33,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Aswin Arun"
                    },
                    {
                        "name": "Christo Kurisummoottil Thomas"
                    },
                    {
                        "name": "Rimalpudi Sarvendranath"
                    },
                    {
                        "name": "Walid Saad"
                    }
                ],
                "author_detail": {
                    "name": "Walid Saad"
                },
                "author": "Walid Saad"
            },
            {
                "id": "http://arxiv.org/abs/2511.10283v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10283v1",
                "title": "Behavior Modeling for Training-free Building of Private Domain Multi Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavior Modeling for Training-free Building of Private Domain Multi Agent System"
                },
                "updated": "2025-11-13T13:14:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    14,
                    6,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10283v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rise of agentic systems that combine orchestration, tool use, and conversational capabilities, has been more visible by the recent advent of large language models (LLMs). While open-domain frameworks exist, applying them in private domains remains difficult due to heterogeneous tool formats, domain-specific jargon, restricted accessibility of APIs, and complex governance. Conventional solutions, such as fine-tuning on synthetic dialogue data, are burdensome and brittle under domain shifts, and risk degrading general performance. In this light, we introduce a framework for private-domain multi-agent conversational systems that avoids training and data generation by adopting behavior modeling and documentation. Our design simply assumes an orchestrator, a tool-calling agent, and a general chat agent, with tool integration defined through structured specifications and domain-informed instructions. This approach enables scalable adaptation to private tools and evolving contexts without continual retraining. The framework supports practical use cases, including lightweight deployment of multi-agent systems, leveraging API specifications as retrieval resources, and generating synthetic dialogue for evaluation -- providing a sustainable method for aligning agent behavior with domain expertise in private conversational ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of agentic systems that combine orchestration, tool use, and conversational capabilities, has been more visible by the recent advent of large language models (LLMs). While open-domain frameworks exist, applying them in private domains remains difficult due to heterogeneous tool formats, domain-specific jargon, restricted accessibility of APIs, and complex governance. Conventional solutions, such as fine-tuning on synthetic dialogue data, are burdensome and brittle under domain shifts, and risk degrading general performance. In this light, we introduce a framework for private-domain multi-agent conversational systems that avoids training and data generation by adopting behavior modeling and documentation. Our design simply assumes an orchestrator, a tool-calling agent, and a general chat agent, with tool integration defined through structured specifications and domain-informed instructions. This approach enables scalable adaptation to private tools and evolving contexts without continual retraining. The framework supports practical use cases, including lightweight deployment of multi-agent systems, leveraging API specifications as retrieval resources, and generating synthetic dialogue for evaluation -- providing a sustainable method for aligning agent behavior with domain expertise in private conversational ecosystems."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:14:06Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    14,
                    6,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "10 pages, 1 figure, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Won Ik Cho"
                    },
                    {
                        "name": "Woonghee Han"
                    },
                    {
                        "name": "Kyung Seo Ki"
                    },
                    {
                        "name": "Young Min Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young Min Kim"
                },
                "author": "Young Min Kim"
            },
            {
                "id": "http://arxiv.org/abs/2511.10281v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10281v1",
                "title": "FactGuard: Event-Centric and Commonsense-Guided Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FactGuard: Event-Centric and Commonsense-Guided Fake News Detection"
                },
                "updated": "2025-11-13T13:11:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    11,
                    42,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10281v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fake news detection methods based on writing style have achieved remarkable progress. However, as adversaries increasingly imitate the style of authentic news, the effectiveness of such approaches is gradually diminishing. Recent research has explored incorporating large language models (LLMs) to enhance fake news detection. Yet, despite their transformative potential, LLMs remain an untapped goldmine for fake news detection, with their real-world adoption hampered by shallow functionality exploration, ambiguous usability, and prohibitive inference costs. In this paper, we propose a novel fake news detection framework, dubbed FactGuard, that leverages LLMs to extract event-centric content, thereby reducing the impact of writing style on detection performance. Furthermore, our approach introduces a dynamic usability mechanism that identifies contradictions and ambiguous cases in factual reasoning, adaptively incorporating LLM advice to improve decision reliability. To ensure efficiency and practical deployment, we employ knowledge distillation to derive FactGuard-D, enabling the framework to operate effectively in cold-start and resource-constrained scenarios. Comprehensive experiments on two benchmark datasets demonstrate that our approach consistently outperforms existing methods in both robustness and accuracy, effectively addressing the challenges of style sensitivity and LLM usability in fake news detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fake news detection methods based on writing style have achieved remarkable progress. However, as adversaries increasingly imitate the style of authentic news, the effectiveness of such approaches is gradually diminishing. Recent research has explored incorporating large language models (LLMs) to enhance fake news detection. Yet, despite their transformative potential, LLMs remain an untapped goldmine for fake news detection, with their real-world adoption hampered by shallow functionality exploration, ambiguous usability, and prohibitive inference costs. In this paper, we propose a novel fake news detection framework, dubbed FactGuard, that leverages LLMs to extract event-centric content, thereby reducing the impact of writing style on detection performance. Furthermore, our approach introduces a dynamic usability mechanism that identifies contradictions and ambiguous cases in factual reasoning, adaptively incorporating LLM advice to improve decision reliability. To ensure efficiency and practical deployment, we employ knowledge distillation to derive FactGuard-D, enabling the framework to operate effectively in cold-start and resource-constrained scenarios. Comprehensive experiments on two benchmark datasets demonstrate that our approach consistently outperforms existing methods in both robustness and accuracy, effectively addressing the challenges of style sensitivity and LLM usability in fake news detection."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:11:42Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    11,
                    42,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jing He"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Yuanhui Xiao"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Shaowen Yao"
                    },
                    {
                        "name": "Renyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Renyang Liu"
                },
                "author": "Renyang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.10277v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10277v1",
                "title": "Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware"
                },
                "updated": "2025-11-13T13:03:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    3,
                    37,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10277v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet their applicability to dialogue systems in computer games remains limited. This limitation arises from their substantial hardware requirements, latency constraints, and the necessity to maintain clearly defined knowledge boundaries within a game setting. In this paper, we propose a modular NPC dialogue system that leverages Small Language Models (SLMs), fine-tuned to encode specific NPC personas and integrated with runtime-swappable memory modules. These memory modules preserve character-specific conversational context and world knowledge, enabling expressive interactions and long-term memory without retraining or model reloading during gameplay. We comprehensively evaluate our system using three open-source SLMs: DistilGPT-2, TinyLlama-1.1B-Chat, and Mistral-7B-Instruct, trained on synthetic persona-aligned data and benchmarked on consumer-grade hardware. While our approach is motivated by applications in gaming, its modular design and persona-driven memory architecture hold significant potential for broader adoption in domains requiring expressive, scalable, and memory-rich conversational agents, such as virtual assistants, customer support bots, or interactive educational systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet their applicability to dialogue systems in computer games remains limited. This limitation arises from their substantial hardware requirements, latency constraints, and the necessity to maintain clearly defined knowledge boundaries within a game setting. In this paper, we propose a modular NPC dialogue system that leverages Small Language Models (SLMs), fine-tuned to encode specific NPC personas and integrated with runtime-swappable memory modules. These memory modules preserve character-specific conversational context and world knowledge, enabling expressive interactions and long-term memory without retraining or model reloading during gameplay. We comprehensively evaluate our system using three open-source SLMs: DistilGPT-2, TinyLlama-1.1B-Chat, and Mistral-7B-Instruct, trained on synthetic persona-aligned data and benchmarked on consumer-grade hardware. While our approach is motivated by applications in gaming, its modular design and persona-driven memory architecture hold significant potential for broader adoption in domains requiring expressive, scalable, and memory-rich conversational agents, such as virtual assistants, customer support bots, or interactive educational systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T13:03:37Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    13,
                    3,
                    37,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Martin Braas"
                    },
                    {
                        "name": "Lukas Esterle"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Esterle"
                },
                "author": "Lukas Esterle"
            },
            {
                "id": "http://arxiv.org/abs/2511.10271v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10271v1",
                "title": "Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics"
                },
                "updated": "2025-11-13T12:56:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    56,
                    7,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10271v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:56:07Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    56,
                    7,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Daniel Ståhl"
                    },
                    {
                        "name": "Kristian Sandahl"
                    },
                    {
                        "name": "Christoph Kessler"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Kessler"
                },
                "author": "Christoph Kessler"
            },
            {
                "id": "http://arxiv.org/abs/2506.19794v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.19794v5",
                "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study"
                },
                "updated": "2025-11-13T12:47:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    47,
                    18,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.19794v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.19794v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-24T17:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    4,
                    23,
                    1,
                    175,
                    0
                ],
                "arxiv_comment": "AAAI 2026 (oral)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Yi Zhong"
                    },
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Ziheng Zhang"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.07250v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07250v2",
                "title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs"
                },
                "updated": "2025-11-13T12:42:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    42,
                    38,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07250v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T16:02:33Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    16,
                    2,
                    33,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "arxiv_journal_ref": "The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)",
                "authors": [
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Haochen Wang"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Gavin Chang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Shihao Li"
                    },
                    {
                        "name": "Yanghai Wang"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Steven Huang"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Liu"
                },
                "author": "Jiaheng Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.10248v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10248v1",
                "title": "Pk-IOTA: Blockchain empowered Programmable Data Plane to secure OPC UA communications in Industry 4.0",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pk-IOTA: Blockchain empowered Programmable Data Plane to secure OPC UA communications in Industry 4.0"
                },
                "updated": "2025-11-13T12:28:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    28,
                    26,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10248v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The OPC UA protocol is becoming the de facto standard for Industry 4.0 machine-to-machine communication. It stands out as one of the few industrial protocols that provide robust security features designed to prevent attackers from manipulating and damaging critical infrastructures. However, prior works showed that significant challenges still exists to set up secure OPC UA deployments in practice, mainly caused by the complexity of certificate management in industrial scenarios and the inconsistent implementation of security features across industrial OPC UA devices. In this paper, we present Pk-IOTA, an automated solution designed to secure OPC UA communications by integrating programmable data plane switches for in-network certificate validation and leveraging the IOTA Tangle for decen- tralized certificate distribution. Our evaluation is performed on a physical testbed representing a real-world industrial scenario and shows that Pk-IOTA introduces a minimal overhead while providing a scalable and tamper-proof mechanism for OPC UA certificate management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The OPC UA protocol is becoming the de facto standard for Industry 4.0 machine-to-machine communication. It stands out as one of the few industrial protocols that provide robust security features designed to prevent attackers from manipulating and damaging critical infrastructures. However, prior works showed that significant challenges still exists to set up secure OPC UA deployments in practice, mainly caused by the complexity of certificate management in industrial scenarios and the inconsistent implementation of security features across industrial OPC UA devices. In this paper, we present Pk-IOTA, an automated solution designed to secure OPC UA communications by integrating programmable data plane switches for in-network certificate validation and leveraging the IOTA Tangle for decen- tralized certificate distribution. Our evaluation is performed on a physical testbed representing a real-world industrial scenario and shows that Pk-IOTA introduces a minimal overhead while providing a scalable and tamper-proof mechanism for OPC UA certificate management."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:28:26Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    28,
                    26,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Rinieri Lorenzo"
                    },
                    {
                        "name": "Gori Giacomo"
                    },
                    {
                        "name": "Melis Andrea"
                    },
                    {
                        "name": "Girau Roberto"
                    },
                    {
                        "name": "Prandini Marco"
                    },
                    {
                        "name": "Callegati Franco"
                    }
                ],
                "author_detail": {
                    "name": "Callegati Franco"
                },
                "author": "Callegati Franco"
            },
            {
                "id": "http://arxiv.org/abs/2511.10240v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10240v1",
                "title": "ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs"
                },
                "updated": "2025-11-13T12:14:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    14,
                    36,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10240v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:14:36Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    14,
                    36,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Minbae Park"
                    },
                    {
                        "name": "Hyemin Yang"
                    },
                    {
                        "name": "Jeonghyun Kim"
                    },
                    {
                        "name": "Kunsoo Park"
                    },
                    {
                        "name": "Hyunjoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunjoon Kim"
                },
                "author": "Hyunjoon Kim"
            },
            {
                "id": "http://arxiv.org/abs/2506.21765v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.21765v2",
                "title": "TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker"
                },
                "updated": "2025-11-13T12:13:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    13,
                    29,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.21765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.21765v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Trackerless freehand ultrasound reconstruction aims to reconstruct 3D volumes from sequences of 2D ultrasound images without relying on external tracking systems. By eliminating the need for optical or electromagnetic trackers, this approach offers a low-cost, portable, and widely deployable alternative to more expensive volumetric ultrasound imaging systems, particularly valuable in resource-constrained clinical settings. However, predicting long-distance transformations and handling complex probe trajectories remain challenging. The TUS-REC2024 Challenge establishes the first benchmark for trackerless 3D freehand ultrasound reconstruction by providing a large publicly available dataset, along with a baseline model and a rigorous evaluation framework. By the submission deadline, the Challenge had attracted 43 registered teams, of which 6 teams submitted 21 valid dockerized solutions. The submitted methods span a wide range of approaches, including the state space model, the recurrent model, the registration-driven volume refinement, the attention mechanism, and the physics-informed model. This paper provides a comprehensive background introduction and literature review in the field, presents an overview of the challenge design and dataset, and offers a comparative analysis of submitted methods across multiple evaluation metrics. These analyses highlight both the progress and the current limitations of state-of-the-art approaches in this domain and provide insights for future research directions. All data and code are publicly available to facilitate ongoing development and reproducibility. As a live and evolving benchmark, it is designed to be continuously iterated and improved. The Challenge was held at MICCAI 2024 and is organised again at MICCAI 2025, reflecting its sustained commitment to advancing this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trackerless freehand ultrasound reconstruction aims to reconstruct 3D volumes from sequences of 2D ultrasound images without relying on external tracking systems. By eliminating the need for optical or electromagnetic trackers, this approach offers a low-cost, portable, and widely deployable alternative to more expensive volumetric ultrasound imaging systems, particularly valuable in resource-constrained clinical settings. However, predicting long-distance transformations and handling complex probe trajectories remain challenging. The TUS-REC2024 Challenge establishes the first benchmark for trackerless 3D freehand ultrasound reconstruction by providing a large publicly available dataset, along with a baseline model and a rigorous evaluation framework. By the submission deadline, the Challenge had attracted 43 registered teams, of which 6 teams submitted 21 valid dockerized solutions. The submitted methods span a wide range of approaches, including the state space model, the recurrent model, the registration-driven volume refinement, the attention mechanism, and the physics-informed model. This paper provides a comprehensive background introduction and literature review in the field, presents an overview of the challenge design and dataset, and offers a comparative analysis of submitted methods across multiple evaluation metrics. These analyses highlight both the progress and the current limitations of state-of-the-art approaches in this domain and provide insights for future research directions. All data and code are publicly available to facilitate ongoing development and reproducibility. As a live and evolving benchmark, it is designed to be continuously iterated and improved. The Challenge was held at MICCAI 2024 and is organised again at MICCAI 2025, reflecting its sustained commitment to advancing this field."
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-26T20:52:18Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    20,
                    52,
                    18,
                    3,
                    177,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV"
                },
                "authors": [
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Shaheer U. Saeed"
                    },
                    {
                        "name": "Yuliang Huang"
                    },
                    {
                        "name": "Mingyuan Luo"
                    },
                    {
                        "name": "Zhongnuo Yan"
                    },
                    {
                        "name": "Jiongquan Chen"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Dong Ni"
                    },
                    {
                        "name": "Nektarios Winter"
                    },
                    {
                        "name": "Phuc Nguyen"
                    },
                    {
                        "name": "Lucas Steinberger"
                    },
                    {
                        "name": "Caelan Haney"
                    },
                    {
                        "name": "Yuan Zhao"
                    },
                    {
                        "name": "Mingjie Jiang"
                    },
                    {
                        "name": "Bowen Ren"
                    },
                    {
                        "name": "SiYeoul Lee"
                    },
                    {
                        "name": "Seonho Kim"
                    },
                    {
                        "name": "MinKyung Seo"
                    },
                    {
                        "name": "MinWoo Kim"
                    },
                    {
                        "name": "Yimeng Dou"
                    },
                    {
                        "name": "Zhiwei Zhang"
                    },
                    {
                        "name": "Yin Li"
                    },
                    {
                        "name": "Tomy Varghese"
                    },
                    {
                        "name": "Dean C. Barratt"
                    },
                    {
                        "name": "Matthew J. Clarkson"
                    },
                    {
                        "name": "Tom Vercauteren"
                    },
                    {
                        "name": "Yipeng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yipeng Hu"
                },
                "author": "Yipeng Hu"
            },
            {
                "id": "http://arxiv.org/abs/2511.10234v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10234v1",
                "title": "Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners"
                },
                "updated": "2025-11-13T12:06:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    6,
                    12,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10234v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While promising, graph reasoners based on Large Language Models (LLMs) lack built-in invariance to symmetries in graph representations. Operating on sequential graph serializations, LLMs can produce different outputs under node reindexing, edge reordering, or formatting changes, raising robustness concerns. We systematically analyze these effects, studying how fine-tuning impacts encoding sensitivity as well generalization on unseen tasks. We propose a principled decomposition of graph serializations into node labeling, edge encoding, and syntax, and evaluate LLM robustness to variations of each of these factors on a comprehensive benchmarking suite. We also contribute a novel set of spectral tasks to further assess generalization abilities of fine-tuned reasoners. Results show that larger (non-fine-tuned) models are more robust. Fine-tuning reduces sensitivity to node relabeling but may increase it to variations in structure and format, while it does not consistently improve performance on unseen tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While promising, graph reasoners based on Large Language Models (LLMs) lack built-in invariance to symmetries in graph representations. Operating on sequential graph serializations, LLMs can produce different outputs under node reindexing, edge reordering, or formatting changes, raising robustness concerns. We systematically analyze these effects, studying how fine-tuning impacts encoding sensitivity as well generalization on unseen tasks. We propose a principled decomposition of graph serializations into node labeling, edge encoding, and syntax, and evaluate LLM robustness to variations of each of these factors on a comprehensive benchmarking suite. We also contribute a novel set of spectral tasks to further assess generalization abilities of fine-tuned reasoners. Results show that larger (non-fine-tuned) models are more robust. Fine-tuning reduces sensitivity to node relabeling but may increase it to variations in structure and format, while it does not consistently improve performance on unseen tasks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:06:12Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    6,
                    12,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "AAAI 2026 Workshop on Graphs and more Complex Structures For Learning and Reasoning (GCLR)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Daniel Herbst"
                    },
                    {
                        "name": "Lea Karbeska"
                    },
                    {
                        "name": "Divyanshu Kumar"
                    },
                    {
                        "name": "Akanksha Ahuja"
                    },
                    {
                        "name": "Fatemeh Gholamzadeh Nasrabadi"
                    },
                    {
                        "name": "Fabrizio Frasca"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Frasca"
                },
                "author": "Fabrizio Frasca"
            },
            {
                "id": "http://arxiv.org/abs/2511.10233v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10233v1",
                "title": "Bridging Synthetic and Real Routing Problems via LLM-Guided Instance Generation and Progressive Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Synthetic and Real Routing Problems via LLM-Guided Instance Generation and Progressive Adaptation"
                },
                "updated": "2025-11-13T12:06:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    6,
                    11,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10233v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in Neural Combinatorial Optimization (NCO) methods have significantly improved the capability of neural solvers to handle synthetic routing instances. Nonetheless, existing neural solvers typically struggle to generalize effectively from synthetic, uniformly-distributed training data to real-world VRP scenarios, including widely recognized benchmark instances from TSPLib and CVRPLib. To bridge this generalization gap, we present Evolutionary Realistic Instance Synthesis (EvoReal), which leverages an evolutionary module guided by large language models (LLMs) to generate synthetic instances characterized by diverse and realistic structural patterns. Specifically, the evolutionary module produces synthetic instances whose structural attributes statistically mimics those observed in authentic real-world instances. Subsequently, pre-trained NCO models are progressively refined, firstly aligning them with these structurally enriched synthetic distributions and then further adapting them through direct fine-tuning on actual benchmark instances. Extensive experimental evaluations demonstrate that EvoReal markedly improves the generalization capabilities of state-of-the-art neural solvers, yielding a notable reduced performance gap compared to the optimal solutions on the TSPLib (1.05%) and CVRPLib (2.71%) benchmarks across a broad spectrum of problem scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Neural Combinatorial Optimization (NCO) methods have significantly improved the capability of neural solvers to handle synthetic routing instances. Nonetheless, existing neural solvers typically struggle to generalize effectively from synthetic, uniformly-distributed training data to real-world VRP scenarios, including widely recognized benchmark instances from TSPLib and CVRPLib. To bridge this generalization gap, we present Evolutionary Realistic Instance Synthesis (EvoReal), which leverages an evolutionary module guided by large language models (LLMs) to generate synthetic instances characterized by diverse and realistic structural patterns. Specifically, the evolutionary module produces synthetic instances whose structural attributes statistically mimics those observed in authentic real-world instances. Subsequently, pre-trained NCO models are progressively refined, firstly aligning them with these structurally enriched synthetic distributions and then further adapting them through direct fine-tuning on actual benchmark instances. Extensive experimental evaluations demonstrate that EvoReal markedly improves the generalization capabilities of state-of-the-art neural solvers, yielding a notable reduced performance gap compared to the optimal solutions on the TSPLib (1.05%) and CVRPLib (2.71%) benchmarks across a broad spectrum of problem scales."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:06:11Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    6,
                    11,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "21 pages; To be published in AAAI-26",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jianghan Zhu"
                    },
                    {
                        "name": "Yaoxin Wu"
                    },
                    {
                        "name": "Zhuoyi Lin"
                    },
                    {
                        "name": "Zhengyuan Zhang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zhiguang Cao"
                    },
                    {
                        "name": "Senthilnath Jayavelu"
                    },
                    {
                        "name": "Xiaoli Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoli Li"
                },
                "author": "Xiaoli Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.10229v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10229v1",
                "title": "LangGPS: Language Separability Guided Data Pre-Selection for Joint Multilingual Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LangGPS: Language Separability Guided Data Pre-Selection for Joint Multilingual Instruction Tuning"
                },
                "updated": "2025-11-13T12:02:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    2,
                    32,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10229v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Joint multilingual instruction tuning is a widely adopted approach to improve the multilingual instruction-following ability and downstream performance of large language models (LLMs), but the resulting multilingual capability remains highly sensitive to the composition and selection of the training data. Existing selection methods, often based on features like text quality, diversity, or task relevance, typically overlook the intrinsic linguistic structure of multilingual data. In this paper, we propose LangGPS, a lightweight two-stage pre-selection framework guided by language separability which quantifies how well samples in different languages can be distinguished in the model's representation space. LangGPS first filters training data based on separability scores and then refines the subset using existing selection methods. Extensive experiments across six benchmarks and 22 languages demonstrate that applying LangGPS on top of existing selection methods improves their effectiveness and generalizability in multilingual training, especially for understanding tasks and low-resource languages. Further analysis reveals that highly separable samples facilitate the formation of clearer language boundaries and support faster adaptation, while low-separability samples tend to function as bridges for cross-lingual alignment. Besides, we also find that language separability can serve as an effective signal for multilingual curriculum learning, where interleaving samples with diverse separability levels yields stable and generalizable gains. Together, we hope our work offers a new perspective on data utility in multilingual contexts and support the development of more linguistically informed LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint multilingual instruction tuning is a widely adopted approach to improve the multilingual instruction-following ability and downstream performance of large language models (LLMs), but the resulting multilingual capability remains highly sensitive to the composition and selection of the training data. Existing selection methods, often based on features like text quality, diversity, or task relevance, typically overlook the intrinsic linguistic structure of multilingual data. In this paper, we propose LangGPS, a lightweight two-stage pre-selection framework guided by language separability which quantifies how well samples in different languages can be distinguished in the model's representation space. LangGPS first filters training data based on separability scores and then refines the subset using existing selection methods. Extensive experiments across six benchmarks and 22 languages demonstrate that applying LangGPS on top of existing selection methods improves their effectiveness and generalizability in multilingual training, especially for understanding tasks and low-resource languages. Further analysis reveals that highly separable samples facilitate the formation of clearer language boundaries and support faster adaptation, while low-separability samples tend to function as bridges for cross-lingual alignment. Besides, we also find that language separability can serve as an effective signal for multilingual curriculum learning, where interleaving samples with diverse separability levels yields stable and generalizable gains. Together, we hope our work offers a new perspective on data utility in multilingual contexts and support the development of more linguistically informed LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T12:02:32Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    12,
                    2,
                    32,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "AAAI2026 Main Track Accepted",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yangfan Ye"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Weitao Ma"
                    },
                    {
                        "name": "Qichen Hong"
                    },
                    {
                        "name": "Yunfei Lu"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin"
            },
            {
                "id": "http://arxiv.org/abs/2511.10222v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10222v1",
                "title": "Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard"
                },
                "updated": "2025-11-13T11:50:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    50,
                    54,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10222v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent progress in large language models (LLMs) has enabled understanding of both speech and non-speech audio, but exposing new safety risks emerging from complex audio inputs that are inadequately handled by current safeguards. We introduce SACRED-Bench (Speech-Audio Composition for RED-teaming) to evaluate the robustness of LLMs under complex audio-based attacks. Unlike existing perturbation-based methods that rely on noise optimization or white-box access, SACRED-Bench exploits speech-audio composition mechanisms. SACRED-Bench adopts three mechanisms: (a) speech overlap and multi-speaker dialogue, which embeds harmful prompts beneath or alongside benign speech; (b) speech-audio mixture, which imply unsafe intent via non-speech audio alongside benign speech or audio; and (c) diverse spoken instruction formats (open-ended QA, yes/no) that evade text-only filters. Experiments show that, even Gemini 2.5 Pro, the state-of-the-art proprietary LLM, still exhibits 66% attack success rate in SACRED-Bench test set, exposing vulnerabilities under cross-modal, speech-audio composition attacks. To bridge this gap, we propose SALMONN-Guard, a safeguard LLM that jointly inspects speech, audio, and text for safety judgments, reducing attack success down to 20%. Our results highlight the need for audio-aware defenses for the safety of multimodal LLMs. The benchmark and SALMONN-Guard checkpoints can be found at https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench. Warning: this paper includes examples that may be offensive or harmful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has enabled understanding of both speech and non-speech audio, but exposing new safety risks emerging from complex audio inputs that are inadequately handled by current safeguards. We introduce SACRED-Bench (Speech-Audio Composition for RED-teaming) to evaluate the robustness of LLMs under complex audio-based attacks. Unlike existing perturbation-based methods that rely on noise optimization or white-box access, SACRED-Bench exploits speech-audio composition mechanisms. SACRED-Bench adopts three mechanisms: (a) speech overlap and multi-speaker dialogue, which embeds harmful prompts beneath or alongside benign speech; (b) speech-audio mixture, which imply unsafe intent via non-speech audio alongside benign speech or audio; and (c) diverse spoken instruction formats (open-ended QA, yes/no) that evade text-only filters. Experiments show that, even Gemini 2.5 Pro, the state-of-the-art proprietary LLM, still exhibits 66% attack success rate in SACRED-Bench test set, exposing vulnerabilities under cross-modal, speech-audio composition attacks. To bridge this gap, we propose SALMONN-Guard, a safeguard LLM that jointly inspects speech, audio, and text for safety judgments, reducing attack success down to 20%. Our results highlight the need for audio-aware defenses for the safety of multimodal LLMs. The benchmark and SALMONN-Guard checkpoints can be found at https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench. Warning: this paper includes examples that may be offensive or harmful."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T11:50:54Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    50,
                    54,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Yudong Yang"
                    },
                    {
                        "name": "Xuezhen Zhang"
                    },
                    {
                        "name": "Zhifeng Han"
                    },
                    {
                        "name": "Siyin Wang"
                    },
                    {
                        "name": "Jimin Zhuang"
                    },
                    {
                        "name": "Zengrui Jin"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.07090v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07090v4",
                "title": "Green AI: A systematic review and meta-analysis of its definitions, lifecycle models, hardware and measurement attempts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Green AI: A systematic review and meta-analysis of its definitions, lifecycle models, hardware and measurement attempts"
                },
                "updated": "2025-11-13T11:33:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    33,
                    2,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07090v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07090v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Across the Artificial Intelligence (AI) lifecycle - from hardware to development, deployment, and reuse - burdens span energy, carbon, water, and embodied impacts. Cloud provider tools improve transparency but remain heterogeneous and often omit water and value chain effects, limiting comparability and reproducibility. Addressing these multi dimensional burdens requires a lifecycle approach linking phase explicit mapping with system levers (hardware, placement, energy mix, cooling, scheduling) and calibrated measurement across facility, system, device, and workload levels. This article (i) establishes a unified, operational definition of Green AI distinct from Sustainable AI; (ii) formalizes a five phase lifecycle mapped to Life Cycle Assessment (LCA) stages, making energy, carbon, water, and embodied impacts first class; (iii) specifies governance via Plan Do Check Act (PDCA) cycles with decision gateways; (iv) systematizes hardware and system level strategies across the edge cloud continuum to reduce embodied burdens; and (v) defines a calibrated measurement framework combining estimator models with direct metering to enable reproducible, provider agnostic comparisons. Combining definition, lifecycle processes, hardware strategies, and calibrated measurement, this article offers actionable, evidence based guidance for researchers, practitioners, and policymakers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across the Artificial Intelligence (AI) lifecycle - from hardware to development, deployment, and reuse - burdens span energy, carbon, water, and embodied impacts. Cloud provider tools improve transparency but remain heterogeneous and often omit water and value chain effects, limiting comparability and reproducibility. Addressing these multi dimensional burdens requires a lifecycle approach linking phase explicit mapping with system levers (hardware, placement, energy mix, cooling, scheduling) and calibrated measurement across facility, system, device, and workload levels. This article (i) establishes a unified, operational definition of Green AI distinct from Sustainable AI; (ii) formalizes a five phase lifecycle mapped to Life Cycle Assessment (LCA) stages, making energy, carbon, water, and embodied impacts first class; (iii) specifies governance via Plan Do Check Act (PDCA) cycles with decision gateways; (iv) systematizes hardware and system level strategies across the edge cloud continuum to reduce embodied burdens; and (v) defines a calibrated measurement framework combining estimator models with direct metering to enable reproducible, provider agnostic comparisons. Combining definition, lifecycle processes, hardware strategies, and calibrated measurement, this article offers actionable, evidence based guidance for researchers, practitioners, and policymakers."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T13:26:06Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    13,
                    26,
                    6,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Marcel Rojahn"
                    },
                    {
                        "name": "Marcus Grum"
                    }
                ],
                "author_detail": {
                    "name": "Marcus Grum"
                },
                "author": "Marcus Grum"
            },
            {
                "id": "http://arxiv.org/abs/2511.10210v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10210v1",
                "title": "Advanced Black-Box Tuning of Large Language Models with Limited API Calls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Black-Box Tuning of Large Language Models with Limited API Calls"
                },
                "updated": "2025-11-13T11:32:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    32,
                    8,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10210v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Black-box tuning is an emerging paradigm for adapting large language models (LLMs) to better achieve desired behaviors, particularly when direct access to model parameters is unavailable. Current strategies, however, often present a dilemma of suboptimal extremes: either separately train a small proxy model and then use it to shift the predictions of the foundation model, offering notable efficiency but often yielding limited improvement; or making API calls in each tuning iteration to the foundation model, which entails prohibitive computational costs. Therefore, we propose a novel advanced black-box tuning method for LLMs with limited API calls. Our core strategy involves training a Gaussian Process (GP) surrogate model with \"LogitMap Pairs\" derived from querying the foundation model on a minimal but highly informative training subset. This surrogate can approximate the outputs of the foundation model to guide the training of the proxy model, thereby effectively reducing the need for direct queries to the foundation model. Extensive experiments verify that our approach elevates pre-trained language model accuracy from 55.92% to 86.85%, reducing the frequency of API queries to merely 1.38%. This significantly outperforms offline approaches that operate entirely without API access. Notably, our method also achieves comparable or superior accuracy to query-intensive approaches, while significantly reducing API costs. This offers a robust and high-efficiency paradigm for language model adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box tuning is an emerging paradigm for adapting large language models (LLMs) to better achieve desired behaviors, particularly when direct access to model parameters is unavailable. Current strategies, however, often present a dilemma of suboptimal extremes: either separately train a small proxy model and then use it to shift the predictions of the foundation model, offering notable efficiency but often yielding limited improvement; or making API calls in each tuning iteration to the foundation model, which entails prohibitive computational costs. Therefore, we propose a novel advanced black-box tuning method for LLMs with limited API calls. Our core strategy involves training a Gaussian Process (GP) surrogate model with \"LogitMap Pairs\" derived from querying the foundation model on a minimal but highly informative training subset. This surrogate can approximate the outputs of the foundation model to guide the training of the proxy model, thereby effectively reducing the need for direct queries to the foundation model. Extensive experiments verify that our approach elevates pre-trained language model accuracy from 55.92% to 86.85%, reducing the frequency of API queries to merely 1.38%. This significantly outperforms offline approaches that operate entirely without API access. Notably, our method also achieves comparable or superior accuracy to query-intensive approaches, while significantly reducing API costs. This offers a robust and high-efficiency paradigm for language model adaptation."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T11:32:08Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    32,
                    8,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "15 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zhikang Xie"
                    },
                    {
                        "name": "Weilin Wan"
                    },
                    {
                        "name": "Peizhu Gong"
                    },
                    {
                        "name": "Weizhong Zhang"
                    },
                    {
                        "name": "Cheng Jin"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Jin"
                },
                "author": "Cheng Jin"
            },
            {
                "id": "http://arxiv.org/abs/2511.10207v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10207v1",
                "title": "Generalized Intelligence for Tactical Decision-Making: Large Language Model-Driven Dynamic Weapon Target Assignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Intelligence for Tactical Decision-Making: Large Language Model-Driven Dynamic Weapon Target Assignment"
                },
                "updated": "2025-11-13T11:25:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    25,
                    58,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10207v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern aerospace defense systems increasingly rely on autonomous decision-making to coordinate large numbers of interceptors against multiple incoming threats. Conventional weapon-target assignment (WTA) algorithms, including mixed-integer programming and auction-based methods, show limitations in dynamic and uncertain tactical environments where human-like reasoning and adaptive prioritization are required. This paper introduces a large language model (LLM) driven WTA framework that integrates generalized intelligence into cooperative missile guidance. The proposed system formulates the tactical decision process as a reasoning problem, in which an LLM evaluates spatial and temporal relationships among interceptors, targets, and defended assets to generate real-time assignments. In contrast to classical optimization methods, the approach leverages contextual mission data such as threat direction, asset priority, and closing velocity to adapt dynamically and reduce assignment switching. A dedicated simulation environment supports both static and dynamic assignment modes. Results demonstrate improved consistency, adaptability, and mission-level prioritization, establishing a foundation for integrating generalized artificial intelligence into tactical guidance systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern aerospace defense systems increasingly rely on autonomous decision-making to coordinate large numbers of interceptors against multiple incoming threats. Conventional weapon-target assignment (WTA) algorithms, including mixed-integer programming and auction-based methods, show limitations in dynamic and uncertain tactical environments where human-like reasoning and adaptive prioritization are required. This paper introduces a large language model (LLM) driven WTA framework that integrates generalized intelligence into cooperative missile guidance. The proposed system formulates the tactical decision process as a reasoning problem, in which an LLM evaluates spatial and temporal relationships among interceptors, targets, and defended assets to generate real-time assignments. In contrast to classical optimization methods, the approach leverages contextual mission data such as threat direction, asset priority, and closing velocity to adapt dynamically and reduce assignment switching. A dedicated simulation environment supports both static and dynamic assignment modes. Results demonstrate improved consistency, adaptability, and mission-level prioritization, establishing a foundation for integrating generalized artificial intelligence into tactical guidance systems."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T11:25:58Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    25,
                    58,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "8 Pages, 6 figures, submitted to IEEE Transactions on Aerospace and Electronic Systems",
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Johannes Autenrieb"
                    },
                    {
                        "name": "Ole Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Ole Ostermann"
                },
                "author": "Ole Ostermann"
            },
            {
                "id": "http://arxiv.org/abs/2510.26495v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.26495v2",
                "title": "Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for Real-world Database Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for Real-world Database Exploration"
                },
                "updated": "2025-11-13T11:17:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    17,
                    29,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.26495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.26495v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in Text-to-SQL have achieved strong results in static, single-turn tasks, where models generate SQL queries from natural language questions. However, these systems fall short in real-world interactive scenarios, where user intents evolve and queries must be refined over multiple turns. In applications such as finance and business analytics, users iteratively adjust query constraints or dimensions based on intermediate results. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a benchmark assessing model performance under evolving user interactions. Unlike previous manually curated datasets, DySQL-Bench is built through an automated two-stage pipeline of task synthesis and verification. Structured tree representations derived from raw database tables guide LLM-based task generation, followed by interaction-oriented filtering and expert validation. Human evaluation confirms 100% correctness of the synthesized data. We further propose a multi-turn evaluation framework simulating realistic interactions among an LLM-simulated user, the model under test, and an executable database. The model must adapt its reasoning and SQL generation as user intents change. DySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling 1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the Pass@5 metric, underscoring the benchmark's difficulty. All code and data are released at https://github.com/Aurora-slz/Real-World-SQL-Bench .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Text-to-SQL have achieved strong results in static, single-turn tasks, where models generate SQL queries from natural language questions. However, these systems fall short in real-world interactive scenarios, where user intents evolve and queries must be refined over multiple turns. In applications such as finance and business analytics, users iteratively adjust query constraints or dimensions based on intermediate results. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a benchmark assessing model performance under evolving user interactions. Unlike previous manually curated datasets, DySQL-Bench is built through an automated two-stage pipeline of task synthesis and verification. Structured tree representations derived from raw database tables guide LLM-based task generation, followed by interaction-oriented filtering and expert validation. Human evaluation confirms 100% correctness of the synthesized data. We further propose a multi-turn evaluation framework simulating realistic interactions among an LLM-simulated user, the model under test, and an executable database. The model must adapt its reasoning and SQL generation as user intents change. DySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling 1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the Pass@5 metric, underscoring the benchmark's difficulty. All code and data are released at https://github.com/Aurora-slz/Real-World-SQL-Bench ."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T13:44:22Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    44,
                    22,
                    3,
                    303,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Linzhuang Sun"
                    },
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Yuying Li"
                    },
                    {
                        "name": "Qifeng Cai"
                    },
                    {
                        "name": "Jingxuan Wei"
                    },
                    {
                        "name": "Bihui Yu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui"
            },
            {
                "id": "http://arxiv.org/abs/2511.10201v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10201v1",
                "title": "EffiReason-Bench: A Unified Benchmark for Evaluating and Advancing Efficient Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EffiReason-Bench: A Unified Benchmark for Evaluating and Advancing Efficient Reasoning in Large Language Models"
                },
                "updated": "2025-11-13T11:14:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    14,
                    46,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10201v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) with Chain-of-Thought (CoT) prompting achieve strong reasoning but often produce unnecessarily long explanations, increasing cost and sometimes reducing accuracy. Fair comparison of efficiency-oriented approaches is hindered by fragmented evaluation practices. We introduce EffiReason-Bench, a unified benchmark for rigorous cross-paradigm evaluation of efficient reasoning methods across three categories: Reasoning Blueprints, Dynamic Execution, and Post-hoc Refinement. To enable step-by-step evaluation, we construct verified CoT annotations for CommonsenseQA and LogiQA via a pipeline that enforces standardized reasoning structures, comprehensive option-wise analysis, and human verification. We evaluate 7 methods across 6 open-source LLMs (1B-70B) on 4 datasets spanning mathematics, commonsense, and logic, and propose the E3-Score, a principled metric inspired by economic trade-off modeling that provides smooth, stable evaluation without discontinuities or heavy reliance on heuristics. Experiments show that no single method universally dominates; optimal strategies depend on backbone scale, task complexity, and architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with Chain-of-Thought (CoT) prompting achieve strong reasoning but often produce unnecessarily long explanations, increasing cost and sometimes reducing accuracy. Fair comparison of efficiency-oriented approaches is hindered by fragmented evaluation practices. We introduce EffiReason-Bench, a unified benchmark for rigorous cross-paradigm evaluation of efficient reasoning methods across three categories: Reasoning Blueprints, Dynamic Execution, and Post-hoc Refinement. To enable step-by-step evaluation, we construct verified CoT annotations for CommonsenseQA and LogiQA via a pipeline that enforces standardized reasoning structures, comprehensive option-wise analysis, and human verification. We evaluate 7 methods across 6 open-source LLMs (1B-70B) on 4 datasets spanning mathematics, commonsense, and logic, and propose the E3-Score, a principled metric inspired by economic trade-off modeling that provides smooth, stable evaluation without discontinuities or heavy reliance on heuristics. Experiments show that no single method universally dominates; optimal strategies depend on backbone scale, task complexity, and architecture."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T11:14:46Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    14,
                    46,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "11 pages, 4 figures, 4 tables. Appendix included",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Junquan Huang"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Yubo Gao"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Junyan Zhang"
                    },
                    {
                        "name": "Yonghua Hei"
                    },
                    {
                        "name": "Song Dai"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Puay Siew Tan"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu"
            },
            {
                "id": "http://arxiv.org/abs/2511.10192v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10192v1",
                "title": "Text2SQL-Flow: A Robust SQL-Aware Data Augmentation Framework for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2SQL-Flow: A Robust SQL-Aware Data Augmentation Framework for Text-to-SQL"
                },
                "updated": "2025-11-13T11:02:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    2,
                    15,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10192v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The data-centric paradigm has become pivotal in AI, especially for Text-to-SQL, where performance is limited by scarce, simplistic, and low-diversity datasets. To address this, we propose Text2SQL-Flow, a SQL-aware data augmentation framework that generates large-scale, semantically valid, and structurally diverse Text-to-SQL pairs from minimal seed data. It operates across six augmentation dimensions and integrates an end-to-end pipeline featuring SQL execution verification, natural language question generation, chain-of-thought reasoning traces, and data classification. A modular Database Manager ensures cross-database compatibility and scalability. Using this framework, we build SQLFlow, a high-quality dataset of 89,544 annotated examples. We evaluate SQLFlow in two settings: (1) For open-source LLMs, fine-tuning on SQLFlow consistently improves performance across benchmarks under the same data budget. (2) For closed-source LLMs, we introduce a masked alignment retrieval method that treats SQLFlow as both knowledge base and training data for the retriever. This enables structure-aware example matching by modeling fine-grained alignments between questions and SQL queries. Experiments show our retrieval strategy outperforms existing methods, underscoring the value of SQLFlow's high-fidelity data and our novel technique. Our work establishes a scalable, data-centric foundation for advancing Text-to-SQL systems and highlights the critical role of high-quality structured data in modern AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The data-centric paradigm has become pivotal in AI, especially for Text-to-SQL, where performance is limited by scarce, simplistic, and low-diversity datasets. To address this, we propose Text2SQL-Flow, a SQL-aware data augmentation framework that generates large-scale, semantically valid, and structurally diverse Text-to-SQL pairs from minimal seed data. It operates across six augmentation dimensions and integrates an end-to-end pipeline featuring SQL execution verification, natural language question generation, chain-of-thought reasoning traces, and data classification. A modular Database Manager ensures cross-database compatibility and scalability. Using this framework, we build SQLFlow, a high-quality dataset of 89,544 annotated examples. We evaluate SQLFlow in two settings: (1) For open-source LLMs, fine-tuning on SQLFlow consistently improves performance across benchmarks under the same data budget. (2) For closed-source LLMs, we introduce a masked alignment retrieval method that treats SQLFlow as both knowledge base and training data for the retriever. This enables structure-aware example matching by modeling fine-grained alignments between questions and SQL queries. Experiments show our retrieval strategy outperforms existing methods, underscoring the value of SQLFlow's high-fidelity data and our novel technique. Our work establishes a scalable, data-centric foundation for advancing Text-to-SQL systems and highlights the critical role of high-quality structured data in modern AI."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T11:02:15Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    2,
                    15,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Qifeng Cai"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui"
            },
            {
                "id": "http://arxiv.org/abs/2510.05162v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.05162v2",
                "title": "Artificial-Intelligence Grading Assistance for Handwritten Components of a Calculus Exam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial-Intelligence Grading Assistance for Handwritten Components of a Calculus Exam"
                },
                "updated": "2025-11-13T10:55:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    10,
                    55,
                    43,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.05162v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.05162v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate whether contemporary multimodal LLMs can assist with grading open-ended calculus at scale without eroding validity. In a large first-year exam, students' handwritten work was graded by GPT-5 against the same rubric used by teaching assistants (TAs), with fractional credit permitted; TA rubric decisions served as ground truth. We calibrated a human-in-the-loop filter that combines a partial-credit threshold with an Item Response Theory (2PL) risk measure based on the deviation between the AI score and the model-expected score for each student-item. Unfiltered AI-TA agreement was moderate, adequate for low-stakes feedback but not for high-stakes use. Confidence filtering made the workload-quality trade-off explicit: under stricter settings, AI delivered human-level accuracy, but also left roughly 70% of the items to be graded by humans. Psychometric patterns were constrained by low stakes on the open-ended portion, a small set of rubric checkpoints, and occasional misalignment between designated answer regions and where work appeared. Practical adjustments such as slightly higher weight and protected time, a few rubric-visible substeps, stronger spatial anchoring should raise ceiling performance. Overall, calibrated confidence and conservative routing enable AI to reliably handle a sizable subset of routine cases while reserving expert judgment for ambiguous or pedagogically rich responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether contemporary multimodal LLMs can assist with grading open-ended calculus at scale without eroding validity. In a large first-year exam, students' handwritten work was graded by GPT-5 against the same rubric used by teaching assistants (TAs), with fractional credit permitted; TA rubric decisions served as ground truth. We calibrated a human-in-the-loop filter that combines a partial-credit threshold with an Item Response Theory (2PL) risk measure based on the deviation between the AI score and the model-expected score for each student-item. Unfiltered AI-TA agreement was moderate, adequate for low-stakes feedback but not for high-stakes use. Confidence filtering made the workload-quality trade-off explicit: under stricter settings, AI delivered human-level accuracy, but also left roughly 70% of the items to be graded by humans. Psychometric patterns were constrained by low stakes on the open-ended portion, a small set of rubric checkpoints, and occasional misalignment between designated answer regions and where work appeared. Practical adjustments such as slightly higher weight and protected time, a few rubric-visible substeps, stronger spatial anchoring should raise ceiling performance. Overall, calibrated confidence and conservative routing enable AI to reliably handle a sizable subset of routine cases while reserving expert judgment for ambiguous or pedagogically rich responses."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-04T15:07:06Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    7,
                    6,
                    5,
                    277,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Gerd Kortemeyer"
                    },
                    {
                        "name": "Alexander Caspar"
                    },
                    {
                        "name": "Daria Horica"
                    }
                ],
                "author_detail": {
                    "name": "Daria Horica"
                },
                "author": "Daria Horica"
            },
            {
                "id": "http://arxiv.org/abs/2511.10182v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10182v1",
                "title": "Beyond the Black Box: Demystifying Multi-Turn LLM Reasoning with VISTA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Black Box: Demystifying Multi-Turn LLM Reasoning with VISTA"
                },
                "updated": "2025-11-13T10:48:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    10,
                    48,
                    20,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10182v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent research has increasingly focused on the reasoning capabilities of Large Language Models (LLMs) in multi-turn interactions, as these scenarios more closely mirror real-world problem-solving. However, analyzing the intricate reasoning processes within these interactions presents a significant challenge due to complex contextual dependencies and a lack of specialized visualization tools, leading to a high cognitive load for researchers. To address this gap, we present VISTA, an web-based Visual Interactive System for Textual Analytics in multi-turn reasoning tasks. VISTA allows users to visualize the influence of context on model decisions and interactively modify conversation histories to conduct \"what-if\" analyses across different models. Furthermore, the platform can automatically parse a session and generate a reasoning dependency tree, offering a transparent view of the model's step-by-step logical path. By providing a unified and interactive framework, VISTA significantly reduces the complexity of analyzing reasoning chains, thereby facilitating a deeper understanding of the capabilities and limitations of current LLMs. The platform is open-source and supports easy integration of custom benchmarks and local models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has increasingly focused on the reasoning capabilities of Large Language Models (LLMs) in multi-turn interactions, as these scenarios more closely mirror real-world problem-solving. However, analyzing the intricate reasoning processes within these interactions presents a significant challenge due to complex contextual dependencies and a lack of specialized visualization tools, leading to a high cognitive load for researchers. To address this gap, we present VISTA, an web-based Visual Interactive System for Textual Analytics in multi-turn reasoning tasks. VISTA allows users to visualize the influence of context on model decisions and interactively modify conversation histories to conduct \"what-if\" analyses across different models. Furthermore, the platform can automatically parse a session and generate a reasoning dependency tree, offering a transparent view of the model's step-by-step logical path. By providing a unified and interactive framework, VISTA significantly reduces the complexity of analyzing reasoning chains, thereby facilitating a deeper understanding of the capabilities and limitations of current LLMs. The platform is open-source and supports easy integration of custom benchmarks and local models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T10:48:20Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    10,
                    48,
                    20,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yiran Zhang"
                    },
                    {
                        "name": "Mingyang Lin"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem"
            },
            {
                "id": "http://arxiv.org/abs/2511.10179v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10179v1",
                "title": "QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuCoWE Quantum Contrastive Word Embeddings with Variational Circuits for NearTerm Quantum Devices"
                },
                "updated": "2025-11-13T10:45:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    10,
                    45,
                    15,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10179v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present QuCoWE a framework that learns quantumnative word embeddings by training shallow hardwareefficient parameterized quantum circuits PQCs with a contrastive skipgram objective Words are encoded by datareuploading circuits with controlled ring entanglement similarity is computed via quantum state fidelity and passed through a logitfidelity head that aligns scores with the shiftedPMI scale of SGNSNoiseContrastive Estimation To maintain trainability we introduce an entanglementbudget regularizer based on singlequbit purity that mitigates barren plateaus On Text8 and WikiText2 QuCoWE attains competitive intrinsic WordSim353 SimLex999 and extrinsic SST2 TREC6 performance versus 50100d classical baselines while using fewer learned parameters per token All experiments are run in classical simulation we analyze depolarizingreadout noise and include errormitigation hooks zeronoise extrapolation randomized compiling to facilitate hardware deployment",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present QuCoWE a framework that learns quantumnative word embeddings by training shallow hardwareefficient parameterized quantum circuits PQCs with a contrastive skipgram objective Words are encoded by datareuploading circuits with controlled ring entanglement similarity is computed via quantum state fidelity and passed through a logitfidelity head that aligns scores with the shiftedPMI scale of SGNSNoiseContrastive Estimation To maintain trainability we introduce an entanglementbudget regularizer based on singlequbit purity that mitigates barren plateaus On Text8 and WikiText2 QuCoWE attains competitive intrinsic WordSim353 SimLex999 and extrinsic SST2 TREC6 performance versus 50100d classical baselines while using fewer learned parameters per token All experiments are run in classical simulation we analyze depolarizingreadout noise and include errormitigation hooks zeronoise extrapolation randomized compiling to facilitate hardware deployment"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T10:45:15Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    10,
                    45,
                    15,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Rabimba Karanjai"
                    },
                    {
                        "name": "Hemanth Hegadehalli Madhavarao"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Weidong Shi"
                    }
                ],
                "author_detail": {
                    "name": "Weidong Shi"
                },
                "author": "Weidong Shi"
            },
            {
                "id": "http://arxiv.org/abs/2510.01299v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.01299v2",
                "title": "Enhancing the development of Cherenkov Telescope Array control software with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the development of Cherenkov Telescope Array control software with Large Language Models"
                },
                "updated": "2025-11-13T10:31:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    10,
                    31,
                    14,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.01299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.01299v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We develop AI agents based on instruction-finetuned large language models (LLMs) to assist in the engineering and operation of the Cherenkov Telescope Array Observatory (CTAO) Control and Data Acquisition Software (ACADA). These agents align with project-specific documentation and codebases, understand contextual information, interact with external APIs, and communicate with users in natural language. We present our progress in integrating these features into CTAO pipelines for operations and offline data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop AI agents based on instruction-finetuned large language models (LLMs) to assist in the engineering and operation of the Cherenkov Telescope Array Observatory (CTAO) Control and Data Acquisition Software (ACADA). These agents align with project-specific documentation and codebases, understand contextual information, interact with external APIs, and communicate with users in natural language. We present our progress in integrating these features into CTAO pipelines for operations and offline data analysis."
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-01T14:14:41Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    14,
                    41,
                    2,
                    274,
                    0
                ],
                "arxiv_comment": "EuCAIFCon 2025 proceedings",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM"
                },
                "authors": [
                    {
                        "name": "Dmitriy Kostunin"
                    },
                    {
                        "name": "Elisa Jones"
                    },
                    {
                        "name": "Vladimir Sotnikov"
                    },
                    {
                        "name": "Valery Sotnikov"
                    },
                    {
                        "name": "Sergo Golovachev"
                    },
                    {
                        "name": "Alexandre Strube"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Strube"
                },
                "author": "Alexandre Strube"
            },
            {
                "id": "http://arxiv.org/abs/2412.16725v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.16725v2",
                "title": "Enhancing Conflict Resolution in Language Models via Abstract Argumentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Conflict Resolution in Language Models via Abstract Argumentation"
                },
                "updated": "2025-11-13T10:12:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    10,
                    12,
                    53,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.16725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.16725v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In recent years, large language models (LLMs) have made significant advancements in developing human-like and engaging dialogue systems. However, in tasks such as consensus-building and persuasion, LLMs often struggle to resolve conflicts arising from incomplete or inconsistent information, revealing their limitations in real-world applications. Given these limitations, abstract argumentation, a specialized logical framework designed to resolve conflicts and inconsistencies, becomes particularly relevant. In this paper, we aim to enhance the conflict-solving capabilities of LLMs by leveraging formal abstract argumentation, integrating language model learning with symbolic computation. To achieve this, we develop and curate a dataset comprising diverse abstract argumentation frameworks, accompanied by detailed explanations of the argument acceptability computation process. Subsequently, we fine-tune LLMs on this dataset, focusing on abstract conflict resolution tasks. As a comparative baseline, LLMs are also evaluated using a chain-of-thought approach, however, they fail to solve the conflict-based arguments effectively. Our experiments demonstrate that process explanations play a crucial role in learning. Models trained with explanations exhibit superior generalization accuracy compared to those trained solely on question-answer pairs. Furthermore, leveraging LLMs' self-explanation capabilities, our approach provides detailed illustrations that mitigate the lack of transparency typically associated with neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have made significant advancements in developing human-like and engaging dialogue systems. However, in tasks such as consensus-building and persuasion, LLMs often struggle to resolve conflicts arising from incomplete or inconsistent information, revealing their limitations in real-world applications. Given these limitations, abstract argumentation, a specialized logical framework designed to resolve conflicts and inconsistencies, becomes particularly relevant. In this paper, we aim to enhance the conflict-solving capabilities of LLMs by leveraging formal abstract argumentation, integrating language model learning with symbolic computation. To achieve this, we develop and curate a dataset comprising diverse abstract argumentation frameworks, accompanied by detailed explanations of the argument acceptability computation process. Subsequently, we fine-tune LLMs on this dataset, focusing on abstract conflict resolution tasks. As a comparative baseline, LLMs are also evaluated using a chain-of-thought approach, however, they fail to solve the conflict-based arguments effectively. Our experiments demonstrate that process explanations play a crucial role in learning. Models trained with explanations exhibit superior generalization accuracy compared to those trained solely on question-answer pairs. Furthermore, leveraging LLMs' self-explanation capabilities, our approach provides detailed illustrations that mitigate the lack of transparency typically associated with neural networks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-21T18:23:06Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    18,
                    23,
                    6,
                    5,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zhaoqun Li"
                    },
                    {
                        "name": "Xiaotong Fang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mengze Li"
                    },
                    {
                        "name": "Beishui Liao"
                    }
                ],
                "author_detail": {
                    "name": "Beishui Liao"
                },
                "author": "Beishui Liao"
            },
            {
                "id": "http://arxiv.org/abs/2510.01252v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.01252v3",
                "title": "GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models"
                },
                "updated": "2025-11-13T10:09:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    10,
                    9,
                    28,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.01252v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.01252v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are trained on massive, unstructured corpora, making it unclear which social patterns and biases they absorb and later reproduce. Existing evaluations typically examine outputs or activations, but rarely connect them back to the pre-training data. We introduce a pipeline that couples LLMs with sparse autoencoders (SAEs) to trace how different themes are encoded during training. As a controlled case study, we trained a GPT-style model on 37 nineteenth-century novels by ten female authors, a corpus centered on themes such as gender, marriage, class, and morality. By applying SAEs across layers and probing with eleven social and moral categories, we mapped sparse features to human-interpretable concepts. The analysis revealed stable thematic backbones (most prominently around gender and kinship) and showed how associations expand and entangle with depth. More broadly, we argue that the LLM+SAEs pipeline offers a scalable framework for auditing how cultural assumptions from the data are embedded in model representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are trained on massive, unstructured corpora, making it unclear which social patterns and biases they absorb and later reproduce. Existing evaluations typically examine outputs or activations, but rarely connect them back to the pre-training data. We introduce a pipeline that couples LLMs with sparse autoencoders (SAEs) to trace how different themes are encoded during training. As a controlled case study, we trained a GPT-style model on 37 nineteenth-century novels by ten female authors, a corpus centered on themes such as gender, marriage, class, and morality. By applying SAEs across layers and probing with eleven social and moral categories, we mapped sparse features to human-interpretable concepts. The analysis revealed stable thematic backbones (most prominently around gender and kinship) and showed how associations expand and entangle with depth. More broadly, we argue that the LLM+SAEs pipeline offers a scalable framework for auditing how cultural assumptions from the data are embedded in model representations."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-24T11:10:16Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    10,
                    16,
                    2,
                    267,
                    0
                ],
                "arxiv_comment": "Preprint. Draft version, subject to revision",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mariam Mahran"
                    },
                    {
                        "name": "Katharina Simbeck"
                    }
                ],
                "author_detail": {
                    "name": "Katharina Simbeck"
                },
                "author": "Katharina Simbeck"
            },
            {
                "id": "http://arxiv.org/abs/2506.01939v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.01939v2",
                "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
                },
                "updated": "2025-11-13T10:08:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    10,
                    8,
                    29,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.01939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.01939v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-02T17:54:39Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    54,
                    39,
                    0,
                    153,
                    0
                ],
                "arxiv_comment": "Accepted to NeurIPS 2025. 25 pages, 17 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shenzhi Wang"
                    },
                    {
                        "name": "Le Yu"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Shixuan Liu"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Kai Dang"
                    },
                    {
                        "name": "Xionghui Chen"
                    },
                    {
                        "name": "Jianxin Yang"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Yuqiong Liu"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Shiji Song"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Gao Huang"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin"
            },
            {
                "id": "http://arxiv.org/abs/2511.10150v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10150v1",
                "title": "Decoupling Bias, Aligning Distributions: Synergistic Fairness Optimization for Deepfake Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling Bias, Aligning Distributions: Synergistic Fairness Optimization for Deepfake Detection"
                },
                "updated": "2025-11-13T10:04:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    10,
                    4,
                    45,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10150v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fairness is a core element in the trustworthy deployment of deepfake detection models, especially in the field of digital identity security. Biases in detection models toward different demographic groups, such as gender and race, may lead to systemic misjudgments, exacerbating the digital divide and social inequities. However, current fairness-enhanced detectors often improve fairness at the cost of detection accuracy. To address this challenge, we propose a dual-mechanism collaborative optimization framework. Our proposed method innovatively integrates structural fairness decoupling and global distribution alignment: decoupling channels sensitive to demographic groups at the model architectural level, and subsequently reducing the distance between the overall sample distribution and the distributions corresponding to each demographic group at the feature level. Experimental results demonstrate that, compared with other methods, our framework improves both inter-group and intra-group fairness while maintaining overall detection accuracy across domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness is a core element in the trustworthy deployment of deepfake detection models, especially in the field of digital identity security. Biases in detection models toward different demographic groups, such as gender and race, may lead to systemic misjudgments, exacerbating the digital divide and social inequities. However, current fairness-enhanced detectors often improve fairness at the cost of detection accuracy. To address this challenge, we propose a dual-mechanism collaborative optimization framework. Our proposed method innovatively integrates structural fairness decoupling and global distribution alignment: decoupling channels sensitive to demographic groups at the model architectural level, and subsequently reducing the distance between the overall sample distribution and the distributions corresponding to each demographic group at the feature level. Experimental results demonstrate that, compared with other methods, our framework improves both inter-group and intra-group fairness while maintaining overall detection accuracy across domains."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T10:04:45Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    10,
                    4,
                    45,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Feng Ding"
                    },
                    {
                        "name": "Wenhui Yi"
                    },
                    {
                        "name": "Yunpeng Zhou"
                    },
                    {
                        "name": "Xinan He"
                    },
                    {
                        "name": "Hong Rao"
                    },
                    {
                        "name": "Shu Hu"
                    }
                ],
                "author_detail": {
                    "name": "Shu Hu"
                },
                "author": "Shu Hu"
            },
            {
                "id": "http://arxiv.org/abs/2304.11954v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2304.11954v5",
                "title": "Spikingformer: A Key Foundation Model for Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spikingformer: A Key Foundation Model for Spiking Neural Networks"
                },
                "updated": "2025-11-13T09:37:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    9,
                    37,
                    0,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2304.11954v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2304.11954v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spiking neural networks (SNNs) offer a promising energy-efficient alternative to artificial neural networks, due to their event-driven spiking computation. However, some foundation SNN backbones (including Spikformer and SEW ResNet) suffer from non-spike computations (integer-float multiplications) caused by the structure of their residual connections. These non-spike computations increase SNNs' power consumption and make them unsuitable for deployment on mainstream neuromorphic hardware. In this paper, we analyze the spike-driven behavior of the residual connection methods in SNNs. We then present Spikingformer, a novel spiking transformer backbone that merges the MS Residual connection with Self-Attention in a biologically plausible way to address the non-spike computation challenge in Spikformer while maintaining global modeling capabilities. We evaluate Spikingformer across 13 datasets spanning large static images, neuromorphic data, and natural language tasks, and demonstrate the effectiveness and universality of Spikingformer, setting a vital benchmark for spiking neural networks. In addition, with the spike-driven features and global modeling capabilities, Spikingformer is expected to become a more efficient general-purpose SNN backbone towards energy-efficient artificial intelligence. Code: https://github.com/TheBrainLab/Spikingformer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking neural networks (SNNs) offer a promising energy-efficient alternative to artificial neural networks, due to their event-driven spiking computation. However, some foundation SNN backbones (including Spikformer and SEW ResNet) suffer from non-spike computations (integer-float multiplications) caused by the structure of their residual connections. These non-spike computations increase SNNs' power consumption and make them unsuitable for deployment on mainstream neuromorphic hardware. In this paper, we analyze the spike-driven behavior of the residual connection methods in SNNs. We then present Spikingformer, a novel spiking transformer backbone that merges the MS Residual connection with Self-Attention in a biologically plausible way to address the non-spike computation challenge in Spikformer while maintaining global modeling capabilities. We evaluate Spikingformer across 13 datasets spanning large static images, neuromorphic data, and natural language tasks, and demonstrate the effectiveness and universality of Spikingformer, setting a vital benchmark for spiking neural networks. In addition, with the spike-driven features and global modeling capabilities, Spikingformer is expected to become a more efficient general-purpose SNN backbone towards energy-efficient artificial intelligence. Code: https://github.com/TheBrainLab/Spikingformer"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-04-24T09:44:24Z",
                "published_parsed": [
                    2023,
                    4,
                    24,
                    9,
                    44,
                    24,
                    0,
                    114,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026 (Oral), Code: https://github.com/TheBrainLab/Spikingformer",
                "arxiv_primary_category": {
                    "term": "cs.NE"
                },
                "authors": [
                    {
                        "name": "Chenlin Zhou"
                    },
                    {
                        "name": "Liutao Yu"
                    },
                    {
                        "name": "Zhaokun Zhou"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Huihui Zhou"
                    },
                    {
                        "name": "Zhengyu Ma"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian"
            },
            {
                "id": "http://arxiv.org/abs/2509.15886v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.15886v3",
                "title": "RangeSAM: On the Potential of Visual Foundation Models for Range-View represented LiDAR segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RangeSAM: On the Potential of Visual Foundation Models for Range-View represented LiDAR segmentation"
                },
                "updated": "2025-11-13T09:29:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    9,
                    29,
                    33,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.15886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.15886v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Point cloud segmentation is central to autonomous driving and 3D scene understanding. While voxel- and point-based methods dominate recent research due to their compatibility with deep architectures and ability to capture fine-grained geometry, they often incur high computational cost, irregular memory access, and limited real-time efficiency. In contrast, range-view methods, though relatively underexplored - can leverage mature 2D semantic segmentation techniques for fast and accurate predictions. Motivated by the rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot recognition, and multimodal tasks, we investigate whether SAM2, the current state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for LiDAR point cloud segmentation in the range view. We present , to our knowledge, the first range-view framework that adapts SAM2 to 3D segmentation, coupling efficient 2D feature extraction with standard projection/back-projection to operate on point clouds. To optimize SAM2 for range-view representations, we implement several architectural modifications to the encoder: (1) a novel module that emphasizes horizontal spatial dependencies inherent in LiDAR range images, (2) a customized configuration of tailored to the geometric properties of spherical projections, and (3) an adapted mechanism in the encoder backbone specifically designed to capture the unique spatial patterns and discontinuities present in range-view pseudo-images. Our approach achieves competitive performance on SemanticKITTI while benefiting from the speed, scalability, and deployment simplicity of 2D-centric pipelines. This work highlights the viability of VFMs as general-purpose backbones for 3D perception and opens a path toward unified, foundation-model-driven LiDAR segmentation. Results lets us conclude that range-view segmentation methods using VFMs leads to promising results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud segmentation is central to autonomous driving and 3D scene understanding. While voxel- and point-based methods dominate recent research due to their compatibility with deep architectures and ability to capture fine-grained geometry, they often incur high computational cost, irregular memory access, and limited real-time efficiency. In contrast, range-view methods, though relatively underexplored - can leverage mature 2D semantic segmentation techniques for fast and accurate predictions. Motivated by the rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot recognition, and multimodal tasks, we investigate whether SAM2, the current state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for LiDAR point cloud segmentation in the range view. We present , to our knowledge, the first range-view framework that adapts SAM2 to 3D segmentation, coupling efficient 2D feature extraction with standard projection/back-projection to operate on point clouds. To optimize SAM2 for range-view representations, we implement several architectural modifications to the encoder: (1) a novel module that emphasizes horizontal spatial dependencies inherent in LiDAR range images, (2) a customized configuration of tailored to the geometric properties of spherical projections, and (3) an adapted mechanism in the encoder backbone specifically designed to capture the unique spatial patterns and discontinuities present in range-view pseudo-images. Our approach achieves competitive performance on SemanticKITTI while benefiting from the speed, scalability, and deployment simplicity of 2D-centric pipelines. This work highlights the viability of VFMs as general-purpose backbones for 3D perception and opens a path toward unified, foundation-model-driven LiDAR segmentation. Results lets us conclude that range-view segmentation methods using VFMs leads to promising results."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-19T11:33:10Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    11,
                    33,
                    10,
                    4,
                    262,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Paul Julius Kühn"
                    },
                    {
                        "name": "Duc Anh Nguyen"
                    },
                    {
                        "name": "Arjan Kuijper"
                    },
                    {
                        "name": "Holger Graf"
                    },
                    {
                        "name": "Saptarshi Neil Sinha"
                    }
                ],
                "author_detail": {
                    "name": "Saptarshi Neil Sinha"
                },
                "author": "Saptarshi Neil Sinha"
            },
            {
                "id": "http://arxiv.org/abs/2511.10117v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10117v1",
                "title": "Cooperative Control of Hybrid FES-Exoskeleton: Dynamic Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Control of Hybrid FES-Exoskeleton: Dynamic Allocation"
                },
                "updated": "2025-11-13T09:24:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    9,
                    24,
                    10,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10117v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Hybrid assistive systems that integrate functional electrical stimulation (FES) and robotic exoskeletons offer a promising approach for neurorehabilitation. However, control of these systems remains challenging due to actuator redundancy and heterogeneous assistive device constraints. This paper introduces a novel cooperative control architecture based on dynamic allocation to address actuator redundancy in a hybrid FES-exoskeleton system. The proposed approach employs a modular control allocator that redistributes required control torques between FES and exoskeleton actuators in real time, accounting for device-specific limitations and user preferences (e.g., prioritizing one assistive device over another). Within this framework, the high-level controller determines the total assistance level, while the allocator dynamically distributes control effort based on these assistive device-specific considerations. Simulation results and experimental validation demonstrate the method's effectiveness in resolving actuator redundancy in the FES-exoskeleton system while reflecting actuator constraints, indicating its potential for deployment in clinical studies to assess patient acceptance and clinical efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid assistive systems that integrate functional electrical stimulation (FES) and robotic exoskeletons offer a promising approach for neurorehabilitation. However, control of these systems remains challenging due to actuator redundancy and heterogeneous assistive device constraints. This paper introduces a novel cooperative control architecture based on dynamic allocation to address actuator redundancy in a hybrid FES-exoskeleton system. The proposed approach employs a modular control allocator that redistributes required control torques between FES and exoskeleton actuators in real time, accounting for device-specific limitations and user preferences (e.g., prioritizing one assistive device over another). Within this framework, the high-level controller determines the total assistance level, while the allocator dynamically distributes control effort based on these assistive device-specific considerations. Simulation results and experimental validation demonstrate the method's effectiveness in resolving actuator redundancy in the FES-exoskeleton system while reflecting actuator constraints, indicating its potential for deployment in clinical studies to assess patient acceptance and clinical efficacy."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T09:24:10Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    9,
                    24,
                    10,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "14 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Hossein Kavianirad"
                    },
                    {
                        "name": "Satoshi Endo"
                    },
                    {
                        "name": "Davide Astarita"
                    },
                    {
                        "name": "Lorenzo Amato"
                    },
                    {
                        "name": "Emilio Trigili"
                    },
                    {
                        "name": "Sandra Hirche"
                    }
                ],
                "author_detail": {
                    "name": "Sandra Hirche"
                },
                "author": "Sandra Hirche"
            },
            {
                "id": "http://arxiv.org/abs/2505.06347v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.06347v2",
                "title": "Scalable Quantum State Preparation via Large-Language-Model-Driven Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Quantum State Preparation via Large-Language-Model-Driven Discovery"
                },
                "updated": "2025-11-13T09:23:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    9,
                    23,
                    43,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.06347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.06347v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Efficient quantum state preparation remains a central challenge in first-principles quantum simulations of dynamics in quantum field theories, where the Hilbert space is intrinsically infinite-dimensional. Here, we introduce a large language model (LLM)-assisted framework for quantum-circuit design that systematically scales state-preparation circuits to large lattice volumes. Applied to a 1+1d XY spin chain, the LLM autonomously discovers a compact 4-parameter circuit that captures boundary-induced symmetry breaking with sub-percent energy deviation, enabling successful validation on the \\texttt{Zuchongzhi} quantum processor. Guided by this insight, we extend the framework to 2+1d quantum field theories, where scalable variational ansätze have remained elusive. For a scalar field theory, the search yields a symmetry-preserving, 3-parameter shallow-depth ansatz whose optimized parameters converge to size-independent constants for lattices $n \\ge 4$, providing, to our knowledge, the first scalable ansatz for this class of 2+1d models. Our results establish a practical route toward AI-assisted, human-guided discovery in quantum simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient quantum state preparation remains a central challenge in first-principles quantum simulations of dynamics in quantum field theories, where the Hilbert space is intrinsically infinite-dimensional. Here, we introduce a large language model (LLM)-assisted framework for quantum-circuit design that systematically scales state-preparation circuits to large lattice volumes. Applied to a 1+1d XY spin chain, the LLM autonomously discovers a compact 4-parameter circuit that captures boundary-induced symmetry breaking with sub-percent energy deviation, enabling successful validation on the \\texttt{Zuchongzhi} quantum processor. Guided by this insight, we extend the framework to 2+1d quantum field theories, where scalable variational ansätze have remained elusive. For a scalar field theory, the search yields a symmetry-preserving, 3-parameter shallow-depth ansatz whose optimized parameters converge to size-independent constants for lattices $n \\ge 4$, providing, to our knowledge, the first scalable ansatz for this class of 2+1d models. Our results establish a practical route toward AI-assisted, human-guided discovery in quantum simulation."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-lat",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-09T18:00:02Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    18,
                    0,
                    2,
                    4,
                    129,
                    0
                ],
                "arxiv_comment": "6 + 3 pages, 9 figures. Substantially update the draft, including a study of the 2+1d scalar field theory",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Qing-Hong Cao"
                    },
                    {
                        "name": "Zong-Yue Hou"
                    },
                    {
                        "name": "Ying-Ying Li"
                    },
                    {
                        "name": "Xiaohui Liu"
                    },
                    {
                        "name": "Zhuo-Yang Song"
                    },
                    {
                        "name": "Liang-Qi Zhang"
                    },
                    {
                        "name": "Shutao Zhang"
                    },
                    {
                        "name": "Ke Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Ke Zhao"
                },
                "author": "Ke Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2511.08066v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08066v3",
                "title": "Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression"
                },
                "updated": "2025-11-13T09:16:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    9,
                    16,
                    44,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08066v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08066v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent years have witnessed the rapid advancements of large language models (LLMs) and their expanding applications, leading to soaring demands for computational resources. The widespread adoption of test-time scaling further aggravates the tension between model capability and resource consumption, highlighting the importance of inference efficiency. However, a unified metric that accurately reflects an LLM's efficiency across different model sizes and architectures remains absent. Motivated by the correlation between compression and intelligence, we introduce information capacity, a measure of model efficiency based on text compression performance relative to computational complexity. Larger models can predict the next token more accurately, achieving greater compression gains but at higher computational costs. Empirical evaluations on mainstream open-source models show that models of varying sizes within a series exhibit consistent information capacity. This metric enables a fair efficiency comparison across model series and accurate performance prediction within a model series. A distinctive feature of information capacity is that it incorporates tokenizer efficiency, which affects both input and output token counts but is often neglected in LLM evaluations. We assess the information capacity of 49 models on 5 heterogeneous datasets and observe consistent results on the influences of tokenizer efficiency, pretraining data, and the mixture-of-experts architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid advancements of large language models (LLMs) and their expanding applications, leading to soaring demands for computational resources. The widespread adoption of test-time scaling further aggravates the tension between model capability and resource consumption, highlighting the importance of inference efficiency. However, a unified metric that accurately reflects an LLM's efficiency across different model sizes and architectures remains absent. Motivated by the correlation between compression and intelligence, we introduce information capacity, a measure of model efficiency based on text compression performance relative to computational complexity. Larger models can predict the next token more accurately, achieving greater compression gains but at higher computational costs. Empirical evaluations on mainstream open-source models show that models of varying sizes within a series exhibit consistent information capacity. This metric enables a fair efficiency comparison across model series and accurate performance prediction within a model series. A distinctive feature of information capacity is that it incorporates tokenizer efficiency, which affects both input and output token counts but is often neglected in LLM evaluations. We assess the information capacity of 49 models on 5 heterogeneous datasets and observe consistent results on the influences of tokenizer efficiency, pretraining data, and the mixture-of-experts architecture."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T10:07:32Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    10,
                    7,
                    32,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "Code: https://github.com/TeleAI-AI-Flow/InformationCapacity. Data: https://huggingface.co/datasets/TeleAI-AI-Flow/InformationCapacity",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Cheng Yuan"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.08903v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08903v2",
                "title": "LLM-Guided Probabilistic Fusion for Label-Efficient Document Layout Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Guided Probabilistic Fusion for Label-Efficient Document Layout Analysis"
                },
                "updated": "2025-11-13T09:03:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    9,
                    3,
                    17,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08903v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Document layout understanding remains data-intensive despite advances in semi-supervised learning. We present a framework that enhances semi-supervised detection by fusing visual predictions with structural priors from text-pretrained LLMs via principled probabilistic weighting. Given unlabeled documents, an OCR-LLM pipeline infers hierarchical regions which are combined with teacher detector outputs through inverse-variance fusion to generate refined pseudo-labels.Our method demonstrates consistent gains across model scales. With a lightweight SwiftFormer backbone (26M params), we achieve 88.2$\\pm$0.3 AP using only 5\\% labels on PubLayNet. When applied to document-pretrained LayoutLMv3 (133M params), our fusion framework reaches 89.7$\\pm$0.4 AP, surpassing both LayoutLMv3 with standard semi-supervised learning (89.1$\\pm$0.4 AP, p=0.02) and matching UDOP~\\cite{udop} (89.8 AP) which requires 100M+ pages of multimodal pretraining. This demonstrates that LLM structural priors are complementary to both lightweight and pretrained architectures. Key findings include: (1) learned instance-adaptive gating improves over fixed weights by +0.9 AP with data-dependent PAC bounds correctly predicting convergence; (2) open-source LLMs enable privacy-preserving deployment with minimal loss (Llama-3-70B: 87.1 AP lightweight, 89.4 AP with LayoutLMv3); (3) LLMs provide targeted semantic disambiguation (18.7\\% of cases, +3.8 AP gain) beyond simple text heuristics.Total system cost includes \\$12 for GPT-4o-mini API or 17 GPU-hours for local Llama-3-70B per 50K pages, amortized across training runs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document layout understanding remains data-intensive despite advances in semi-supervised learning. We present a framework that enhances semi-supervised detection by fusing visual predictions with structural priors from text-pretrained LLMs via principled probabilistic weighting. Given unlabeled documents, an OCR-LLM pipeline infers hierarchical regions which are combined with teacher detector outputs through inverse-variance fusion to generate refined pseudo-labels.Our method demonstrates consistent gains across model scales. With a lightweight SwiftFormer backbone (26M params), we achieve 88.2$\\pm$0.3 AP using only 5\\% labels on PubLayNet. When applied to document-pretrained LayoutLMv3 (133M params), our fusion framework reaches 89.7$\\pm$0.4 AP, surpassing both LayoutLMv3 with standard semi-supervised learning (89.1$\\pm$0.4 AP, p=0.02) and matching UDOP~\\cite{udop} (89.8 AP) which requires 100M+ pages of multimodal pretraining. This demonstrates that LLM structural priors are complementary to both lightweight and pretrained architectures. Key findings include: (1) learned instance-adaptive gating improves over fixed weights by +0.9 AP with data-dependent PAC bounds correctly predicting convergence; (2) open-source LLMs enable privacy-preserving deployment with minimal loss (Llama-3-70B: 87.1 AP lightweight, 89.4 AP with LayoutLMv3); (3) LLMs provide targeted semantic disambiguation (18.7\\% of cases, +3.8 AP gain) beyond simple text heuristics.Total system cost includes \\$12 for GPT-4o-mini API or 17 GPU-hours for local Llama-3-70B per 50K pages, amortized across training runs."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T02:25:58Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    2,
                    25,
                    58,
                    2,
                    316,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Sanjeda Akter"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma"
            },
            {
                "id": "http://arxiv.org/abs/2502.13595v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.13595v4",
                "title": "MMTEB: Massive Multilingual Text Embedding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMTEB: Massive Multilingual Text Embedding Benchmark"
                },
                "updated": "2025-11-13T08:51:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    8,
                    51,
                    24,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.13595v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.13595v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-19T10:13:43Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    13,
                    43,
                    2,
                    50,
                    0
                ],
                "arxiv_comment": "Accepted for ICLR: https://openreview.net/forum?id=zl3pfz4VCV",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kenneth Enevoldsen"
                    },
                    {
                        "name": "Isaac Chung"
                    },
                    {
                        "name": "Imene Kerboua"
                    },
                    {
                        "name": "Márton Kardos"
                    },
                    {
                        "name": "Ashwin Mathur"
                    },
                    {
                        "name": "David Stap"
                    },
                    {
                        "name": "Jay Gala"
                    },
                    {
                        "name": "Wissam Siblini"
                    },
                    {
                        "name": "Dominik Krzemiński"
                    },
                    {
                        "name": "Genta Indra Winata"
                    },
                    {
                        "name": "Saba Sturua"
                    },
                    {
                        "name": "Saiteja Utpala"
                    },
                    {
                        "name": "Mathieu Ciancone"
                    },
                    {
                        "name": "Marion Schaeffer"
                    },
                    {
                        "name": "Gabriel Sequeira"
                    },
                    {
                        "name": "Diganta Misra"
                    },
                    {
                        "name": "Shreeya Dhakal"
                    },
                    {
                        "name": "Jonathan Rystrøm"
                    },
                    {
                        "name": "Roman Solomatin"
                    },
                    {
                        "name": "Ömer Çağatan"
                    },
                    {
                        "name": "Akash Kundu"
                    },
                    {
                        "name": "Martin Bernstorff"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Akshita Sukhlecha"
                    },
                    {
                        "name": "Bhavish Pahwa"
                    },
                    {
                        "name": "Rafał Poświata"
                    },
                    {
                        "name": "Kranthi Kiran GV"
                    },
                    {
                        "name": "Shawon Ashraf"
                    },
                    {
                        "name": "Daniel Auras"
                    },
                    {
                        "name": "Björn Plüster"
                    },
                    {
                        "name": "Jan Philipp Harries"
                    },
                    {
                        "name": "Loïc Magne"
                    },
                    {
                        "name": "Isabelle Mohr"
                    },
                    {
                        "name": "Mariya Hendriksen"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Hippolyte Gisserot-Boukhlef"
                    },
                    {
                        "name": "Tom Aarsen"
                    },
                    {
                        "name": "Jan Kostkan"
                    },
                    {
                        "name": "Konrad Wojtasik"
                    },
                    {
                        "name": "Taemin Lee"
                    },
                    {
                        "name": "Marek Šuppa"
                    },
                    {
                        "name": "Crystina Zhang"
                    },
                    {
                        "name": "Roberta Rocca"
                    },
                    {
                        "name": "Mohammed Hamdy"
                    },
                    {
                        "name": "Andrianos Michail"
                    },
                    {
                        "name": "John Yang"
                    },
                    {
                        "name": "Manuel Faysse"
                    },
                    {
                        "name": "Aleksei Vatolin"
                    },
                    {
                        "name": "Nandan Thakur"
                    },
                    {
                        "name": "Manan Dey"
                    },
                    {
                        "name": "Dipam Vasani"
                    },
                    {
                        "name": "Pranjal Chitale"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Nguyen Tai"
                    },
                    {
                        "name": "Artem Snegirev"
                    },
                    {
                        "name": "Michael Günther"
                    },
                    {
                        "name": "Mengzhou Xia"
                    },
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Xing Han Lù"
                    },
                    {
                        "name": "Jordan Clive"
                    },
                    {
                        "name": "Gayatri Krishnakumar"
                    },
                    {
                        "name": "Anna Maksimova"
                    },
                    {
                        "name": "Silvan Wehrli"
                    },
                    {
                        "name": "Maria Tikhonova"
                    },
                    {
                        "name": "Henil Panchal"
                    },
                    {
                        "name": "Aleksandr Abramov"
                    },
                    {
                        "name": "Malte Ostendorff"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Simon Clematide"
                    },
                    {
                        "name": "Lester James Miranda"
                    },
                    {
                        "name": "Alena Fenogenova"
                    },
                    {
                        "name": "Guangyu Song"
                    },
                    {
                        "name": "Ruqiya Bin Safi"
                    },
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Alessia Borghini"
                    },
                    {
                        "name": "Federico Cassano"
                    },
                    {
                        "name": "Hongjin Su"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Howard Yen"
                    },
                    {
                        "name": "Lasse Hansen"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Vaibhav Adlakha"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Muennighoff"
                },
                "author": "Niklas Muennighoff"
            },
            {
                "id": "http://arxiv.org/abs/2511.10091v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10091v1",
                "title": "SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition"
                },
                "updated": "2025-11-13T08:45:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    8,
                    45,
                    24,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10091v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) hold rich implicit knowledge and powerful transferability. In this paper, we explore the combination of LLMs with the human skeleton to perform action classification and description. However, when treating LLM as a recognizer, two questions arise: 1) How can LLMs understand skeleton? 2) How can LLMs distinguish among actions? To address these problems, we introduce a novel paradigm named learning Skeleton representation with visUal-motion knowledGe for Action Recognition (SUGAR). In our pipeline, we first utilize off-the-shelf large-scale video models as a knowledge base to generate visual, motion information related to actions. Then, we propose to supervise skeleton learning through this prior knowledge to yield discrete representations. Finally, we use the LLM with untouched pre-training weights to understand these representations and generate the desired action targets and descriptions. Notably, we present a Temporal Query Projection (TQP) module to continuously model the skeleton signals with long sequences. Experiments on several skeleton-based action classification benchmarks demonstrate the efficacy of our SUGAR. Moreover, experiments on zero-shot scenarios show that SUGAR is more versatile than linear-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hold rich implicit knowledge and powerful transferability. In this paper, we explore the combination of LLMs with the human skeleton to perform action classification and description. However, when treating LLM as a recognizer, two questions arise: 1) How can LLMs understand skeleton? 2) How can LLMs distinguish among actions? To address these problems, we introduce a novel paradigm named learning Skeleton representation with visUal-motion knowledGe for Action Recognition (SUGAR). In our pipeline, we first utilize off-the-shelf large-scale video models as a knowledge base to generate visual, motion information related to actions. Then, we propose to supervise skeleton learning through this prior knowledge to yield discrete representations. Finally, we use the LLM with untouched pre-training weights to understand these representations and generate the desired action targets and descriptions. Notably, we present a Temporal Query Projection (TQP) module to continuously model the skeleton signals with long sequences. Experiments on several skeleton-based action classification benchmarks demonstrate the efficacy of our SUGAR. Moreover, experiments on zero-shot scenarios show that SUGAR is more versatile than linear-based methods."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T08:45:24Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    8,
                    45,
                    24,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026 Main Track",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Qilang Ye"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Lian He"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Xuanming Guo"
                    },
                    {
                        "name": "Jiayu Zhang"
                    },
                    {
                        "name": "Mingkui Tan"
                    },
                    {
                        "name": "Weicheng Xie"
                    },
                    {
                        "name": "Yue Sun"
                    },
                    {
                        "name": "Tao Tan"
                    },
                    {
                        "name": "Xiaochen Yuan"
                    },
                    {
                        "name": "Ghada Khoriba"
                    },
                    {
                        "name": "Zitong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zitong Yu"
                },
                "author": "Zitong Yu"
            },
            {
                "id": "http://arxiv.org/abs/2511.10089v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10089v1",
                "title": "T2IBias: Uncovering Societal Bias Encoded in the Latent Space of Text-to-Image Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T2IBias: Uncovering Societal Bias Encoded in the Latent Space of Text-to-Image Generative Models"
                },
                "updated": "2025-11-13T08:43:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    8,
                    43,
                    2,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10089v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text-to-image (T2I) generative models are largely used in AI-powered real-world applications and value creation. However, their strategic deployment raises critical concerns for responsible AI management, particularly regarding the reproduction and amplification of race- and gender-related stereotypes that can undermine organizational ethics. In this work, we investigate whether such societal biases are systematically encoded within the pretrained latent spaces of state-of-the-art T2I models. We conduct an empirical study across the five most popular open-source models, using ten neutral, profession-related prompts to generate 100 images per profession, resulting in a dataset of 5,000 images evaluated by diverse human assessors representing different races and genders. We demonstrate that all five models encode and amplify pronounced societal skew: caregiving and nursing roles are consistently feminized, while high-status professions such as corporate CEO, politician, doctor, and lawyer are overwhelmingly represented by males and mostly White individuals. We further identify model-specific patterns, such as QWEN-Image's near-exclusive focus on East Asian outputs, Kandinsky's dominance of White individuals, and SDXL's comparatively broader but still biased distributions. These results provide critical insights for AI project managers and practitioners, enabling them to select equitable AI models and customized prompts that generate images in alignment with the principles of responsible AI. We conclude by discussing the risks of these biases and proposing actionable strategies for bias mitigation in building responsible GenAI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generative models are largely used in AI-powered real-world applications and value creation. However, their strategic deployment raises critical concerns for responsible AI management, particularly regarding the reproduction and amplification of race- and gender-related stereotypes that can undermine organizational ethics. In this work, we investigate whether such societal biases are systematically encoded within the pretrained latent spaces of state-of-the-art T2I models. We conduct an empirical study across the five most popular open-source models, using ten neutral, profession-related prompts to generate 100 images per profession, resulting in a dataset of 5,000 images evaluated by diverse human assessors representing different races and genders. We demonstrate that all five models encode and amplify pronounced societal skew: caregiving and nursing roles are consistently feminized, while high-status professions such as corporate CEO, politician, doctor, and lawyer are overwhelmingly represented by males and mostly White individuals. We further identify model-specific patterns, such as QWEN-Image's near-exclusive focus on East Asian outputs, Kandinsky's dominance of White individuals, and SDXL's comparatively broader but still biased distributions. These results provide critical insights for AI project managers and practitioners, enabling them to select equitable AI models and customized prompts that generate images in alignment with the principles of responsible AI. We conclude by discussing the risks of these biases and proposing actionable strategies for bias mitigation in building responsible GenAI systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T08:43:02Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    8,
                    43,
                    2,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "This manuscript has been accepted for presentation in the First Interdisciplinary Workshop on Responsible AI for Value Creation. Dec 1, Copenhagen. The final version will be submitted for inclusion in a Springer LNCS Volume. (15 pages, 7 figures)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Abu Sufian"
                    },
                    {
                        "name": "Cosimo Distante"
                    },
                    {
                        "name": "Marco Leo"
                    },
                    {
                        "name": "Hanan Salam"
                    }
                ],
                "author_detail": {
                    "name": "Hanan Salam"
                },
                "author": "Hanan Salam"
            },
            {
                "id": "http://arxiv.org/abs/2511.10087v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10087v1",
                "title": "Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning"
                },
                "updated": "2025-11-13T08:42:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    8,
                    42,
                    20,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10087v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Offline-to-online reinforcement learning (O2O-RL) has emerged as a promising paradigm for safe and efficient robotic policy deployment but suffers from two fundamental challenges: limited coverage of multimodal behaviors and distributional shifts during online adaptation. We propose UEPO, a unified generative framework inspired by large language model pretraining and fine-tuning strategies. Our contributions are threefold: (1) a multi-seed dynamics-aware diffusion policy that efficiently captures diverse modalities without training multiple models; (2) a dynamic divergence regularization mechanism that enforces physically meaningful policy diversity; and (3) a diffusion-based data augmentation module that enhances dynamics model generalization. On the D4RL benchmark, UEPO achieves +5.9\\% absolute improvement over Uni-O4 on locomotion tasks and +12.4\\% on dexterous manipulation, demonstrating strong generalization and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline-to-online reinforcement learning (O2O-RL) has emerged as a promising paradigm for safe and efficient robotic policy deployment but suffers from two fundamental challenges: limited coverage of multimodal behaviors and distributional shifts during online adaptation. We propose UEPO, a unified generative framework inspired by large language model pretraining and fine-tuning strategies. Our contributions are threefold: (1) a multi-seed dynamics-aware diffusion policy that efficiently captures diverse modalities without training multiple models; (2) a dynamic divergence regularization mechanism that enforces physically meaningful policy diversity; and (3) a diffusion-based data augmentation module that enhances dynamics model generalization. On the D4RL benchmark, UEPO achieves +5.9\\% absolute improvement over Uni-O4 on locomotion tasks and +12.4\\% on dexterous manipulation, demonstrating strong generalization and scalability."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T08:42:20Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    8,
                    42,
                    20,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted by NeurIPS 2025 Workshop on Embodied World Models for Decision Making",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Haidong Huang"
                    },
                    {
                        "name": "Haiyue Zhu. Jiayu Song"
                    },
                    {
                        "name": "Xixin Zhao"
                    },
                    {
                        "name": "Yaohua Zhou"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yuze Zhai"
                    },
                    {
                        "name": "Xiaocong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaocong Li"
                },
                "author": "Xiaocong Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.10075v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10075v1",
                "title": "Format Matters: The Robustness of Multimodal LLMs in Reviewing Evidence from Tables and Charts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Format Matters: The Robustness of Multimodal LLMs in Reviewing Evidence from Tables and Charts"
                },
                "updated": "2025-11-13T08:29:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    8,
                    29,
                    47,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10075v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the growing number of submitted scientific papers, there is an increasing demand for systems that can assist reviewers in evaluating research claims. Experimental results are a core component of scientific work, often presented in varying formats such as tables or charts. Understanding how robust current multimodal large language models (multimodal LLMs) are at verifying scientific claims across different evidence formats remains an important and underexplored challenge. In this paper, we design and conduct a series of experiments to assess the ability of multimodal LLMs to verify scientific claims using both tables and charts as evidence. To enable this evaluation, we adapt two existing datasets of scientific papers by incorporating annotations and structures necessary for a multimodal claim verification task. Using this adapted dataset, we evaluate 12 multimodal LLMs and find that current models perform better with table-based evidence while struggling with chart-based evidence. We further conduct human evaluations and observe that humans maintain strong performance across both formats, unlike the models. Our analysis also reveals that smaller multimodal LLMs (under 8B) show weak correlation in performance between table-based and chart-based tasks, indicating limited cross-modal generalization. These findings highlight a critical gap in current models' multimodal reasoning capabilities. We suggest that future multimodal LLMs should place greater emphasis on improving chart understanding to better support scientific claim verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing number of submitted scientific papers, there is an increasing demand for systems that can assist reviewers in evaluating research claims. Experimental results are a core component of scientific work, often presented in varying formats such as tables or charts. Understanding how robust current multimodal large language models (multimodal LLMs) are at verifying scientific claims across different evidence formats remains an important and underexplored challenge. In this paper, we design and conduct a series of experiments to assess the ability of multimodal LLMs to verify scientific claims using both tables and charts as evidence. To enable this evaluation, we adapt two existing datasets of scientific papers by incorporating annotations and structures necessary for a multimodal claim verification task. Using this adapted dataset, we evaluate 12 multimodal LLMs and find that current models perform better with table-based evidence while struggling with chart-based evidence. We further conduct human evaluations and observe that humans maintain strong performance across both formats, unlike the models. Our analysis also reveals that smaller multimodal LLMs (under 8B) show weak correlation in performance between table-based and chart-based tasks, indicating limited cross-modal generalization. These findings highlight a critical gap in current models' multimodal reasoning capabilities. We suggest that future multimodal LLMs should place greater emphasis on improving chart understanding to better support scientific claim verification."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T08:29:47Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    8,
                    29,
                    47,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Accepted at AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Xanh Ho"
                    },
                    {
                        "name": "Yun-Ang Wu"
                    },
                    {
                        "name": "Sunisth Kumar"
                    },
                    {
                        "name": "Florian Boudin"
                    },
                    {
                        "name": "Atsuhiro Takasu"
                    },
                    {
                        "name": "Akiko Aizawa"
                    }
                ],
                "author_detail": {
                    "name": "Akiko Aizawa"
                },
                "author": "Akiko Aizawa"
            }
        ]
    }
]