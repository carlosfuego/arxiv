[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.06483v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06483v4",
                "updated": "2025-11-07T18:03:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    3,
                    32,
                    4,
                    311,
                    0
                ],
                "published": "2024-06-10T17:22:17Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    22,
                    17,
                    0,
                    162,
                    0
                ],
                "title": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance"
                },
                "summary": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity."
                },
                "authors": [
                    {
                        "name": "Joshua J. Daymude"
                    },
                    {
                        "name": "Antonio M. Espinoza"
                    },
                    {
                        "name": "Holly Bergen"
                    },
                    {
                        "name": "Benjamin Mixon-Baca"
                    },
                    {
                        "name": "Jeffrey Knockel"
                    },
                    {
                        "name": "Jedidiah R. Crandall"
                    }
                ],
                "author_detail": {
                    "name": "Jedidiah R. Crandall"
                },
                "author": "Jedidiah R. Crandall",
                "arxiv_comment": "37 pages, 11 figures, 2 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06483v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06483v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05345v2",
                "updated": "2025-11-07T16:42:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    42,
                    30,
                    4,
                    311,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Hyper-Scaling with KV Cache Compression"
                },
                "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference latency and memory load. For\ninstance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and\n9.7 on LiveCodeBench on average for an equivalent number of memory reads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference latency and memory load. For\ninstance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and\n9.7 on LiveCodeBench on average for an equivalent number of memory reads."
                },
                "authors": [
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05299v1",
                "updated": "2025-11-07T15:00:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    0,
                    37,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:00:37Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    0,
                    37,
                    4,
                    311,
                    0
                ],
                "title": "LiveStar: Live Streaming Assistant for Real-World Online Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveStar: Live Streaming Assistant for Real-World Online Video\n  Understanding"
                },
                "summary": "Despite significant progress in Video Large Language Models (Video-LLMs) for\noffline video understanding, existing online Video-LLMs typically struggle to\nsimultaneously process continuous frame-by-frame inputs and determine optimal\nresponse timing, often compromising real-time responsiveness and narrative\ncoherence. To address these limitations, we introduce LiveStar, a pioneering\nlive streaming assistant that achieves always-on proactive responses through\nadaptive streaming decoding. Specifically, LiveStar incorporates: (1) a\ntraining strategy enabling incremental video-language alignment for\nvariable-length video streams, preserving temporal consistency across\ndynamically evolving frame sequences; (2) a response-silence decoding framework\nthat determines optimal proactive response timing via a single forward pass\nverification; (3) memory-aware acceleration via peak-end memory compression for\nonline inference on 10+ minute videos, combined with streaming key-value cache\nto achieve 1.53x faster inference. We also construct an OmniStar dataset, a\ncomprehensive dataset for training and benchmarking that encompasses 15 diverse\nreal-world scenarios and 5 evaluation tasks for online video understanding.\nExtensive experiments across three benchmarks demonstrate LiveStar's\nstate-of-the-art performance, achieving an average 19.5% improvement in\nsemantic correctness with 18.1% reduced timing difference compared to existing\nonline Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.\nOur model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress in Video Large Language Models (Video-LLMs) for\noffline video understanding, existing online Video-LLMs typically struggle to\nsimultaneously process continuous frame-by-frame inputs and determine optimal\nresponse timing, often compromising real-time responsiveness and narrative\ncoherence. To address these limitations, we introduce LiveStar, a pioneering\nlive streaming assistant that achieves always-on proactive responses through\nadaptive streaming decoding. Specifically, LiveStar incorporates: (1) a\ntraining strategy enabling incremental video-language alignment for\nvariable-length video streams, preserving temporal consistency across\ndynamically evolving frame sequences; (2) a response-silence decoding framework\nthat determines optimal proactive response timing via a single forward pass\nverification; (3) memory-aware acceleration via peak-end memory compression for\nonline inference on 10+ minute videos, combined with streaming key-value cache\nto achieve 1.53x faster inference. We also construct an OmniStar dataset, a\ncomprehensive dataset for training and benchmarking that encompasses 15 diverse\nreal-world scenarios and 5 evaluation tasks for online video understanding.\nExtensive experiments across three benchmarks demonstrate LiveStar's\nstate-of-the-art performance, achieving an average 19.5% improvement in\nsemantic correctness with 18.1% reduced timing difference compared to existing\nonline Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.\nOur model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar."
                },
                "authors": [
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Kairui Zhang"
                    },
                    {
                        "name": "Yuhang Hu"
                    },
                    {
                        "name": "Bing Wang"
                    },
                    {
                        "name": "Shengsheng Qian"
                    },
                    {
                        "name": "Bin Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Weiming Dong"
                    },
                    {
                        "name": "Changsheng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Changsheng Xu"
                },
                "author": "Changsheng Xu",
                "arxiv_comment": "NeurIPS 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05060v1",
                "updated": "2025-11-07T08:07:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    8,
                    7,
                    19,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T08:07:19Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    8,
                    7,
                    19,
                    4,
                    311,
                    0
                ],
                "title": "kV-Class Lateral NiOx/GaN Super-Heterojunction Diode via Ammonia\n  Molecular Beam Epitaxy (NH3-MBE)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "kV-Class Lateral NiOx/GaN Super-Heterojunction Diode via Ammonia\n  Molecular Beam Epitaxy (NH3-MBE)"
                },
                "summary": "This work reports the demonstration of lateral p-NiOx/p-GaN/n-GaN-based\nsuper-heterojunction (SHJ) diodes using p-GaN with additional sputtered p-type\nnickel oxide (NiOx) layers to realize charge-balanced structures. The\nheterojunction diode capacitance-voltage (C-V) model is applied to extract\neffective the acceptor concentration from the p-NiOx. Net donor and acceptor\nconcentration in n-GaN and p-GaN are extracted by using\nmetal-oxide-semiconductor (MOS) test structures. The fabricated\np-NiOx/p-GaN/n-GaN SHJ diodes with charge-balanced region between anode and\ncathode exhibit a forward on-state current density of 10-30 mA/mm across an\nanode-to-cathode distance (LAC) from 16 {\\mu}m to 80 {\\mu}m. The SHJ diodes\nshow rectifying behavior with a maximum on/off ratio of 10^9 and a low reverse\nleakage density. The highest breakdown voltage achieved for the SHJ diodes is\n~2.8 kV with reverse leakage density of 10^-4 mA/mm at ~80% of devices\ncatastrophic breakdown voltage. The SHJ diodes across all types of dimensions\nexhibit significant breakdown voltage improvements (~6X on average) with\nultra-low reverse leakage current compared to corresponding reference\nstructures without a charge-balanced extension, clearly demonstrating the\nsuperjunction effect for devices fabricated on GaN epitaxial layer with ~10^17\ncm^-3 electron density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the demonstration of lateral p-NiOx/p-GaN/n-GaN-based\nsuper-heterojunction (SHJ) diodes using p-GaN with additional sputtered p-type\nnickel oxide (NiOx) layers to realize charge-balanced structures. The\nheterojunction diode capacitance-voltage (C-V) model is applied to extract\neffective the acceptor concentration from the p-NiOx. Net donor and acceptor\nconcentration in n-GaN and p-GaN are extracted by using\nmetal-oxide-semiconductor (MOS) test structures. The fabricated\np-NiOx/p-GaN/n-GaN SHJ diodes with charge-balanced region between anode and\ncathode exhibit a forward on-state current density of 10-30 mA/mm across an\nanode-to-cathode distance (LAC) from 16 {\\mu}m to 80 {\\mu}m. The SHJ diodes\nshow rectifying behavior with a maximum on/off ratio of 10^9 and a low reverse\nleakage density. The highest breakdown voltage achieved for the SHJ diodes is\n~2.8 kV with reverse leakage density of 10^-4 mA/mm at ~80% of devices\ncatastrophic breakdown voltage. The SHJ diodes across all types of dimensions\nexhibit significant breakdown voltage improvements (~6X on average) with\nultra-low reverse leakage current compared to corresponding reference\nstructures without a charge-balanced extension, clearly demonstrating the\nsuperjunction effect for devices fabricated on GaN epitaxial layer with ~10^17\ncm^-3 electron density."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Zachary J. Biegler"
                    },
                    {
                        "name": "Ashley E. Wissel-Garcia"
                    },
                    {
                        "name": "James S. Speck"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05022v1",
                "updated": "2025-11-07T06:53:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    53,
                    48,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T06:53:48Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    53,
                    48,
                    4,
                    311,
                    0
                ],
                "title": "AWARE: Evaluating PriorityFresh Caching for Offline Emergency Warning\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AWARE: Evaluating PriorityFresh Caching for Offline Emergency Warning\n  Systems"
                },
                "summary": "PriorityFresh is a semantic, actionability-first caching policy designed for\noffline emergency warning systems. Within the AWARE system's simulation\nenvironment, PriorityFresh optimizes which alerts to retain and surface under\nconstrained connectivity. Experiments indicate improved actionability-first\nperformance without harming efficiency. A separate Priority Forecasting model\nis used only to synthesize realistic alert sequences for controlled experiments\nand does not influence caching or push decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PriorityFresh is a semantic, actionability-first caching policy designed for\noffline emergency warning systems. Within the AWARE system's simulation\nenvironment, PriorityFresh optimizes which alerts to retain and surface under\nconstrained connectivity. Experiments indicate improved actionability-first\nperformance without harming efficiency. A separate Priority Forecasting model\nis used only to synthesize realistic alert sequences for controlled experiments\nand does not influence caching or push decisions."
                },
                "authors": [
                    {
                        "name": "Charles Melvin"
                    },
                    {
                        "name": "N. Rich Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "N. Rich Nguyen"
                },
                "author": "N. Rich Nguyen",
                "arxiv_comment": "Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; H.3.4; H.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05017v1",
                "updated": "2025-11-07T06:39:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    39,
                    54,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T06:39:54Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    39,
                    54,
                    4,
                    311,
                    0
                ],
                "title": "Towards Mitigating Hallucinations in Large Vision-Language Models by\n  Refining Textual Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Mitigating Hallucinations in Large Vision-Language Models by\n  Refining Textual Embeddings"
                },
                "summary": "In this work, we identify an inherent bias in prevailing LVLM architectures\ntoward the language modality, largely resulting from the common practice of\nsimply appending visual embeddings to the input text sequence. To address this,\nwe propose a simple yet effective method that refines textual embeddings by\nintegrating average-pooled visual features. Our approach demonstrably improves\nvisual grounding and significantly reduces hallucinations on established\nbenchmarks. While average pooling offers a straightforward, robust, and\nefficient means of incorporating visual information, we believe that more\nsophisticated fusion methods could further enhance visual grounding and\ncross-modal alignment. Given that the primary focus of this work is to\nhighlight the modality imbalance and its impact on hallucinations -- and to\nshow that refining textual embeddings with visual information mitigates this\nissue -- we leave exploration of advanced fusion strategies for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we identify an inherent bias in prevailing LVLM architectures\ntoward the language modality, largely resulting from the common practice of\nsimply appending visual embeddings to the input text sequence. To address this,\nwe propose a simple yet effective method that refines textual embeddings by\nintegrating average-pooled visual features. Our approach demonstrably improves\nvisual grounding and significantly reduces hallucinations on established\nbenchmarks. While average pooling offers a straightforward, robust, and\nefficient means of incorporating visual information, we believe that more\nsophisticated fusion methods could further enhance visual grounding and\ncross-modal alignment. Given that the primary focus of this work is to\nhighlight the modality imbalance and its impact on hallucinations -- and to\nshow that refining textual embeddings with visual information mitigates this\nissue -- we leave exploration of advanced fusion strategies for future work."
                },
                "authors": [
                    {
                        "name": "Aakriti Agrawal"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Rohith Aralikatti"
                    },
                    {
                        "name": "Gauri Jagatap"
                    },
                    {
                        "name": "Jiaxin Yuan"
                    },
                    {
                        "name": "Vijay Kamarshi"
                    },
                    {
                        "name": "Andrea Fanelli"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24095v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24095v2",
                "updated": "2025-11-06T21:05:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    21,
                    5,
                    23,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-30T00:46:18Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    46,
                    18,
                    4,
                    150,
                    0
                ],
                "title": "SkyWalker: A Locality-Aware Cross-Region Load Balancer for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyWalker: A Locality-Aware Cross-Region Load Balancer for LLM Inference"
                },
                "summary": "Serving Large Language Models (LLMs) efficiently in multi-region setups\nremains a challenge. Due to cost and GPU availability concerns, providers\ntypically deploy LLMs in multiple regions using instance with long-term\ncommitments, like reserved instances or on-premise clusters, which are often\nunderutilized due to their region-local traffic handling and diurnal traffic\nvariance. In this paper, we introduce SkyWalker, a multi-region load balancer\nfor LLM inference that aggregates regional diurnal patterns through\ncross-region traffic handling. By doing so, SkyWalker enables providers to\nreserve instances based on expected global demand, rather than peak demand in\neach individual region. Meanwhile, SkyWalker preserves KV-Cache locality and\nload balancing, ensuring cost efficiency without sacrificing performance.\nSkyWalker achieves this with a cache-aware cross-region traffic handler and a\nselective pushing based load balancing mechanism. Our evaluation on real-world\nworkloads shows that it achieves 1.12-2.06x higher throughput and 1.74-6.30x\nlower latency compared to existing load balancers, while reducing total serving\ncost by 25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) efficiently in multi-region setups\nremains a challenge. Due to cost and GPU availability concerns, providers\ntypically deploy LLMs in multiple regions using instance with long-term\ncommitments, like reserved instances or on-premise clusters, which are often\nunderutilized due to their region-local traffic handling and diurnal traffic\nvariance. In this paper, we introduce SkyWalker, a multi-region load balancer\nfor LLM inference that aggregates regional diurnal patterns through\ncross-region traffic handling. By doing so, SkyWalker enables providers to\nreserve instances based on expected global demand, rather than peak demand in\neach individual region. Meanwhile, SkyWalker preserves KV-Cache locality and\nload balancing, ensuring cost efficiency without sacrificing performance.\nSkyWalker achieves this with a cache-aware cross-region traffic handler and a\nselective pushing based load balancing mechanism. Our evaluation on real-world\nworkloads shows that it achieves 1.12-2.06x higher throughput and 1.74-6.30x\nlower latency compared to existing load balancers, while reducing total serving\ncost by 25%."
                },
                "authors": [
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Jamison Kerney"
                    },
                    {
                        "name": "Ethan J. Jackson"
                    },
                    {
                        "name": "Zhifei Li"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24095v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24095v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04804v1",
                "updated": "2025-11-06T20:49:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    20,
                    49,
                    13,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T20:49:13Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    20,
                    49,
                    13,
                    3,
                    310,
                    0
                ],
                "title": "Simplex-FEM Networks (SiFEN): Learning A Triangulated Function\n  Approximator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplex-FEM Networks (SiFEN): Learning A Triangulated Function\n  Approximator"
                },
                "summary": "We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial\npredictor that represents f: R^d -> R^k as a globally C^r finite-element field\non a learned simplicial mesh in an optionally warped input space. Each query\nactivates exactly one simplex and at most d+1 basis functions via barycentric\ncoordinates, yielding explicit locality, controllable smoothness, and\ncache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with\na light invertible warp and trains end-to-end with shape regularization,\nsemi-discrete OT coverage, and differentiable edge flips. Under standard\nshape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic\nFEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic\napproximation tasks, tabular regression/classification, and as a drop-in head\non compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter\nbudgets, improves calibration (lower ECE/Brier), and reduces inference latency\ndue to geometric locality. These properties make SiFEN a compact,\ninterpretable, and theoretically grounded alternative to dense MLPs and\nedge-spline networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial\npredictor that represents f: R^d -> R^k as a globally C^r finite-element field\non a learned simplicial mesh in an optionally warped input space. Each query\nactivates exactly one simplex and at most d+1 basis functions via barycentric\ncoordinates, yielding explicit locality, controllable smoothness, and\ncache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with\na light invertible warp and trains end-to-end with shape regularization,\nsemi-discrete OT coverage, and differentiable edge flips. Under standard\nshape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic\nFEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic\napproximation tasks, tabular regression/classification, and as a drop-in head\non compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter\nbudgets, improves calibration (lower ECE/Brier), and reduces inference latency\ndue to geometric locality. These properties make SiFEN a compact,\ninterpretable, and theoretically grounded alternative to dense MLPs and\nedge-spline networks."
                },
                "authors": [
                    {
                        "name": "Chaymae Yahyati"
                    },
                    {
                        "name": "Ismail Lamaakal"
                    },
                    {
                        "name": "Khalid El Makkaoui"
                    },
                    {
                        "name": "Ibrahim Ouahbi"
                    },
                    {
                        "name": "Yassine Maleh"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Maleh"
                },
                "author": "Yassine Maleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04791v1",
                "updated": "2025-11-06T20:18:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    20,
                    18,
                    34,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T20:18:34Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    20,
                    18,
                    34,
                    3,
                    310,
                    0
                ],
                "title": "DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive\n  GPU Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive\n  GPU Multiplexing"
                },
                "summary": "Modern LLM serving systems must sustain high throughput while meeting strict\nlatency SLOs across two distinct inference phases: compute-intensive prefill\nand memory-bound decode phases. Existing approaches either (1) aggregate both\nphases on shared GPUs, leading to interference between prefill and decode\nphases, which degrades time-between-tokens (TBT); or (2) disaggregate the two\nphases across GPUs, improving latency but wasting resources through duplicated\nmodels and KV cache transfers. We present DuetServe, a unified LLM serving\nframework that achieves disaggregation-level isolation within a single GPU.\nDuetServe operates in aggregated mode by default and dynamically activates\nSM-level GPU spatial multiplexing when TBT degradation is predicted. Its key\nidea is to decouple prefill and decode execution only when needed through\nfine-grained, adaptive SM partitioning that provides phase isolation only when\ncontention threatens latency service level objectives (SLOs). DuetServe\nintegrates (1) an attention-aware roofline model to forecast iteration latency,\n(2) a partitioning optimizer that selects the optimal SM split to maximize\nthroughput under TBT constraints, and (3) an interruption-free execution engine\nthat eliminates CPU-GPU synchronization overhead. Evaluations show that\nDuetServe improves total throughput by up to 1.3x while maintaining low\ngeneration latency compared to state-of-the-art frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM serving systems must sustain high throughput while meeting strict\nlatency SLOs across two distinct inference phases: compute-intensive prefill\nand memory-bound decode phases. Existing approaches either (1) aggregate both\nphases on shared GPUs, leading to interference between prefill and decode\nphases, which degrades time-between-tokens (TBT); or (2) disaggregate the two\nphases across GPUs, improving latency but wasting resources through duplicated\nmodels and KV cache transfers. We present DuetServe, a unified LLM serving\nframework that achieves disaggregation-level isolation within a single GPU.\nDuetServe operates in aggregated mode by default and dynamically activates\nSM-level GPU spatial multiplexing when TBT degradation is predicted. Its key\nidea is to decouple prefill and decode execution only when needed through\nfine-grained, adaptive SM partitioning that provides phase isolation only when\ncontention threatens latency service level objectives (SLOs). DuetServe\nintegrates (1) an attention-aware roofline model to forecast iteration latency,\n(2) a partitioning optimizer that selects the optimal SM split to maximize\nthroughput under TBT constraints, and (3) an interruption-free execution engine\nthat eliminates CPU-GPU synchronization overhead. Evaluations show that\nDuetServe improves total throughput by up to 1.3x while maintaining low\ngeneration latency compared to state-of-the-art frameworks."
                },
                "authors": [
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03092v2",
                "updated": "2025-11-06T18:27:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    18,
                    27,
                    11,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-05T00:38:31Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    0,
                    38,
                    31,
                    2,
                    309,
                    0
                ],
                "title": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators"
                },
                "summary": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+\ncontext length support have resulted in increasing demands for on-chip memory\nto support large KV caches. Techniques such as StreamingLLM and SnapKV\ndemonstrate how to control KV cache size while maintaining model accuracy. Yet,\nthese techniques are not commonly used within industrial deployments using\nframeworks like vLLM or SGLang. The reason is twofold: on one hand, the static\ngraphs and continuous batching methodology employed by these frameworks make it\ndifficult to admit modifications to the standard multi-head attention\nalgorithm, while on the other hand, the accuracy implications of such\ntechniques on modern instruction-following and reasoning models are not well\nunderstood, obfuscating the need for implementing these techniques. In this\npaper, we explore these accuracy implications on Llama-3.1-8B-Instruct and\nDeepSeek-R1, and develop SnapStream, a KV cache compression method that can be\ndeployed at scale. We demonstrate the efficacy of SnapStream in a 16-way\ntensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators\nrunning at 128k context length and up to 1832 tokens per second in a real\nproduction setting. SnapStream enables $4\\times$ improved on-chip memory usage\nand introduces minimal accuracy degradation on LongBench-v2, AIME24 and\nLiveCodeBench. To the best of our knowledge, this is the first implementation\nof sparse KV attention techniques deployed in a production inference system\nwith static graphs and continuous batching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+\ncontext length support have resulted in increasing demands for on-chip memory\nto support large KV caches. Techniques such as StreamingLLM and SnapKV\ndemonstrate how to control KV cache size while maintaining model accuracy. Yet,\nthese techniques are not commonly used within industrial deployments using\nframeworks like vLLM or SGLang. The reason is twofold: on one hand, the static\ngraphs and continuous batching methodology employed by these frameworks make it\ndifficult to admit modifications to the standard multi-head attention\nalgorithm, while on the other hand, the accuracy implications of such\ntechniques on modern instruction-following and reasoning models are not well\nunderstood, obfuscating the need for implementing these techniques. In this\npaper, we explore these accuracy implications on Llama-3.1-8B-Instruct and\nDeepSeek-R1, and develop SnapStream, a KV cache compression method that can be\ndeployed at scale. We demonstrate the efficacy of SnapStream in a 16-way\ntensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators\nrunning at 128k context length and up to 1832 tokens per second in a real\nproduction setting. SnapStream enables $4\\times$ improved on-chip memory usage\nand introduces minimal accuracy degradation on LongBench-v2, AIME24 and\nLiveCodeBench. To the best of our knowledge, this is the first implementation\nof sparse KV attention techniques deployed in a production inference system\nwith static graphs and continuous batching."
                },
                "authors": [
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Nasim Farahini"
                    },
                    {
                        "name": "Evgenii Iuliugin"
                    },
                    {
                        "name": "Magnus Vesterlund"
                    },
                    {
                        "name": "Christian Haggstrom"
                    },
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Ayush Sachdeva"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Faline Fu"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Ayesha Siddiqua"
                    },
                    {
                        "name": "John Long"
                    },
                    {
                        "name": "Tuowen Zhao"
                    },
                    {
                        "name": "Matheen Musaddiq"
                    },
                    {
                        "name": "Hakan Zeffer"
                    },
                    {
                        "name": "Yun Du"
                    },
                    {
                        "name": "Mingran Wang"
                    },
                    {
                        "name": "Qinghua Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    },
                    {
                        "name": "Raghu Prabhakar"
                    }
                ],
                "author_detail": {
                    "name": "Raghu Prabhakar"
                },
                "author": "Raghu Prabhakar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05410v2",
                "updated": "2025-11-06T17:09:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    9,
                    52,
                    3,
                    310,
                    0
                ],
                "published": "2025-06-04T16:10:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    10,
                    44,
                    2,
                    155,
                    0
                ],
                "title": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache\n  Asymmetry for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache\n  Asymmetry for Long-Context LLMs"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights ({\\it local homogeneity}), adjacent values\ndemonstrate distinct {\\it heterogeneous} distributions. This key-value\nasymmetry reveals a critical limitation in existing compression methods that\ntreat keys and values uniformly. To address the limitation, we propose a\ntraining-free compression framework (AsymKV) that combines homogeneity-based\nkey merging with a mathematically proven lossless value compression. Extensive\nexperiments demonstrate that AsymKV consistently outperforms existing\nlong-context methods across various tasks and base models. For example, on\nLLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing\nSOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in\nthis link:https://github.com/the-scale-lab/Asymkv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights ({\\it local homogeneity}), adjacent values\ndemonstrate distinct {\\it heterogeneous} distributions. This key-value\nasymmetry reveals a critical limitation in existing compression methods that\ntreat keys and values uniformly. To address the limitation, we propose a\ntraining-free compression framework (AsymKV) that combines homogeneity-based\nkey merging with a mathematically proven lossless value compression. Extensive\nexperiments demonstrate that AsymKV consistently outperforms existing\nlong-context methods across various tasks and base models. For example, on\nLLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing\nSOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in\nthis link:https://github.com/the-scale-lab/Asymkv."
                },
                "authors": [
                    {
                        "name": "Wanyun Cui"
                    },
                    {
                        "name": "Mingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mingwei Xu"
                },
                "author": "Mingwei Xu",
                "arxiv_comment": "14 pages,7 figures;Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04489v1",
                "updated": "2025-11-06T16:08:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    8,
                    24,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T16:08:24Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    8,
                    24,
                    3,
                    310,
                    0
                ],
                "title": "Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear\n  Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear\n  Fusion"
                },
                "summary": "EIRENE [1] is a Monte Carlo neutral transport solver heavily used in the\nfusion community. EIRENE does not implement domain decomposition, making it\nimpossible to use for simulations where the grid data does not fit on one\ncompute node (see e.g. [2]). This paper presents a domain-decomposed Monte\nCarlo (DDMC) algorithm implemented in a new open source Monte Carlo code,\nEiron. Two parallel algorithms currently used in EIRENE are also implemented in\nEiron, and the three algorithms are compared by running strong scaling tests,\nwith DDMC performing better than the other two algorithms in nearly all cases.\nOn the supercomputer Mahti [3], DDMC strong scaling is superlinear for grids\nthat do not fit into an L3 cache slice (4 MiB). The DDMC algorithm is also\nscaled up to 16384 cores in weak scaling tests, with a weak scaling efficiency\nof 45% in a high-collisional (heavier compute load) case, and 26% in a\nlow-collisional (lighter compute load) case. We conclude that implementing this\ndomain decomposition algorithm in EIRENE would improve performance and enable\nsimulations that are currently impossible due to memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EIRENE [1] is a Monte Carlo neutral transport solver heavily used in the\nfusion community. EIRENE does not implement domain decomposition, making it\nimpossible to use for simulations where the grid data does not fit on one\ncompute node (see e.g. [2]). This paper presents a domain-decomposed Monte\nCarlo (DDMC) algorithm implemented in a new open source Monte Carlo code,\nEiron. Two parallel algorithms currently used in EIRENE are also implemented in\nEiron, and the three algorithms are compared by running strong scaling tests,\nwith DDMC performing better than the other two algorithms in nearly all cases.\nOn the supercomputer Mahti [3], DDMC strong scaling is superlinear for grids\nthat do not fit into an L3 cache slice (4 MiB). The DDMC algorithm is also\nscaled up to 16384 cores in weak scaling tests, with a weak scaling efficiency\nof 45% in a high-collisional (heavier compute load) case, and 26% in a\nlow-collisional (lighter compute load) case. We conclude that implementing this\ndomain decomposition algorithm in EIRENE would improve performance and enable\nsimulations that are currently impossible due to memory constraints."
                },
                "authors": [
                    {
                        "name": "Oskar Lappi"
                    },
                    {
                        "name": "Huw Leggate"
                    },
                    {
                        "name": "Yannick Marandet"
                    },
                    {
                        "name": "Jan Åström"
                    },
                    {
                        "name": "Keijo Heljanko"
                    },
                    {
                        "name": "Dmitriy V. Borodin"
                    }
                ],
                "author_detail": {
                    "name": "Dmitriy V. Borodin"
                },
                "author": "Dmitriy V. Borodin",
                "arxiv_comment": "19 pages, 3 figures, submitted to Journal of Computational Physics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68W10 (Primary), 68W15, 65C05 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.3; G.3; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04464v1",
                "updated": "2025-11-06T15:37:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    37,
                    11,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T15:37:11Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    37,
                    11,
                    3,
                    310,
                    0
                ],
                "title": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context"
                },
                "summary": "Traditional vehicle routing systems efficiently optimize singular metrics\nlike time or distance, and when considering multiple metrics, they need more\nprocesses to optimize . However, they lack the capability to interpret and\nintegrate the complex, semantic, and dynamic contexts of human drivers, such as\nmulti-step tasks, situational constraints, or urgent needs. This paper\nintroduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a\nhybrid agentic assistant designed to augment classical pathfinding algorithms\nwith contextual reasoning. Our approach employs a Large Language Model (LLM)\nagent that operates on a candidate set of routes generated by a multi-objective\n(time, CO2) Dijkstra algorithm. The agent evaluates these options against\nuser-provided tasks, preferences, and avoidance rules by leveraging a\npre-processed geospatial cache of urban Points of Interest (POIs). In a\nbenchmark of realistic urban scenarios, PAVe successfully used complex user\nintent into appropriate route modifications, achieving over 88% accuracy in its\ninitial route selections with a local model. We conclude that combining\nclassical routing algorithms with an LLM-based semantic reasoning layer is a\nrobust and effective approach for creating personalized, adaptive, and scalable\nsolutions for urban mobility optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional vehicle routing systems efficiently optimize singular metrics\nlike time or distance, and when considering multiple metrics, they need more\nprocesses to optimize . However, they lack the capability to interpret and\nintegrate the complex, semantic, and dynamic contexts of human drivers, such as\nmulti-step tasks, situational constraints, or urgent needs. This paper\nintroduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a\nhybrid agentic assistant designed to augment classical pathfinding algorithms\nwith contextual reasoning. Our approach employs a Large Language Model (LLM)\nagent that operates on a candidate set of routes generated by a multi-objective\n(time, CO2) Dijkstra algorithm. The agent evaluates these options against\nuser-provided tasks, preferences, and avoidance rules by leveraging a\npre-processed geospatial cache of urban Points of Interest (POIs). In a\nbenchmark of realistic urban scenarios, PAVe successfully used complex user\nintent into appropriate route modifications, achieving over 88% accuracy in its\ninitial route selections with a local model. We conclude that combining\nclassical routing algorithms with an LLM-based semantic reasoning layer is a\nrobust and effective approach for creating personalized, adaptive, and scalable\nsolutions for urban mobility optimization."
                },
                "authors": [
                    {
                        "name": "Carnot Braun"
                    },
                    {
                        "name": "Rafael O. Jarczewski"
                    },
                    {
                        "name": "Gabriel U. Talasso"
                    },
                    {
                        "name": "Leandro A. Villas"
                    },
                    {
                        "name": "Allan M. de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Allan M. de Souza"
                },
                "author": "Allan M. de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04421v1",
                "updated": "2025-11-06T14:52:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    52,
                    54,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T14:52:54Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    52,
                    54,
                    3,
                    310,
                    0
                ],
                "title": "Temporal Action Selection for Action Chunking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Action Selection for Action Chunking"
                },
                "summary": "Action chunking is a widely adopted approach in Learning from Demonstration\n(LfD). By modeling multi-step action chunks rather than single-step actions,\naction chunking significantly enhances modeling capabilities for human expert\npolicies. However, the reduced decision frequency restricts the utilization of\nrecent observations, degrading reactivity - particularly evident in the\ninadequate adaptation to sensor noise and dynamic environmental changes.\nExisting efforts to address this issue have primarily resorted to trading off\nreactivity against decision consistency, without achieving both. To address\nthis limitation, we propose a novel algorithm, Temporal Action Selector (TAS),\nwhich caches predicted action chunks from multiple timesteps and dynamically\nselects the optimal action through a lightweight selector network. TAS achieves\nbalanced optimization across three critical dimensions: reactivity, decision\nconsistency, and motion coherence. Experiments across multiple tasks with\ndiverse base policies show that TAS significantly improves success rates -\nyielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a\nbase policy with residual reinforcement learning (RL) substantially enhances\ntraining efficiency and elevates the performance plateau. Experiments in both\nsimulation and physical robots confirm the method's efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action chunking is a widely adopted approach in Learning from Demonstration\n(LfD). By modeling multi-step action chunks rather than single-step actions,\naction chunking significantly enhances modeling capabilities for human expert\npolicies. However, the reduced decision frequency restricts the utilization of\nrecent observations, degrading reactivity - particularly evident in the\ninadequate adaptation to sensor noise and dynamic environmental changes.\nExisting efforts to address this issue have primarily resorted to trading off\nreactivity against decision consistency, without achieving both. To address\nthis limitation, we propose a novel algorithm, Temporal Action Selector (TAS),\nwhich caches predicted action chunks from multiple timesteps and dynamically\nselects the optimal action through a lightweight selector network. TAS achieves\nbalanced optimization across three critical dimensions: reactivity, decision\nconsistency, and motion coherence. Experiments across multiple tasks with\ndiverse base policies show that TAS significantly improves success rates -\nyielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a\nbase policy with residual reinforcement learning (RL) substantially enhances\ntraining efficiency and elevates the performance plateau. Experiments in both\nsimulation and physical robots confirm the method's efficacy."
                },
                "authors": [
                    {
                        "name": "Yueyang Weng"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    },
                    {
                        "name": "Yongjin Mu"
                    },
                    {
                        "name": "Yingcong Zhu"
                    },
                    {
                        "name": "Yanjie Li"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04406v1",
                "updated": "2025-11-06T14:33:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    33,
                    29,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T14:33:29Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    33,
                    29,
                    3,
                    310,
                    0
                ],
                "title": "Dynamic Jointly Batch Selection for Data Efficient Machine Translation\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Jointly Batch Selection for Data Efficient Machine Translation\n  Fine-Tuning"
                },
                "summary": "Data quality and its effective selection are fundamental to improving the\nperformance of machine translation models, serving as cornerstones for\nachieving robust and reliable translation systems. This paper presents a data\nselection methodology specifically designed for fine-tuning machine translation\nsystems, which leverages the synergy between a learner model and a pre-trained\nreference model to enhance overall training effectiveness. By defining a\nlearnability score, our approach systematically evaluates the utility of data\npoints for training, ensuring that only the most relevant and impactful\nexamples contribute to the fine-tuning process. Furthermore, our method employs\na batch selection strategy which considers interdependencies among data points,\noptimizing the efficiency of the training process while maintaining a focus on\ndata relevance. Experiments on English to Persian and several other language\npairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that\nour method can achieve up to a fivefold improvement in data efficiency compared\nto an iid baseline. Experimental results indicate that our approach improves\ncomputational efficiency by 24 when utilizing cached embeddings, as it requires\nfewer training data points. Additionally, it enhances generalization, resulting\nin superior translation performance compared to random selection method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data quality and its effective selection are fundamental to improving the\nperformance of machine translation models, serving as cornerstones for\nachieving robust and reliable translation systems. This paper presents a data\nselection methodology specifically designed for fine-tuning machine translation\nsystems, which leverages the synergy between a learner model and a pre-trained\nreference model to enhance overall training effectiveness. By defining a\nlearnability score, our approach systematically evaluates the utility of data\npoints for training, ensuring that only the most relevant and impactful\nexamples contribute to the fine-tuning process. Furthermore, our method employs\na batch selection strategy which considers interdependencies among data points,\noptimizing the efficiency of the training process while maintaining a focus on\ndata relevance. Experiments on English to Persian and several other language\npairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that\nour method can achieve up to a fivefold improvement in data efficiency compared\nto an iid baseline. Experimental results indicate that our approach improves\ncomputational efficiency by 24 when utilizing cached embeddings, as it requires\nfewer training data points. Additionally, it enhances generalization, resulting\nin superior translation performance compared to random selection method."
                },
                "authors": [
                    {
                        "name": "Mohammad Amin Ghanizadeh"
                    },
                    {
                        "name": "Mohammad Javad Dousti"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Javad Dousti"
                },
                "author": "Mohammad Javad Dousti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01110v3",
                "updated": "2025-11-06T13:44:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    44,
                    12,
                    3,
                    310,
                    0
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory"
                },
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Thomas Köhler"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01110v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01110v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04002v1",
                "updated": "2025-11-06T02:55:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    2,
                    55,
                    7,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T02:55:07Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    2,
                    55,
                    7,
                    3,
                    310,
                    0
                ],
                "title": "Memory- and Latency-Constrained Inference of Large Language Models via\n  Adaptive Split Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and Latency-Constrained Inference of Large Language Models via\n  Adaptive Split Computing"
                },
                "summary": "Large language models (LLMs) have achieved near-human performance across\ndiverse reasoning tasks, yet their deployment on resource-constrained\nInternet-of-Things (IoT) devices remains impractical due to massive parameter\nfootprints and memory-intensive autoregressive decoding. While split computing\noffers a promising solution by partitioning model execution between edge\ndevices and cloud servers, existing approaches fail to address the unique\nchallenges of autoregressive inference, particularly the iterative token\ngeneration process and expanding key-value (KV) cache requirements. This work\nintroduces the first autoregressive-aware split computing framework designed\nexplicitly for LLM deployment on edge devices. Our approach makes three key\ncontributions. First, we develop one-point split compression (OPSC), a\nmixed-precision quantization scheme that prevents out-of-memory failures by\nstrategically partitioning models into front-end and back-end segments with\ndifferent precision levels. Second, we propose a two-stage intermediate\ncompression pipeline that combines threshold splitting (TS) and token-wise\nadaptive bit quantization (TAB-Q) to preserve accuracy-critical activations\nwhile dramatically reducing communication overhead. Third, we formulate a\nunified optimization framework that jointly selects optimal split points,\nquantization settings, and sequence lengths to satisfy strict memory and\nlatency constraints. Extensive evaluations across diverse LLMs and hardware\nplatforms demonstrate superior performance compared to state-of-the-art\nquantization methods, including SmoothQuant, OmniQuant, and Atom. The framework\nachieves a 1.49 inference speedup and significant communication overhead\nreduction while maintaining or improving model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved near-human performance across\ndiverse reasoning tasks, yet their deployment on resource-constrained\nInternet-of-Things (IoT) devices remains impractical due to massive parameter\nfootprints and memory-intensive autoregressive decoding. While split computing\noffers a promising solution by partitioning model execution between edge\ndevices and cloud servers, existing approaches fail to address the unique\nchallenges of autoregressive inference, particularly the iterative token\ngeneration process and expanding key-value (KV) cache requirements. This work\nintroduces the first autoregressive-aware split computing framework designed\nexplicitly for LLM deployment on edge devices. Our approach makes three key\ncontributions. First, we develop one-point split compression (OPSC), a\nmixed-precision quantization scheme that prevents out-of-memory failures by\nstrategically partitioning models into front-end and back-end segments with\ndifferent precision levels. Second, we propose a two-stage intermediate\ncompression pipeline that combines threshold splitting (TS) and token-wise\nadaptive bit quantization (TAB-Q) to preserve accuracy-critical activations\nwhile dramatically reducing communication overhead. Third, we formulate a\nunified optimization framework that jointly selects optimal split points,\nquantization settings, and sequence lengths to satisfy strict memory and\nlatency constraints. Extensive evaluations across diverse LLMs and hardware\nplatforms demonstrate superior performance compared to state-of-the-art\nquantization methods, including SmoothQuant, OmniQuant, and Atom. The framework\nachieves a 1.49 inference speedup and significant communication overhead\nreduction while maintaining or improving model accuracy."
                },
                "authors": [
                    {
                        "name": "Mingyu Sung"
                    },
                    {
                        "name": "Vikas Palakonda"
                    },
                    {
                        "name": "Suhwan Im"
                    },
                    {
                        "name": "Sunghwan Moon"
                    },
                    {
                        "name": "Il-Min Kim"
                    },
                    {
                        "name": "Sangseok Yun"
                    },
                    {
                        "name": "Jae-Mo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Mo Kang"
                },
                "author": "Jae-Mo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24722v2",
                "updated": "2025-11-06T01:18:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    1,
                    18,
                    3,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-30T15:42:42Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts"
                },
                "summary": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining."
                },
                "authors": [
                    {
                        "name": "Neil He"
                    },
                    {
                        "name": "Rishabh Anand"
                    },
                    {
                        "name": "Hiren Madhu"
                    },
                    {
                        "name": "Ali Maatouk"
                    },
                    {
                        "name": "Smita Krishnaswamy"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03944v1",
                "updated": "2025-11-06T00:42:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    0,
                    42,
                    29,
                    3,
                    310,
                    0
                ],
                "published": "2025-11-06T00:42:29Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    0,
                    42,
                    29,
                    3,
                    310,
                    0
                ],
                "title": "From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era\n  Memory Hierarchies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era\n  Memory Hierarchies"
                },
                "summary": "In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a\nsimple, storage-memory-economics-based heuristic for deciding when data should\nlive in DRAM rather than on storage. Subsequent revisits to the rule largely\nretained that economics-only view, leaving host costs, feasibility limits, and\nworkload behavior out of scope. This paper revisits the rule from first\nprinciples, integrating host costs, DRAM bandwidth/capacity, and\nphysics-grounded models of SSD performance and cost, and then embedding these\nelements in a constraint- and workload-aware framework that yields actionable\nprovisioning guidance. We show that, for modern AI platforms, especially\nGPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained\nrandom access, the DRAM-to-flash caching threshold collapses from minutes to a\nfew seconds. This shift reframes NAND flash memory as an active data tier and\nexposes a broad research space across the hardware-software stack. We further\nintroduce MQSim-Next, a calibrated SSD simulator that supports validation and\nsensitivity analysis and facilitates future architectural and system research.\nFinally, we present two concrete case studies that showcase the software system\ndesign space opened by such memory hierarchy paradigm shift. Overall, we turn a\nclassical heuristic into an actionable, feasibility-aware analysis and\nprovisioning framework and set the stage for further research on AI-era memory\nhierarchy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a\nsimple, storage-memory-economics-based heuristic for deciding when data should\nlive in DRAM rather than on storage. Subsequent revisits to the rule largely\nretained that economics-only view, leaving host costs, feasibility limits, and\nworkload behavior out of scope. This paper revisits the rule from first\nprinciples, integrating host costs, DRAM bandwidth/capacity, and\nphysics-grounded models of SSD performance and cost, and then embedding these\nelements in a constraint- and workload-aware framework that yields actionable\nprovisioning guidance. We show that, for modern AI platforms, especially\nGPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained\nrandom access, the DRAM-to-flash caching threshold collapses from minutes to a\nfew seconds. This shift reframes NAND flash memory as an active data tier and\nexposes a broad research space across the hardware-software stack. We further\nintroduce MQSim-Next, a calibrated SSD simulator that supports validation and\nsensitivity analysis and facilitates future architectural and system research.\nFinally, we present two concrete case studies that showcase the software system\ndesign space opened by such memory hierarchy paradigm shift. Overall, we turn a\nclassical heuristic into an actionable, feasibility-aware analysis and\nprovisioning framework and set the stage for further research on AI-era memory\nhierarchy."
                },
                "authors": [
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Chris J. Newburn"
                    },
                    {
                        "name": "Teresa Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jiangpeng Li"
                    },
                    {
                        "name": "Hao Zhong"
                    },
                    {
                        "name": "Wen-Mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Mei Hwu"
                },
                "author": "Wen-Mei Hwu",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22913v2",
                "updated": "2025-11-06T00:11:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    0,
                    11,
                    18,
                    3,
                    310,
                    0
                ],
                "published": "2025-05-28T22:32:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM\n  Inference"
                },
                "summary": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar."
                },
                "authors": [
                    {
                        "name": "Donghyeon Joo"
                    },
                    {
                        "name": "Helya Hosseini"
                    },
                    {
                        "name": "Ramyad Hadidi"
                    },
                    {
                        "name": "Bahar Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Bahar Asgari"
                },
                "author": "Bahar Asgari",
                "arxiv_comment": "20 pages, 9 figures, NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03830v1",
                "updated": "2025-11-05T19:53:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    19,
                    53,
                    51,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T19:53:51Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    19,
                    53,
                    51,
                    2,
                    309,
                    0
                ],
                "title": "Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label\n  LLM-Based Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label\n  LLM-Based Classification"
                },
                "summary": "We introduce a method for efficient multi-label text classification with\nlarge language models (LLMs), built on reformulating classification tasks as\nsequences of dichotomic (yes/no) decisions. Instead of generating all labels in\na single structured response, each target dimension is queried independently,\nwhich, combined with a prefix caching mechanism, yields substantial efficiency\ngains for short-text inference without loss of accuracy. To demonstrate the\napproach, we focus on affective text analysis, covering 24 dimensions including\nemotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator\nmodel (DeepSeek-V3) provides multiple annotations per text, which are\naggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,\nGemma3-1B). The fine-tuned models show significant improvements over zero-shot\nbaselines, particularly on the dimensions seen during training. Our findings\nsuggest that decomposing multi-label classification into dichotomic queries,\ncombined with distillation and cache-aware inference, offers a scalable and\neffective framework for LLM-based classification. While we validate the method\non affective states, the approach is general and applicable across domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for efficient multi-label text classification with\nlarge language models (LLMs), built on reformulating classification tasks as\nsequences of dichotomic (yes/no) decisions. Instead of generating all labels in\na single structured response, each target dimension is queried independently,\nwhich, combined with a prefix caching mechanism, yields substantial efficiency\ngains for short-text inference without loss of accuracy. To demonstrate the\napproach, we focus on affective text analysis, covering 24 dimensions including\nemotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator\nmodel (DeepSeek-V3) provides multiple annotations per text, which are\naggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,\nGemma3-1B). The fine-tuned models show significant improvements over zero-shot\nbaselines, particularly on the dimensions seen during training. Our findings\nsuggest that decomposing multi-label classification into dichotomic queries,\ncombined with distillation and cache-aware inference, offers a scalable and\neffective framework for LLM-based classification. While we validate the method\non affective states, the approach is general and applicable across domains."
                },
                "authors": [
                    {
                        "name": "Mikołaj Langner"
                    },
                    {
                        "name": "Jan Eliasz"
                    },
                    {
                        "name": "Ewa Rudnicka"
                    },
                    {
                        "name": "Jan Kocoń"
                    }
                ],
                "author_detail": {
                    "name": "Jan Kocoń"
                },
                "author": "Jan Kocoń",
                "arxiv_comment": "9 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v2",
                "updated": "2025-11-05T14:29:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    29,
                    12,
                    2,
                    309,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness. The code is available at\nhttps://github.com/OpenMOSS/Sparse-dLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness. The code is available at\nhttps://github.com/OpenMOSS/Sparse-dLLM."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03475v1",
                "updated": "2025-11-05T13:59:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T13:59:01Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "title": "RAGBoost: Efficient Retrieval-Augmented Generation with\n  Accuracy-Preserving Context Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGBoost: Efficient Retrieval-Augmented Generation with\n  Accuracy-Preserving Context Reuse"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith retrieved context but often suffers from downgraded prefill performance as\nmodern applications demand longer and more complex inputs. Existing caching\ntechniques either preserve accuracy with low cache reuse or improve reuse at\nthe cost of degraded reasoning quality. We present RAGBoost, an efficient RAG\nsystem that achieves high cache reuse without sacrificing accuracy through\naccuracy-preserving context reuse. RAGBoost detects overlapping retrieved items\nacross concurrent sessions and multi-turn interactions, using efficient context\nindexing, ordering, and de-duplication to maximize reuse, while lightweight\ncontextual hints maintain reasoning fidelity. It integrates seamlessly with\nexisting LLM inference engines and improves their prefill performance by 1.5-3X\nover state-of-the-art methods, while preserving or even enhancing reasoning\naccuracy across diverse RAG and agentic AI workloads. Our code is released at:\nhttps://github.com/Edinburgh-AgenticAI/RAGBoost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith retrieved context but often suffers from downgraded prefill performance as\nmodern applications demand longer and more complex inputs. Existing caching\ntechniques either preserve accuracy with low cache reuse or improve reuse at\nthe cost of degraded reasoning quality. We present RAGBoost, an efficient RAG\nsystem that achieves high cache reuse without sacrificing accuracy through\naccuracy-preserving context reuse. RAGBoost detects overlapping retrieved items\nacross concurrent sessions and multi-turn interactions, using efficient context\nindexing, ordering, and de-duplication to maximize reuse, while lightweight\ncontextual hints maintain reasoning fidelity. It integrates seamlessly with\nexisting LLM inference engines and improves their prefill performance by 1.5-3X\nover state-of-the-art methods, while preserving or even enhancing reasoning\naccuracy across diverse RAG and agentic AI workloads. Our code is released at:\nhttps://github.com/Edinburgh-AgenticAI/RAGBoost."
                },
                "authors": [
                    {
                        "name": "Yinsicheng Jiang"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Liang Cheng"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xuan Sun"
                    },
                    {
                        "name": "Luo Mai"
                    }
                ],
                "author_detail": {
                    "name": "Luo Mai"
                },
                "author": "Luo Mai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09045v2",
                "updated": "2025-11-05T09:18:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    18,
                    48,
                    2,
                    309,
                    0
                ],
                "published": "2025-06-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagCache: Fast Video Generation with Magnitude-Aware Cache"
                },
                "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically, steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.10x-2.68x\nspeedups on Open-Sora, CogVideoX, Wan 2.1, and HunyuanVideo, while preserving\nsuperior visual fidelity. It significantly outperforms existing methods in\nLPIPS, SSIM, and PSNR, under similar computational budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically, steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.10x-2.68x\nspeedups on Open-Sora, CogVideoX, Wan 2.1, and HunyuanVideo, while preserving\nsuperior visual fidelity. It significantly outperforms existing methods in\nLPIPS, SSIM, and PSNR, under similar computational budgets."
                },
                "authors": [
                    {
                        "name": "Zehong Ma"
                    },
                    {
                        "name": "Longhui Wei"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "arxiv_comment": "Project Page: https://zehong-ma.github.io/MagCache Accepted by\n  NeurIPS 2025",
                "arxiv_journal_ref": "In Proceedings of NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03159v1",
                "updated": "2025-11-05T03:40:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    3,
                    40,
                    44,
                    2,
                    309,
                    0
                ],
                "published": "2025-11-05T03:40:44Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    3,
                    40,
                    44,
                    2,
                    309,
                    0
                ],
                "title": "Joint Optimization of DNN Model Caching and Request Routing in Mobile\n  Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Optimization of DNN Model Caching and Request Routing in Mobile\n  Edge Computing"
                },
                "summary": "Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near\nend-users, providing low-latency services and improving users' quality of\nexperience (QoE). However, caching all DNN models at edge servers with limited\ncapacity is difficult, and the impact of model loading time on QoE remains\nunderexplored. Hence, we introduce dynamic DNNs in edge scenarios,\ndisassembling a complete DNN model into interrelated submodels for more\nfine-grained and flexible model caching and request routing solutions. This\nraises the pressing issue of jointly deciding request routing and submodel\ncaching for dynamic DNNs to balance model inference precision and loading\nlatency for QoE optimization. In this paper, we study the joint dynamic model\ncaching and request routing problem in MEC networks, aiming to maximize user\nrequest inference precision under constraints of server resources, latency, and\nmodel loading time. To tackle this problem, we propose CoCaR, an offline\nalgorithm based on linear programming and random rounding that leverages\ndynamic DNNs to optimize caching and routing schemes, achieving near-optimal\nperformance. Furthermore, we develop an online variant of CoCaR, named\nCoCaR-OL, enabling effective adaptation to dynamic and unpredictable online\nrequest patterns. The simulation results demonstrate that the proposed CoCaR\nimproves the average inference precision of user requests by 46\\% compared to\nstate-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves\nan improvement of no less than 32.3\\% in user QoE over competitive baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near\nend-users, providing low-latency services and improving users' quality of\nexperience (QoE). However, caching all DNN models at edge servers with limited\ncapacity is difficult, and the impact of model loading time on QoE remains\nunderexplored. Hence, we introduce dynamic DNNs in edge scenarios,\ndisassembling a complete DNN model into interrelated submodels for more\nfine-grained and flexible model caching and request routing solutions. This\nraises the pressing issue of jointly deciding request routing and submodel\ncaching for dynamic DNNs to balance model inference precision and loading\nlatency for QoE optimization. In this paper, we study the joint dynamic model\ncaching and request routing problem in MEC networks, aiming to maximize user\nrequest inference precision under constraints of server resources, latency, and\nmodel loading time. To tackle this problem, we propose CoCaR, an offline\nalgorithm based on linear programming and random rounding that leverages\ndynamic DNNs to optimize caching and routing schemes, achieving near-optimal\nperformance. Furthermore, we develop an online variant of CoCaR, named\nCoCaR-OL, enabling effective adaptation to dynamic and unpredictable online\nrequest patterns. The simulation results demonstrate that the proposed CoCaR\nimproves the average inference precision of user requests by 46\\% compared to\nstate-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves\nan improvement of no less than 32.3\\% in user QoE over competitive baselines."
                },
                "authors": [
                    {
                        "name": "Shuting Qiu"
                    },
                    {
                        "name": "Fang Dong"
                    },
                    {
                        "name": "Siyu Tan"
                    },
                    {
                        "name": "Ruiting Zhou"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Patrick P. C. Lee"
                    },
                    {
                        "name": "Qilin Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qilin Fan"
                },
                "author": "Qilin Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02919v1",
                "updated": "2025-11-04T19:02:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    19,
                    2,
                    29,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T19:02:29Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    19,
                    2,
                    29,
                    1,
                    308,
                    0
                ],
                "title": "Cache Mechanism for Agent RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Mechanism for Agent RAG Systems"
                },
                "summary": "Recent advances in Large Language Model (LLM)-based agents have been\npropelled by Retrieval-Augmented Generation (RAG), which grants the models\naccess to vast external knowledge bases. Despite RAG's success in improving\nagent performance, agent-level cache management, particularly constructing,\nmaintaining, and updating a compact, relevant corpus dynamically tailored to\neach agent's need, remains underexplored. Therefore, we introduce ARC (Agent\nRAG Cache Mechanism), a novel, annotation-free caching framework that\ndynamically manages small, high-value corpora for each agent. By synthesizing\nhistorical query distribution patterns with the intrinsic geometry of cached\nitems in the embedding space, ARC automatically maintains a high-relevance\ncache. With comprehensive experiments on three retrieval datasets, our\nexperimental results demonstrate that ARC reduces storage requirements to\n0.015% of the original corpus while offering up to 79.8% has-answer rate and\nreducing average retrieval latency by 80%. Our results demonstrate that ARC can\ndrastically enhance efficiency and effectiveness in RAG-powered LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Model (LLM)-based agents have been\npropelled by Retrieval-Augmented Generation (RAG), which grants the models\naccess to vast external knowledge bases. Despite RAG's success in improving\nagent performance, agent-level cache management, particularly constructing,\nmaintaining, and updating a compact, relevant corpus dynamically tailored to\neach agent's need, remains underexplored. Therefore, we introduce ARC (Agent\nRAG Cache Mechanism), a novel, annotation-free caching framework that\ndynamically manages small, high-value corpora for each agent. By synthesizing\nhistorical query distribution patterns with the intrinsic geometry of cached\nitems in the embedding space, ARC automatically maintains a high-relevance\ncache. With comprehensive experiments on three retrieval datasets, our\nexperimental results demonstrate that ARC reduces storage requirements to\n0.015% of the original corpus while offering up to 79.8% has-answer rate and\nreducing average retrieval latency by 80%. Our results demonstrate that ARC can\ndrastically enhance efficiency and effectiveness in RAG-powered LLM agents."
                },
                "authors": [
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Zhencan Peng"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Xiao Lin"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02761v1",
                "updated": "2025-11-04T17:40:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    40,
                    31,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:40:31Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    40,
                    31,
                    1,
                    308,
                    0
                ],
                "title": "Non-Contact Manipulation of Induced Magnetic Dipoles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Contact Manipulation of Induced Magnetic Dipoles"
                },
                "summary": "Extending the field of magnetic manipulation to conductive, non-magnetic\nobjects opens the door for a wide array of applications previously limited to\nhard or soft magnetic materials. Of particular interest is the recycling of\nspace debris through the use of oscillating magnetic fields, which represent a\ncache of raw materials in an environment particularly suited to the low forces\ngenerated from inductive magnetic manipulation. Building upon previous work\nthat demonstrated 3D open-loop position control by leveraging the opposing\ndipole moment created from induced eddy currents, this work demonstrates\nclosed-loop position control of a semi-buoyant aluminum sphere in lab tests,\nand the efficacy of varying methods for force inversion is explored. The\nclosed-loop methods represent a critical first step towards wider applications\nfor 3-DOF position control of induced magnetic dipoles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the field of magnetic manipulation to conductive, non-magnetic\nobjects opens the door for a wide array of applications previously limited to\nhard or soft magnetic materials. Of particular interest is the recycling of\nspace debris through the use of oscillating magnetic fields, which represent a\ncache of raw materials in an environment particularly suited to the low forces\ngenerated from inductive magnetic manipulation. Building upon previous work\nthat demonstrated 3D open-loop position control by leveraging the opposing\ndipole moment created from induced eddy currents, this work demonstrates\nclosed-loop position control of a semi-buoyant aluminum sphere in lab tests,\nand the efficacy of varying methods for force inversion is explored. The\nclosed-loop methods represent a critical first step towards wider applications\nfor 3-DOF position control of induced magnetic dipoles."
                },
                "authors": [
                    {
                        "name": "Seth Stewart"
                    },
                    {
                        "name": "Joseph Pawelski"
                    },
                    {
                        "name": "Steve Ward"
                    },
                    {
                        "name": "Andrew J. Petruska"
                    }
                ],
                "author_detail": {
                    "name": "Andrew J. Petruska"
                },
                "author": "Andrew J. Petruska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02749v1",
                "updated": "2025-11-04T17:22:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    49,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:22:49Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    49,
                    1,
                    308,
                    0
                ],
                "title": "Using Span Queries to Optimize for Cache and Attention Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Span Queries to Optimize for Cache and Attention Locality"
                },
                "summary": "Clients are evolving beyond chat completion, and now include a variety of\ninnovative inference-time scaling and deep reasoning techniques. At the same\ntime, inference servers remain heavily optimized for chat completion. Prior\nwork has shown that large improvements to KV cache hit rate are possible if\ninference servers evolve towards these non-chat use cases. However, they offer\nsolutions that are also optimized for a single use case, RAG. In this paper, we\nintroduce the span query to generalize the interface to the inference server.\nWe demonstrate that chat, RAG, inference-time scaling, and agentic workloads\ncan all be expressed as span queries. We show how the critical distinction that\nhad been assumed by prior work lies in whether the order of the inputs matter\n-- do they commute? In chat, they do not. In RAG, they often do. This paper\nintroduces span queries, which are expression trees of inference calls, linked\ntogether with commutativity constraints. We describe span query syntax and\nsemantics. We show how they can be automatically optimized to improve KV cache\nlocality. We show how a small change to vLLM (affecting only 492 lines) can\nenable high-performance execution of span queries. Using this stack, we\ndemonstrate that span queries can achieve 10-20x reductions in TTFT for two\ndistinct non-chat use cases. Finally, we show that span queries can also be\noptimized to improve attention locality, so as to avoid the so-called\nlost-in-the-middle problem. We demonstrate that an attention-optimized span\nquery on a 2b parameter model vastly outperforms the accuracy of a stock\ninference server using an 8b model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clients are evolving beyond chat completion, and now include a variety of\ninnovative inference-time scaling and deep reasoning techniques. At the same\ntime, inference servers remain heavily optimized for chat completion. Prior\nwork has shown that large improvements to KV cache hit rate are possible if\ninference servers evolve towards these non-chat use cases. However, they offer\nsolutions that are also optimized for a single use case, RAG. In this paper, we\nintroduce the span query to generalize the interface to the inference server.\nWe demonstrate that chat, RAG, inference-time scaling, and agentic workloads\ncan all be expressed as span queries. We show how the critical distinction that\nhad been assumed by prior work lies in whether the order of the inputs matter\n-- do they commute? In chat, they do not. In RAG, they often do. This paper\nintroduces span queries, which are expression trees of inference calls, linked\ntogether with commutativity constraints. We describe span query syntax and\nsemantics. We show how they can be automatically optimized to improve KV cache\nlocality. We show how a small change to vLLM (affecting only 492 lines) can\nenable high-performance execution of span queries. Using this stack, we\ndemonstrate that span queries can achieve 10-20x reductions in TTFT for two\ndistinct non-chat use cases. Finally, we show that span queries can also be\noptimized to improve attention locality, so as to avoid the so-called\nlost-in-the-middle problem. We demonstrate that an attention-optimized span\nquery on a 2b parameter model vastly outperforms the accuracy of a stock\ninference server using an 8b model."
                },
                "authors": [
                    {
                        "name": "Paul Castro"
                    },
                    {
                        "name": "Nick Mitchell"
                    },
                    {
                        "name": "Nathan Ordonez"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Mudhakar Srivatsa"
                    },
                    {
                        "name": "Antoni Viros i Martin"
                    }
                ],
                "author_detail": {
                    "name": "Antoni Viros i Martin"
                },
                "author": "Antoni Viros i Martin",
                "arxiv_comment": "12 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02651v1",
                "updated": "2025-11-04T15:17:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    43,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:17:43Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    43,
                    1,
                    308,
                    0
                ],
                "title": "Apriel-H1: Towards Efficient Enterprise Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apriel-H1: Towards Efficient Enterprise Reasoning Models"
                },
                "summary": "Large Language Models (LLMs) achieve remarkable reasoning capabilities\nthrough transformer architectures with attention mechanisms. However,\ntransformers suffer from quadratic time and memory complexity in the attention\nmodule (MHA) and require caching key-value states during inference, which\nseverely limits throughput and scalability. High inference throughput is\ncritical for agentic tasks, long-context reasoning, efficient deployment under\nhigh request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with\nlinear inference complexity and a constant memory footprint via recurrent\ncomputation with fixed-size hidden states. In this technical report we\nintroduce the Apriel-H1 family of hybrid LLMs that combine transformer\nattention and SSM sequence mixers for efficient reasoning at 15B model size.\nThese models are obtained through incremental distillation from a pretrained\nreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing\nless critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with\ndifferent SSM-to-MHA ratios and analyse how reasoning performance degrades as\nmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant\nof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,\nachieving over 2x higher inference throughput when deployed in the\nproduction-ready vLLM environment, with minimal degradation in reasoning\nperformance. This shows that distilled hybrid SSM-Transformer architectures can\ndeliver substantial efficiency gains over the pretrained transformer equivalent\nwithout substantially compromising the reasoning quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve remarkable reasoning capabilities\nthrough transformer architectures with attention mechanisms. However,\ntransformers suffer from quadratic time and memory complexity in the attention\nmodule (MHA) and require caching key-value states during inference, which\nseverely limits throughput and scalability. High inference throughput is\ncritical for agentic tasks, long-context reasoning, efficient deployment under\nhigh request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with\nlinear inference complexity and a constant memory footprint via recurrent\ncomputation with fixed-size hidden states. In this technical report we\nintroduce the Apriel-H1 family of hybrid LLMs that combine transformer\nattention and SSM sequence mixers for efficient reasoning at 15B model size.\nThese models are obtained through incremental distillation from a pretrained\nreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing\nless critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with\ndifferent SSM-to-MHA ratios and analyse how reasoning performance degrades as\nmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant\nof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,\nachieving over 2x higher inference throughput when deployed in the\nproduction-ready vLLM environment, with minimal degradation in reasoning\nperformance. This shows that distilled hybrid SSM-Transformer architectures can\ndeliver substantial efficiency gains over the pretrained transformer equivalent\nwithout substantially compromising the reasoning quality."
                },
                "authors": [
                    {
                        "name": "Oleksiy Ostapenko"
                    },
                    {
                        "name": "Luke Kumar"
                    },
                    {
                        "name": "Raymond Li"
                    },
                    {
                        "name": "Denis Kocetkov"
                    },
                    {
                        "name": "Joel Lamy-Poirier"
                    },
                    {
                        "name": "Shruthan Radhakrishna"
                    },
                    {
                        "name": "Soham Parikh"
                    },
                    {
                        "name": "Shambhavi Mishra"
                    },
                    {
                        "name": "Sebastien Paquet"
                    },
                    {
                        "name": "Srinivas Sunkara"
                    },
                    {
                        "name": "Valérie Bécaert"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Torsten Scholak"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Scholak"
                },
                "author": "Torsten Scholak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02647v1",
                "updated": "2025-11-04T15:14:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    14,
                    58,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:14:58Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    14,
                    58,
                    1,
                    308,
                    0
                ],
                "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM\n  Inference over Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Attention: A Distributed Paradigm for Collaborative LLM\n  Inference over Edge Networks"
                },
                "summary": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments."
                },
                "authors": [
                    {
                        "name": "Xiumei Deng"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Binbin Chen"
                    },
                    {
                        "name": "Dong In Kim"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "H. Vincent Poor"
                    }
                ],
                "author_detail": {
                    "name": "H. Vincent Poor"
                },
                "author": "H. Vincent Poor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v5",
                "updated": "2025-11-04T12:04:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    4,
                    6,
                    1,
                    308,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "To appear on NeurIPS 2025 (spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02381v1",
                "updated": "2025-11-04T09:03:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    3,
                    9,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T09:03:09Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    3,
                    9,
                    1,
                    308,
                    0
                ],
                "title": "Laser diagnostics for negative ion source optimization: insights from\n  SPIDER at the ITER Neutral Beam Test Facility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser diagnostics for negative ion source optimization: insights from\n  SPIDER at the ITER Neutral Beam Test Facility"
                },
                "summary": "The ITER Heating Neutral Beams (HNBs) require large, high-energy H/D atom\nbeams (285/330 A/m^2 extracted current density, and 1/0.87 MeV acceleration\nenergy, respectively for H and D). To address the associated challenges, the\nSPIDER negative ion RF beam source at the Neutral Beam Test Facility (NBTF) in\nPadova (Italy) serves as a full-scale source prototype with a 100 kV triode\naccelerator, for design validation and performance verification. SPIDER is\nequipped with two advanced laser diagnostics to monitor key plasma parameters;\nCavity Ring-Down Spectroscopy (CRDS) is used to measure H$^-$\\slash D$^-$ ion\ndensities, while Laser Absorption Spectroscopy (LAS) tracks caesium neutral\ndensity in the source. These measurements are essential for optimizing negative\nion production and meeting ITER source targets. We present diagnostic upgrade\ndetails, recent experimental results, and correlations with other machine\nparameters. Since CRDS relies on a single 4.637-meter-long optical cavity, the\nlongest used in such sources, it has demonstrated sensitivity to alignment.\nBased on recent experimental experience, structural improvements are being\nimplemented to enhance both stability and measurement reliability. LAS has\nmainly been employed as a tool to monitor the caesium conditioning status of\nSPIDER. Additionally, due to a distributed measurement over four lines of\nsight, LAS has proven effective in monitoring the caesium distribution within\nthe source. This work demonstrates the essential role of laser diagnostics in\ndeveloping ITER-relevant plasma sources and informs ongoing efforts to improve\nmeasurement accuracy in challenging environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ITER Heating Neutral Beams (HNBs) require large, high-energy H/D atom\nbeams (285/330 A/m^2 extracted current density, and 1/0.87 MeV acceleration\nenergy, respectively for H and D). To address the associated challenges, the\nSPIDER negative ion RF beam source at the Neutral Beam Test Facility (NBTF) in\nPadova (Italy) serves as a full-scale source prototype with a 100 kV triode\naccelerator, for design validation and performance verification. SPIDER is\nequipped with two advanced laser diagnostics to monitor key plasma parameters;\nCavity Ring-Down Spectroscopy (CRDS) is used to measure H$^-$\\slash D$^-$ ion\ndensities, while Laser Absorption Spectroscopy (LAS) tracks caesium neutral\ndensity in the source. These measurements are essential for optimizing negative\nion production and meeting ITER source targets. We present diagnostic upgrade\ndetails, recent experimental results, and correlations with other machine\nparameters. Since CRDS relies on a single 4.637-meter-long optical cavity, the\nlongest used in such sources, it has demonstrated sensitivity to alignment.\nBased on recent experimental experience, structural improvements are being\nimplemented to enhance both stability and measurement reliability. LAS has\nmainly been employed as a tool to monitor the caesium conditioning status of\nSPIDER. Additionally, due to a distributed measurement over four lines of\nsight, LAS has proven effective in monitoring the caesium distribution within\nthe source. This work demonstrates the essential role of laser diagnostics in\ndeveloping ITER-relevant plasma sources and informs ongoing efforts to improve\nmeasurement accuracy in challenging environments."
                },
                "authors": [
                    {
                        "name": "R. Agnello"
                    },
                    {
                        "name": "M. Barbisan"
                    },
                    {
                        "name": "R. Pasqualotto"
                    },
                    {
                        "name": "B. Pouradier-Duteil"
                    },
                    {
                        "name": "E. Sartori"
                    },
                    {
                        "name": "A. Tiso"
                    },
                    {
                        "name": "B. Zaniol"
                    }
                ],
                "author_detail": {
                    "name": "B. Zaniol"
                },
                "author": "B. Zaniol",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02230v1",
                "updated": "2025-11-04T03:43:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    3,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T03:43:05Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    3,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV\n  Cache Time-to-Live",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV\n  Cache Time-to-Live"
                },
                "summary": "Agentic LLM applications interleave LLM generation requests with tool calls.\nThese tool calls break the continuity of the workflow by creating pauses\nbetween LLM requests, bringing many challenges for the serving system,\nespecially under multi-turn scenarios. Each pause potentially causes KV cache\neviction and extra waiting time before entering the continuous batch for the\nfollowing LLM request. Since these pauses happen for each call, this problem\nbecomes increasingly severe as turn number grow for agentic programs. Previous\nworks either fail to incorporate information from the tool call, evicting KV\ncache that leads to repetitive prefill or loading, or ignore the continuity of\na multi-turn program, creating waiting time between turns that increases\nper-request latency.\n  We present Continuum, a serving system to optimize job completion time for\nmulti-turn agent workloads by combining tool-aware KV cache timeout with\nprogram-level scheduling. By predicting tool call durations in agentic\nworkflows, Continuum selectively pins the KV cache in GPU memory with a\ntime-to-live value based on total turn number. When combined with program-level\nfirst-come-first-serve, Continuum prevents scheduling bubbles, preserves\nmulti-turn continuity, and optimizes for throughput for complex agentic\nworkflows. By modeling the variability of tool call and agent program\ncontinuity, Continuum outperforms state-of-the-art baselines. Our evaluation on\nreal-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models\nshows that Continuum significantly improves the average job completion times,\nand remains performant across different hardware setups and DRAM offloading\nschemes. Preview code is available at:\nhttps://github.com/Hanchenli/vllm-continuum",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic LLM applications interleave LLM generation requests with tool calls.\nThese tool calls break the continuity of the workflow by creating pauses\nbetween LLM requests, bringing many challenges for the serving system,\nespecially under multi-turn scenarios. Each pause potentially causes KV cache\neviction and extra waiting time before entering the continuous batch for the\nfollowing LLM request. Since these pauses happen for each call, this problem\nbecomes increasingly severe as turn number grow for agentic programs. Previous\nworks either fail to incorporate information from the tool call, evicting KV\ncache that leads to repetitive prefill or loading, or ignore the continuity of\na multi-turn program, creating waiting time between turns that increases\nper-request latency.\n  We present Continuum, a serving system to optimize job completion time for\nmulti-turn agent workloads by combining tool-aware KV cache timeout with\nprogram-level scheduling. By predicting tool call durations in agentic\nworkflows, Continuum selectively pins the KV cache in GPU memory with a\ntime-to-live value based on total turn number. When combined with program-level\nfirst-come-first-serve, Continuum prevents scheduling bubbles, preserves\nmulti-turn continuity, and optimizes for throughput for complex agentic\nworkflows. By modeling the variability of tool call and agent program\ncontinuity, Continuum outperforms state-of-the-art baselines. Our evaluation on\nreal-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models\nshows that Continuum significantly improves the average job completion times,\nand remains performant across different hardware setups and DRAM offloading\nschemes. Preview code is available at:\nhttps://github.com/Hanchenli/vllm-continuum"
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Qiuyang Mang"
                    },
                    {
                        "name": "Runyuan He"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Huanzhi Mao"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Alvin Cheung"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02132v1",
                "updated": "2025-11-03T23:48:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    23,
                    48,
                    39,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T23:48:39Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    23,
                    48,
                    39,
                    0,
                    307,
                    0
                ],
                "title": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA\n  Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA\n  Effects"
                },
                "summary": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in\nlarge-scale attention workloads: non-uniform memory access (NUMA). As\nmulti-chiplet designs become the norm for scaling compute capabilities, memory\nlatency and bandwidth vary sharply across compute regions, undermining the\nperformance of traditional GPU kernel scheduling strategies that assume uniform\nmemory access. We identify how these NUMA effects distort locality in\nmulti-head attention (MHA) and present Swizzled Head-first Mapping, a\nspatially-aware scheduling strategy that aligns attention heads with GPU NUMA\ndomains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our\nmethod achieves up to 50% higher performance over state-of-the-art attention\nalgorithms using conventional scheduling techniques and sustains consistently\nhigh L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware\nscheduling is now fundamental to achieving full efficiency on next-generation\ndisaggregated GPUs, offering a path forward for scalable AI training and\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in\nlarge-scale attention workloads: non-uniform memory access (NUMA). As\nmulti-chiplet designs become the norm for scaling compute capabilities, memory\nlatency and bandwidth vary sharply across compute regions, undermining the\nperformance of traditional GPU kernel scheduling strategies that assume uniform\nmemory access. We identify how these NUMA effects distort locality in\nmulti-head attention (MHA) and present Swizzled Head-first Mapping, a\nspatially-aware scheduling strategy that aligns attention heads with GPU NUMA\ndomains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our\nmethod achieves up to 50% higher performance over state-of-the-art attention\nalgorithms using conventional scheduling techniques and sustains consistently\nhigh L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware\nscheduling is now fundamental to achieving full efficiency on next-generation\ndisaggregated GPUs, offering a path forward for scalable AI training and\ninference."
                },
                "authors": [
                    {
                        "name": "Mansi Choudhary"
                    },
                    {
                        "name": "Karthik Sangaiah"
                    },
                    {
                        "name": "Sonali Singh"
                    },
                    {
                        "name": "Muhammad Osama"
                    },
                    {
                        "name": "Lisa Wu Wills"
                    },
                    {
                        "name": "Ganesh Dasika"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Dasika"
                },
                "author": "Ganesh Dasika",
                "arxiv_comment": "11 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01815v1",
                "updated": "2025-11-03T18:20:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    20,
                    35,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T18:20:35Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    20,
                    35,
                    0,
                    307,
                    0
                ],
                "title": "KV Cache Transform Coding for Compact Storage in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Transform Coding for Compact Storage in LLM Inference"
                },
                "summary": "Serving large language models (LLMs) at scale necessitates efficient\nkey-value (KV) cache management. KV caches can be reused across conversation\nturns via shared-prefix prompts that are common in iterative code editing and\nchat. However, stale caches consume scarce GPU memory, require offloading, or\nforce recomputation. We present KVTC, a lightweight transform coder that\ncompresses KV caches for compact on-GPU and off-GPU storage. Drawing on\nclassical media compression, KVTC combines PCA-based feature decorrelation,\nadaptive quantization, and entropy coding. It requires only a brief initial\ncalibration and leaves model parameters unchanged. By exploiting redundancies\nin KV caches, KVTC achieves up to 20$\\times$ compression while maintaining\nreasoning and long-context accuracy, and 40$\\times$ or higher for specific use\ncases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across\nbenchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and\nMATH-500. It consistently outperforms inference-time baselines such as token\neviction, quantization, and SVD-based methods, while achieving higher\ncompression ratios. These results support KVTC as a practical building block\nfor memory-efficient LLM serving with reusable KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) at scale necessitates efficient\nkey-value (KV) cache management. KV caches can be reused across conversation\nturns via shared-prefix prompts that are common in iterative code editing and\nchat. However, stale caches consume scarce GPU memory, require offloading, or\nforce recomputation. We present KVTC, a lightweight transform coder that\ncompresses KV caches for compact on-GPU and off-GPU storage. Drawing on\nclassical media compression, KVTC combines PCA-based feature decorrelation,\nadaptive quantization, and entropy coding. It requires only a brief initial\ncalibration and leaves model parameters unchanged. By exploiting redundancies\nin KV caches, KVTC achieves up to 20$\\times$ compression while maintaining\nreasoning and long-context accuracy, and 40$\\times$ or higher for specific use\ncases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across\nbenchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and\nMATH-500. It consistently outperforms inference-time baselines such as token\neviction, quantization, and SVD-based methods, while achieving higher\ncompression ratios. These results support KVTC as a practical building block\nfor memory-efficient LLM serving with reusable KV caches."
                },
                "authors": [
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Adrian Łańcucki"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Łańcucki"
                },
                "author": "Adrian Łańcucki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01633v1",
                "updated": "2025-11-03T14:42:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    14,
                    42,
                    53,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T14:42:53Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    14,
                    42,
                    53,
                    0,
                    307,
                    0
                ],
                "title": "Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with\n  Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with\n  Efficient LLM Serving"
                },
                "summary": "Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to\nperform step-by-step reasoning over graph-structured knowledge, but existing\npipelines suffer from low accuracy, excessive token usage, high latency, and\nlow throughput due to single-agent monolithic prompts, repeated context\nre-encoding, and inefficient serving execution. We present GLM, the first\nmulti-agent Graph-CoT system co-designed with an optimized LLM serving\narchitecture. GLM decomposes reasoning into specialized agents for\nclassification, reasoning, action generation, and graph retrieval, enabling\nbranching and selective context sharing to reduce prompt length and reasoning\niterations while preserving reasoning quality, thereby improving accuracy and\nreducing overall token consumption. To scale inference, we introduce a\nGraph-CoT-aware LLM inference mechanism with graph-specific KV-cache\nmanagement, priority-based eviction, and pipelined execution to improve serving\nefficiency. Experiments demonstrate that GLM improves answer accuracy by up to\n38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and\nachieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT\nbaselines, enabling efficient adoption for complex real-world reasoning at\nscale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to\nperform step-by-step reasoning over graph-structured knowledge, but existing\npipelines suffer from low accuracy, excessive token usage, high latency, and\nlow throughput due to single-agent monolithic prompts, repeated context\nre-encoding, and inefficient serving execution. We present GLM, the first\nmulti-agent Graph-CoT system co-designed with an optimized LLM serving\narchitecture. GLM decomposes reasoning into specialized agents for\nclassification, reasoning, action generation, and graph retrieval, enabling\nbranching and selective context sharing to reduce prompt length and reasoning\niterations while preserving reasoning quality, thereby improving accuracy and\nreducing overall token consumption. To scale inference, we introduce a\nGraph-CoT-aware LLM inference mechanism with graph-specific KV-cache\nmanagement, priority-based eviction, and pipelined execution to improve serving\nefficiency. Experiments demonstrate that GLM improves answer accuracy by up to\n38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and\nachieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT\nbaselines, enabling efficient adoption for complex real-world reasoning at\nscale."
                },
                "authors": [
                    {
                        "name": "Chengying Huan"
                    },
                    {
                        "name": "Ziheng Meng"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Zhengyi Yang"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yue Yun"
                    },
                    {
                        "name": "Shipeng Li"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Xiabao Wu"
                    },
                    {
                        "name": "Haitao Zhang"
                    },
                    {
                        "name": "Chuntao Hong"
                    },
                    {
                        "name": "Shaonan Ma"
                    },
                    {
                        "name": "Guihai Chen"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21590v2",
                "updated": "2025-11-03T11:32:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    11,
                    32,
                    13,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-18T05:07:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "title": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation"
                },
                "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Salim I. Amoukou"
                    },
                    {
                        "name": "Francesco Leofante"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "Accepted at NeurIPS 2025. Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01385v1",
                "updated": "2025-11-03T09:36:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    36,
                    11,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T09:36:11Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    36,
                    11,
                    0,
                    307,
                    0
                ],
                "title": "Memory-Efficient Training with In-Place FFT Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training with In-Place FFT Implementation"
                },
                "summary": "Fast Fourier Transforms (FFT) are widely used to reduce memory and\ncomputational costs in deep learning. However, existing implementations,\nincluding standard FFT and real FFT (rFFT), cannot achieve true in-place\ncomputation. In particular, rFFT maps an input of size n to a complex output of\nsize n/2+1, causing dimensional mismatch and requiring additional memory\nallocation. We propose the first real-domain, fully in-place FFT framework\n(rdFFT) that preserves input-output memory space consistency. By leveraging\nbutterfly operation symmetry and conjugate properties in the frequency domain,\nwe design an implicit complex encoding scheme that eliminates intermediate\ncache usage entirely. Experiments on multiple natural language understanding\ntasks demonstrate the method effectiveness in reducing training memory cost,\noffering a promising direction for frequency-domain lightweight adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Fourier Transforms (FFT) are widely used to reduce memory and\ncomputational costs in deep learning. However, existing implementations,\nincluding standard FFT and real FFT (rFFT), cannot achieve true in-place\ncomputation. In particular, rFFT maps an input of size n to a complex output of\nsize n/2+1, causing dimensional mismatch and requiring additional memory\nallocation. We propose the first real-domain, fully in-place FFT framework\n(rdFFT) that preserves input-output memory space consistency. By leveraging\nbutterfly operation symmetry and conjugate properties in the frequency domain,\nwe design an implicit complex encoding scheme that eliminates intermediate\ncache usage entirely. Experiments on multiple natural language understanding\ntasks demonstrate the method effectiveness in reducing training memory cost,\noffering a promising direction for frequency-domain lightweight adaptation."
                },
                "authors": [
                    {
                        "name": "Xinyu Ding"
                    },
                    {
                        "name": "Bangtian Liu"
                    },
                    {
                        "name": "Siyu Liao"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang",
                "arxiv_comment": "Accepted at NeurIPS 2025. Presents a real-domain in-place FFT (rdFFT)\n  operator for memory-efficient fine-tuning of large language models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; G.1.2; D.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01266v1",
                "updated": "2025-11-03T06:37:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    37,
                    53,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T06:37:53Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    37,
                    53,
                    0,
                    307,
                    0
                ],
                "title": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls"
                },
                "summary": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience."
                },
                "authors": [
                    {
                        "name": "Joonghyuk Shin"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Jaesik Park"
                    },
                    {
                        "name": "Eli Schechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project webpage: https://joonghyuk.com/motionstream-web/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13544v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13544v3",
                "updated": "2025-11-02T20:27:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    20,
                    27,
                    27,
                    6,
                    306,
                    0
                ],
                "published": "2025-05-19T02:09:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    9,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Multi-head Temporal Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Temporal Latent Attention"
                },
                "summary": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality."
                },
                "authors": [
                    {
                        "name": "Keqi Deng"
                    },
                    {
                        "name": "Philip C. Woodland"
                    }
                ],
                "author_detail": {
                    "name": "Philip C. Woodland"
                },
                "author": "Philip C. Woodland",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13544v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00868v1",
                "updated": "2025-11-02T09:33:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    9,
                    33,
                    12,
                    6,
                    306,
                    0
                ],
                "published": "2025-11-02T09:33:12Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    9,
                    33,
                    12,
                    6,
                    306,
                    0
                ],
                "title": "FlexiCache: Leveraging Temporal Stability of Attention Heads for\n  Efficient KV Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiCache: Leveraging Temporal Stability of Attention Heads for\n  Efficient KV Cache Management"
                },
                "summary": "Large Language Model (LLM) serving is increasingly constrained by the growing\nsize of the key-value (KV) cache, which scales with both context length and\ngeneration length. Prior work shows that attention is dominated by a small\nsubset of critical tokens, yet existing systems struggle to exploit this\nefficiently without degrading accuracy, especially in long generation. We make\na key observation: the temporal stability of these critical tokens varies\nsignificantly across KV heads: some heads consistently focus on the same\ntokens, while others shift frequently. Building on this insight, we introduce\nFlexiCache, a hierarchical KV-cache management system that leverages the\ntemporal stability of KV heads to reduce GPU memory usage and computation\noverhead, while preserving model accuracy. FlexiCache classifies KV heads as\nstable or unstable: it retains all KV-cache pages from unstable heads in GPU\nmemory, whereas for stable heads, it keeps only the top-K pages on the GPU and\noffloads the rest to host memory. By exploiting temporal stability, FlexiCache\nperforms periodic reranking for stable heads to fetch newly promoted top pages.\nImplemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context\nrequests by up to 70%, improves offline serving throughput by 1.38-1.55x, and\nlowers online token latency by 1.6-2.1x, all while maintaining accuracy in\nlong-context, long-generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) serving is increasingly constrained by the growing\nsize of the key-value (KV) cache, which scales with both context length and\ngeneration length. Prior work shows that attention is dominated by a small\nsubset of critical tokens, yet existing systems struggle to exploit this\nefficiently without degrading accuracy, especially in long generation. We make\na key observation: the temporal stability of these critical tokens varies\nsignificantly across KV heads: some heads consistently focus on the same\ntokens, while others shift frequently. Building on this insight, we introduce\nFlexiCache, a hierarchical KV-cache management system that leverages the\ntemporal stability of KV heads to reduce GPU memory usage and computation\noverhead, while preserving model accuracy. FlexiCache classifies KV heads as\nstable or unstable: it retains all KV-cache pages from unstable heads in GPU\nmemory, whereas for stable heads, it keeps only the top-K pages on the GPU and\noffloads the rest to host memory. By exploiting temporal stability, FlexiCache\nperforms periodic reranking for stable heads to fetch newly promoted top pages.\nImplemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context\nrequests by up to 70%, improves offline serving throughput by 1.38-1.55x, and\nlowers online token latency by 1.6-2.1x, all while maintaining accuracy in\nlong-context, long-generation scenarios."
                },
                "authors": [
                    {
                        "name": "Nazmul Takbir"
                    },
                    {
                        "name": "Hamidreza Alikhani"
                    },
                    {
                        "name": "Nikil Dutt"
                    },
                    {
                        "name": "Sangeetha Abdu Jyothi"
                    }
                ],
                "author_detail": {
                    "name": "Sangeetha Abdu Jyothi"
                },
                "author": "Sangeetha Abdu Jyothi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00819v1",
                "updated": "2025-11-02T06:15:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    6,
                    15,
                    14,
                    6,
                    306,
                    0
                ],
                "published": "2025-11-02T06:15:14Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    6,
                    15,
                    14,
                    6,
                    306,
                    0
                ],
                "title": "Optimizing Native Sparse Attention with Latent Attention and Local\n  Global Alternating Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Native Sparse Attention with Latent Attention and Local\n  Global Alternating Strategies"
                },
                "summary": "In this work, we conduct a systematic analysis of Native Sparse Attention\n(NSA) and propose targeted improvements that enhance long-context modeling. A\nkey insight is that alternating between local (sliding-window) and global\n(compression, selective) attention across layers, rather than using fixed\npatterns, enables more effective propagation of long-range dependencies and\nsubstantially boosts performance on long-sequence tasks. Meanwhile, we further\nrefine NSA's branches with Latent Attention that the sliding-window branch is\nenhanced with Multi-head Latent Attention (MLA) while compression and selective\nbranches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache\nmemory by 50\\% versus NSA while improving the model's common-sense reasoning\nand long-text understanding capabilities. Experiments on models from 340M to\n1.3B parameters (trained on 15B and 100B tokens) show our method matches or\nexceeds full attention and native sparse attention in both common-sense\nreasoning and long-context understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we conduct a systematic analysis of Native Sparse Attention\n(NSA) and propose targeted improvements that enhance long-context modeling. A\nkey insight is that alternating between local (sliding-window) and global\n(compression, selective) attention across layers, rather than using fixed\npatterns, enables more effective propagation of long-range dependencies and\nsubstantially boosts performance on long-sequence tasks. Meanwhile, we further\nrefine NSA's branches with Latent Attention that the sliding-window branch is\nenhanced with Multi-head Latent Attention (MLA) while compression and selective\nbranches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache\nmemory by 50\\% versus NSA while improving the model's common-sense reasoning\nand long-text understanding capabilities. Experiments on models from 340M to\n1.3B parameters (trained on 15B and 100B tokens) show our method matches or\nexceeds full attention and native sparse attention in both common-sense\nreasoning and long-context understanding tasks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Wen Zan"
                    },
                    {
                        "name": "Pingwei Sun"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Yerui Sun"
                    },
                    {
                        "name": "Yuchen Xie"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00745v1",
                "updated": "2025-11-02T00:04:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    0,
                    4,
                    54,
                    6,
                    306,
                    0
                ],
                "published": "2025-11-02T00:04:54Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    0,
                    4,
                    54,
                    6,
                    306,
                    0
                ],
                "title": "High-Power Dual-Channel Field Chamber for High-Frequency Magnetic\n  Neuromodulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Power Dual-Channel Field Chamber for High-Frequency Magnetic\n  Neuromodulation"
                },
                "summary": "Several novel methods, including magnetogenetics and magnetoelectric\nstimulation, use high frequency alternating magnetic fields to precisely\nmanipulate neural activity. To quantify the behavioral effects of such\ninterventions in a freely moving mouse, we developed a dual-channel magnetic\nchamber, specifically designed for rate-sensitive magnetothermal-genetic\nstimulation, and adaptable for other uses of alternating magnetic fields.\nThrough an optimized coil design, the system allows independent control of two\nspatially orthogonal uniform magnetic fields delivered at different frequencies\nwithin a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal\nfrequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5\nmT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV\nand currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling\nsystem enables magnetic field generation for second-level duration, and an\nobservation port and camera allow video capture of the animal's behavior within\nthe chamber. The system generates high-amplitude magnetic fields across two\nwidely separated frequency channels with negligible interference (< 1%).\nRelatively uniform magnetic field distribution (+/-10% across 94% of the\nchamber volume) is maintained throughout the chamber, and temperature increase\nof the inner side of the coil enclosure during the operation is limited to <\n0.35 {\\deg}C/s to ensure in vivo safety. Using cobalt-doped and undoped iron\noxide nanoparticles, we demonstrate channel-specific heating rates of 3.5\n{\\deg}C/s and 1.5 {\\deg}C/s, respectively, validating frequency-selectivity.\nBoth channels can run continuously for four seconds stably.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several novel methods, including magnetogenetics and magnetoelectric\nstimulation, use high frequency alternating magnetic fields to precisely\nmanipulate neural activity. To quantify the behavioral effects of such\ninterventions in a freely moving mouse, we developed a dual-channel magnetic\nchamber, specifically designed for rate-sensitive magnetothermal-genetic\nstimulation, and adaptable for other uses of alternating magnetic fields.\nThrough an optimized coil design, the system allows independent control of two\nspatially orthogonal uniform magnetic fields delivered at different frequencies\nwithin a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal\nfrequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5\nmT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV\nand currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling\nsystem enables magnetic field generation for second-level duration, and an\nobservation port and camera allow video capture of the animal's behavior within\nthe chamber. The system generates high-amplitude magnetic fields across two\nwidely separated frequency channels with negligible interference (< 1%).\nRelatively uniform magnetic field distribution (+/-10% across 94% of the\nchamber volume) is maintained throughout the chamber, and temperature increase\nof the inner side of the coil enclosure during the operation is limited to <\n0.35 {\\deg}C/s to ensure in vivo safety. Using cobalt-doped and undoped iron\noxide nanoparticles, we demonstrate channel-specific heating rates of 3.5\n{\\deg}C/s and 1.5 {\\deg}C/s, respectively, validating frequency-selectivity.\nBoth channels can run continuously for four seconds stably."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Tian"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Boshuo Wang"
                    },
                    {
                        "name": "Jinshui Zhang"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Jeannette Ingabire"
                    },
                    {
                        "name": "Samantha Coffler"
                    },
                    {
                        "name": "Guillaume Duret"
                    },
                    {
                        "name": "Quoc-Khanh Pham"
                    },
                    {
                        "name": "Gang Bao"
                    },
                    {
                        "name": "Jacob T. Robinson"
                    },
                    {
                        "name": "Stefan M. Goetz"
                    },
                    {
                        "name": "Angel V. Peterchev"
                    }
                ],
                "author_detail": {
                    "name": "Angel V. Peterchev"
                },
                "author": "Angel V. Peterchev",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26692v2",
                "updated": "2025-11-01T12:05:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    12,
                    5,
                    18,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-30T16:59:43Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    43,
                    3,
                    303,
                    0
                ],
                "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi Linear: An Expressive, Efficient Attention Architecture"
                },
                "summary": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints."
                },
                "authors": [
                    {
                        "name": "Kimi Team"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zongyu Lin"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Fanqing Meng"
                    },
                    {
                        "name": "Chengyin Liu"
                    },
                    {
                        "name": "Xin Men"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Wentao Li"
                    },
                    {
                        "name": "Enzhe Lu"
                    },
                    {
                        "name": "Weizhou Liu"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Weixin Xu"
                    },
                    {
                        "name": "Longhui Yu"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Enming Yuan"
                    },
                    {
                        "name": "Dehao Zhang"
                    },
                    {
                        "name": "Yizhi Zhang"
                    },
                    {
                        "name": "T. Y. Liu"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Shengjun Fang"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Jiezhong Qiu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Zhejun Jiang"
                    },
                    {
                        "name": "Weixiao Huang"
                    },
                    {
                        "name": "Bohong Yin"
                    },
                    {
                        "name": "Jiacheng You"
                    },
                    {
                        "name": "Chu Wei"
                    },
                    {
                        "name": "Zhengtao Wang"
                    },
                    {
                        "name": "Chao Hong"
                    },
                    {
                        "name": "Yutian Chen"
                    },
                    {
                        "name": "Guanduo Chen"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Huabin Zheng"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Mengnan Dong"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Siyuan Pan"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Longyu Guan"
                    },
                    {
                        "name": "Jiawen Tao"
                    },
                    {
                        "name": "Guohong Fu"
                    },
                    {
                        "name": "Xinran Xu"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Yulun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Du"
                },
                "author": "Yulun Du",
                "arxiv_comment": "Kimi Linear tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00473v1",
                "updated": "2025-11-01T09:53:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    9,
                    53,
                    47,
                    5,
                    305,
                    0
                ],
                "published": "2025-11-01T09:53:47Z",
                "published_parsed": [
                    2025,
                    11,
                    1,
                    9,
                    53,
                    47,
                    5,
                    305,
                    0
                ],
                "title": "Drinfeld associators and Kashiwara-Vergne associators in higher genera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drinfeld associators and Kashiwara-Vergne associators in higher genera"
                },
                "summary": "For $g\\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by\nAlekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in\nrelation to the formality problem of the Goldman-Turaev Lie bialgebra on an\noriented surface with a framing, is directly constructed from a genus $g$\nanalogue of a Drinfeld associator formulated by Gonzalez, which we call a\nGonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus\n0. The framing is automatically determined from the choice of a\nGonzalez-Drinfeld associator, and in the case of genus 1, we show that only one\nparticular framing is realised by our construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For $g\\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by\nAlekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in\nrelation to the formality problem of the Goldman-Turaev Lie bialgebra on an\noriented surface with a framing, is directly constructed from a genus $g$\nanalogue of a Drinfeld associator formulated by Gonzalez, which we call a\nGonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus\n0. The framing is automatically determined from the choice of a\nGonzalez-Drinfeld associator, and in the case of genus 1, we show that only one\nparticular framing is realised by our construction."
                },
                "authors": [
                    {
                        "name": "Toyo Taniguchi"
                    }
                ],
                "author_detail": {
                    "name": "Toyo Taniguchi"
                },
                "author": "Toyo Taniguchi",
                "arxiv_comment": "40 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.QA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "57K20(primary), 16T05, 16W70, 18M75, 20F36, 20F40, 57M05 (secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19755v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19755v3",
                "updated": "2025-11-01T08:49:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    8,
                    49,
                    20,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-22T16:46:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    46,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation"
                },
                "summary": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Zhikai Wang"
                    },
                    {
                        "name": "Peiru Wang"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Zhengan Yan"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Zhengyi Shi"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "22 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19755v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19755v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12872v2",
                "updated": "2025-11-01T08:26:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    8,
                    26,
                    24,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-14T18:00:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems"
                },
                "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms."
                },
                "authors": [
                    {
                        "name": "Hancheng Ye"
                    },
                    {
                        "name": "Zhengqi Gao"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Qinsi Wang"
                    },
                    {
                        "name": "Yuzhe Fu"
                    },
                    {
                        "name": "Ming-Yu Chung"
                    },
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "Accepted for publication in NeurIPS2025. Code is available at\n  \\url{https://github.com/FastMAS/KVCOMM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22765v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22765v2",
                "updated": "2025-11-01T07:01:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    7,
                    1,
                    0,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-26T17:28:05Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    17,
                    28,
                    5,
                    6,
                    299,
                    0
                ],
                "title": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval"
                },
                "summary": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released."
                },
                "authors": [
                    {
                        "name": "Binxiao Xu"
                    },
                    {
                        "name": "Junyu Feng"
                    },
                    {
                        "name": "Shaolin Lu"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22765v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v7",
                "updated": "2025-11-01T04:26:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    4,
                    26,
                    3,
                    5,
                    305,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only O(1) additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only O(1) additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025.\n  https://neurips.cc/virtual/2025/poster/116615",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00321v1",
                "updated": "2025-10-31T23:50:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    23,
                    50,
                    44,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T23:50:44Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    23,
                    50,
                    44,
                    4,
                    304,
                    0
                ],
                "title": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled\n  KV-Cache Management Beyond GPU Limits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled\n  KV-Cache Management Beyond GPU Limits"
                },
                "summary": "The expansion of context windows in large language models (LLMs) to\nmulti-million tokens introduces severe memory and compute bottlenecks,\nparticularly in managing the growing Key-Value (KV) cache. While Compute\nExpress Link (CXL) enables non-eviction frameworks that offload the full\nKV-cache to scalable external memory, these frameworks still suffer from costly\ndata transfers when recalling non-resident KV tokens to limited GPU memory as\ncontext lengths increase. This work proposes scalable Processing-Near-Memory\n(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that\ncoordinates memory and computation beyond GPU limits. Our design offloads token\npage selection to a PNM accelerator within CXL memory, eliminating costly\nrecalls and enabling larger GPU batch sizes. We further introduce a hybrid\nparallelization strategy and a steady-token selection mechanism to enhance\ncompute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM\nsystem, our solution delivers consistent performance gains for LLMs with up to\n405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)\nand GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x\nthroughput improvement, up to 60x lower energy per token, and up to 7.3x better\ntotal cost efficiency than the baseline, demonstrating that CXL-enabled\nmulti-PNM architectures can serve as a scalable backbone for future\nlong-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of context windows in large language models (LLMs) to\nmulti-million tokens introduces severe memory and compute bottlenecks,\nparticularly in managing the growing Key-Value (KV) cache. While Compute\nExpress Link (CXL) enables non-eviction frameworks that offload the full\nKV-cache to scalable external memory, these frameworks still suffer from costly\ndata transfers when recalling non-resident KV tokens to limited GPU memory as\ncontext lengths increase. This work proposes scalable Processing-Near-Memory\n(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that\ncoordinates memory and computation beyond GPU limits. Our design offloads token\npage selection to a PNM accelerator within CXL memory, eliminating costly\nrecalls and enabling larger GPU batch sizes. We further introduce a hybrid\nparallelization strategy and a steady-token selection mechanism to enhance\ncompute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM\nsystem, our solution delivers consistent performance gains for LLMs with up to\n405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)\nand GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x\nthroughput improvement, up to 60x lower energy per token, and up to 7.3x better\ntotal cost efficiency than the baseline, demonstrating that CXL-enabled\nmulti-PNM architectures can serve as a scalable backbone for future\nlong-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Dowon Kim"
                    },
                    {
                        "name": "MinJae Lee"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "HyuckSung Kwon"
                    },
                    {
                        "name": "Hyeonggyu Jeong"
                    },
                    {
                        "name": "Sang-Soo Park"
                    },
                    {
                        "name": "Minyong Yoon"
                    },
                    {
                        "name": "Si-Dong Roh"
                    },
                    {
                        "name": "Yongsuk Kwon"
                    },
                    {
                        "name": "Jinin So"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25979v2",
                "updated": "2025-10-31T18:19:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    18,
                    19,
                    55,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-29T21:26:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    26,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache"
                },
                "summary": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Shangye Chen"
                    },
                    {
                        "name": "Cyril Guyot"
                    },
                    {
                        "name": "Filip Blagojevic"
                    },
                    {
                        "name": "Hyeran Jeon"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_comment": "10 pages, 6 figures, submitted to Ninth Annual Conference on Machine\n  Learning and Systems (MLSys'26)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27641v1",
                "updated": "2025-10-31T17:12:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:12:34Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "title": "SpecAttn: Speculating Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecAttn: Speculating Sparse Attention"
                },
                "summary": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation."
                },
                "authors": [
                    {
                        "name": "Harsh Shah"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Shah"
                },
                "author": "Harsh Shah",
                "arxiv_comment": "Accepted to NeurIPS 2025 Workshop on Structured Probabilistic\n  Inference & Generative Modeling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27617v1",
                "updated": "2025-10-31T16:40:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:40:58Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation"
                },
                "summary": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training."
                },
                "authors": [
                    {
                        "name": "Heng Ping"
                    },
                    {
                        "name": "Arijit Bhattacharjee"
                    },
                    {
                        "name": "Peiyu Zhang"
                    },
                    {
                        "name": "Shixuan Li"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Anzhe Cheng"
                    },
                    {
                        "name": "Xiaole Zhang"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Ali Jannesari"
                    },
                    {
                        "name": "Nesreen Ahmed"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04525v2",
                "updated": "2025-10-31T05:31:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    5,
                    31,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-06T06:30:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion"
                },
                "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers."
                },
                "authors": [
                    {
                        "name": "Satoshi Hayakawa"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Masaaki Imaizumi"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "23 pages, fixed cleveref-related issue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27171v1",
                "updated": "2025-10-31T04:47:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    47,
                    14,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T04:47:14Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    47,
                    14,
                    4,
                    304,
                    0
                ],
                "title": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance\n  Acceleration of Generative Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance\n  Acceleration of Generative Diffusion Models"
                },
                "summary": "Diffusion models have emerged as state-of-the-art in image generation, but\ntheir practical deployment is hindered by the significant computational cost of\ntheir iterative denoising process. While existing caching techniques can\naccelerate inference, they often create a challenging trade-off between speed\nand fidelity, suffering from quality degradation and high computational\noverhead. To address these limitations, we introduce H2-Cache, a novel\nhierarchical caching mechanism designed for modern generative diffusion model\narchitectures. Our method is founded on the key insight that the denoising\nprocess can be functionally separated into a structure-defining stage and a\ndetail-refining stage. H2-cache leverages this by employing a dual-threshold\nsystem, using independent thresholds to selectively cache each stage. To ensure\nthe efficiency of our dual-check approach, we introduce pooled feature\nsummarization (PFS), a lightweight technique for robust and fast similarity\nestimation. Extensive experiments on the Flux architecture demonstrate that\nH2-cache achieves significant acceleration (up to 5.08x) while maintaining\nimage quality nearly identical to the baseline, quantitatively and\nqualitatively outperforming existing caching methods. Our work presents a\nrobust and practical solution that effectively resolves the speed-quality\ndilemma, significantly lowering the barrier for the real-world application of\nhigh-fidelity diffusion models. Source code is available at\nhttps://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as state-of-the-art in image generation, but\ntheir practical deployment is hindered by the significant computational cost of\ntheir iterative denoising process. While existing caching techniques can\naccelerate inference, they often create a challenging trade-off between speed\nand fidelity, suffering from quality degradation and high computational\noverhead. To address these limitations, we introduce H2-Cache, a novel\nhierarchical caching mechanism designed for modern generative diffusion model\narchitectures. Our method is founded on the key insight that the denoising\nprocess can be functionally separated into a structure-defining stage and a\ndetail-refining stage. H2-cache leverages this by employing a dual-threshold\nsystem, using independent thresholds to selectively cache each stage. To ensure\nthe efficiency of our dual-check approach, we introduce pooled feature\nsummarization (PFS), a lightweight technique for robust and fast similarity\nestimation. Extensive experiments on the Flux architecture demonstrate that\nH2-cache achieves significant acceleration (up to 5.08x) while maintaining\nimage quality nearly identical to the baseline, quantitatively and\nqualitatively outperforming existing caching methods. Our work presents a\nrobust and practical solution that effectively resolves the speed-quality\ndilemma, significantly lowering the barrier for the real-world application of\nhigh-fidelity diffusion models. Source code is available at\nhttps://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache."
                },
                "authors": [
                    {
                        "name": "Mingyu Sung"
                    },
                    {
                        "name": "Il-Min Kim"
                    },
                    {
                        "name": "Sangseok Yun"
                    },
                    {
                        "name": "Jae-Mo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Mo Kang"
                },
                "author": "Jae-Mo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18586v2",
                "updated": "2025-10-31T04:17:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    17,
                    5,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-21T12:39:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25977v2",
                "updated": "2025-10-31T01:52:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    1,
                    52,
                    13,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-29T21:22:08Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    22,
                    8,
                    2,
                    302,
                    0
                ],
                "title": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium"
                },
                "summary": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference."
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Jierui Xu"
                    },
                    {
                        "name": "Weichu Yang"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_comment": "12 pages, 8 figures, submitted to the Proceedings of the Twenty-First\n  European Conference on Computer Systems (EuroSys'26)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27070v1",
                "updated": "2025-10-31T00:39:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T00:39:27Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "title": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review"
                },
                "summary": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures."
                },
                "authors": [
                    {
                        "name": "Dong Tong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Tong"
                },
                "author": "Dong Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00413v2",
                "updated": "2025-10-30T21:11:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    21,
                    11,
                    33,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-31T06:10:10Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding"
                },
                "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26944v1",
                "updated": "2025-10-30T18:58:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    18,
                    58,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T18:58:02Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    18,
                    58,
                    2,
                    3,
                    303,
                    0
                ],
                "title": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache\n  Hierarchies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache\n  Hierarchies"
                },
                "summary": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios."
                },
                "authors": [
                    {
                        "name": "Hoa Nguyen"
                    },
                    {
                        "name": "Pongstorn Maidee"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    },
                    {
                        "name": "Alireza Kaviani"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Kaviani"
                },
                "author": "Alireza Kaviani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26730v1",
                "updated": "2025-10-30T17:29:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:29:27Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference"
                },
                "summary": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints."
                },
                "authors": [
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Runxin Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20499v2",
                "updated": "2025-10-30T13:43:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    43,
                    31,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-23T12:39:59Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    39,
                    59,
                    3,
                    296,
                    0
                ],
                "title": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming"
                },
                "summary": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset."
                },
                "authors": [
                    {
                        "name": "Akif Çördük"
                    },
                    {
                        "name": "Piotr Sielski"
                    },
                    {
                        "name": "Alice Boucher"
                    },
                    {
                        "name": "Kumar Aatish"
                    }
                ],
                "author_detail": {
                    "name": "Kumar Aatish"
                },
                "author": "Kumar Aatish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26486v1",
                "updated": "2025-10-30T13:39:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:39:08Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "title": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks"
                },
                "summary": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks."
                },
                "authors": [
                    {
                        "name": "Dipak Meher"
                    },
                    {
                        "name": "Carlotta Domeniconi"
                    },
                    {
                        "name": "Guadalupe Correa-Cabrera"
                    }
                ],
                "author_detail": {
                    "name": "Guadalupe Correa-Cabrera"
                },
                "author": "Guadalupe Correa-Cabrera",
                "arxiv_comment": "Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25160v2",
                "updated": "2025-10-30T08:52:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    52,
                    17,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T04:29:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    29,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "Model-Document Protocol for AI Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Document Protocol for AI Search"
                },
                "summary": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18480v2",
                "updated": "2025-10-30T08:46:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    46,
                    37,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices"
                },
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs."
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao",
                "arxiv_comment": "Withdrawn by the authors to better delineate the related work from\n  the paper's original contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26234v1",
                "updated": "2025-10-30T08:12:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    12,
                    53,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T08:12:53Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    12,
                    53,
                    3,
                    303,
                    0
                ],
                "title": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS"
                },
                "summary": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies."
                },
                "authors": [
                    {
                        "name": "Mathis Engelbart"
                    },
                    {
                        "name": "Mike Kosek"
                    },
                    {
                        "name": "Lars Eggert"
                    },
                    {
                        "name": "Jörg Ott"
                    }
                ],
                "author_detail": {
                    "name": "Jörg Ott"
                },
                "author": "Jörg Ott",
                "arxiv_doi": "10.1145/3772356.3772416",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772356.3772416",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.26234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "HotNets 2025",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00090v1",
                "updated": "2025-10-30T04:57:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    4,
                    57,
                    26,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T04:57:26Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    4,
                    57,
                    26,
                    3,
                    303,
                    0
                ],
                "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based\n  Video Generation"
                },
                "summary": "We present LeMiCa, a training-free and efficient acceleration framework for\ndiffusion-based video generation. While existing caching strategies primarily\nfocus on reducing local heuristic errors, they often overlook the accumulation\nof global errors, leading to noticeable content degradation between accelerated\nand original videos. To address this issue, we formulate cache scheduling as a\ndirected graph with error-weighted edges and introduce a Lexicographic Minimax\nPath Optimization strategy that explicitly bounds the worst-case path error.\nThis approach substantially improves the consistency of global content and\nstyle across generated frames. Extensive experiments on multiple text-to-video\nbenchmarks demonstrate that LeMiCa delivers dual improvements in both inference\nspeed and generation quality. Notably, our method achieves a 2.9x speedup on\nthe Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming\nprior caching techniques. Importantly, these gains come with minimal perceptual\nquality degradation, making LeMiCa a robust and generalizable paradigm for\naccelerating diffusion-based video generation. We believe this approach can\nserve as a strong foundation for future research on efficient and reliable\nvideo synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LeMiCa, a training-free and efficient acceleration framework for\ndiffusion-based video generation. While existing caching strategies primarily\nfocus on reducing local heuristic errors, they often overlook the accumulation\nof global errors, leading to noticeable content degradation between accelerated\nand original videos. To address this issue, we formulate cache scheduling as a\ndirected graph with error-weighted edges and introduce a Lexicographic Minimax\nPath Optimization strategy that explicitly bounds the worst-case path error.\nThis approach substantially improves the consistency of global content and\nstyle across generated frames. Extensive experiments on multiple text-to-video\nbenchmarks demonstrate that LeMiCa delivers dual improvements in both inference\nspeed and generation quality. Notably, our method achieves a 2.9x speedup on\nthe Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming\nprior caching techniques. Importantly, these gains come with minimal perceptual\nquality degradation, making LeMiCa a robust and generalizable paradigm for\naccelerating diffusion-based video generation. We believe this approach can\nserve as a strong foundation for future research on efficient and reliable\nvideo synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa"
                },
                "authors": [
                    {
                        "name": "Huanlin Gao"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Fuyuan Shi"
                    },
                    {
                        "name": "Chao Tan"
                    },
                    {
                        "name": "Zhaoxiang Liu"
                    },
                    {
                        "name": "Fang Zhao"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Shiguo Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shiguo Lian"
                },
                "author": "Shiguo Lian",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25600v2",
                "updated": "2025-10-30T03:43:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    43,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T15:10:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    10,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models"
                },
                "summary": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation."
                },
                "authors": [
                    {
                        "name": "Zhonghua Jiang"
                    },
                    {
                        "name": "Kunxi Li"
                    },
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Sihao Liu"
                    },
                    {
                        "name": "Zhaode Wang"
                    },
                    {
                        "name": "Chengfei lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26104v1",
                "updated": "2025-10-30T03:30:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    30,
                    12,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T03:30:12Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    30,
                    12,
                    3,
                    303,
                    0
                ],
                "title": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender"
                },
                "summary": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests."
                },
                "authors": [
                    {
                        "name": "Zhaoqi Zhang"
                    },
                    {
                        "name": "Haolei Pei"
                    },
                    {
                        "name": "Jun Guo"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Yufei Feng"
                    },
                    {
                        "name": "Hui Sun"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Aixin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Aixin Sun"
                },
                "author": "Aixin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v2",
                "updated": "2025-10-29T21:56:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    56,
                    19,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "Oneiros: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oneiros: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce Oneiros, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that Oneiros significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM. Source code of Oneiros is available at\nhttps://github.com/UT-SysML/Oneiros/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce Oneiros, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that Oneiros significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM. Source code of Oneiros is available at\nhttps://github.com/UT-SysML/Oneiros/."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26835v1",
                "updated": "2025-10-29T19:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    19,
                    59,
                    45,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T19:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    19,
                    59,
                    45,
                    2,
                    302,
                    0
                ],
                "title": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads"
                },
                "summary": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Xunzhuo Liu"
                    },
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Priya Nagpurkar"
                    },
                    {
                        "name": "Huamin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Chen"
                },
                "author": "Huamin Chen",
                "arxiv_comment": "13 pages including reference, position paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25695v1",
                "updated": "2025-10-29T17:00:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:00:16Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "title": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate"
                },
                "summary": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices."
                },
                "authors": [
                    {
                        "name": "Emerson J. Hollar"
                    },
                    {
                        "name": "Esmat Farzana"
                    }
                ],
                "author_detail": {
                    "name": "Esmat Farzana"
                },
                "author": "Esmat Farzana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25604v1",
                "updated": "2025-10-29T15:12:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:12:35Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "title": "Quickest Change Point Detection with Measurements over a Lossy Link",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quickest Change Point Detection with Measurements over a Lossy Link"
                },
                "summary": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime."
                },
                "authors": [
                    {
                        "name": "Krishna Chaythanya KV"
                    },
                    {
                        "name": "Saqib Abbas Baba"
                    },
                    {
                        "name": "Anurag Kumar"
                    },
                    {
                        "name": "Arpan Chattopadhyay"
                    },
                    {
                        "name": "Rajesh Sundaresan"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Sundaresan"
                },
                "author": "Rajesh Sundaresan",
                "arxiv_comment": "17 pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25590v1",
                "updated": "2025-10-29T14:58:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T14:58:37Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing"
                },
                "summary": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved."
                },
                "authors": [
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Maosen Zhao"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Bangyin Xiang"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "26 pages, 10 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v2",
                "updated": "2025-10-29T14:46:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    46,
                    17,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hüger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Accepted by NeurIPS 2025 - main track. Project page:\n  https://focus-mllm-vqa.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25412v1",
                "updated": "2025-10-29T11:29:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:29:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "Serve Programs, Not Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serve Programs, Not Prompts"
                },
                "summary": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3713082.3730398",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730398",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.25412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotOS 2025. Follow-up implementation work (SOSP 2025) is available at\n  arXiv:2510.24051",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25152v1",
                "updated": "2025-10-29T04:09:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T04:09:50Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "title": "Off-Centered WoS-Type Solvers with Statistical Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-Centered WoS-Type Solvers with Statistical Weighting"
                },
                "summary": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems."
                },
                "authors": [
                    {
                        "name": "Anchang Bao"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Enya Shen"
                    },
                    {
                        "name": "Jianmin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianmin Wang"
                },
                "author": "Jianmin Wang",
                "arxiv_comment": "SIGGRAPH Asia 2025 conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25122v1",
                "updated": "2025-10-29T03:00:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T03:00:36Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "title": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies"
                },
                "summary": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Jiahong Chen"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Chuwei Cai"
                    },
                    {
                        "name": "Jinghui Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jinghui Lu"
                },
                "author": "Jinghui Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24824v1",
                "updated": "2025-10-28T15:35:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:35:50Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling"
                },
                "summary": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Xiang Luo"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Qifan Yu"
                    },
                    {
                        "name": "Fan Xia"
                    },
                    {
                        "name": "Tianqi Zhang"
                    },
                    {
                        "name": "Hongrui Zhan"
                    },
                    {
                        "name": "Zheng Zhong"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Siyuan Qiao"
                    },
                    {
                        "name": "Xingyan Bin"
                    }
                ],
                "author_detail": {
                    "name": "Xingyan Bin"
                },
                "author": "Xingyan Bin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24359v1",
                "updated": "2025-10-28T12:28:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:28:02Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine"
                },
                "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual."
                },
                "authors": [
                    {
                        "name": "Pedram Fard"
                    },
                    {
                        "name": "Alaleh Azhir"
                    },
                    {
                        "name": "Neguine Rezaii"
                    },
                    {
                        "name": "Jiazi Tian"
                    },
                    {
                        "name": "Hossein Estiri"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Estiri"
                },
                "author": "Hossein Estiri",
                "arxiv_comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24273v1",
                "updated": "2025-10-28T10:32:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:32:52Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "title": "SALS: Sparse Attention in Latent Space for KV cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALS: Sparse Attention in Latent Space for KV cache Compression"
                },
                "summary": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively."
                },
                "authors": [
                    {
                        "name": "Junlin Mu"
                    },
                    {
                        "name": "Hantao Huang"
                    },
                    {
                        "name": "Jihang Zhang"
                    },
                    {
                        "name": "Minghui Yu"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Yidong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yidong Li"
                },
                "author": "Yidong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24051v1",
                "updated": "2025-10-28T04:17:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T04:17:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "Pie: A Programmable Serving System for Emerging LLM Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: A Programmable Serving System for Emerging LLM Applications"
                },
                "summary": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Zhiyao Ma"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3731569.3764814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.24051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SOSP 2025. Source code available at\n  https://github.com/pie-project/pie",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v4",
                "updated": "2025-10-28T04:00:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    0,
                    18,
                    1,
                    301,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14969v2",
                "updated": "2025-10-27T21:48:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    21,
                    48,
                    48,
                    0,
                    300,
                    0
                ],
                "published": "2025-05-20T23:12:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STree: Speculative Tree Decoding for Hybrid State-Space Models"
                },
                "summary": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree."
                },
                "authors": [
                    {
                        "name": "Yangchao Wu"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Alex Wong"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12362v2",
                "updated": "2025-10-27T17:31:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    17,
                    31,
                    15,
                    0,
                    300,
                    0
                ],
                "published": "2024-04-18T17:45:19Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    17,
                    45,
                    19,
                    3,
                    109,
                    0
                ],
                "title": "KV-weights are all you need for skipless transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-weights are all you need for skipless transformers"
                },
                "summary": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks."
                },
                "authors": [
                    {
                        "name": "Nils Graef"
                    }
                ],
                "author_detail": {
                    "name": "Nils Graef"
                },
                "author": "Nils Graef",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v3",
                "updated": "2025-10-27T16:20:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    16,
                    20,
                    28,
                    0,
                    300,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_doi": "10.1145/3721462.3770776",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721462.3770776",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at Middleware '25",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v2",
                "updated": "2025-10-27T14:59:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    14,
                    59,
                    46,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving"
                },
                "summary": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Accepted in a computer science workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v2",
                "updated": "2025-10-27T11:55:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    11,
                    55,
                    7,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gaëtan Hadjeres"
                    },
                    {
                        "name": "Gaël Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_doi": "10.5334/TISMIR.251",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5334/TISMIR.251",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Transactions of the International Society for Music Information\n  Retrieval, 8(1): 334-352 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22876v1",
                "updated": "2025-10-26T23:59:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T23:59:23Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "title": "Batch Speculative Decoding Done Right",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Speculative Decoding Done Right"
                },
                "summary": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec."
                },
                "authors": [
                    {
                        "name": "Ranran Haoran Zhang"
                    },
                    {
                        "name": "Soumik Dey"
                    },
                    {
                        "name": "Ashirbad Mishra"
                    },
                    {
                        "name": "Hansi Wu"
                    },
                    {
                        "name": "Binbin Li"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v3",
                "updated": "2025-10-26T13:31:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    13,
                    31,
                    41,
                    6,
                    299,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22556v1",
                "updated": "2025-10-26T07:17:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T07:17:10Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "title": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size"
                },
                "summary": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length."
                },
                "authors": [
                    {
                        "name": "Jinhan Chen"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Xianjun Gao"
                    },
                    {
                        "name": "Shilong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shilong Wang"
                },
                "author": "Shilong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v3",
                "updated": "2025-10-26T04:25:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    4,
                    25,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23657v1",
                "updated": "2025-10-26T01:25:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T01:25:24Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "title": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops"
                },
                "summary": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture."
                },
                "authors": [
                    {
                        "name": "Saklain Niam"
                    },
                    {
                        "name": "Tashfiqur Rahman"
                    },
                    {
                        "name": "Md. Amjad Patwary"
                    },
                    {
                        "name": "Mukarram Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Mukarram Hossain"
                },
                "author": "Mukarram Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22467v1",
                "updated": "2025-10-26T00:50:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T00:50:12Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "title": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints"
                },
                "summary": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore)."
                },
                "authors": [
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Kaitong Cai"
                    },
                    {
                        "name": "Yijia Fan"
                    },
                    {
                        "name": "Yufeng Yang"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v3",
                "updated": "2025-10-25T14:12:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    14,
                    12,
                    56,
                    5,
                    298,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "38 pages, 9 figures, 17 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23649v1",
                "updated": "2025-10-25T11:43:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models"
                },
                "summary": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK."
                },
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22145v1",
                "updated": "2025-10-25T03:34:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T03:34:34Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "title": "Fundamental Limits of Coded Caching with Fixed Subpacketization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Coded Caching with Fixed Subpacketization"
                },
                "summary": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Yifei Huang"
                    },
                    {
                        "name": "Youlong Wu"
                    },
                    {
                        "name": "Jinyan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinyan Wang"
                },
                "author": "Jinyan Wang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v3",
                "updated": "2025-10-25T02:29:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    2,
                    29,
                    47,
                    5,
                    298,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22049v1",
                "updated": "2025-10-24T22:17:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T22:17:49Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "title": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders"
                },
                "summary": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users."
                },
                "authors": [
                    {
                        "name": "Zhimin Chen"
                    },
                    {
                        "name": "Chenyu Zhao"
                    },
                    {
                        "name": "Ka Chun Mo"
                    },
                    {
                        "name": "Yunjiang Jiang"
                    },
                    {
                        "name": "Jane H. Lee"
                    },
                    {
                        "name": "Shouwei Chen"
                    },
                    {
                        "name": "Khushhall Chandra Mahajan"
                    },
                    {
                        "name": "Ning Jiang"
                    },
                    {
                        "name": "Kai Ren"
                    },
                    {
                        "name": "Jinhui Li"
                    },
                    {
                        "name": "Wen-Yun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Yun Yang"
                },
                "author": "Wen-Yun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21696v1",
                "updated": "2025-10-24T17:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "title": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character"
                },
                "summary": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training."
                },
                "authors": [
                    {
                        "name": "Han Yan"
                    },
                    {
                        "name": "Xibin Song"
                    },
                    {
                        "name": "Yifu Wang"
                    },
                    {
                        "name": "Hongdong Li"
                    },
                    {
                        "name": "Pan Ji"
                    },
                    {
                        "name": "Chao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chao Ma"
                },
                "author": "Chao Ma",
                "arxiv_comment": "Project page: https://wolfball.github.io/bachvid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.05487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05487v1",
                "updated": "2025-11-07T18:56:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    56,
                    18,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T18:56:18Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    56,
                    18,
                    4,
                    311,
                    0
                ],
                "title": "Function on Scalar Regression with Complex Survey Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function on Scalar Regression with Complex Survey Designs"
                },
                "summary": "Large health surveys increasingly collect high-dimensional functional data\nfrom wearable devices, and function on scalar regression (FoSR) is often used\nto quantify the relationship between these functional outcomes and scalar\ncovariates such as age and sex. However, existing methods for FoSR fail to\naccount for complex survey design. We introduce inferential methods for FoSR\nfor studies with complex survey designs. The method combines fast univariate\ninference (FUI) developed for functional data outcomes and survey sampling\ninferential methods developed for scalar outcomes. Our approach consists of\nthree steps: (1) fit survey weighted GLMs at each point along the functional\ndomain, (2) smooth coefficients along the functional domain, and (3) use\nbalanced repeated replication (BRR) or the Rao-Wu-Yue-Beaumont (RWYB) bootstrap\nto obtain pointwise and joint confidence bands for the functional coefficients.\nThe method is motivated by association studies between continuous physical\nactivity data and covariates collected in the National Health and Nutrition\nExamination Survey (NHANES). A first-of-its-kind analytical simulation study\nand empirical simulation using the NHANES data demonstrates that our method\nperforms better than existing methods that do not account for the survey\nstructure. Finally, application of the method in NHANES shows the practical\nimplications of accounting for survey structure. The method is implemented in\nthe R package svyfosr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large health surveys increasingly collect high-dimensional functional data\nfrom wearable devices, and function on scalar regression (FoSR) is often used\nto quantify the relationship between these functional outcomes and scalar\ncovariates such as age and sex. However, existing methods for FoSR fail to\naccount for complex survey design. We introduce inferential methods for FoSR\nfor studies with complex survey designs. The method combines fast univariate\ninference (FUI) developed for functional data outcomes and survey sampling\ninferential methods developed for scalar outcomes. Our approach consists of\nthree steps: (1) fit survey weighted GLMs at each point along the functional\ndomain, (2) smooth coefficients along the functional domain, and (3) use\nbalanced repeated replication (BRR) or the Rao-Wu-Yue-Beaumont (RWYB) bootstrap\nto obtain pointwise and joint confidence bands for the functional coefficients.\nThe method is motivated by association studies between continuous physical\nactivity data and covariates collected in the National Health and Nutrition\nExamination Survey (NHANES). A first-of-its-kind analytical simulation study\nand empirical simulation using the NHANES data demonstrates that our method\nperforms better than existing methods that do not account for the survey\nstructure. Finally, application of the method in NHANES shows the practical\nimplications of accounting for survey structure. The method is implemented in\nthe R package svyfosr."
                },
                "authors": [
                    {
                        "name": "Lily Koffman"
                    },
                    {
                        "name": "Sunan Gao"
                    },
                    {
                        "name": "Xinkai Zhou"
                    },
                    {
                        "name": "Andrew Leroux"
                    },
                    {
                        "name": "Ciprian Crainiceanu"
                    },
                    {
                        "name": "John Muschelli III"
                    }
                ],
                "author_detail": {
                    "name": "John Muschelli III"
                },
                "author": "John Muschelli III",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05484v1",
                "updated": "2025-11-07T18:53:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    53,
                    39,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T18:53:39Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    53,
                    39,
                    4,
                    311,
                    0
                ],
                "title": "Non-Gaussian Galaxy Stochasticity and the Noise-Field Formulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Gaussian Galaxy Stochasticity and the Noise-Field Formulation"
                },
                "summary": "We revisit the stochastic, or noise, contributions to the galaxy density\nfield within the effective field theory (EFT) of large-scale structure.\nStarting from the general, all-order expression of the EFT partition function,\nwe elucidate how the stochastic contributions can be described by local\nnonlinear couplings of a single Gaussian noise field. We introduce an\nalternative formulation of the partition function in terms of such a noise\nfield, and derive the corresponding field-level likelihood for biased tracers.\nThis noise-field formulation can capture the complete set of stochastic\ncontributions to the galaxy density at the field level in a normalized,\npositive-definite probability density which is suitable for numerical sampling.\nWe illustrate this by presenting the first results of EFT-based field-level\ninference with non-Gaussian and density-dependent stochasticity on dark matter\nhalos using LEFTfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We revisit the stochastic, or noise, contributions to the galaxy density\nfield within the effective field theory (EFT) of large-scale structure.\nStarting from the general, all-order expression of the EFT partition function,\nwe elucidate how the stochastic contributions can be described by local\nnonlinear couplings of a single Gaussian noise field. We introduce an\nalternative formulation of the partition function in terms of such a noise\nfield, and derive the corresponding field-level likelihood for biased tracers.\nThis noise-field formulation can capture the complete set of stochastic\ncontributions to the galaxy density at the field level in a normalized,\npositive-definite probability density which is suitable for numerical sampling.\nWe illustrate this by presenting the first results of EFT-based field-level\ninference with non-Gaussian and density-dependent stochasticity on dark matter\nhalos using LEFTfield."
                },
                "authors": [
                    {
                        "name": "Henrique Rubira"
                    },
                    {
                        "name": "Fabian Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Schmidt"
                },
                "author": "Fabian Schmidt",
                "arxiv_comment": "33 pages, 2 figures; comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05479v1",
                "updated": "2025-11-07T18:44:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    44,
                    50,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T18:44:50Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    44,
                    50,
                    4,
                    311,
                    0
                ],
                "title": "FPGA-Based Real-Time Waveform Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FPGA-Based Real-Time Waveform Classification"
                },
                "summary": "For self-triggered readout of SiPM sum signals, a waveform classification can\naid a simple threshold trigger to reliably extract calorimetric particle hit\ninformation online at an early stage and thus reduce the volume of transmitted\ndata. Typically, the ADC data acquisition is based on FPGAs for edge data\nprocessing. In this study, we consider look-up-table-based neural-networks and\naddress challenges of binary multi-layer neural networks' layout, footprint,\nperformance and training. We show that these structures can be trained using a\ngenetic algorithm and achieve the inference latency compatible with dead-time\nfree processing online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For self-triggered readout of SiPM sum signals, a waveform classification can\naid a simple threshold trigger to reliably extract calorimetric particle hit\ninformation online at an early stage and thus reduce the volume of transmitted\ndata. Typically, the ADC data acquisition is based on FPGAs for edge data\nprocessing. In this study, we consider look-up-table-based neural-networks and\naddress challenges of binary multi-layer neural networks' layout, footprint,\nperformance and training. We show that these structures can be trained using a\ngenetic algorithm and achieve the inference latency compatible with dead-time\nfree processing online."
                },
                "authors": [
                    {
                        "name": "Alperen Aksoy"
                    },
                    {
                        "name": "Ilja Bekman"
                    },
                    {
                        "name": "Chimezie Eguzo"
                    },
                    {
                        "name": "Christian Grewing"
                    },
                    {
                        "name": "Andre Zambanini"
                    }
                ],
                "author_detail": {
                    "name": "Andre Zambanini"
                },
                "author": "Andre Zambanini",
                "arxiv_comment": "TWEPP25 proceedings paper pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05476v1",
                "updated": "2025-11-07T18:38:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    38,
                    54,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T18:38:54Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    38,
                    54,
                    4,
                    311,
                    0
                ],
                "title": "A Metamorphic Testing Perspective on Knowledge Distillation for Language\n  Models of Code: Does the Student Deeply Mimic the Teacher?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Metamorphic Testing Perspective on Knowledge Distillation for Language\n  Models of Code: Does the Student Deeply Mimic the Teacher?"
                },
                "summary": "Transformer-based language models of code have achieved state-of-the-art\nperformance across a wide range of software analytics tasks, but their\npractical deployment remains limited due to high computational costs, slow\ninference speeds, and significant environmental impact. To address these\nchallenges, recent research has increasingly explored knowledge distillation as\na method for compressing a large language model of code (the teacher) into a\nsmaller model (the student) while maintaining performance. However, the degree\nto which a student model deeply mimics the predictive behavior and internal\nrepresentations of its teacher remains largely unexplored, as current\naccuracy-based evaluation provides only a surface-level view of model quality\nand often fails to capture more profound discrepancies in behavioral fidelity\nbetween the teacher and student models. To address this gap, we empirically\nshow that the student model often fails to deeply mimic the teacher model,\nresulting in up to 285% greater performance drop under adversarial attacks,\nwhich is not captured by traditional accuracy-based evaluation. Therefore, we\npropose MetaCompress, a metamorphic testing framework that systematically\nevaluates behavioral fidelity by comparing the outputs of teacher and student\nmodels under a set of behavior-preserving metamorphic relations. We evaluate\nMetaCompress on two widely studied tasks, using compressed versions of popular\nlanguage models of code, obtained via three different knowledge distillation\ntechniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress\nidentifies up to 62% behavioral discrepancies in student models, underscoring\nthe need for behavioral fidelity evaluation within the knowledge distillation\npipeline and establishing MetaCompress as a practical framework for testing\ncompressed language models of code derived through knowledge distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based language models of code have achieved state-of-the-art\nperformance across a wide range of software analytics tasks, but their\npractical deployment remains limited due to high computational costs, slow\ninference speeds, and significant environmental impact. To address these\nchallenges, recent research has increasingly explored knowledge distillation as\na method for compressing a large language model of code (the teacher) into a\nsmaller model (the student) while maintaining performance. However, the degree\nto which a student model deeply mimics the predictive behavior and internal\nrepresentations of its teacher remains largely unexplored, as current\naccuracy-based evaluation provides only a surface-level view of model quality\nand often fails to capture more profound discrepancies in behavioral fidelity\nbetween the teacher and student models. To address this gap, we empirically\nshow that the student model often fails to deeply mimic the teacher model,\nresulting in up to 285% greater performance drop under adversarial attacks,\nwhich is not captured by traditional accuracy-based evaluation. Therefore, we\npropose MetaCompress, a metamorphic testing framework that systematically\nevaluates behavioral fidelity by comparing the outputs of teacher and student\nmodels under a set of behavior-preserving metamorphic relations. We evaluate\nMetaCompress on two widely studied tasks, using compressed versions of popular\nlanguage models of code, obtained via three different knowledge distillation\ntechniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress\nidentifies up to 62% behavioral discrepancies in student models, underscoring\nthe need for behavioral fidelity evaluation within the knowledge distillation\npipeline and establishing MetaCompress as a practical framework for testing\ncompressed language models of code derived through knowledge distillation."
                },
                "authors": [
                    {
                        "name": "Md. Abdul Awal"
                    },
                    {
                        "name": "Mrigank Rochan"
                    },
                    {
                        "name": "Chanchal K. Roy"
                    }
                ],
                "author_detail": {
                    "name": "Chanchal K. Roy"
                },
                "author": "Chanchal K. Roy",
                "arxiv_comment": "The paper is currently under review at a peer-reviewed journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05471v1",
                "updated": "2025-11-07T18:33:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    33,
                    40,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T18:33:40Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    33,
                    40,
                    4,
                    311,
                    0
                ],
                "title": "Precipitation nowcasting of satellite data using physically conditioned\n  neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precipitation nowcasting of satellite data using physically conditioned\n  neural networks"
                },
                "summary": "Accurate short-term precipitation forecasts predominantly rely on dense\nweather-radar networks, limiting operational value in places most exposed to\nclimate extremes. We present TUPANN (Transferable and Universal Physics-Aligned\nNowcasting Network), a satellite-only model trained on GOES-16 RRQPE. Unlike\nmost deep learning models for nowcasting, TUPANN decomposes the forecast into\nphysically meaningful components: a variational encoder-decoder infers motion\nand intensity fields from recent imagery under optical-flow supervision, a\nlead-time-conditioned MaxViT evolves the latent state, and a differentiable\nadvection operator reconstructs future frames. We evaluate TUPANN on both\nGOES-16 and IMERG data, in up to four distinct climates (Rio de Janeiro,\nManaus, Miami, La Paz) at 10-180min lead times using the CSI and HSS metrics\nover 4-64 mm/h thresholds. Comparisons against optical-flow, deep learning and\nhybrid baselines show that TUPANN achieves the best or second-best skill in\nmost settings, with pronounced gains at higher thresholds. Training on multiple\ncities further improves performance, while cross-city experiments show modest\ndegradation and occasional gains for rare heavy-rain regimes. The model\nproduces smooth, interpretable motion fields aligned with numerical optical\nflow and runs in near real time due to the low latency of GOES-16. These\nresults indicate that physically aligned learning can provide nowcasts that are\nskillful, transferable and global.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate short-term precipitation forecasts predominantly rely on dense\nweather-radar networks, limiting operational value in places most exposed to\nclimate extremes. We present TUPANN (Transferable and Universal Physics-Aligned\nNowcasting Network), a satellite-only model trained on GOES-16 RRQPE. Unlike\nmost deep learning models for nowcasting, TUPANN decomposes the forecast into\nphysically meaningful components: a variational encoder-decoder infers motion\nand intensity fields from recent imagery under optical-flow supervision, a\nlead-time-conditioned MaxViT evolves the latent state, and a differentiable\nadvection operator reconstructs future frames. We evaluate TUPANN on both\nGOES-16 and IMERG data, in up to four distinct climates (Rio de Janeiro,\nManaus, Miami, La Paz) at 10-180min lead times using the CSI and HSS metrics\nover 4-64 mm/h thresholds. Comparisons against optical-flow, deep learning and\nhybrid baselines show that TUPANN achieves the best or second-best skill in\nmost settings, with pronounced gains at higher thresholds. Training on multiple\ncities further improves performance, while cross-city experiments show modest\ndegradation and occasional gains for rare heavy-rain regimes. The model\nproduces smooth, interpretable motion fields aligned with numerical optical\nflow and runs in near real time due to the low latency of GOES-16. These\nresults indicate that physically aligned learning can provide nowcasts that are\nskillful, transferable and global."
                },
                "authors": [
                    {
                        "name": "Antônio Catão"
                    },
                    {
                        "name": "Melvin Poveda"
                    },
                    {
                        "name": "Leonardo Voltarelli"
                    },
                    {
                        "name": "Paulo Orenstein"
                    }
                ],
                "author_detail": {
                    "name": "Paulo Orenstein"
                },
                "author": "Paulo Orenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05470v1",
                "updated": "2025-11-07T18:30:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    30,
                    44,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T18:30:44Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    30,
                    44,
                    4,
                    311,
                    0
                ],
                "title": "$\\texttt{unimpeded}$: A Public Nested Sampling Database for Bayesian\n  Cosmology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{unimpeded}$: A Public Nested Sampling Database for Bayesian\n  Cosmology"
                },
                "summary": "Bayesian inference is central to modern cosmology. While parameter estimation\nis achievable with unnormalised posteriors traditionally obtained via MCMC\nmethods, comprehensive model comparison and tension quantification require\nBayesian evidences and normalised posteriors, which remain computationally\nprohibitive for many researchers. To address this, we present\n$\\texttt{unimpeded}$, a publicly available Python library and data repository\nproviding DiRAC-funded (DP192 and 264) pre-computed nested sampling and MCMC\nchains with their normalised posterior samples, computed using\n$\\texttt{Cobaya}$ and the Boltzmann solver $\\texttt{CAMB}$.\n$\\texttt{unimpeded}$ delivers systematic analysis across a grid of eight\ncosmological models (including $\\Lambda$CDM and seven extensions) and 39 modern\ncosmological datasets (comprising individual probes and their pairwise\ncombinations). The built-in tension statistics calculator enables rapid\ncomputation of six tension quantification metrics. All chains are hosted on\nZenodo with permanent access via the unimpeded API, analogous to the renowned\nPlanck Legacy Archive but utilising nested sampling in addition to traditional\nMCMC methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference is central to modern cosmology. While parameter estimation\nis achievable with unnormalised posteriors traditionally obtained via MCMC\nmethods, comprehensive model comparison and tension quantification require\nBayesian evidences and normalised posteriors, which remain computationally\nprohibitive for many researchers. To address this, we present\n$\\texttt{unimpeded}$, a publicly available Python library and data repository\nproviding DiRAC-funded (DP192 and 264) pre-computed nested sampling and MCMC\nchains with their normalised posterior samples, computed using\n$\\texttt{Cobaya}$ and the Boltzmann solver $\\texttt{CAMB}$.\n$\\texttt{unimpeded}$ delivers systematic analysis across a grid of eight\ncosmological models (including $\\Lambda$CDM and seven extensions) and 39 modern\ncosmological datasets (comprising individual probes and their pairwise\ncombinations). The built-in tension statistics calculator enables rapid\ncomputation of six tension quantification metrics. All chains are hosted on\nZenodo with permanent access via the unimpeded API, analogous to the renowned\nPlanck Legacy Archive but utilising nested sampling in addition to traditional\nMCMC methods."
                },
                "authors": [
                    {
                        "name": "Dily Duan Yi Ong"
                    },
                    {
                        "name": "Will Handley"
                    }
                ],
                "author_detail": {
                    "name": "Will Handley"
                },
                "author": "Will Handley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05464v1",
                "updated": "2025-11-07T18:08:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    8,
                    4,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T18:08:04Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    8,
                    4,
                    4,
                    311,
                    0
                ],
                "title": "Photo Dating by Facial Age Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photo Dating by Facial Age Aggregation"
                },
                "summary": "We introduce a novel method for Photo Dating which estimates the year a\nphotograph was taken by leveraging information from the faces of people present\nin the image. To facilitate this research, we publicly release CSFD-1.6M, a new\ndataset containing over 1.6 million annotated faces, primarily from movie\nstills, with identity and birth year annotations. Uniquely, our dataset\nprovides annotations for multiple individuals within a single image, enabling\nthe study of multi-face information aggregation. We propose a probabilistic\nframework that formally combines visual evidence from modern face recognition\nand age estimation models, and career-based temporal priors to infer the photo\ncapture year. Our experiments demonstrate that aggregating evidence from\nmultiple faces consistently improves the performance and the approach\nsignificantly outperforms strong, scene-based baselines, particularly for\nimages containing several identifiable individuals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel method for Photo Dating which estimates the year a\nphotograph was taken by leveraging information from the faces of people present\nin the image. To facilitate this research, we publicly release CSFD-1.6M, a new\ndataset containing over 1.6 million annotated faces, primarily from movie\nstills, with identity and birth year annotations. Uniquely, our dataset\nprovides annotations for multiple individuals within a single image, enabling\nthe study of multi-face information aggregation. We propose a probabilistic\nframework that formally combines visual evidence from modern face recognition\nand age estimation models, and career-based temporal priors to infer the photo\ncapture year. Our experiments demonstrate that aggregating evidence from\nmultiple faces consistently improves the performance and the approach\nsignificantly outperforms strong, scene-based baselines, particularly for\nimages containing several identifiable individuals."
                },
                "authors": [
                    {
                        "name": "Jakub Paplham"
                    },
                    {
                        "name": "Vojtech Franc"
                    }
                ],
                "author_detail": {
                    "name": "Vojtech Franc"
                },
                "author": "Vojtech Franc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11628v2",
                "updated": "2025-11-07T18:04:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    4,
                    29,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-13T17:10:25Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    10,
                    25,
                    0,
                    286,
                    0
                ],
                "title": "Bayesian Self-Calibration and Parametric Channel Estimation for 6G\n  Antenna Arrays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Self-Calibration and Parametric Channel Estimation for 6G\n  Antenna Arrays"
                },
                "summary": "Accurate channel estimation is essential for both high-rate communication and\nhigh-precision sensing in 6G wireless systems. However, a major performance\nlimitation arises from calibration mismatches when operating phased-array\nantennas under real-world conditions. To address this issue, we propose to\nintegrate antenna element self-calibration into a variational sparse Bayesian\nlearning (VSBL) algorithm for parametric channel estimation. We model antenna\ngain and phase deviations as latent variables and derive explicit update\nequations to jointly infer these calibration parameters and the channel\nparameters: the model order, complex amplitudes, delays, angles, and the noise\nvariance. The resulting algorithm operates online and adapts in real time to\nhardware-induced mismatches. We assess its performance in terms of the root\nmean square error (RMSE) and the optimal subpattern-assignment (OSPA) metric,\ndemonstrating consistent improvements over conventional VSBL without\ncalibration. Our results demonstrate that embedding self-calibration within\nBayesian inference significantly enhances the robustness of channel estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate channel estimation is essential for both high-rate communication and\nhigh-precision sensing in 6G wireless systems. However, a major performance\nlimitation arises from calibration mismatches when operating phased-array\nantennas under real-world conditions. To address this issue, we propose to\nintegrate antenna element self-calibration into a variational sparse Bayesian\nlearning (VSBL) algorithm for parametric channel estimation. We model antenna\ngain and phase deviations as latent variables and derive explicit update\nequations to jointly infer these calibration parameters and the channel\nparameters: the model order, complex amplitudes, delays, angles, and the noise\nvariance. The resulting algorithm operates online and adapts in real time to\nhardware-induced mismatches. We assess its performance in terms of the root\nmean square error (RMSE) and the optimal subpattern-assignment (OSPA) metric,\ndemonstrating consistent improvements over conventional VSBL without\ncalibration. Our results demonstrate that embedding self-calibration within\nBayesian inference significantly enhances the robustness of channel estimation."
                },
                "authors": [
                    {
                        "name": "Patrick Hödl"
                    },
                    {
                        "name": "Jakob Möderl"
                    },
                    {
                        "name": "Erik Leitinger"
                    },
                    {
                        "name": "Klaus Witrisal"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Witrisal"
                },
                "author": "Klaus Witrisal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06483v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06483v4",
                "updated": "2025-11-07T18:03:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    3,
                    32,
                    4,
                    311,
                    0
                ],
                "published": "2024-06-10T17:22:17Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    22,
                    17,
                    0,
                    162,
                    0
                ],
                "title": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance"
                },
                "summary": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity."
                },
                "authors": [
                    {
                        "name": "Joshua J. Daymude"
                    },
                    {
                        "name": "Antonio M. Espinoza"
                    },
                    {
                        "name": "Holly Bergen"
                    },
                    {
                        "name": "Benjamin Mixon-Baca"
                    },
                    {
                        "name": "Jeffrey Knockel"
                    },
                    {
                        "name": "Jedidiah R. Crandall"
                    }
                ],
                "author_detail": {
                    "name": "Jedidiah R. Crandall"
                },
                "author": "Jedidiah R. Crandall",
                "arxiv_comment": "37 pages, 11 figures, 2 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06483v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06483v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05459v1",
                "updated": "2025-11-07T18:01:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    1,
                    32,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T18:01:32Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    1,
                    32,
                    4,
                    311,
                    0
                ],
                "title": "SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for\n  Large Language Models"
                },
                "summary": "Evaluating large language models (LLMs) for software engineering has been\nlimited by narrow task coverage, language bias, and insufficient alignment with\nreal-world developer workflows. Existing benchmarks often focus on algorithmic\nproblems or Python-centric bug fixing, leaving critical dimensions of software\nengineering underexplored. To address these gaps, we introduce SWE-Compass1, a\ncomprehensive benchmark that unifies heterogeneous code-related evaluations\ninto a structured and production-aligned framework. SWE-Compass spans 8 task\ntypes, 8 programming scenarios, and 10 programming languages, with 2000\nhigh-quality instances curated from authentic GitHub pull requests and refined\nthrough systematic filtering and validation. We benchmark ten state-of-the-art\nLLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear\nhierarchy of difficulty across task types, languages, and scenarios. Moreover,\nby aligning evaluation with real-world developer practices, SWE-Compass\nprovides a rigorous and reproducible foundation for diagnosing and advancing\nagentic coding capabilities in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) for software engineering has been\nlimited by narrow task coverage, language bias, and insufficient alignment with\nreal-world developer workflows. Existing benchmarks often focus on algorithmic\nproblems or Python-centric bug fixing, leaving critical dimensions of software\nengineering underexplored. To address these gaps, we introduce SWE-Compass1, a\ncomprehensive benchmark that unifies heterogeneous code-related evaluations\ninto a structured and production-aligned framework. SWE-Compass spans 8 task\ntypes, 8 programming scenarios, and 10 programming languages, with 2000\nhigh-quality instances curated from authentic GitHub pull requests and refined\nthrough systematic filtering and validation. We benchmark ten state-of-the-art\nLLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear\nhierarchy of difficulty across task types, languages, and scenarios. Moreover,\nby aligning evaluation with real-world developer practices, SWE-Compass\nprovides a rigorous and reproducible foundation for diagnosing and advancing\nagentic coding capabilities in large language models."
                },
                "authors": [
                    {
                        "name": "Jingxuan Xu"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Weihao Li"
                    },
                    {
                        "name": "Songwei Yu"
                    },
                    {
                        "name": "Huaixi Tang"
                    },
                    {
                        "name": "Haoyang Huang"
                    },
                    {
                        "name": "Zhiyi Lai"
                    },
                    {
                        "name": "Zizheng Zhan"
                    },
                    {
                        "name": "Yanan Wu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Kepeng Lei"
                    },
                    {
                        "name": "Yifan Yao"
                    },
                    {
                        "name": "Xinping Lei"
                    },
                    {
                        "name": "Wenqiang Zhu"
                    },
                    {
                        "name": "Zongxian Feng"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Junqi Xiong"
                    },
                    {
                        "name": "Dailin Li"
                    },
                    {
                        "name": "Zuchen Gao"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Wen Xiang"
                    },
                    {
                        "name": "Ziqi Zhan"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Wuxuan Gong"
                    },
                    {
                        "name": "Ziyuan Gao"
                    },
                    {
                        "name": "Guanxiang Wang"
                    },
                    {
                        "name": "Yirong Xue"
                    },
                    {
                        "name": "Xiaojiang Zhang"
                    },
                    {
                        "name": "Jinghui Wang"
                    },
                    {
                        "name": "Huiming Wang"
                    },
                    {
                        "name": "Wenhao Zhuang"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Yuqun Zhang"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Jiaheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Liu"
                },
                "author": "Jiaheng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04864v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04864v3",
                "updated": "2025-11-07T17:52:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    17,
                    52,
                    25,
                    4,
                    311,
                    0
                ],
                "published": "2025-04-07T09:19:28Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    19,
                    28,
                    0,
                    97,
                    0
                ],
                "title": "Statistical parametric simulation studies based on real data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical parametric simulation studies based on real data"
                },
                "summary": "Simulation studies are indispensable for evaluating statistical methods and\nubiquitous in statistical research. The most common simulation approach is\nparametric simulation, where the data-generating mechanism (DGM) corresponds to\na parametric model from which observations are drawn. While many simulation\nstudies aim to give practical guidance on method suitability, parametric\nsimulations in particular are often criticized for being unrealistic. To\novercome this drawback, it is sensible to employ real data for constructing the\nparametric DGMs. However, although real-data-based parametric DGMs are widely\nused, the specific ways in which DGM components are inferred vary, and their\nimplications may not be fully understood. Additionally, researchers often rely\non a limited selection of real datasets, with the rationale for their selection\nbeing unclear. This paper reviews and formally discusses how components of\nparametric DGMs can be inferred from real data and how dataset selection can be\nperformed more systematically. By doing so, we aim to support researchers in\nconducting simulation studies with a lower risk of overgeneralization and\nmisinterpretation. We illustrate the construction of parametric DGMs based on a\nsystematically selected set of real datasets using two examples: one on ordinal\noutcomes in randomized controlled trials and one on differential gene\nexpression analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation studies are indispensable for evaluating statistical methods and\nubiquitous in statistical research. The most common simulation approach is\nparametric simulation, where the data-generating mechanism (DGM) corresponds to\na parametric model from which observations are drawn. While many simulation\nstudies aim to give practical guidance on method suitability, parametric\nsimulations in particular are often criticized for being unrealistic. To\novercome this drawback, it is sensible to employ real data for constructing the\nparametric DGMs. However, although real-data-based parametric DGMs are widely\nused, the specific ways in which DGM components are inferred vary, and their\nimplications may not be fully understood. Additionally, researchers often rely\non a limited selection of real datasets, with the rationale for their selection\nbeing unclear. This paper reviews and formally discusses how components of\nparametric DGMs can be inferred from real data and how dataset selection can be\nperformed more systematically. By doing so, we aim to support researchers in\nconducting simulation studies with a lower risk of overgeneralization and\nmisinterpretation. We illustrate the construction of parametric DGMs based on a\nsystematically selected set of real datasets using two examples: one on ordinal\noutcomes in randomized controlled trials and one on differential gene\nexpression analysis."
                },
                "authors": [
                    {
                        "name": "Christina Sauer"
                    },
                    {
                        "name": "F. Julian D. Lange"
                    },
                    {
                        "name": "Maria Thurow"
                    },
                    {
                        "name": "Ina Dormuth"
                    },
                    {
                        "name": "Anne-Laure Boulesteix"
                    }
                ],
                "author_detail": {
                    "name": "Anne-Laure Boulesteix"
                },
                "author": "Anne-Laure Boulesteix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04864v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04864v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23247v2",
                "updated": "2025-11-07T17:49:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    17,
                    49,
                    33,
                    4,
                    311,
                    0
                ],
                "published": "2025-07-31T05:10:38Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    5,
                    10,
                    38,
                    3,
                    212,
                    0
                ],
                "title": "P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication"
                },
                "summary": "Although explainability and interpretability have received significant\nattention in artificial intelligence (AI) and natural language processing (NLP)\nfor mental health, reasoning has not been examined in the same depth.\nAddressing this gap is essential to bridge NLP and mental health through\ninterpretable and reasoning-capable AI systems. To this end, we investigate the\npragmatic reasoning capability of large-language models (LLMs) in the mental\nhealth domain. We introduce PRiMH dataset, and propose pragmatic reasoning\ntasks in mental health with pragmatic implicature and presupposition phenomena.\nIn particular, we formulate two tasks in implicature and one task in\npresupposition. To benchmark the dataset and the tasks presented, we consider\nfour models: Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the\nexperiments suggest that Mistral and Qwen show substantial reasoning abilities\nin the domain. Subsequently, we study the behavior of MentaLLaMA on the\nproposed reasoning tasks with the rollout attention mechanism. In addition, we\nalso propose three StiPRompts to study the stigma around mental health with the\nstate-of-the-art LLMs, GPT4o-mini, Deepseek-chat, and Claude-3.5-haiku. Our\nevaluated findings show that Claude-3.5-haiku deals with stigma more\nresponsibly compared to the other two LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although explainability and interpretability have received significant\nattention in artificial intelligence (AI) and natural language processing (NLP)\nfor mental health, reasoning has not been examined in the same depth.\nAddressing this gap is essential to bridge NLP and mental health through\ninterpretable and reasoning-capable AI systems. To this end, we investigate the\npragmatic reasoning capability of large-language models (LLMs) in the mental\nhealth domain. We introduce PRiMH dataset, and propose pragmatic reasoning\ntasks in mental health with pragmatic implicature and presupposition phenomena.\nIn particular, we formulate two tasks in implicature and one task in\npresupposition. To benchmark the dataset and the tasks presented, we consider\nfour models: Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the\nexperiments suggest that Mistral and Qwen show substantial reasoning abilities\nin the domain. Subsequently, we study the behavior of MentaLLaMA on the\nproposed reasoning tasks with the rollout attention mechanism. In addition, we\nalso propose three StiPRompts to study the stigma around mental health with the\nstate-of-the-art LLMs, GPT4o-mini, Deepseek-chat, and Claude-3.5-haiku. Our\nevaluated findings show that Claude-3.5-haiku deals with stigma more\nresponsibly compared to the other two LLMs."
                },
                "authors": [
                    {
                        "name": "Sneha Oram"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05449v1",
                "updated": "2025-11-07T17:38:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    17,
                    38,
                    1,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T17:38:01Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    17,
                    38,
                    1,
                    4,
                    311,
                    0
                ],
                "title": "How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?"
                },
                "summary": "Recent advances in 3D point cloud transformers have led to state-of-the-art\nresults in tasks such as semantic segmentation and reconstruction. However,\nthese models typically rely on dense token representations, incurring high\ncomputational and memory costs during training and inference. In this work, we\npresent the finding that tokens are remarkably redundant, leading to\nsubstantial inefficiency. We introduce gitmerge3D, a globally informed graph\ntoken merging method that can reduce the token count by up to 90-95% while\nmaintaining competitive performance. This finding challenges the prevailing\nassumption that more tokens inherently yield better performance and highlights\nthat many current models are over-tokenized and under-optimized for\nscalability. We validate our method across multiple 3D vision tasks and show\nconsistent improvements in computational efficiency. This work is the first to\nassess redundancy in large-scale 3D transformer models, providing insights into\nthe development of more efficient 3D foundation architectures. Our code and\ncheckpoints are publicly available at https://gitmerge3d.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in 3D point cloud transformers have led to state-of-the-art\nresults in tasks such as semantic segmentation and reconstruction. However,\nthese models typically rely on dense token representations, incurring high\ncomputational and memory costs during training and inference. In this work, we\npresent the finding that tokens are remarkably redundant, leading to\nsubstantial inefficiency. We introduce gitmerge3D, a globally informed graph\ntoken merging method that can reduce the token count by up to 90-95% while\nmaintaining competitive performance. This finding challenges the prevailing\nassumption that more tokens inherently yield better performance and highlights\nthat many current models are over-tokenized and under-optimized for\nscalability. We validate our method across multiple 3D vision tasks and show\nconsistent improvements in computational efficiency. This work is the first to\nassess redundancy in large-scale 3D transformer models, providing insights into\nthe development of more efficient 3D foundation architectures. Our code and\ncheckpoints are publicly available at https://gitmerge3d.github.io"
                },
                "authors": [
                    {
                        "name": "Tuan Anh Tran"
                    },
                    {
                        "name": "Duy M. H. Nguyen"
                    },
                    {
                        "name": "Hoai-Chau Tran"
                    },
                    {
                        "name": "Michael Barz"
                    },
                    {
                        "name": "Khoa D. Doan"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    },
                    {
                        "name": "Ngo Anh Vien"
                    },
                    {
                        "name": "Mathias Niepert"
                    },
                    {
                        "name": "Daniel Sonntag"
                    },
                    {
                        "name": "Paul Swoboda"
                    }
                ],
                "author_detail": {
                    "name": "Paul Swoboda"
                },
                "author": "Paul Swoboda",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02043v3",
                "updated": "2025-11-07T17:26:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    17,
                    26,
                    2,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-03T20:25:19Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    20,
                    25,
                    19,
                    0,
                    307,
                    0
                ],
                "title": "Flashlight: PyTorch Compiler Extensions to Accelerate Attention Variants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flashlight: PyTorch Compiler Extensions to Accelerate Attention Variants"
                },
                "summary": "Attention is a fundamental building block of large language models (LLMs), so\nthere have been many efforts to implement it efficiently. For example,\nFlashAttention leverages tiling and kernel fusion to optimize attention.\nRecently, a number of variants of attention have been introduced to enhance\nmodel quality or efficiency. Supporting them efficiently remains difficult\nsince they usually require specialized kernels or hand-tuned implementations.\nFlexAttention recently addressed part of this gap by using static programming\ntemplates to support FlashAttention-like kernels for a subset of attention\nvariants.\n  In this paper, we introduce Flashlight, a compiler-native framework within\nthe PyTorch ecosystem that automatically generates fused, FlashAttention-style\nkernels for arbitrary attention-based programs, without relying on static\ntemplates or predefined kernel specializations. Flashlight leverages PyTorch's\ncompilation workflow to fuse and tile attention computations transparently,\nenabling efficient execution for diverse attention patterns. Not only does it\nsupport all variants expressible in the FlexAttention model but it also handles\nmore general, data-dependent attention formulations that are beyond the\ncapabilities of FlexAttention.\n  Our results show that Flashlight produces kernels with competitive or\nsuperior performance to FlexAttention, while offering the flexibility of native\nPyTorch code, enabling developers to rapidly explore new attention models\nwithout sacrificing performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention is a fundamental building block of large language models (LLMs), so\nthere have been many efforts to implement it efficiently. For example,\nFlashAttention leverages tiling and kernel fusion to optimize attention.\nRecently, a number of variants of attention have been introduced to enhance\nmodel quality or efficiency. Supporting them efficiently remains difficult\nsince they usually require specialized kernels or hand-tuned implementations.\nFlexAttention recently addressed part of this gap by using static programming\ntemplates to support FlashAttention-like kernels for a subset of attention\nvariants.\n  In this paper, we introduce Flashlight, a compiler-native framework within\nthe PyTorch ecosystem that automatically generates fused, FlashAttention-style\nkernels for arbitrary attention-based programs, without relying on static\ntemplates or predefined kernel specializations. Flashlight leverages PyTorch's\ncompilation workflow to fuse and tile attention computations transparently,\nenabling efficient execution for diverse attention patterns. Not only does it\nsupport all variants expressible in the FlexAttention model but it also handles\nmore general, data-dependent attention formulations that are beyond the\ncapabilities of FlexAttention.\n  Our results show that Flashlight produces kernels with competitive or\nsuperior performance to FlexAttention, while offering the flexibility of native\nPyTorch code, enabling developers to rapidly explore new attention models\nwithout sacrificing performance."
                },
                "authors": [
                    {
                        "name": "Bozhi You"
                    },
                    {
                        "name": "Irene Wang"
                    },
                    {
                        "name": "Zelal Su Mustafaoglu"
                    },
                    {
                        "name": "Abhinav Jangda"
                    },
                    {
                        "name": "Angélica Moreira"
                    },
                    {
                        "name": "Roshan Dathathri"
                    },
                    {
                        "name": "Divya Mahajan"
                    },
                    {
                        "name": "Keshav Pingali"
                    }
                ],
                "author_detail": {
                    "name": "Keshav Pingali"
                },
                "author": "Keshav Pingali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13557v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13557v5",
                "updated": "2025-11-07T17:21:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    17,
                    21,
                    8,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-16T21:52:04Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    21,
                    52,
                    4,
                    1,
                    259,
                    0
                ],
                "title": "MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for\n  CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for\n  CGRAs"
                },
                "summary": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO-- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO-- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design."
                },
                "authors": [
                    {
                        "name": "Zesong Jiang"
                    },
                    {
                        "name": "Yuqi Sun"
                    },
                    {
                        "name": "Qing Zhong"
                    },
                    {
                        "name": "Mahathi Krishna"
                    },
                    {
                        "name": "Deepak Patil"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    },
                    {
                        "name": "Jeff Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Zhang"
                },
                "author": "Jeff Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13557v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13557v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06265v3",
                "updated": "2025-11-07T17:11:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    17,
                    11,
                    12,
                    4,
                    311,
                    0
                ],
                "published": "2025-04-08T17:59:57Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    57,
                    1,
                    98,
                    0
                ],
                "title": "Large language models as uncertainty-calibrated optimizers for\n  experimental discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models as uncertainty-calibrated optimizers for\n  experimental discovery"
                },
                "summary": "Scientific discovery increasingly depends on efficient experimental\noptimization to navigate vast design spaces under time and resource\nconstraints. Traditional approaches often require extensive domain expertise\nand feature engineering. While large language models, with their vast\nscientific knowledge, circumvent the feature engineering limitations, they lack\nthe calibrated uncertainty estimates required for high-stakes decision making.\nHence, current optimization methods force a choice between domain knowledge and\nreliability, with no principled approach that affords both. In this work, we\nshow that training language models through the uncertainty-aware objectives of\ntraditional optimization methods enables their use as reliable optimizers\nguided by natural language. By teaching LLMs from experimental outcomes under\nuncertainty, we transform their overconfidence from a fundamental limitation\ninto a precise calibration mechanism. Applied to Buchwald-Hartwig reactions, a\ncornerstone of pharmaceutical synthesis, our method nearly doubles the\ndiscovery rate of high-yielding reaction conditions, from 24% to 43% in 50\nexperimental iterations starting from 10 unsuccessful conditions. Across 19\ndiverse optimization problems spanning organic synthesis, materials science and\ncatalysis, process chemistry, and molecular design, our approach ranks first on\naverage, establishing a new paradigm for reliable, uncertainty-guided\noptimization with LLMs. Our approach can accelerate discovery by lowering the\nbarrier to using powerful optimization methods, replacing the need for\ndomain-specific feature engineering with more accessible natural language\ninterfaces. These findings highlight that ensuring reliability through\nprincipled uncertainty quantification is critical for realizing the full\npotential of AI-guided experimentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific discovery increasingly depends on efficient experimental\noptimization to navigate vast design spaces under time and resource\nconstraints. Traditional approaches often require extensive domain expertise\nand feature engineering. While large language models, with their vast\nscientific knowledge, circumvent the feature engineering limitations, they lack\nthe calibrated uncertainty estimates required for high-stakes decision making.\nHence, current optimization methods force a choice between domain knowledge and\nreliability, with no principled approach that affords both. In this work, we\nshow that training language models through the uncertainty-aware objectives of\ntraditional optimization methods enables their use as reliable optimizers\nguided by natural language. By teaching LLMs from experimental outcomes under\nuncertainty, we transform their overconfidence from a fundamental limitation\ninto a precise calibration mechanism. Applied to Buchwald-Hartwig reactions, a\ncornerstone of pharmaceutical synthesis, our method nearly doubles the\ndiscovery rate of high-yielding reaction conditions, from 24% to 43% in 50\nexperimental iterations starting from 10 unsuccessful conditions. Across 19\ndiverse optimization problems spanning organic synthesis, materials science and\ncatalysis, process chemistry, and molecular design, our approach ranks first on\naverage, establishing a new paradigm for reliable, uncertainty-guided\noptimization with LLMs. Our approach can accelerate discovery by lowering the\nbarrier to using powerful optimization methods, replacing the need for\ndomain-specific feature engineering with more accessible natural language\ninterfaces. These findings highlight that ensuring reliability through\nprincipled uncertainty quantification is critical for realizing the full\npotential of AI-guided experimentation."
                },
                "authors": [
                    {
                        "name": "Bojana Ranković"
                    },
                    {
                        "name": "Ryan-Rhys Griffiths"
                    },
                    {
                        "name": "Philippe Schwaller"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Schwaller"
                },
                "author": "Philippe Schwaller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05432v1",
                "updated": "2025-11-07T17:07:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    17,
                    7,
                    56,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T17:07:56Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    17,
                    7,
                    56,
                    4,
                    311,
                    0
                ],
                "title": "Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis"
                },
                "summary": "We propose a text-to-talking-face synthesis framework leveraging latent\nspeech representations from HierSpeech++. A Text-to-Vec module generates\nWav2Vec2 embeddings from text, which jointly condition speech and face\ngeneration. To handle distribution shifts between clean and TTS-predicted\nfeatures, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and\nfinetuning on TTS outputs. This enables tight audio-visual alignment, preserves\nspeaker identity, and produces natural, expressive speech and synchronized\nfacial motion without ground-truth audio at inference. Experiments show that\nconditioning on TTS-predicted latent features outperforms cascaded pipelines,\nimproving both lip-sync and visual realism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a text-to-talking-face synthesis framework leveraging latent\nspeech representations from HierSpeech++. A Text-to-Vec module generates\nWav2Vec2 embeddings from text, which jointly condition speech and face\ngeneration. To handle distribution shifts between clean and TTS-predicted\nfeatures, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and\nfinetuning on TTS outputs. This enables tight audio-visual alignment, preserves\nspeaker identity, and produces natural, expressive speech and synchronized\nfacial motion without ground-truth audio at inference. Experiments show that\nconditioning on TTS-predicted latent features outperforms cascaded pipelines,\nimproving both lip-sync and visual realism."
                },
                "authors": [
                    {
                        "name": "Dogucan Yaman"
                    },
                    {
                        "name": "Seymanur Akti"
                    },
                    {
                        "name": "Fevziye Irem Eyiokur"
                    },
                    {
                        "name": "Alexander Waibel"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Waibel"
                },
                "author": "Alexander Waibel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15162v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15162v2",
                "updated": "2025-11-07T17:00:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    17,
                    0,
                    31,
                    4,
                    311,
                    0
                ],
                "published": "2025-08-21T01:59:59Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    1,
                    59,
                    59,
                    3,
                    233,
                    0
                ],
                "title": "A Unified Framework for Inference with General Missingness Patterns and\n  Machine Learning Imputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Inference with General Missingness Patterns and\n  Machine Learning Imputation"
                },
                "summary": "Pre-trained machine learning (ML) predictions have been increasingly used to\ncomplement incomplete data to enable downstream scientific inquiries, but their\nnaive integration risks biased inferences. Recently, multiple methods have been\ndeveloped to provide valid inference with ML imputations regardless of\nprediction quality and to enhance efficiency relative to complete-case\nanalyses. However, existing approaches are often limited to missing outcomes\nunder a missing-completely-at-random (MCAR) assumption, failing to handle\ngeneral missingness patterns (missing in both the outcome and exposures) under\nthe more realistic missing-at-random (MAR) assumption. This paper develops a\nnovel method that delivers a valid statistical inference framework for general\nZ-estimation problems using ML imputations under the MAR assumption and for\ngeneral missingness patterns. The core technical idea is to stratify\nobservations by distinct missingness patterns and construct an estimator by\nappropriately weighting and aggregating pattern-specific information through a\nmasking-and-imputation procedure on the complete cases. We provide theoretical\nguarantees of asymptotic normality of the proposed estimator and efficiency\ndominance over weighted complete-case analyses. Practically, the method affords\nsimple implementations by leveraging existing weighted complete-case analysis\nsoftware. Extensive simulations are carried out to validate theoretical\nresults. A real data example is provided to further illustrate the practical\nutility of the proposed method. The paper concludes with a brief discussion on\npractical implications, limitations, and potential future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained machine learning (ML) predictions have been increasingly used to\ncomplement incomplete data to enable downstream scientific inquiries, but their\nnaive integration risks biased inferences. Recently, multiple methods have been\ndeveloped to provide valid inference with ML imputations regardless of\nprediction quality and to enhance efficiency relative to complete-case\nanalyses. However, existing approaches are often limited to missing outcomes\nunder a missing-completely-at-random (MCAR) assumption, failing to handle\ngeneral missingness patterns (missing in both the outcome and exposures) under\nthe more realistic missing-at-random (MAR) assumption. This paper develops a\nnovel method that delivers a valid statistical inference framework for general\nZ-estimation problems using ML imputations under the MAR assumption and for\ngeneral missingness patterns. The core technical idea is to stratify\nobservations by distinct missingness patterns and construct an estimator by\nappropriately weighting and aggregating pattern-specific information through a\nmasking-and-imputation procedure on the complete cases. We provide theoretical\nguarantees of asymptotic normality of the proposed estimator and efficiency\ndominance over weighted complete-case analyses. Practically, the method affords\nsimple implementations by leveraging existing weighted complete-case analysis\nsoftware. Extensive simulations are carried out to validate theoretical\nresults. A real data example is provided to further illustrate the practical\nutility of the proposed method. The paper concludes with a brief discussion on\npractical implications, limitations, and potential future directions."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Tyler McCormick"
                    },
                    {
                        "name": "Bhramar Mukherjee"
                    },
                    {
                        "name": "Zhenke Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenke Wu"
                },
                "author": "Zhenke Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15162v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15162v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19132v4",
                "updated": "2025-11-07T16:56:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    56,
                    42,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-21T23:44:55Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    23,
                    44,
                    55,
                    1,
                    294,
                    0
                ],
                "title": "Evidence of Energy Injection in the Short and Distant GRB 250221A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence of Energy Injection in the Short and Distant GRB 250221A"
                },
                "summary": "We present the photometric and spectroscopic analysis of the short-duration\nGRB 250221A ($T_{90}=1.80\\pm0.32$ s), using a data set from the optical\nfacilities COLIBR\\'I, the Harlingten 50 cm Telescope, and the Very Large\nTelescope. We complement these observations with data from the \\textit{Neil\nGehrels Swift Observatory} and the \\textit{Einstein Probe}, as well as radio\nobservations from the Very Large Array. GRB 250221A is among the few short GRBs\nwith direct afterglow spectroscopy, which gives a secure redshift determination\nof $z=0.768$ and allows the unambiguous identification of the host as a galaxy\nwith a star-formation rate of $\\sim3\\,M_\\odot\\,{\\rm yr}^{-1}$. The X-ray and\noptical light curves up to $T_0+10$ ks (where $T_0$ refers to the GRB trigger\ntime) are well described by forward-shock synchrotron emission in the\nslow-cooling regime within the standard fireball framework. However, at\n$T_0+0.6$ days, both the X-ray and optical bands exhibit an excess over the\nsame interval, which we interpret as evidence of energy injection into a jet\nwith a half-opening angle of $\\theta_j=11.5^{\\circ}$ through a refreshed shock\npowered by late central engine activity or a radially stratified ejecta. The\nburst properties (duration, spectral hardness, peak energy, and location in the\nAmati plane) all favour a compact binary merger origin. However, our modelling\nof the afterglow suggests a dense circumburst medium ($n\\sim80$ cm$^{-3}$),\nwhich is more typical of a Collapsar environment. This tension over the\nclassification of this burst (short-hard vs. long-soft) as inferred from the\nprompt and afterglow emissions makes GRB~250221A an unusual event and\nunderscores the limitations of duration-based classifications and the\nimportance of multi-wavelength, time-resolved follow-up observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the photometric and spectroscopic analysis of the short-duration\nGRB 250221A ($T_{90}=1.80\\pm0.32$ s), using a data set from the optical\nfacilities COLIBR\\'I, the Harlingten 50 cm Telescope, and the Very Large\nTelescope. We complement these observations with data from the \\textit{Neil\nGehrels Swift Observatory} and the \\textit{Einstein Probe}, as well as radio\nobservations from the Very Large Array. GRB 250221A is among the few short GRBs\nwith direct afterglow spectroscopy, which gives a secure redshift determination\nof $z=0.768$ and allows the unambiguous identification of the host as a galaxy\nwith a star-formation rate of $\\sim3\\,M_\\odot\\,{\\rm yr}^{-1}$. The X-ray and\noptical light curves up to $T_0+10$ ks (where $T_0$ refers to the GRB trigger\ntime) are well described by forward-shock synchrotron emission in the\nslow-cooling regime within the standard fireball framework. However, at\n$T_0+0.6$ days, both the X-ray and optical bands exhibit an excess over the\nsame interval, which we interpret as evidence of energy injection into a jet\nwith a half-opening angle of $\\theta_j=11.5^{\\circ}$ through a refreshed shock\npowered by late central engine activity or a radially stratified ejecta. The\nburst properties (duration, spectral hardness, peak energy, and location in the\nAmati plane) all favour a compact binary merger origin. However, our modelling\nof the afterglow suggests a dense circumburst medium ($n\\sim80$ cm$^{-3}$),\nwhich is more typical of a Collapsar environment. This tension over the\nclassification of this burst (short-hard vs. long-soft) as inferred from the\nprompt and afterglow emissions makes GRB~250221A an unusual event and\nunderscores the limitations of duration-based classifications and the\nimportance of multi-wavelength, time-resolved follow-up observations."
                },
                "authors": [
                    {
                        "name": "Camila Angulo-Valdez"
                    },
                    {
                        "name": "Rosa L. Becerra"
                    },
                    {
                        "name": "Ramandeep Gill"
                    },
                    {
                        "name": "Noémie Globus"
                    },
                    {
                        "name": "William H. Lee"
                    },
                    {
                        "name": "Diego López-Cámara"
                    },
                    {
                        "name": "Cassidy Mihalenko"
                    },
                    {
                        "name": "Enrique Moreno-Méndez"
                    },
                    {
                        "name": "Roberto Ricci"
                    },
                    {
                        "name": "Karelle Siellez"
                    },
                    {
                        "name": "Alan M. Watson"
                    },
                    {
                        "name": "Muskan Yadav"
                    },
                    {
                        "name": "Yu-han Yang"
                    },
                    {
                        "name": "Dalya Akl"
                    },
                    {
                        "name": "Sarah Antier"
                    },
                    {
                        "name": "Jean-Luc Atteia"
                    },
                    {
                        "name": "Stéphane Basa"
                    },
                    {
                        "name": "Nathaniel R. Butler"
                    },
                    {
                        "name": "Simone Dichiara"
                    },
                    {
                        "name": "Damien Dornic"
                    },
                    {
                        "name": "Jean-Grégoire Ducoin"
                    },
                    {
                        "name": "Francis Fortin"
                    },
                    {
                        "name": "Leonardo García-García"
                    },
                    {
                        "name": "Kin Ocelotl López"
                    },
                    {
                        "name": "Francesco Magnani"
                    },
                    {
                        "name": "Brendan O'Connor"
                    },
                    {
                        "name": "Margarita Pereyra"
                    },
                    {
                        "name": "Ny Avo Rakotondrainibe"
                    },
                    {
                        "name": "Fredd Sánchez-Álvarez"
                    },
                    {
                        "name": "Benjamin Schneider"
                    },
                    {
                        "name": "Eleonora Troja"
                    },
                    {
                        "name": "Antonio de Ugarte Postigo"
                    }
                ],
                "author_detail": {
                    "name": "Antonio de Ugarte Postigo"
                },
                "author": "Antonio de Ugarte Postigo",
                "arxiv_comment": "19 pages, 13 figures. Fixed typo in Table 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00801v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00801v3",
                "updated": "2025-11-07T16:53:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    53,
                    2,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-02T04:46:43Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    4,
                    46,
                    43,
                    6,
                    306,
                    0
                ],
                "title": "Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided\n  Medical Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided\n  Medical Image Editing"
                },
                "summary": "Medical image editing has emerged as a pivotal technology with broad\napplications in data augmentation, model interpretability, medical education,\nand treatment simulation. However, the lack of large-scale, high-quality, and\nopenly accessible datasets tailored for medical contexts with strict anatomical\nand clinical constraints has significantly hindered progress in this domain. To\nbridge this gap, we introduce Med-Banana-50K, a comprehensive dataset of over\n50k medically curated image edits spanning chest X-ray, brain MRI, and fundus\nphotography across 23 diseases. Each sample supports bidirectional lesion\nediting (addition and removal) and is constructed using Gemini-2.5-Flash-Image\nbased on real clinical images. A key differentiator of our dataset is the\nmedically grounded quality control protocol: we employ an LLM-as-Judge\nevaluation framework with criteria such as instruction compliance, structural\nplausibility, image realism, and fidelity preservation, alongside iterative\nrefinement over up to five rounds. Additionally, Med-Banana-50K includes around\n37,000 failed editing attempts with full evaluation logs to support preference\nlearning and alignment research. By offering a large-scale, medically rigorous,\nand fully documented resource, Med-Banana-50K establishes a critical foundation\nfor developing and evaluating reliable medical image editing systems. Our\ndataset and code are publicly available.\n[https://github.com/richardChenzhihui/med-banana-50k].",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image editing has emerged as a pivotal technology with broad\napplications in data augmentation, model interpretability, medical education,\nand treatment simulation. However, the lack of large-scale, high-quality, and\nopenly accessible datasets tailored for medical contexts with strict anatomical\nand clinical constraints has significantly hindered progress in this domain. To\nbridge this gap, we introduce Med-Banana-50K, a comprehensive dataset of over\n50k medically curated image edits spanning chest X-ray, brain MRI, and fundus\nphotography across 23 diseases. Each sample supports bidirectional lesion\nediting (addition and removal) and is constructed using Gemini-2.5-Flash-Image\nbased on real clinical images. A key differentiator of our dataset is the\nmedically grounded quality control protocol: we employ an LLM-as-Judge\nevaluation framework with criteria such as instruction compliance, structural\nplausibility, image realism, and fidelity preservation, alongside iterative\nrefinement over up to five rounds. Additionally, Med-Banana-50K includes around\n37,000 failed editing attempts with full evaluation logs to support preference\nlearning and alignment research. By offering a large-scale, medically rigorous,\nand fully documented resource, Med-Banana-50K establishes a critical foundation\nfor developing and evaluating reliable medical image editing systems. Our\ndataset and code are publicly available.\n[https://github.com/richardChenzhihui/med-banana-50k]."
                },
                "authors": [
                    {
                        "name": "Zhihui Chen"
                    },
                    {
                        "name": "Mengling Feng"
                    }
                ],
                "author_detail": {
                    "name": "Mengling Feng"
                },
                "author": "Mengling Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00801v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00801v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05421v1",
                "updated": "2025-11-07T16:52:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    52,
                    42,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T16:52:42Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    52,
                    42,
                    4,
                    311,
                    0
                ],
                "title": "Sharing the Learned Knowledge-base to Estimate Convolutional Filter\n  Parameters for Continual Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharing the Learned Knowledge-base to Estimate Convolutional Filter\n  Parameters for Continual Image Restoration"
                },
                "summary": "Continual learning is an emerging topic in the field of deep learning, where\na model is expected to learn continuously for new upcoming tasks without\nforgetting previous experiences. This field has witnessed numerous\nadvancements, but few works have been attempted in the direction of image\nrestoration. Handling large image sizes and the divergent nature of various\ndegradation poses a unique challenge in the restoration domain. However,\nexisting works require heavily engineered architectural modifications for new\ntask adaptation, resulting in significant computational overhead.\nRegularization-based methods are unsuitable for restoration, as different\nrestoration challenges require different kinds of feature processing. In this\ndirection, we propose a simple modification of the convolution layer to adapt\nthe knowledge from previous restoration tasks without touching the main\nbackbone architecture. Therefore, it can be seamlessly applied to any deep\narchitecture without any structural modifications. Unlike other approaches, we\ndemonstrate that our model can increase the number of trainable parameters\nwithout significantly increasing computational overhead or inference time.\nExperimental validation demonstrates that new restoration tasks can be\nintroduced without compromising the performance of existing tasks. We also show\nthat performance on new restoration tasks improves by adapting the knowledge\nfrom the knowledge base created by previous restoration tasks. The code is\navailable at https://github.com/aupendu/continual-restore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning is an emerging topic in the field of deep learning, where\na model is expected to learn continuously for new upcoming tasks without\nforgetting previous experiences. This field has witnessed numerous\nadvancements, but few works have been attempted in the direction of image\nrestoration. Handling large image sizes and the divergent nature of various\ndegradation poses a unique challenge in the restoration domain. However,\nexisting works require heavily engineered architectural modifications for new\ntask adaptation, resulting in significant computational overhead.\nRegularization-based methods are unsuitable for restoration, as different\nrestoration challenges require different kinds of feature processing. In this\ndirection, we propose a simple modification of the convolution layer to adapt\nthe knowledge from previous restoration tasks without touching the main\nbackbone architecture. Therefore, it can be seamlessly applied to any deep\narchitecture without any structural modifications. Unlike other approaches, we\ndemonstrate that our model can increase the number of trainable parameters\nwithout significantly increasing computational overhead or inference time.\nExperimental validation demonstrates that new restoration tasks can be\nintroduced without compromising the performance of existing tasks. We also show\nthat performance on new restoration tasks improves by adapting the knowledge\nfrom the knowledge base created by previous restoration tasks. The code is\navailable at https://github.com/aupendu/continual-restore."
                },
                "authors": [
                    {
                        "name": "Aupendu Kar"
                    },
                    {
                        "name": "Krishnendu Ghosh"
                    },
                    {
                        "name": "Prabir Kumar Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Prabir Kumar Biswas"
                },
                "author": "Prabir Kumar Biswas",
                "arxiv_comment": "This paper has been accepted to ACM ICVGIP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03505v2",
                "updated": "2025-11-07T16:49:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    49,
                    47,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-03T17:39:08Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    39,
                    8,
                    2,
                    246,
                    0
                ],
                "title": "LimiX: Unleashing Structured-Data Modeling Capability for Generalist\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LimiX: Unleashing Structured-Data Modeling Capability for Generalist\n  Intelligence"
                },
                "summary": "We argue that progress toward general intelligence requires complementary\nfoundation models grounded in language, the physical world, and structured\ndata. This report presents LimiX-16M and LimiX-2M, two instantiations of our\nlarge structured-data models (LDMs). Both models treat structured data as a\njoint distribution over variables and missingness, thus capable of addressing a\nwide range of tabular tasks through query-based conditional prediction via a\nsingle model. They are pretrained using masked joint-distribution modeling with\nan episodic, context-conditional objective, supporting rapid, training-free\nadaptation at inference. We evaluate LimiX models across 11 large\nstructured-data benchmarks with broad regimes of sample size, feature\ndimensionality, class number, categorical-to-numerical feature ratio,\nmissingness, and sample-to-feature ratios. LimiX-16M consistently surpasses\nstrong baselines, as shown in Figure 1 and Figure 2. The superiority holds\nacross a wide range of tasks, such as classification, regression, missing value\nimputation, and data generation, often by substantial margins, while avoiding\ntask-specific architectures or bespoke training per task. Notably, LimiX-2M\ndelivers strong results under tight compute and memory budgets. We also present\nthe first scaling law study for LDMs, revealing how data and model scaling\njointly influence downstream performance and offering quantitative guidance for\ntabular foundation modeling. All LimiX models are publicly accessible under\nApache 2.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We argue that progress toward general intelligence requires complementary\nfoundation models grounded in language, the physical world, and structured\ndata. This report presents LimiX-16M and LimiX-2M, two instantiations of our\nlarge structured-data models (LDMs). Both models treat structured data as a\njoint distribution over variables and missingness, thus capable of addressing a\nwide range of tabular tasks through query-based conditional prediction via a\nsingle model. They are pretrained using masked joint-distribution modeling with\nan episodic, context-conditional objective, supporting rapid, training-free\nadaptation at inference. We evaluate LimiX models across 11 large\nstructured-data benchmarks with broad regimes of sample size, feature\ndimensionality, class number, categorical-to-numerical feature ratio,\nmissingness, and sample-to-feature ratios. LimiX-16M consistently surpasses\nstrong baselines, as shown in Figure 1 and Figure 2. The superiority holds\nacross a wide range of tasks, such as classification, regression, missing value\nimputation, and data generation, often by substantial margins, while avoiding\ntask-specific architectures or bespoke training per task. Notably, LimiX-2M\ndelivers strong results under tight compute and memory budgets. We also present\nthe first scaling law study for LDMs, revealing how data and model scaling\njointly influence downstream performance and offering quantitative guidance for\ntabular foundation modeling. All LimiX models are publicly accessible under\nApache 2.0."
                },
                "authors": [
                    {
                        "name": "Xingxuan Zhang"
                    },
                    {
                        "name": "Gang Ren"
                    },
                    {
                        "name": "Han Yu"
                    },
                    {
                        "name": "Hao Yuan"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiansheng Li"
                    },
                    {
                        "name": "Jiayun Wu"
                    },
                    {
                        "name": "Lang Mo"
                    },
                    {
                        "name": "Li Mao"
                    },
                    {
                        "name": "Mingchao Hao"
                    },
                    {
                        "name": "Ningbo Dai"
                    },
                    {
                        "name": "Renzhe Xu"
                    },
                    {
                        "name": "Shuyang Li"
                    },
                    {
                        "name": "Tianyang Zhang"
                    },
                    {
                        "name": "Yue He"
                    },
                    {
                        "name": "Yuanrui Wang"
                    },
                    {
                        "name": "Yunjia Zhang"
                    },
                    {
                        "name": "Zijing Xu"
                    },
                    {
                        "name": "Dongzhe Li"
                    },
                    {
                        "name": "Fang Gao"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Jiandong Liu"
                    },
                    {
                        "name": "Jiashuo Liu"
                    },
                    {
                        "name": "Jiawei Xu"
                    },
                    {
                        "name": "Kaijie Cheng"
                    },
                    {
                        "name": "Kehan Li"
                    },
                    {
                        "name": "Linjun Zhou"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Shaohua Fan"
                    },
                    {
                        "name": "Xiaoyu Lin"
                    },
                    {
                        "name": "Xinyan Han"
                    },
                    {
                        "name": "Xuanyue Li"
                    },
                    {
                        "name": "Yan Lu"
                    },
                    {
                        "name": "Yuan Xue"
                    },
                    {
                        "name": "Yuanyuan Jiang"
                    },
                    {
                        "name": "Zimu Wang"
                    },
                    {
                        "name": "Zhenlei Wang"
                    },
                    {
                        "name": "Peng Cui"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cui"
                },
                "author": "Peng Cui",
                "arxiv_comment": "61 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05345v2",
                "updated": "2025-11-07T16:42:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    42,
                    30,
                    4,
                    311,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Hyper-Scaling with KV Cache Compression"
                },
                "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference latency and memory load. For\ninstance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and\n9.7 on LiveCodeBench on average for an equivalent number of memory reads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference latency and memory load. For\ninstance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and\n9.7 on LiveCodeBench on average for an equivalent number of memory reads."
                },
                "authors": [
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15487v2",
                "updated": "2025-11-07T16:41:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    41,
                    20,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-18T23:45:50Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    23,
                    45,
                    50,
                    3,
                    261,
                    0
                ],
                "title": "DiskMINT: Self-Consistent Thermochemical Disk Models with Radially\n  Varying Gas and Dust -- Application to the Massive, CO-Rich Disk of IM Lup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskMINT: Self-Consistent Thermochemical Disk Models with Radially\n  Varying Gas and Dust -- Application to the Massive, CO-Rich Disk of IM Lup"
                },
                "summary": "Disks around young stars are the birthplaces of planets, and the spatial\ndistribution of their gas and dust masses is critical for understanding where\nand what types of planets can form. We present self-consistent thermochemical\ndisk models built with DiskMINT, which extends its initial framework to allow\nfor spatially decoupled gas and dust distributions. DiskMINT calculates the gas\ntemperature based on thermal equilibrium with dust grains, solves vertical gas\nhydrostatic equilibrium, and includes key processes for the CO chemistry,\nspecifically selective photodissociation, and freeze-out with conversion\nCO/CO$_2$ ice. We apply DiskMINT to study the IM Lup disk, a large massive\ndisk, yet with an inferred CO depletion of up to 100 based on earlier\nthermochemical models. By fitting the multi-wavelength SED along with the\nmillimeter continuum, ${\\rm C^{18}O}$ radial emission profiles, we find\n$0.02-0.08\\,{\\rm M_\\odot}$ for the gas disk mass, which are consistent with the\ndynamical-based mass within the uncertainties. We further compare the derived\nsurface densities for dust and gas and find that the outer disk is\ndrift-dominated, with a dust-to-gas mass ratio of approximately 0.01-0.02,\nwhich is likely insufficient to meet the conditions for the streaming\ninstability to occur. Our results suggest that when interpreted with\nself-consistent thermochemical models, ${\\rm C^{18}O}$ alone can serve as a\nreliable tracer of both the total gas mass and its radial distribution. This\napproach enables gas mass estimates in lower-mass disks, where dynamical\nconstraints are not available, and in fainter systems where rare species like\n${\\rm N_2H^+}$ are too weak to detect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disks around young stars are the birthplaces of planets, and the spatial\ndistribution of their gas and dust masses is critical for understanding where\nand what types of planets can form. We present self-consistent thermochemical\ndisk models built with DiskMINT, which extends its initial framework to allow\nfor spatially decoupled gas and dust distributions. DiskMINT calculates the gas\ntemperature based on thermal equilibrium with dust grains, solves vertical gas\nhydrostatic equilibrium, and includes key processes for the CO chemistry,\nspecifically selective photodissociation, and freeze-out with conversion\nCO/CO$_2$ ice. We apply DiskMINT to study the IM Lup disk, a large massive\ndisk, yet with an inferred CO depletion of up to 100 based on earlier\nthermochemical models. By fitting the multi-wavelength SED along with the\nmillimeter continuum, ${\\rm C^{18}O}$ radial emission profiles, we find\n$0.02-0.08\\,{\\rm M_\\odot}$ for the gas disk mass, which are consistent with the\ndynamical-based mass within the uncertainties. We further compare the derived\nsurface densities for dust and gas and find that the outer disk is\ndrift-dominated, with a dust-to-gas mass ratio of approximately 0.01-0.02,\nwhich is likely insufficient to meet the conditions for the streaming\ninstability to occur. Our results suggest that when interpreted with\nself-consistent thermochemical models, ${\\rm C^{18}O}$ alone can serve as a\nreliable tracer of both the total gas mass and its radial distribution. This\napproach enables gas mass estimates in lower-mass disks, where dynamical\nconstraints are not available, and in fainter systems where rare species like\n${\\rm N_2H^+}$ are too weak to detect."
                },
                "authors": [
                    {
                        "name": "Dingshan Deng"
                    },
                    {
                        "name": "Uma Gorti"
                    },
                    {
                        "name": "Ilaria Pascucci"
                    },
                    {
                        "name": "Maxime Ruaud"
                    }
                ],
                "author_detail": {
                    "name": "Maxime Ruaud"
                },
                "author": "Maxime Ruaud",
                "arxiv_comment": "Accepted for publication in ApJ. 25 pages, 15 figures. Typos in Table\n  5 and Appendix B are corrected in this updated version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.03165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.03165v2",
                "updated": "2025-11-07T16:40:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    40,
                    17,
                    4,
                    311,
                    0
                ],
                "published": "2023-09-06T17:07:21Z",
                "published_parsed": [
                    2023,
                    9,
                    6,
                    17,
                    7,
                    21,
                    2,
                    249,
                    0
                ],
                "title": "A semiparametric generalized exponential regression model with a\n  principled distance-based prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A semiparametric generalized exponential regression model with a\n  principled distance-based prior"
                },
                "summary": "The generalized exponential distribution is a well-known probability model in\nlifetime data analysis and several other research areas, including\nprecipitation modeling. Despite having broad applications for independently and\nidentically distributed observations, its uses as a generalized linear model\nfor non-identically distributed data are limited. This paper introduces a\nsemiparametric Bayesian generalized exponential (GE) regression model. Our\nproposed approach involves modeling the GE rate parameter within a generalized\nadditive model framework. An important feature is the integration of a\nprincipled distance-based prior for the GE shape parameter; this allows the\nmodel to shrink to an exponential regression model that retains the advantages\nof the exponential family. We draw inferences using the Markov chain Monte\nCarlo algorithm and discuss some theoretical results pertaining to Bayesian\nasymptotics. Extensive simulations demonstrate that the proposed model\noutperforms simpler alternatives. The Western Ghats mountain range holds\ncritical importance in regulating monsoon rainfall across Southern India,\nprofoundly impacting regional agriculture. Here, we analyze daily wet-day\nrainfall data for the monsoon months between 1901--2022 for the Northern,\nMiddle, and Southern Western Ghats regions. Applying the proposed model to\nanalyze the rainfall data over 122 years provides insights into model\nparameters, short-term temporal patterns, and the impact of climate change. We\nobserve a significant decreasing trend in wet-day rainfall for the Southern\nWestern Ghats region.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generalized exponential distribution is a well-known probability model in\nlifetime data analysis and several other research areas, including\nprecipitation modeling. Despite having broad applications for independently and\nidentically distributed observations, its uses as a generalized linear model\nfor non-identically distributed data are limited. This paper introduces a\nsemiparametric Bayesian generalized exponential (GE) regression model. Our\nproposed approach involves modeling the GE rate parameter within a generalized\nadditive model framework. An important feature is the integration of a\nprincipled distance-based prior for the GE shape parameter; this allows the\nmodel to shrink to an exponential regression model that retains the advantages\nof the exponential family. We draw inferences using the Markov chain Monte\nCarlo algorithm and discuss some theoretical results pertaining to Bayesian\nasymptotics. Extensive simulations demonstrate that the proposed model\noutperforms simpler alternatives. The Western Ghats mountain range holds\ncritical importance in regulating monsoon rainfall across Southern India,\nprofoundly impacting regional agriculture. Here, we analyze daily wet-day\nrainfall data for the monsoon months between 1901--2022 for the Northern,\nMiddle, and Southern Western Ghats regions. Applying the proposed model to\nanalyze the rainfall data over 122 years provides insights into model\nparameters, short-term temporal patterns, and the impact of climate change. We\nobserve a significant decreasing trend in wet-day rainfall for the Southern\nWestern Ghats region."
                },
                "authors": [
                    {
                        "name": "Arijit Dey"
                    },
                    {
                        "name": "Arnab Hazra"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Hazra"
                },
                "author": "Arnab Hazra",
                "arxiv_comment": "34 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.03165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.03165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P12, 62F15, 62G08, 62J12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03299v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03299v6",
                "updated": "2025-11-07T16:36:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    36,
                    35,
                    4,
                    311,
                    0
                ],
                "published": "2024-02-05T18:54:43Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    18,
                    54,
                    43,
                    0,
                    36,
                    0
                ],
                "title": "GUARD: Role-playing to Generate Natural-language Jailbreakings to Test\n  Guideline Adherence of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUARD: Role-playing to Generate Natural-language Jailbreakings to Test\n  Guideline Adherence of Large Language Models"
                },
                "summary": "The discovery of \"jailbreaks\" to bypass safety filters of Large Language\nModels (LLMs) and harmful responses have encouraged the community to implement\nsafety measures. One major safety measure is to proactively test the LLMs with\njailbreaks prior to the release. Therefore, such testing will require a method\nthat can generate jailbreaks massively and efficiently. In this paper, we\nfollow a novel yet intuitive strategy to generate jailbreaks in the style of\nthe human generation. We propose a role-playing system that assigns four\ndifferent roles to the user LLMs to collaborate on new jailbreaks. Furthermore,\nwe collect existing jailbreaks and split them into different independent\ncharacteristics using clustering frequency and semantic patterns sentence by\nsentence. We organize these characteristics into a knowledge graph, making them\nmore accessible and easier to retrieve. Our system of different roles will\nleverage this knowledge graph to generate new jailbreaks, which have proved\neffective in inducing LLMs to generate unethical or guideline-violating\nresponses. In addition, we also pioneer a setting in our system that will\nautomatically follow the government-issued guidelines to generate jailbreaks to\ntest whether LLMs follow the guidelines accordingly. We refer to our system as\nGUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have\nempirically validated the effectiveness of GUARD on three cutting-edge\nopen-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a\nwidely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the\nrealm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing\nGUARD's versatility and contributing valuable insights for the development of\nsafer, more reliable LLM-based applications across diverse modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of \"jailbreaks\" to bypass safety filters of Large Language\nModels (LLMs) and harmful responses have encouraged the community to implement\nsafety measures. One major safety measure is to proactively test the LLMs with\njailbreaks prior to the release. Therefore, such testing will require a method\nthat can generate jailbreaks massively and efficiently. In this paper, we\nfollow a novel yet intuitive strategy to generate jailbreaks in the style of\nthe human generation. We propose a role-playing system that assigns four\ndifferent roles to the user LLMs to collaborate on new jailbreaks. Furthermore,\nwe collect existing jailbreaks and split them into different independent\ncharacteristics using clustering frequency and semantic patterns sentence by\nsentence. We organize these characteristics into a knowledge graph, making them\nmore accessible and easier to retrieve. Our system of different roles will\nleverage this knowledge graph to generate new jailbreaks, which have proved\neffective in inducing LLMs to generate unethical or guideline-violating\nresponses. In addition, we also pioneer a setting in our system that will\nautomatically follow the government-issued guidelines to generate jailbreaks to\ntest whether LLMs follow the guidelines accordingly. We refer to our system as\nGUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have\nempirically validated the effectiveness of GUARD on three cutting-edge\nopen-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a\nwidely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the\nrealm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing\nGUARD's versatility and contributing valuable insights for the development of\nsafer, more reliable LLM-based applications across diverse modalities."
                },
                "authors": [
                    {
                        "name": "Haibo Jin"
                    },
                    {
                        "name": "Ruoxi Chen"
                    },
                    {
                        "name": "Peiyan Zhang"
                    },
                    {
                        "name": "Andy Zhou"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "28 papges",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03299v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03299v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05408v1",
                "updated": "2025-11-07T16:34:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    34,
                    16,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T16:34:16Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    34,
                    16,
                    4,
                    311,
                    0
                ],
                "title": "Steering Language Models with Weight Arithmetic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Language Models with Weight Arithmetic"
                },
                "summary": "Providing high-quality feedback to Large Language Models (LLMs) on a diverse\ntraining distribution can be difficult and expensive, and providing feedback\nonly on a narrow distribution can result in unintended generalizations. To\nbetter leverage narrow training data, we propose contrastive weight steering, a\nsimple post-training method that edits the model parameters using weight\narithmetic. We isolate a behavior direction in weight-space by subtracting the\nweight deltas from two small fine-tunes -- one that induces the desired\nbehavior and another that induces its opposite -- and then add or remove this\ndirection to modify the model's weights. We apply this technique to mitigate\nsycophancy and induce misalignment, and find that weight steering often\ngeneralizes further than activation steering, achieving stronger\nout-of-distribution behavioral control before degrading general capabilities.\nWe also show that, in the context of task-specific fine-tuning, weight steering\ncan partially mitigate undesired behavioral drift: it can reduce sycophancy and\nunder-refusals introduced during fine-tuning while preserving task performance\ngains. Finally, we provide preliminary evidence that emergent misalignment can\nbe detected by measuring the similarity between fine-tuning updates and an\n\"evil\" weight direction, suggesting that it may be possible to monitor the\nevolution of weights during training and detect rare misaligned behaviors that\nnever manifest during training or evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing high-quality feedback to Large Language Models (LLMs) on a diverse\ntraining distribution can be difficult and expensive, and providing feedback\nonly on a narrow distribution can result in unintended generalizations. To\nbetter leverage narrow training data, we propose contrastive weight steering, a\nsimple post-training method that edits the model parameters using weight\narithmetic. We isolate a behavior direction in weight-space by subtracting the\nweight deltas from two small fine-tunes -- one that induces the desired\nbehavior and another that induces its opposite -- and then add or remove this\ndirection to modify the model's weights. We apply this technique to mitigate\nsycophancy and induce misalignment, and find that weight steering often\ngeneralizes further than activation steering, achieving stronger\nout-of-distribution behavioral control before degrading general capabilities.\nWe also show that, in the context of task-specific fine-tuning, weight steering\ncan partially mitigate undesired behavioral drift: it can reduce sycophancy and\nunder-refusals introduced during fine-tuning while preserving task performance\ngains. Finally, we provide preliminary evidence that emergent misalignment can\nbe detected by measuring the similarity between fine-tuning updates and an\n\"evil\" weight direction, suggesting that it may be possible to monitor the\nevolution of weights during training and detect rare misaligned behaviors that\nnever manifest during training or evaluations."
                },
                "authors": [
                    {
                        "name": "Constanza Fierro"
                    },
                    {
                        "name": "Fabien Roger"
                    }
                ],
                "author_detail": {
                    "name": "Fabien Roger"
                },
                "author": "Fabien Roger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05406v1",
                "updated": "2025-11-07T16:32:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    32,
                    41,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T16:32:41Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    32,
                    41,
                    4,
                    311,
                    0
                ],
                "title": "Large Language Models for Explainable Threat Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Explainable Threat Intelligence"
                },
                "summary": "As cyber threats continue to grow in complexity, traditional security\nmechanisms struggle to keep up. Large language models (LLMs) offer significant\npotential in cybersecurity due to their advanced capabilities in text\nprocessing and generation. This paper explores the use of LLMs with\nretrieval-augmented generation (RAG) to obtain threat intelligence by combining\nreal-time information retrieval with domain-specific data. The proposed system,\nRAGRecon, uses a LLM with RAG to answer questions about cybersecurity threats.\nMoreover, it makes this form of Artificial Intelligence (AI) explainable by\ngenerating and visually presenting to the user a knowledge graph for every\nreply. This increases the transparency and interpretability of the reasoning of\nthe model, allowing analysts to better understand the connections made by the\nsystem based on the context recovered by the RAG system. We evaluated RAGRecon\nexperimentally with two datasets and seven different LLMs and the responses\nmatched the reference responses more than 91% of the time for the best\ncombinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As cyber threats continue to grow in complexity, traditional security\nmechanisms struggle to keep up. Large language models (LLMs) offer significant\npotential in cybersecurity due to their advanced capabilities in text\nprocessing and generation. This paper explores the use of LLMs with\nretrieval-augmented generation (RAG) to obtain threat intelligence by combining\nreal-time information retrieval with domain-specific data. The proposed system,\nRAGRecon, uses a LLM with RAG to answer questions about cybersecurity threats.\nMoreover, it makes this form of Artificial Intelligence (AI) explainable by\ngenerating and visually presenting to the user a knowledge graph for every\nreply. This increases the transparency and interpretability of the reasoning of\nthe model, allowing analysts to better understand the connections made by the\nsystem based on the context recovered by the RAG system. We evaluated RAGRecon\nexperimentally with two datasets and seven different LLMs and the responses\nmatched the reference responses more than 91% of the time for the best\ncombinations."
                },
                "authors": [
                    {
                        "name": "Tiago Dinis"
                    },
                    {
                        "name": "Miguel Correia"
                    },
                    {
                        "name": "Roger Tavares"
                    }
                ],
                "author_detail": {
                    "name": "Roger Tavares"
                },
                "author": "Roger Tavares",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07869v2",
                "updated": "2025-11-07T16:21:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    21,
                    31,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-09T15:56:51Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    56,
                    51,
                    1,
                    252,
                    0
                ],
                "title": "Are Humans as Brittle as Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Humans as Brittle as Large Language Models?"
                },
                "summary": "The output of large language models (LLMs) is unstable, due both to\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\nprompt changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The output of large language models (LLMs) is unstable, due both to\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\nprompt changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Sean Papay"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21700v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21700v3",
                "updated": "2025-11-07T16:21:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    21,
                    6,
                    4,
                    311,
                    0
                ],
                "published": "2025-04-30T14:44:24Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    44,
                    24,
                    2,
                    120,
                    0
                ],
                "title": "XBreaking: Understanding how LLMs security alignment can be broken",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBreaking: Understanding how LLMs security alignment can be broken"
                },
                "summary": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. These mechanisms maintain the\nintegrity of LLM alignment by guaranteeing that the models respond safely and\nethically. In response to this, attacks on LLMs are a significant threat to\nsuch protections, and many previous approaches have already demonstrated their\neffectiveness across diverse domains. Existing LLM attacks mostly adopt a\ngenerate-and-test strategy to craft malicious input. To improve the\ncomprehension of censoring mechanisms and design a targeted attack, we propose\nan Explainable-AI solution that comparatively analyzes the behavior of censored\nand uncensored models to derive unique exploitable alignment patterns. Then, we\npropose XBreaking, a novel approach that exploits these unique patterns to\nbreak the security and alignment constraints of LLMs by targeted noise\ninjection. Our thorough experimental campaign returns important insights about\nthe censoring mechanisms and demonstrates the effectiveness and performance of\nour approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. These mechanisms maintain the\nintegrity of LLM alignment by guaranteeing that the models respond safely and\nethically. In response to this, attacks on LLMs are a significant threat to\nsuch protections, and many previous approaches have already demonstrated their\neffectiveness across diverse domains. Existing LLM attacks mostly adopt a\ngenerate-and-test strategy to craft malicious input. To improve the\ncomprehension of censoring mechanisms and design a targeted attack, we propose\nan Explainable-AI solution that comparatively analyzes the behavior of censored\nand uncensored models to derive unique exploitable alignment patterns. Then, we\npropose XBreaking, a novel approach that exploits these unique patterns to\nbreak the security and alignment constraints of LLMs by targeted noise\ninjection. Our thorough experimental campaign returns important insights about\nthe censoring mechanisms and demonstrates the effectiveness and performance of\nour approach."
                },
                "authors": [
                    {
                        "name": "Marco Arazzi"
                    },
                    {
                        "name": "Vignesh Kumar Kembu"
                    },
                    {
                        "name": "Antonino Nocera"
                    },
                    {
                        "name": "Vinod P"
                    }
                ],
                "author_detail": {
                    "name": "Vinod P"
                },
                "author": "Vinod P",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21700v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21700v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05389v1",
                "updated": "2025-11-07T16:11:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    11,
                    18,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T16:11:18Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    11,
                    18,
                    4,
                    311,
                    0
                ],
                "title": "Block-structured Operator Inference for coupled multiphysics model\n  reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-structured Operator Inference for coupled multiphysics model\n  reduction"
                },
                "summary": "This paper presents a block-structured formulation of Operator Inference as a\nway to learn structured reduced-order models for multiphysics systems. The\napproach specifies the governing equation structure for each physics component\nand the structure of the coupling terms. Once the multiphysics structure is\nspecified, the reduced-order model is learned from snapshot data following the\nnonintrusive Operator Inference methodology. In addition to preserving physical\nsystem structure, which in turn permits preservation of system properties such\nas stability and second-order structure, the block-structured approach has the\nadvantages of reducing the overall dimensionality of the learning problem and\nadmitting tailored regularization for each physics component. The numerical\nadvantages of the block-structured formulation over a monolithic Operator\nInference formulation are demonstrated for aeroelastic analysis, which couples\naerodynamic and structural models. For the benchmark test case of the AGARD\n445.6 wing, block-structured Operator Inference provides an average 20% online\nprediction speedup over monolithic Operator Inference across subsonic and\nsupersonic flow conditions in both the stable and fluttering parameter regimes\nwhile preserving the accuracy achieved with monolithic Operator Inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a block-structured formulation of Operator Inference as a\nway to learn structured reduced-order models for multiphysics systems. The\napproach specifies the governing equation structure for each physics component\nand the structure of the coupling terms. Once the multiphysics structure is\nspecified, the reduced-order model is learned from snapshot data following the\nnonintrusive Operator Inference methodology. In addition to preserving physical\nsystem structure, which in turn permits preservation of system properties such\nas stability and second-order structure, the block-structured approach has the\nadvantages of reducing the overall dimensionality of the learning problem and\nadmitting tailored regularization for each physics component. The numerical\nadvantages of the block-structured formulation over a monolithic Operator\nInference formulation are demonstrated for aeroelastic analysis, which couples\naerodynamic and structural models. For the benchmark test case of the AGARD\n445.6 wing, block-structured Operator Inference provides an average 20% online\nprediction speedup over monolithic Operator Inference across subsonic and\nsupersonic flow conditions in both the stable and fluttering parameter regimes\nwhile preserving the accuracy achieved with monolithic Operator Inference."
                },
                "authors": [
                    {
                        "name": "Benjamin G. Zastrow"
                    },
                    {
                        "name": "Anirban Chaudhuri"
                    },
                    {
                        "name": "Karen E. Willcox"
                    },
                    {
                        "name": "Anthony Ashley"
                    },
                    {
                        "name": "Michael Chamberlain Henson"
                    }
                ],
                "author_detail": {
                    "name": "Michael Chamberlain Henson"
                },
                "author": "Michael Chamberlain Henson",
                "arxiv_comment": "28 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03724v2",
                "updated": "2025-11-07T16:11:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    11,
                    8,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-05T18:58:18Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    58,
                    18,
                    2,
                    309,
                    0
                ],
                "title": "Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via\n  Self-Play and Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via\n  Self-Play and Reinforcement Learning"
                },
                "summary": "AI researchers have long focused on poker-like games as a testbed for\nenvironments characterized by multi-player dynamics, imperfect information, and\nreasoning under uncertainty. While recent breakthroughs have matched elite\nhuman play at no-limit Texas hold'em, the multi-player dynamics are subdued:\nmost hands converge quickly with only two players engaged through multiple\nrounds of bidding. In this paper, we present Solly, the first AI agent to\nachieve elite human play in reduced-format Liar's Poker, a game characterized\nby extensive multi-player engagement. We trained Solly using self-play with a\nmodel-free, actor-critic, deep reinforcement learning algorithm. Solly played\nat an elite human level as measured by win rate (won over 50% of hands) and\nequity (money won) in heads-up and multi-player Liar's Poker. Solly also\noutperformed large language models (LLMs), including those with reasoning\nabilities, on the same metrics. Solly developed novel bidding strategies,\nrandomized play effectively, and was not easily exploitable by world-class\nhuman players.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI researchers have long focused on poker-like games as a testbed for\nenvironments characterized by multi-player dynamics, imperfect information, and\nreasoning under uncertainty. While recent breakthroughs have matched elite\nhuman play at no-limit Texas hold'em, the multi-player dynamics are subdued:\nmost hands converge quickly with only two players engaged through multiple\nrounds of bidding. In this paper, we present Solly, the first AI agent to\nachieve elite human play in reduced-format Liar's Poker, a game characterized\nby extensive multi-player engagement. We trained Solly using self-play with a\nmodel-free, actor-critic, deep reinforcement learning algorithm. Solly played\nat an elite human level as measured by win rate (won over 50% of hands) and\nequity (money won) in heads-up and multi-player Liar's Poker. Solly also\noutperformed large language models (LLMs), including those with reasoning\nabilities, on the same metrics. Solly developed novel bidding strategies,\nrandomized play effectively, and was not easily exploitable by world-class\nhuman players."
                },
                "authors": [
                    {
                        "name": "Richard Dewey"
                    },
                    {
                        "name": "Janos Botyanszki"
                    },
                    {
                        "name": "Ciamac C. Moallemi"
                    },
                    {
                        "name": "Andrew T. Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Andrew T. Zheng"
                },
                "author": "Andrew T. Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05385v1",
                "updated": "2025-11-07T16:08:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    8,
                    34,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T16:08:34Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    8,
                    34,
                    4,
                    311,
                    0
                ],
                "title": "TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation\n  Framework"
                },
                "summary": "Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment\nLarge Language Models' (LLMs) reliability. For flexibility, agentic RAG employs\nautonomous, multi-round retrieval and reasoning to resolve queries. Although\nrecent agentic RAG has improved via reinforcement learning, they often incur\nsubstantial token overhead from search and reasoning processes. This trade-off\nprioritizes accuracy over efficiency. To address this issue, this work proposes\nTeaRAG, a token-efficient agentic RAG framework capable of compressing both\nretrieval content and reasoning steps. 1) First, the retrieved content is\ncompressed by augmenting chunk-based semantic retrieval with a graph retrieval\nusing concise triplets. A knowledge association graph is then built from\nsemantic similarity and co-occurrence. Finally, Personalized PageRank is\nleveraged to highlight key knowledge within this graph, reducing the number of\ntokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative\nProcess-aware Direct Preference Optimization (IP-DPO) is proposed.\nSpecifically, our reward function evaluates the knowledge sufficiency by a\nknowledge matching mechanism, while penalizing excessive reasoning steps. This\ndesign can produce high-quality preference-pair datasets, supporting iterative\nDPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the\naverage Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on\nLlama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/TeaRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment\nLarge Language Models' (LLMs) reliability. For flexibility, agentic RAG employs\nautonomous, multi-round retrieval and reasoning to resolve queries. Although\nrecent agentic RAG has improved via reinforcement learning, they often incur\nsubstantial token overhead from search and reasoning processes. This trade-off\nprioritizes accuracy over efficiency. To address this issue, this work proposes\nTeaRAG, a token-efficient agentic RAG framework capable of compressing both\nretrieval content and reasoning steps. 1) First, the retrieved content is\ncompressed by augmenting chunk-based semantic retrieval with a graph retrieval\nusing concise triplets. A knowledge association graph is then built from\nsemantic similarity and co-occurrence. Finally, Personalized PageRank is\nleveraged to highlight key knowledge within this graph, reducing the number of\ntokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative\nProcess-aware Direct Preference Optimization (IP-DPO) is proposed.\nSpecifically, our reward function evaluates the knowledge sufficiency by a\nknowledge matching mechanism, while penalizing excessive reasoning steps. This\ndesign can produce high-quality preference-pair datasets, supporting iterative\nDPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the\naverage Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on\nLlama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/TeaRAG."
                },
                "authors": [
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Derong Xu"
                    },
                    {
                        "name": "Haoxin Zhang"
                    },
                    {
                        "name": "Yuanjie Lyu"
                    },
                    {
                        "name": "Yuhao Chen"
                    },
                    {
                        "name": "Shuochen Liu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05383v1",
                "updated": "2025-11-07T16:05:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    5,
                    21,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T16:05:21Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    5,
                    21,
                    4,
                    311,
                    0
                ],
                "title": "Connectomics Informed by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connectomics Informed by Large Language Models"
                },
                "summary": "Tractography is a unique method for mapping white matter connections in the\nbrain, but tractography algorithms suffer from an inherent trade-off between\nsensitivity and specificity that limits accuracy. Incorporating prior knowledge\nof white matter anatomy is an effective strategy for improving accuracy and has\nbeen successful for reducing false positives and false negatives in\nbundle-mapping protocols. However, it is challenging to scale this approach for\nconnectomics due to the difficulty in synthesising information relating to many\nthousands of possible connections. In this work, we develop and evaluate a\npipeline using large language models (LLMs) to generate quantitative priors for\nconnectomics, based on their knowledge of neuroanatomy. We benchmark our\napproach against an evaluation set derived from a gold-standard tractography\natlas, identifying prompting techniques to elicit accurate connectivity\ninformation from the LLMs. We further identify strategies for incorporating\nexternal knowledge sources into the pipeline, which can provide grounding for\nthe LLM and improve accuracy. Finally, we demonstrate how the LLM-derived\npriors can augment existing tractography filtering approaches by identifying\ntrue-positive connections to retain during the filtering process. We show that\nthese additional connections can improve the accuracy of a connectome-based\nmodel of pathology spread, which provides supporting evidence that the\nconnections preserved by the LLM are valid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tractography is a unique method for mapping white matter connections in the\nbrain, but tractography algorithms suffer from an inherent trade-off between\nsensitivity and specificity that limits accuracy. Incorporating prior knowledge\nof white matter anatomy is an effective strategy for improving accuracy and has\nbeen successful for reducing false positives and false negatives in\nbundle-mapping protocols. However, it is challenging to scale this approach for\nconnectomics due to the difficulty in synthesising information relating to many\nthousands of possible connections. In this work, we develop and evaluate a\npipeline using large language models (LLMs) to generate quantitative priors for\nconnectomics, based on their knowledge of neuroanatomy. We benchmark our\napproach against an evaluation set derived from a gold-standard tractography\natlas, identifying prompting techniques to elicit accurate connectivity\ninformation from the LLMs. We further identify strategies for incorporating\nexternal knowledge sources into the pipeline, which can provide grounding for\nthe LLM and improve accuracy. Finally, we demonstrate how the LLM-derived\npriors can augment existing tractography filtering approaches by identifying\ntrue-positive connections to retain during the filtering process. We show that\nthese additional connections can improve the accuracy of a connectome-based\nmodel of pathology spread, which provides supporting evidence that the\nconnections preserved by the LLM are valid."
                },
                "authors": [
                    {
                        "name": "Elinor Thompson"
                    },
                    {
                        "name": "Tiantian He"
                    },
                    {
                        "name": "Anna Schroder"
                    },
                    {
                        "name": "Ahmed Abdulaal"
                    },
                    {
                        "name": "Alec Sargood"
                    },
                    {
                        "name": "Sonja Soskic"
                    },
                    {
                        "name": "Henry F. J. Tregidgo"
                    },
                    {
                        "name": "Daniel C. Alexander"
                    }
                ],
                "author_detail": {
                    "name": "Daniel C. Alexander"
                },
                "author": "Daniel C. Alexander",
                "arxiv_comment": "35 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05378v1",
                "updated": "2025-11-07T16:01:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    1,
                    38,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T16:01:38Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    1,
                    38,
                    4,
                    311,
                    0
                ],
                "title": "A scaling relationship for non-thermal radio emission from ordered\n  magnetospheres - II. Investigating the efficiency of relativistic electron\n  production in magnetospheres of BA-type stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A scaling relationship for non-thermal radio emission from ordered\n  magnetospheres - II. Investigating the efficiency of relativistic electron\n  production in magnetospheres of BA-type stars"
                },
                "summary": "Magnetic BA stars host dipole-like magnetospheres. When detected as radio\nsources, their luminosities correlate with the magnetic field and rotation.\nRotation is crucial because the mechanism undergirding the relativistic\nelectron production is powered by centrifugal breakouts. CBOs occur wherever\nmagnetic tension does not balance centrifugal force; the resulting magnetic\nreconnection provides particle acceleration. To investigate how physical\nconditions at the site of the CBOs affect the efficiency of the acceleration\nmechanism, we broadly explore the parameter space governing radio emission by\nincreasing the sample of radio-loud magnetic stars. High-sensitivity VLA\nobservations of 32 stars were performed in the hope of identifying new\ncentrifugal magnetospheres and associated CBOs. We calculated gyro-synchrotron\nspectra using 3D modeling of a dipole-shaped magnetosphere. We evaluated\ncombinations of parameters. The number of relativistic electrons was\nconstrained by the need to produce the emission level predicted by the scaling\nrelationship for the radio emission from magnetic BA stars. About half of the\nobserved stars were detected, with luminosities in agreement with the expected\nvalues, reinforcing the robust nature of the scaling relationship for\nCBO-powered radio emission. Comparing the competing centrifugal and magnetic\neffects on plasma locked in a rigidly rotating magnetosphere, we located the\nsite of CBOs and inferred the local plasma density. We then estimated the\nefficiency of the acceleration mechanism needed to produce enough non-thermal\nelectrons to support the radio emission level. Given a constant acceleration\nefficiency, relativistic electrons represent a fixed fraction of the local\nthermal plasma. Thus, dense magnetospheres host more energetic particles than\nless dense ones; consequently, with other parameters similar, they are\nintrinsically brighter radio sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic BA stars host dipole-like magnetospheres. When detected as radio\nsources, their luminosities correlate with the magnetic field and rotation.\nRotation is crucial because the mechanism undergirding the relativistic\nelectron production is powered by centrifugal breakouts. CBOs occur wherever\nmagnetic tension does not balance centrifugal force; the resulting magnetic\nreconnection provides particle acceleration. To investigate how physical\nconditions at the site of the CBOs affect the efficiency of the acceleration\nmechanism, we broadly explore the parameter space governing radio emission by\nincreasing the sample of radio-loud magnetic stars. High-sensitivity VLA\nobservations of 32 stars were performed in the hope of identifying new\ncentrifugal magnetospheres and associated CBOs. We calculated gyro-synchrotron\nspectra using 3D modeling of a dipole-shaped magnetosphere. We evaluated\ncombinations of parameters. The number of relativistic electrons was\nconstrained by the need to produce the emission level predicted by the scaling\nrelationship for the radio emission from magnetic BA stars. About half of the\nobserved stars were detected, with luminosities in agreement with the expected\nvalues, reinforcing the robust nature of the scaling relationship for\nCBO-powered radio emission. Comparing the competing centrifugal and magnetic\neffects on plasma locked in a rigidly rotating magnetosphere, we located the\nsite of CBOs and inferred the local plasma density. We then estimated the\nefficiency of the acceleration mechanism needed to produce enough non-thermal\nelectrons to support the radio emission level. Given a constant acceleration\nefficiency, relativistic electrons represent a fixed fraction of the local\nthermal plasma. Thus, dense magnetospheres host more energetic particles than\nless dense ones; consequently, with other parameters similar, they are\nintrinsically brighter radio sources."
                },
                "authors": [
                    {
                        "name": "P. Leto"
                    },
                    {
                        "name": "S. Owocki"
                    },
                    {
                        "name": "C. Trigilio"
                    },
                    {
                        "name": "F. Cavallaro"
                    },
                    {
                        "name": "B. Das"
                    },
                    {
                        "name": "M. E. Shultz"
                    },
                    {
                        "name": "C. S. Buemi"
                    },
                    {
                        "name": "G. Umana"
                    },
                    {
                        "name": "L. Fossati"
                    },
                    {
                        "name": "R. Ignace"
                    },
                    {
                        "name": "J. Krticka"
                    },
                    {
                        "name": "L. M. Oskinova"
                    },
                    {
                        "name": "I. Pillitteri"
                    },
                    {
                        "name": "C. Bordiu"
                    },
                    {
                        "name": "F. Bufano"
                    },
                    {
                        "name": "L. Cerrigone"
                    },
                    {
                        "name": "A. Ingallinera"
                    },
                    {
                        "name": "S. Loru"
                    },
                    {
                        "name": "S. Riggi"
                    },
                    {
                        "name": "A. C. Ruggeri"
                    },
                    {
                        "name": "A. ud-Doula"
                    },
                    {
                        "name": "F. Leone"
                    }
                ],
                "author_detail": {
                    "name": "F. Leone"
                },
                "author": "F. Leone",
                "arxiv_comment": "Under review for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04576v2",
                "updated": "2025-11-07T15:58:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    58,
                    37,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-04T18:01:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    18,
                    1,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "Communication-Efficient Collaborative LLM Inference via Distributed\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication-Efficient Collaborative LLM Inference via Distributed\n  Speculative Decoding"
                },
                "summary": "Speculative decoding is an emerging technique that accelerates large language\nmodel (LLM) inference by allowing a smaller draft model to predict multiple\ntokens in advance, which are then verified or corrected by a larger target\nmodel. In AI-native radio access networks (AI-RAN), this paradigm is\nwell-suited for collaborative inference between resource-constrained end\ndevices and more capable edge servers or base stations (BSs). However, existing\ndistributed speculative decoding requires transmitting the full vocabulary\nprobability distribution from the draft model on the device to the target model\nat the BS, which leads to prohibitive uplink communication overhead. To address\nthis issue, we propose a ``Top-K Sparse Logits Transmission (TK-SLT)`` scheme,\nwhere the draft model transmits only the top-K token raw probabilities and the\ncorresponding token indices instead of the entire distribution. This approach\nsignificantly reduces bandwidth consumption while maintaining inference\nperformance. We further derive an analytical expression for the optimal draft\nlength that maximizes inference throughput, and provide a theoretical analysis\nof the achievable speedup ratio under TK-SLT. Experimental results validate\nboth the efficiency and effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an emerging technique that accelerates large language\nmodel (LLM) inference by allowing a smaller draft model to predict multiple\ntokens in advance, which are then verified or corrected by a larger target\nmodel. In AI-native radio access networks (AI-RAN), this paradigm is\nwell-suited for collaborative inference between resource-constrained end\ndevices and more capable edge servers or base stations (BSs). However, existing\ndistributed speculative decoding requires transmitting the full vocabulary\nprobability distribution from the draft model on the device to the target model\nat the BS, which leads to prohibitive uplink communication overhead. To address\nthis issue, we propose a ``Top-K Sparse Logits Transmission (TK-SLT)`` scheme,\nwhere the draft model transmits only the top-K token raw probabilities and the\ncorresponding token indices instead of the entire distribution. This approach\nsignificantly reduces bandwidth consumption while maintaining inference\nperformance. We further derive an analytical expression for the optimal draft\nlength that maximizes inference throughput, and provide a theoretical analysis\nof the achievable speedup ratio under TK-SLT. Experimental results validate\nboth the efficiency and effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "arxiv_comment": "Accepted in the Seventeenth International Conference on Wireless\n  Communications and Signal Processing Oct. 23-25, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05368v1",
                "updated": "2025-11-07T15:54:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    54,
                    31,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:54:31Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    54,
                    31,
                    4,
                    311,
                    0
                ],
                "title": "Near-Efficient and Non-Asymptotic Multiway Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Efficient and Non-Asymptotic Multiway Inference"
                },
                "summary": "We establish non-asymptotic efficiency guarantees for tensor\ndecomposition-based inference in count data models. Under a Poisson framework,\nwe consider two related goals: (i) parametric inference, the estimation of the\nfull distributional parameter tensor, and (ii) multiway analysis, the recovery\nof its canonical polyadic (CP) decomposition factors. Our main result shows\nthat in the rank-one setting, a rank-constrained maximum-likelihood estimator\nachieves multiway analysis with variance matching the Cram\\'{e}r-Rao Lower\nBound (CRLB) up to absolute constants and logarithmic factors. This provides a\ngeneral framework for studying \"near-efficient\" multiway estimators in\nfinite-sample settings. For higher ranks, we illustrate that our multiway\nestimator may not attain the CRLB; nevertheless, CP-based parametric inference\nremains nearly minimax optimal, with error bounds that improve on prior work by\noffering more favorable dependence on the CP rank. Numerical experiments\ncorroborate near-efficiency in the rank-one case and highlight the efficiency\ngap in higher-rank scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We establish non-asymptotic efficiency guarantees for tensor\ndecomposition-based inference in count data models. Under a Poisson framework,\nwe consider two related goals: (i) parametric inference, the estimation of the\nfull distributional parameter tensor, and (ii) multiway analysis, the recovery\nof its canonical polyadic (CP) decomposition factors. Our main result shows\nthat in the rank-one setting, a rank-constrained maximum-likelihood estimator\nachieves multiway analysis with variance matching the Cram\\'{e}r-Rao Lower\nBound (CRLB) up to absolute constants and logarithmic factors. This provides a\ngeneral framework for studying \"near-efficient\" multiway estimators in\nfinite-sample settings. For higher ranks, we illustrate that our multiway\nestimator may not attain the CRLB; nevertheless, CP-based parametric inference\nremains nearly minimax optimal, with error bounds that improve on prior work by\noffering more favorable dependence on the CP rank. Numerical experiments\ncorroborate near-efficiency in the rank-one case and highlight the efficiency\ngap in higher-rank scenarios."
                },
                "authors": [
                    {
                        "name": "Oscar López"
                    },
                    {
                        "name": "Arvind Prasadan"
                    },
                    {
                        "name": "Carlos Llosa-Vite"
                    },
                    {
                        "name": "Richard B. Lehoucq"
                    },
                    {
                        "name": "Daniel M. Dunlavy"
                    }
                ],
                "author_detail": {
                    "name": "Daniel M. Dunlavy"
                },
                "author": "Daniel M. Dunlavy",
                "arxiv_comment": "28 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F99 (Primary) 15A69 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05367v1",
                "updated": "2025-11-07T15:53:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    53,
                    55,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:53:55Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    53,
                    55,
                    4,
                    311,
                    0
                ],
                "title": "Linking Warm Dark Matter to Merger Tree Histories via Deep Learning\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linking Warm Dark Matter to Merger Tree Histories via Deep Learning\n  Networks"
                },
                "summary": "Dark matter (DM) halos form hierarchically in the Universe through a series\nof merger events. Cosmological simulations can represent this series of mergers\nas a graph-like ``tree'' structure. Previous work has shown these merger trees\nare sensitive to cosmology simulation parameters, but as DM structures, the\noutstanding question of their sensitivity to DM models remains unanswered. In\nthis work, we investigate the feasibility of deep learning methods trained on\nmerger trees to infer Warm Dark Matter (WDM) particles masses from the DREAMS\nsimulation suite. We organize the merger trees from 1,024 zoom-in simulations\ninto graphs with nodes representing the merger history of galaxies and edges\ndenoting hereditary links. We vary the complexity of the node features included\nin the graphs ranging from a single node feature up through an array of several\ngalactic properties (e.g., halo mass, star formation rate, etc.). We train a\nGraph Neural Network (GNN) to predict the WDM mass using the graph\nrepresentation of the merger tree as input. We find that the GNN can predict\nthe mass of the WDM particle ($R^2$ from 0.07 to 0.95), with success depending\non the graph complexity and node features. We extend the same methods to\nsupernovae and active galactic nuclei feedback parameters $A_\\text{SN1}$,\n$A_\\text{SN2}$, and $A_\\text{AGN}$, successfully inferring the supernovae\nparameters. The GNN can even infer the WDM mass from merger tree histories\nwithout any node features, indicating that the structure of merger trees alone\ninherits information about the cosmological parameters of the simulations from\nwhich they form.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark matter (DM) halos form hierarchically in the Universe through a series\nof merger events. Cosmological simulations can represent this series of mergers\nas a graph-like ``tree'' structure. Previous work has shown these merger trees\nare sensitive to cosmology simulation parameters, but as DM structures, the\noutstanding question of their sensitivity to DM models remains unanswered. In\nthis work, we investigate the feasibility of deep learning methods trained on\nmerger trees to infer Warm Dark Matter (WDM) particles masses from the DREAMS\nsimulation suite. We organize the merger trees from 1,024 zoom-in simulations\ninto graphs with nodes representing the merger history of galaxies and edges\ndenoting hereditary links. We vary the complexity of the node features included\nin the graphs ranging from a single node feature up through an array of several\ngalactic properties (e.g., halo mass, star formation rate, etc.). We train a\nGraph Neural Network (GNN) to predict the WDM mass using the graph\nrepresentation of the merger tree as input. We find that the GNN can predict\nthe mass of the WDM particle ($R^2$ from 0.07 to 0.95), with success depending\non the graph complexity and node features. We extend the same methods to\nsupernovae and active galactic nuclei feedback parameters $A_\\text{SN1}$,\n$A_\\text{SN2}$, and $A_\\text{AGN}$, successfully inferring the supernovae\nparameters. The GNN can even infer the WDM mass from merger tree histories\nwithout any node features, indicating that the structure of merger trees alone\ninherits information about the cosmological parameters of the simulations from\nwhich they form."
                },
                "authors": [
                    {
                        "name": "Ilem Leisher"
                    },
                    {
                        "name": "Paul Torrey"
                    },
                    {
                        "name": "Alex M. Garcia"
                    },
                    {
                        "name": "Jonah C. Rose"
                    },
                    {
                        "name": "Francisco Villaescusa-Navarro"
                    },
                    {
                        "name": "Zachary Lubberts"
                    },
                    {
                        "name": "Arya Farahi"
                    },
                    {
                        "name": "Stephanie O'Neil"
                    },
                    {
                        "name": "Xuejian Shen"
                    },
                    {
                        "name": "Olivia Mostow"
                    },
                    {
                        "name": "Nitya Kallivayalil"
                    },
                    {
                        "name": "Dhruv Zimmerman"
                    },
                    {
                        "name": "Desika Narayanan"
                    },
                    {
                        "name": "Mark Vogelsberger"
                    }
                ],
                "author_detail": {
                    "name": "Mark Vogelsberger"
                },
                "author": "Mark Vogelsberger",
                "arxiv_comment": "20 pages, 9 figures, submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10335v2",
                "updated": "2025-11-07T15:53:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    53,
                    49,
                    4,
                    311,
                    0
                ],
                "published": "2025-04-14T15:44:45Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    44,
                    45,
                    0,
                    104,
                    0
                ],
                "title": "MorphTok: Morphologically Grounded Tokenization for Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MorphTok: Morphologically Grounded Tokenization for Indian Languages"
                },
                "summary": "Tokenization is a crucial step in NLP, especially with the rise of large\nlanguage models (LLMs), impacting downstream performance, computational cost,\nand efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE)\nalgorithm for subword tokenization that greedily merges frequent character\nbigrams, often leading to segmentation that does not align with linguistically\nmeaningful units. To address this, we propose morphology-aware segmentation as\na pre-tokenization step before applying BPE. To facilitate morphology-aware\nsegmentation, we create a novel dataset for Hindi and Marathi, incorporating\nsandhi splitting to enhance the subword tokenization. Experiments on downstream\ntasks show that morphologically grounded tokenization improves machine\ntranslation and language modeling performance. Additionally, to handle the\ndependent vowels common in syllable-based writing systems used by Indic\nlanguages, we propose Constrained BPE (CBPE), an extension to the standard BPE\nalgorithm incorporating script-specific constraints. In particular, CBPE\nhandles dependent vowels to form a cohesive unit with other characters instead\nof occurring as a single unit. Our results show that CBPE achieves a 1.68\\%\nreduction in fertility scores while maintaining comparable or improved\ndownstream performance in machine translation and language modeling, offering a\ncomputationally efficient alternative to standard BPE. Moreover, to evaluate\nsegmentation across different tokenization algorithms, we introduce a new human\nevaluation metric, \\textit{EvalTok}, enabling more human-grounded assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a crucial step in NLP, especially with the rise of large\nlanguage models (LLMs), impacting downstream performance, computational cost,\nand efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE)\nalgorithm for subword tokenization that greedily merges frequent character\nbigrams, often leading to segmentation that does not align with linguistically\nmeaningful units. To address this, we propose morphology-aware segmentation as\na pre-tokenization step before applying BPE. To facilitate morphology-aware\nsegmentation, we create a novel dataset for Hindi and Marathi, incorporating\nsandhi splitting to enhance the subword tokenization. Experiments on downstream\ntasks show that morphologically grounded tokenization improves machine\ntranslation and language modeling performance. Additionally, to handle the\ndependent vowels common in syllable-based writing systems used by Indic\nlanguages, we propose Constrained BPE (CBPE), an extension to the standard BPE\nalgorithm incorporating script-specific constraints. In particular, CBPE\nhandles dependent vowels to form a cohesive unit with other characters instead\nof occurring as a single unit. Our results show that CBPE achieves a 1.68\\%\nreduction in fertility scores while maintaining comparable or improved\ndownstream performance in machine translation and language modeling, offering a\ncomputationally efficient alternative to standard BPE. Moreover, to evaluate\nsegmentation across different tokenization algorithms, we introduce a new human\nevaluation metric, \\textit{EvalTok}, enabling more human-grounded assessment."
                },
                "authors": [
                    {
                        "name": "Maharaj Brahma"
                    },
                    {
                        "name": "N J Karthika"
                    },
                    {
                        "name": "Atul Singh"
                    },
                    {
                        "name": "Devaraj Adiga"
                    },
                    {
                        "name": "Smruti Bhate"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    },
                    {
                        "name": "Rohit Saluja"
                    },
                    {
                        "name": "Maunendra Sankar Desarkar"
                    }
                ],
                "author_detail": {
                    "name": "Maunendra Sankar Desarkar"
                },
                "author": "Maunendra Sankar Desarkar",
                "arxiv_comment": "Accepted at Tokenization Workshop (TokShop), ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05366v1",
                "updated": "2025-11-07T15:53:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    53,
                    10,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:53:10Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    53,
                    10,
                    4,
                    311,
                    0
                ],
                "title": "Coarse-graining nonequilibrium diffusions with Markov chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-graining nonequilibrium diffusions with Markov chains"
                },
                "summary": "We investigate nonequilibrium steady-state dynamics in both continuous- and\ndiscrete-state stochastic processes. Our analysis focuses on planar diffusion\ndynamics and their coarse-grained approximations by discrete-state Markov\nchains. Using finite-volume approximations, we derive an approximate master\nequation directly from the underlying diffusion and show that this\ndiscretisation preserves key features of the nonequilibrium steady-state. In\nparticular, we show that the entropy production rate of the approximation\nconverges as the number of discrete states goes to the limit. These results are\nillustrated with analytically solvable diffusions and numerical experiments on\nnonlinear processes, demonstrating how this approach can be used to explore the\ndependence of entropy production rate on model parameters. Finally, we address\nthe problem of inferring discrete-state Markov models from continuous\nstochastic trajectories. We show that discrete-state models significantly\nunderestimate the true entropy production rate. However, we also show that they\ncan provide tests to determine if a stationary planar diffusion is out of\nequilibrium. This property is illustrated with both simulated data and\nempirical trajectories from schooling fish.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate nonequilibrium steady-state dynamics in both continuous- and\ndiscrete-state stochastic processes. Our analysis focuses on planar diffusion\ndynamics and their coarse-grained approximations by discrete-state Markov\nchains. Using finite-volume approximations, we derive an approximate master\nequation directly from the underlying diffusion and show that this\ndiscretisation preserves key features of the nonequilibrium steady-state. In\nparticular, we show that the entropy production rate of the approximation\nconverges as the number of discrete states goes to the limit. These results are\nillustrated with analytically solvable diffusions and numerical experiments on\nnonlinear processes, demonstrating how this approach can be used to explore the\ndependence of entropy production rate on model parameters. Finally, we address\nthe problem of inferring discrete-state Markov models from continuous\nstochastic trajectories. We show that discrete-state models significantly\nunderestimate the true entropy production rate. However, we also show that they\ncan provide tests to determine if a stationary planar diffusion is out of\nequilibrium. This property is illustrated with both simulated data and\nempirical trajectories from schooling fish."
                },
                "authors": [
                    {
                        "name": "Ramón Nartallo-Kaluarachchi"
                    },
                    {
                        "name": "Renaud Lambiotte"
                    },
                    {
                        "name": "Alain Goriely"
                    }
                ],
                "author_detail": {
                    "name": "Alain Goriely"
                },
                "author": "Alain Goriely",
                "arxiv_comment": "22 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05365v1",
                "updated": "2025-11-07T15:52:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    52,
                    53,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:52:53Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    52,
                    53,
                    4,
                    311,
                    0
                ],
                "title": "Mapping the positions of Two-Level-Systems on the surface of a\n  superconducting transmon qubit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping the positions of Two-Level-Systems on the surface of a\n  superconducting transmon qubit"
                },
                "summary": "The coherence of superconducting quantum computers is severely limited by\nmaterial defects that create parasitic two-level-systems (TLS). Progress is\ncomplicated by lacking understanding how TLS are created and in which parts of\na qubit circuit they are most detrimental. Here, we present a method to\ndetermine the individual positions of TLS at the surface of a transmon qubit.\nWe employ a set of on-chip gate electrodes near the qubit to generate local DC\nelectric fields that are used to tune the TLS' resonance frequencies. The TLS\nposition is inferred from the strengths at which TLS couple to different\nelectrodes and comparing them to electric field simulations. We found that the\nmajority of detectable surface-TLS was residing on the leads of the qubit's\nJosephson junction, despite the dominant contribution of its coplanar capacitor\nto electric field energy and surface area. This indicates that the TLS density\nis significantly enhanced near shadow-evaporated electrodes fabricated by\nlift-off techniques. Our method is useful to identify critical circuit regions\nwhere TLS contribute most to decoherence, and can guide improvements in qubit\ndesign and fabrication methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coherence of superconducting quantum computers is severely limited by\nmaterial defects that create parasitic two-level-systems (TLS). Progress is\ncomplicated by lacking understanding how TLS are created and in which parts of\na qubit circuit they are most detrimental. Here, we present a method to\ndetermine the individual positions of TLS at the surface of a transmon qubit.\nWe employ a set of on-chip gate electrodes near the qubit to generate local DC\nelectric fields that are used to tune the TLS' resonance frequencies. The TLS\nposition is inferred from the strengths at which TLS couple to different\nelectrodes and comparing them to electric field simulations. We found that the\nmajority of detectable surface-TLS was residing on the leads of the qubit's\nJosephson junction, despite the dominant contribution of its coplanar capacitor\nto electric field energy and surface area. This indicates that the TLS density\nis significantly enhanced near shadow-evaporated electrodes fabricated by\nlift-off techniques. Our method is useful to identify critical circuit regions\nwhere TLS contribute most to decoherence, and can guide improvements in qubit\ndesign and fabrication methods."
                },
                "authors": [
                    {
                        "name": "Jürgen Lisenfeld"
                    },
                    {
                        "name": "Alexander K. Händel"
                    },
                    {
                        "name": "Etienne Daum"
                    },
                    {
                        "name": "Benedikt Berlitz"
                    },
                    {
                        "name": "Alexander Bilmes"
                    },
                    {
                        "name": "Alexey V. Ustinov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey V. Ustinov"
                },
                "author": "Alexey V. Ustinov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05352v1",
                "updated": "2025-11-07T15:45:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    45,
                    13,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:45:13Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    45,
                    13,
                    4,
                    311,
                    0
                ],
                "title": "A Latent-Variable Formulation of the Poisson Canonical Polyadic Tensor\n  Model: Maximum Likelihood Estimation and Fisher Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Latent-Variable Formulation of the Poisson Canonical Polyadic Tensor\n  Model: Maximum Likelihood Estimation and Fisher Information"
                },
                "summary": "We establish parameter inference for the Poisson canonical polyadic (PCP)\ntensor model through a latent-variable formulation. Our approach exploits the\nobservation that any random PCP tensor can be derived by marginalizing an\nunobservable random tensor of one dimension larger. The loglikelihood of this\nlarger dimensional tensor, referred to as the \"complete\" loglikelihood, is\ncomprised of multiple rank one PCP loglikelihoods. Using this methodology, we\nfirst derive non-iterative maximum likelihood estimators for the PCP model and\ndemonstrate that several existing algorithms for fitting non-negative matrix\nand tensor factorizations are Expectation-Maximization algorithms. Next, we\nderive the observed and expected Fisher information matrices for the PCP model.\nThe Fisher information provides us crucial insights into the well-posedness of\nthe tensor model, such as the role that tensor rank plays in identifiability\nand indeterminacy. For the special case of rank one PCP models, we demonstrate\nthat these results are greatly simplified.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We establish parameter inference for the Poisson canonical polyadic (PCP)\ntensor model through a latent-variable formulation. Our approach exploits the\nobservation that any random PCP tensor can be derived by marginalizing an\nunobservable random tensor of one dimension larger. The loglikelihood of this\nlarger dimensional tensor, referred to as the \"complete\" loglikelihood, is\ncomprised of multiple rank one PCP loglikelihoods. Using this methodology, we\nfirst derive non-iterative maximum likelihood estimators for the PCP model and\ndemonstrate that several existing algorithms for fitting non-negative matrix\nand tensor factorizations are Expectation-Maximization algorithms. Next, we\nderive the observed and expected Fisher information matrices for the PCP model.\nThe Fisher information provides us crucial insights into the well-posedness of\nthe tensor model, such as the role that tensor rank plays in identifiability\nand indeterminacy. For the special case of rank one PCP models, we demonstrate\nthat these results are greatly simplified."
                },
                "authors": [
                    {
                        "name": "Carlos Llosa-Vite"
                    },
                    {
                        "name": "Daniel M. Dunlavy"
                    },
                    {
                        "name": "Richard B. Lehoucq"
                    },
                    {
                        "name": "Oscar López"
                    },
                    {
                        "name": "Arvind Prasadan"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Prasadan"
                },
                "author": "Arvind Prasadan",
                "arxiv_comment": "24 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F99 (Primary) 15A69 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05347v1",
                "updated": "2025-11-07T15:41:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    41,
                    27,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:41:27Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    41,
                    27,
                    4,
                    311,
                    0
                ],
                "title": "Efficient CNN Inference on Ultra-Low-Power MCUs via Saturation-Aware\n  Convolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient CNN Inference on Ultra-Low-Power MCUs via Saturation-Aware\n  Convolution"
                },
                "summary": "Deploying lightweight CNN inference tasks on ultra-low-power MCUs is often\nnot limited by space constraint, thanks to the compact size of models, yet\ninference latency is crucial for preserving energy. We reveal that quantized\nCNN inference on ultra-low-power MCUs executes unnecessary computations in\nneurons that produce saturated output values: often times, these neurons still\nproduce the correct output value without fully completing the computation,\nsince the neuron value is too extreme and is eventually systematically clamped\nat the boundaries allowed by the neuron. We show that with carefully designed\ncondition checks, it is possible to identify and skip these unnecessary\ncomputations without impacting the neuron output. Based on this, we present\nsaturation-aware convolution: an inference technique whereby computations in\nconvolution kernels are executed in an altered order to induce earlier\nsaturation, and saturation checks are inserted to omit unnecessary\ncomputations. We integrate our implementation into MCUNet's TinyEngine, the\nstate-of-the-art neural network code generation and inference framework, and\nconduct experiments on a Cortex-M0+ MCU. The result based on 7 open-source CNN\nmodels displays up to 24% inference time saving, with strictly zero impact on\nneural network accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying lightweight CNN inference tasks on ultra-low-power MCUs is often\nnot limited by space constraint, thanks to the compact size of models, yet\ninference latency is crucial for preserving energy. We reveal that quantized\nCNN inference on ultra-low-power MCUs executes unnecessary computations in\nneurons that produce saturated output values: often times, these neurons still\nproduce the correct output value without fully completing the computation,\nsince the neuron value is too extreme and is eventually systematically clamped\nat the boundaries allowed by the neuron. We show that with carefully designed\ncondition checks, it is possible to identify and skip these unnecessary\ncomputations without impacting the neuron output. Based on this, we present\nsaturation-aware convolution: an inference technique whereby computations in\nconvolution kernels are executed in an altered order to induce earlier\nsaturation, and saturation checks are inserted to omit unnecessary\ncomputations. We integrate our implementation into MCUNet's TinyEngine, the\nstate-of-the-art neural network code generation and inference framework, and\nconduct experiments on a Cortex-M0+ MCU. The result based on 7 open-source CNN\nmodels displays up to 24% inference time saving, with strictly zero impact on\nneural network accuracy."
                },
                "authors": [
                    {
                        "name": "Shiming Li"
                    },
                    {
                        "name": "Luca Mottola"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Stefanos Kaxiras"
                    }
                ],
                "author_detail": {
                    "name": "Stefanos Kaxiras"
                },
                "author": "Stefanos Kaxiras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18312v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18312v3",
                "updated": "2025-11-07T15:28:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    28,
                    43,
                    4,
                    311,
                    0
                ],
                "published": "2025-08-23T16:00:30Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    16,
                    0,
                    30,
                    5,
                    235,
                    0
                ],
                "title": "What Matters in Data for DPO?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Matters in Data for DPO?"
                },
                "summary": "Direct Preference Optimization (DPO) has emerged as a simple and effective\napproach for aligning large language models (LLMs) with human preferences,\nbypassing the need for a learned reward model. Despite its growing adoption, a\nfundamental question remains open: what characteristics of preference data are\nmost critical for DPO performance? In this work, we provide a systematic study\nof how preference data distribution influences DPO, from both theoretical and\nempirical perspectives. We show that the quality of chosen responses plays a\ndominant role in optimizing the DPO objective, while the quality of rejected\nresponses may have relatively limited impact. Our theoretical analysis\ncharacterizes the optimal response distribution under DPO and reveals how\ncontrastiveness between responses helps primarily by improving the chosen\nsamples. We further study an online DPO setting and show it effectively reduces\nto supervised fine-tuning on the chosen responses. Extensive experiments across\ndiverse tasks confirm our findings: improving the quality of chosen responses\nconsistently boosts performance regardless of the quality of the rejected\nresponses. We also investigate the benefit of mixing the on-policy data. Our\nresults interpret the mechanism behind some widely adopted strategies and offer\npractical insights for constructing high-impact preference datasets for LLM\nalignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has emerged as a simple and effective\napproach for aligning large language models (LLMs) with human preferences,\nbypassing the need for a learned reward model. Despite its growing adoption, a\nfundamental question remains open: what characteristics of preference data are\nmost critical for DPO performance? In this work, we provide a systematic study\nof how preference data distribution influences DPO, from both theoretical and\nempirical perspectives. We show that the quality of chosen responses plays a\ndominant role in optimizing the DPO objective, while the quality of rejected\nresponses may have relatively limited impact. Our theoretical analysis\ncharacterizes the optimal response distribution under DPO and reveals how\ncontrastiveness between responses helps primarily by improving the chosen\nsamples. We further study an online DPO setting and show it effectively reduces\nto supervised fine-tuning on the chosen responses. Extensive experiments across\ndiverse tasks confirm our findings: improving the quality of chosen responses\nconsistently boosts performance regardless of the quality of the rejected\nresponses. We also investigate the benefit of mixing the on-policy data. Our\nresults interpret the mechanism behind some widely adopted strategies and offer\npractical insights for constructing high-impact preference datasets for LLM\nalignment."
                },
                "authors": [
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Zhongze Cai"
                    },
                    {
                        "name": "Guanting Chen"
                    },
                    {
                        "name": "Huaiyang Zhong"
                    },
                    {
                        "name": "Chonghuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chonghuan Wang"
                },
                "author": "Chonghuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18312v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18312v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05331v1",
                "updated": "2025-11-07T15:28:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    28,
                    35,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:28:35Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    28,
                    35,
                    4,
                    311,
                    0
                ],
                "title": "EMPEROR I. Exoplanet MCMC parallel tempering for RV orbit retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMPEROR I. Exoplanet MCMC parallel tempering for RV orbit retrieval"
                },
                "summary": "We present \\texttt{EMPEROR}, an open-source Python framework designed for\nefficient exoplanet detection and characterisation with radial velocities (RV).\n\\texttt{EMPEROR} integrates Dynamic Nested Sampling (DNS) and Adaptive Parallel\nTempering (APT) Markov Chain Monte Carlo (MCMC), supporting multiple noise\nmodels such as Gaussian Processes (GPs) and Moving Averages (MA). The framework\nenables systematic model comparison using statistical metrics, including\nBayesian evidence ($\\ln{\\mathcal{Z}}$) and Bayesian Information Criterion\n(BIC), while providing automated, publish-ready visualisations.\n\\texttt{EMPEROR} is evaluated across three distinct systems to assess its\ncapabilities in different detection scenarios. Sampling performance, model\nselection, and the search for Earth-mass planets are evaluated in data for 51\nPegasi, HD 55693 and Barnard's Star (GJ 699). For 51 Pegasi, APT achieves an\neffective sampling increase over DNS by a factor 3.76, while retrieving tighter\nparameter estimates. For HD 55693 the stellar rotation\n$P_{\\text{rot}}=29.72^{+0.01}_{-0.02}$ and magnetic cycle\n$P_{\\text{mag}}=2557.0^{+70.1}_{-36.7}$ are recovered, while demonstrating the\nsensitivity of $\\ln{\\mathcal{Z}}$ to prior selection. For Barnard's star,\nseveral noise models are compared, and the confirmed planet parameters are\nsuccessfully retrieved with all of them. The best model shows a period of\n3.1536$\\pm$0.0003~d, minimum mass of 0.38$\\pm$0.03 M$_{\\rm{\\oplus}}$, and\nsemi-major axis of 0.02315$\\pm$0.00039~AU. Purely statistical inference might\nbe insufficient on its own for robust exoplanet detection. Effective\nmethodologies must integrate domain knowledge, heuristic criteria, and\nmulti-faceted model comparisons. The versatility of \\texttt{EMPEROR} in\nhandling diverse noise structures, its systematic model selection, and its\nimproved performance make it a valuable tool for RV exoplanetary studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \\texttt{EMPEROR}, an open-source Python framework designed for\nefficient exoplanet detection and characterisation with radial velocities (RV).\n\\texttt{EMPEROR} integrates Dynamic Nested Sampling (DNS) and Adaptive Parallel\nTempering (APT) Markov Chain Monte Carlo (MCMC), supporting multiple noise\nmodels such as Gaussian Processes (GPs) and Moving Averages (MA). The framework\nenables systematic model comparison using statistical metrics, including\nBayesian evidence ($\\ln{\\mathcal{Z}}$) and Bayesian Information Criterion\n(BIC), while providing automated, publish-ready visualisations.\n\\texttt{EMPEROR} is evaluated across three distinct systems to assess its\ncapabilities in different detection scenarios. Sampling performance, model\nselection, and the search for Earth-mass planets are evaluated in data for 51\nPegasi, HD 55693 and Barnard's Star (GJ 699). For 51 Pegasi, APT achieves an\neffective sampling increase over DNS by a factor 3.76, while retrieving tighter\nparameter estimates. For HD 55693 the stellar rotation\n$P_{\\text{rot}}=29.72^{+0.01}_{-0.02}$ and magnetic cycle\n$P_{\\text{mag}}=2557.0^{+70.1}_{-36.7}$ are recovered, while demonstrating the\nsensitivity of $\\ln{\\mathcal{Z}}$ to prior selection. For Barnard's star,\nseveral noise models are compared, and the confirmed planet parameters are\nsuccessfully retrieved with all of them. The best model shows a period of\n3.1536$\\pm$0.0003~d, minimum mass of 0.38$\\pm$0.03 M$_{\\rm{\\oplus}}$, and\nsemi-major axis of 0.02315$\\pm$0.00039~AU. Purely statistical inference might\nbe insufficient on its own for robust exoplanet detection. Effective\nmethodologies must integrate domain knowledge, heuristic criteria, and\nmulti-faceted model comparisons. The versatility of \\texttt{EMPEROR} in\nhandling diverse noise structures, its systematic model selection, and its\nimproved performance make it a valuable tool for RV exoplanetary studies."
                },
                "authors": [
                    {
                        "name": "Pablo A. Peña R."
                    },
                    {
                        "name": "James S. Jenkins"
                    }
                ],
                "author_detail": {
                    "name": "James S. Jenkins"
                },
                "author": "James S. Jenkins",
                "arxiv_comment": "Accepted for publication in A&A Sect. 15. Numerical methods and\n  codes. The official acceptance date is 19/10/2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04024v3",
                "updated": "2025-11-07T15:23:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    23,
                    53,
                    4,
                    311,
                    0
                ],
                "published": "2024-04-05T11:07:57Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    11,
                    7,
                    57,
                    4,
                    96,
                    0
                ],
                "title": "Colored Gaussian directed acyclic graphical models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colored Gaussian directed acyclic graphical models"
                },
                "summary": "We study submodels of Gaussian DAG models defined by partial homogeneity\nconstraints imposed on the model error variances and structural coefficients.\nWe represent these models with colored DAGs and investigate their properties\nfor use in statistical and causal inference. Local and global Markov properties\nare provided and shown to characterize the colored DAG model. Additional\nproperties relevant to causal discovery are studied, including the existence\nand non-existence of faithful distributions and structural identifiability.\nExtending prior work of Peters and B\\\"uhlmann and Wu and Drton, we prove\nstructural identifiability under the assumption of homogeneous structural\ncoefficients, as well as for a family of models with partially homogeneous\nstructural coefficients. The latter models, termed BPEC-DAGs, capture\nadditional causal insights by clustering the direct causes of each node into\ncommunities according to their effect on their common target. An analogue of\nthe GES algorithm for learning BPEC-DAGs is given and evaluated on real and\nsynthetic data. Regarding model geometry, we provide a proof of a conjecture of\nSullivant which generalizes to colored DAG models, colored undirected graphical\nmodels and directed ancestral graph models. The proof yields a tool for\nidentification of Markov properties for any rationally parameterized model with\nglobally, rationally identifiable parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study submodels of Gaussian DAG models defined by partial homogeneity\nconstraints imposed on the model error variances and structural coefficients.\nWe represent these models with colored DAGs and investigate their properties\nfor use in statistical and causal inference. Local and global Markov properties\nare provided and shown to characterize the colored DAG model. Additional\nproperties relevant to causal discovery are studied, including the existence\nand non-existence of faithful distributions and structural identifiability.\nExtending prior work of Peters and B\\\"uhlmann and Wu and Drton, we prove\nstructural identifiability under the assumption of homogeneous structural\ncoefficients, as well as for a family of models with partially homogeneous\nstructural coefficients. The latter models, termed BPEC-DAGs, capture\nadditional causal insights by clustering the direct causes of each node into\ncommunities according to their effect on their common target. An analogue of\nthe GES algorithm for learning BPEC-DAGs is given and evaluated on real and\nsynthetic data. Regarding model geometry, we provide a proof of a conjecture of\nSullivant which generalizes to colored DAG models, colored undirected graphical\nmodels and directed ancestral graph models. The proof yields a tool for\nidentification of Markov properties for any rationally parameterized model with\nglobally, rationally identifiable parameters."
                },
                "authors": [
                    {
                        "name": "Tobias Boege"
                    },
                    {
                        "name": "Kaie Kubjas"
                    },
                    {
                        "name": "Pratik Misra"
                    },
                    {
                        "name": "Liam Solus"
                    }
                ],
                "author_detail": {
                    "name": "Liam Solus"
                },
                "author": "Liam Solus",
                "arxiv_comment": "53 pages; v3: another major revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H22 (primary) 62R01, 62D20, 13C70, 13P25 (secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05321v1",
                "updated": "2025-11-07T15:19:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    19,
                    14,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:19:14Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    19,
                    14,
                    4,
                    311,
                    0
                ],
                "title": "MultiVic: A Time-Predictable RISC-V Multi-Core Processor Optimized for\n  Neural Network Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiVic: A Time-Predictable RISC-V Multi-Core Processor Optimized for\n  Neural Network Inference"
                },
                "summary": "Real-time systems, particularly those used in domains like automated driving,\nare increasingly adopting neural networks. From this trend arises the need for\nhigh-performance hardware exhibiting predictable timing behavior. While\nstate-of-the-art real-time hardware often suffers from limited memory and\ncompute resources, modern AI accelerators typically lack the crucial\npredictability due to memory interference.\n  We present a new hardware architecture to bridge this gap between performance\nand predictability. The architecture features a multi-core vector processor\nwith predictable cores, each equipped with local scratchpad memories. A central\nmanagement core orchestrates access to shared external memory following a\nstatically determined schedule.\n  To evaluate the proposed hardware architecture, we analyze different variants\nof our parameterized design. We compare these variants to a baseline\narchitecture consisting of a single-core vector processor with large vector\nregisters. We find that configurations with a larger number of smaller cores\nachieve better performance due to increased effective memory bandwidth and\nhigher clock frequencies. Crucially for real-time systems, execution time\nfluctuation remains very low, demonstrating the platform's time predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time systems, particularly those used in domains like automated driving,\nare increasingly adopting neural networks. From this trend arises the need for\nhigh-performance hardware exhibiting predictable timing behavior. While\nstate-of-the-art real-time hardware often suffers from limited memory and\ncompute resources, modern AI accelerators typically lack the crucial\npredictability due to memory interference.\n  We present a new hardware architecture to bridge this gap between performance\nand predictability. The architecture features a multi-core vector processor\nwith predictable cores, each equipped with local scratchpad memories. A central\nmanagement core orchestrates access to shared external memory following a\nstatically determined schedule.\n  To evaluate the proposed hardware architecture, we analyze different variants\nof our parameterized design. We compare these variants to a baseline\narchitecture consisting of a single-core vector processor with large vector\nregisters. We find that configurations with a larger number of smaller cores\nachieve better performance due to increased effective memory bandwidth and\nhigher clock frequencies. Crucially for real-time systems, execution time\nfluctuation remains very low, demonstrating the platform's time predictability."
                },
                "authors": [
                    {
                        "name": "Maximilian Kirschner"
                    },
                    {
                        "name": "Konstantin Dudzik"
                    },
                    {
                        "name": "Ben Krusekamp"
                    },
                    {
                        "name": "Jürgen Becker"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Becker"
                },
                "author": "Jürgen Becker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05320v1",
                "updated": "2025-11-07T15:17:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    17,
                    45,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:17:45Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    17,
                    45,
                    4,
                    311,
                    0
                ],
                "title": "What Are the Facts? Automated Extraction of Court-Established Facts from\n  Criminal-Court Opinions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Are the Facts? Automated Extraction of Court-Established Facts from\n  Criminal-Court Opinions"
                },
                "summary": "Criminal justice administrative data contain only a limited amount of\ninformation about the committed offense. However, there is an unused source of\nextensive information in continental European courts' decisions: descriptions\nof criminal behaviors in verdicts by which offenders are found guilty. In this\npaper, we study the feasibility of extracting these descriptions from publicly\navailable court decisions from Slovakia. We use two different approaches for\nretrieval: regular expressions and large language models (LLMs). Our baseline\nwas a simple method employing regular expressions to identify typical words\noccurring before and after the description. The advanced regular expression\napproach further focused on \"sparing\" and its normalization (insertion of\nspaces between individual letters), typical for delineating the description.\nThe LLM approach involved prompting the Gemini Flash 2.0 model to extract the\ndescriptions using predefined instructions. Although the baseline identified\ndescriptions in only 40.5% of verdicts, both methods significantly outperformed\nit, achieving 97% with advanced regular expressions and 98.75% with LLMs, and\n99.5% when combined. Evaluation by law students showed that both advanced\nmethods matched human annotations in about 90% of cases, compared to just 34.5%\nfor the baseline. LLMs fully matched human-labeled descriptions in 91.75% of\ninstances, and a combination of advanced regular expressions with LLMs reached\n92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Criminal justice administrative data contain only a limited amount of\ninformation about the committed offense. However, there is an unused source of\nextensive information in continental European courts' decisions: descriptions\nof criminal behaviors in verdicts by which offenders are found guilty. In this\npaper, we study the feasibility of extracting these descriptions from publicly\navailable court decisions from Slovakia. We use two different approaches for\nretrieval: regular expressions and large language models (LLMs). Our baseline\nwas a simple method employing regular expressions to identify typical words\noccurring before and after the description. The advanced regular expression\napproach further focused on \"sparing\" and its normalization (insertion of\nspaces between individual letters), typical for delineating the description.\nThe LLM approach involved prompting the Gemini Flash 2.0 model to extract the\ndescriptions using predefined instructions. Although the baseline identified\ndescriptions in only 40.5% of verdicts, both methods significantly outperformed\nit, achieving 97% with advanced regular expressions and 98.75% with LLMs, and\n99.5% when combined. Evaluation by law students showed that both advanced\nmethods matched human annotations in about 90% of cases, compared to just 34.5%\nfor the baseline. LLMs fully matched human-labeled descriptions in 91.75% of\ninstances, and a combination of advanced regular expressions with LLMs reached\n92%."
                },
                "authors": [
                    {
                        "name": "Klára Bendová"
                    },
                    {
                        "name": "Tomáš Knap"
                    },
                    {
                        "name": "Jan Černý"
                    },
                    {
                        "name": "Vojtěch Pour"
                    },
                    {
                        "name": "Jaromir Savelka"
                    },
                    {
                        "name": "Ivana Kvapilíková"
                    },
                    {
                        "name": "Jakub Drápal"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Drápal"
                },
                "author": "Jakub Drápal",
                "arxiv_comment": "Paper accepted to the proceedings of ASAIL 2025 Workshop under ICAIL\n  conference for publication. Paper contains 6 pages (references included) and\n  2 appendices. It contains 8 tables, no figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05319v1",
                "updated": "2025-11-07T15:17:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    17,
                    40,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:17:40Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    17,
                    40,
                    4,
                    311,
                    0
                ],
                "title": "$\\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language\n  Models"
                },
                "summary": "Although steganography has made significant advancements in recent years, it\nstill struggles to embed semantically rich, sentence-level information into\ncarriers. However, in the era of AIGC, the capacity of steganography is more\ncritical than ever. In this work, we present Sentence-to-Image Steganography,\nan instance of Semantic Steganography, a novel task that enables the hiding of\narbitrary sentence-level messages within a cover image. Furthermore, we\nestablish a benchmark named Invisible Text (IVT), comprising a diverse set of\nsentence-level texts as secret messages for evaluation. Finally, we present\n$\\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large\nlanguage models (LLMs) to embed high-level textual information, such as\nsentences or even paragraphs, into images. Unlike traditional bit-level\ncounterparts, $\\mathrm{S^2LM}$ enables the integration of semantically rich\ncontent through a newly designed pipeline in which the LLM is involved\nthroughout the entire process. Both quantitative and qualitative experiments\ndemonstrate that our method effectively unlocks new semantic steganographic\ncapabilities for LLMs. The source code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although steganography has made significant advancements in recent years, it\nstill struggles to embed semantically rich, sentence-level information into\ncarriers. However, in the era of AIGC, the capacity of steganography is more\ncritical than ever. In this work, we present Sentence-to-Image Steganography,\nan instance of Semantic Steganography, a novel task that enables the hiding of\narbitrary sentence-level messages within a cover image. Furthermore, we\nestablish a benchmark named Invisible Text (IVT), comprising a diverse set of\nsentence-level texts as secret messages for evaluation. Finally, we present\n$\\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large\nlanguage models (LLMs) to embed high-level textual information, such as\nsentences or even paragraphs, into images. Unlike traditional bit-level\ncounterparts, $\\mathrm{S^2LM}$ enables the integration of semantically rich\ncontent through a newly designed pipeline in which the LLM is involved\nthroughout the entire process. Both quantitative and qualitative experiments\ndemonstrate that our method effectively unlocks new semantic steganographic\ncapabilities for LLMs. The source code will be released soon."
                },
                "authors": [
                    {
                        "name": "Huanqi Wu"
                    },
                    {
                        "name": "Huangbiao Xu"
                    },
                    {
                        "name": "Runfeng Xie"
                    },
                    {
                        "name": "Jiaxin Cai"
                    },
                    {
                        "name": "Kaixin Zhang"
                    },
                    {
                        "name": "Xiao Ke"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Ke"
                },
                "author": "Xiao Ke",
                "arxiv_comment": "35 Pages, 20 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05311v1",
                "updated": "2025-11-07T15:12:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    12,
                    49,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:12:49Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    12,
                    49,
                    4,
                    311,
                    0
                ],
                "title": "Cleaning Maintenance Logs with LLM Agents for Improved Predictive\n  Maintenance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cleaning Maintenance Logs with LLM Agents for Improved Predictive\n  Maintenance"
                },
                "summary": "Economic constraints, limited availability of datasets for reproducibility\nand shortages of specialized expertise have long been recognized as key\nchallenges to the adoption and advancement of predictive maintenance (PdM) in\nthe automotive sector. Recent progress in large language models (LLMs) presents\nan opportunity to overcome these barriers and speed up the transition of PdM\nfrom research to industrial practice. Under these conditions, we explore the\npotential of LLM-based agents to support PdM cleaning pipelines. Specifically,\nwe focus on maintenance logs, a critical data source for training\nwell-performing machine learning (ML) models, but one often affected by errors\nsuch as typos, missing fields, near-duplicate entries, and incorrect dates. We\nevaluate LLM agents on cleaning tasks involving six distinct types of noise.\nOur findings show that LLMs are effective at handling generic cleaning tasks\nand offer a promising foundation for future industrial applications. While\ndomain-specific errors remain challenging, these results highlight the\npotential for further improvements through specialized training and enhanced\nagentic capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Economic constraints, limited availability of datasets for reproducibility\nand shortages of specialized expertise have long been recognized as key\nchallenges to the adoption and advancement of predictive maintenance (PdM) in\nthe automotive sector. Recent progress in large language models (LLMs) presents\nan opportunity to overcome these barriers and speed up the transition of PdM\nfrom research to industrial practice. Under these conditions, we explore the\npotential of LLM-based agents to support PdM cleaning pipelines. Specifically,\nwe focus on maintenance logs, a critical data source for training\nwell-performing machine learning (ML) models, but one often affected by errors\nsuch as typos, missing fields, near-duplicate entries, and incorrect dates. We\nevaluate LLM agents on cleaning tasks involving six distinct types of noise.\nOur findings show that LLMs are effective at handling generic cleaning tasks\nand offer a promising foundation for future industrial applications. While\ndomain-specific errors remain challenging, these results highlight the\npotential for further improvements through specialized training and enhanced\nagentic capabilities."
                },
                "authors": [
                    {
                        "name": "Valeriu Dimidov"
                    },
                    {
                        "name": "Faisal Hawlader"
                    },
                    {
                        "name": "Sasan Jafarnejad"
                    },
                    {
                        "name": "Raphaël Frank"
                    }
                ],
                "author_detail": {
                    "name": "Raphaël Frank"
                },
                "author": "Raphaël Frank",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05302v1",
                "updated": "2025-11-07T15:02:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    2,
                    42,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:02:42Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    2,
                    42,
                    4,
                    311,
                    0
                ],
                "title": "Code Review Automation using Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Review Automation using Retrieval Augmented Generation"
                },
                "summary": "Code review is essential for maintaining software quality but is\nlabor-intensive. Automated code review generation offers a promising solution\nto this challenge. Both deep learning-based generative techniques and\nretrieval-based methods have demonstrated strong performance in this task.\nHowever, despite these advancements, there are still some limitations where\ngenerated reviews can be either off-point or overly general. To address these\nissues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages\nRetrieval-Augmented Generation (RAG) to combine retrieval-based and generative\nmethods, explicitly incorporating external domain knowledge into the code\nreview process. RARe uses a dense retriever to select the most relevant reviews\nfrom the codebase, which then enrich the input for a neural generator,\nutilizing the contextual learning capacity of large language models (LLMs), to\nproduce the final review. RARe outperforms state-of-the-art methods on two\nbenchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively.\nIts effectiveness is further validated through a detailed human evaluation and\na case study using an interpretability tool, demonstrating its practical\nutility and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is essential for maintaining software quality but is\nlabor-intensive. Automated code review generation offers a promising solution\nto this challenge. Both deep learning-based generative techniques and\nretrieval-based methods have demonstrated strong performance in this task.\nHowever, despite these advancements, there are still some limitations where\ngenerated reviews can be either off-point or overly general. To address these\nissues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages\nRetrieval-Augmented Generation (RAG) to combine retrieval-based and generative\nmethods, explicitly incorporating external domain knowledge into the code\nreview process. RARe uses a dense retriever to select the most relevant reviews\nfrom the codebase, which then enrich the input for a neural generator,\nutilizing the contextual learning capacity of large language models (LLMs), to\nproduce the final review. RARe outperforms state-of-the-art methods on two\nbenchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively.\nIts effectiveness is further validated through a detailed human evaluation and\na case study using an interpretability tool, demonstrating its practical\nutility and reliability."
                },
                "authors": [
                    {
                        "name": "Qianru Meng"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Zhaochen Ren"
                    },
                    {
                        "name": "Joost Visser"
                    }
                ],
                "author_detail": {
                    "name": "Joost Visser"
                },
                "author": "Joost Visser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05301v1",
                "updated": "2025-11-07T15:01:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    1,
                    38,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:01:38Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    1,
                    38,
                    4,
                    311,
                    0
                ],
                "title": "QUESTER: Query Specification for Generative Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUESTER: Query Specification for Generative Retrieval"
                },
                "summary": "Generative Retrieval (GR) differs from the traditional index-then-retrieve\npipeline by storing relevance in model parameters and directly generating\ndocument identifiers. However, GR often struggles to generalize and is costly\nto scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval),\nwhich reframes GR as query specification generation - in this work, a simple\nkeyword query handled by BM25 - using a (small) LLM. The policy is trained\nusing reinforcement learning techniques (GRPO). Across in- and out-of-domain\nevaluations, we show that our model is more effective than BM25, and\ncompetitive with neural IR models, while maintaining a good efficiency",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Retrieval (GR) differs from the traditional index-then-retrieve\npipeline by storing relevance in model parameters and directly generating\ndocument identifiers. However, GR often struggles to generalize and is costly\nto scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval),\nwhich reframes GR as query specification generation - in this work, a simple\nkeyword query handled by BM25 - using a (small) LLM. The policy is trained\nusing reinforcement learning techniques (GRPO). Across in- and out-of-domain\nevaluations, we show that our model is more effective than BM25, and\ncompetitive with neural IR models, while maintaining a good efficiency"
                },
                "authors": [
                    {
                        "name": "Arthur Satouf"
                    },
                    {
                        "name": "Yuxuan Zong"
                    },
                    {
                        "name": "Habiboulaye Amadou-Boubacar"
                    },
                    {
                        "name": "Pablo Piantanida"
                    },
                    {
                        "name": "Benjamin Piwowarski"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Piwowarski"
                },
                "author": "Benjamin Piwowarski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68P20, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05299v1",
                "updated": "2025-11-07T15:00:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    0,
                    37,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:00:37Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    0,
                    37,
                    4,
                    311,
                    0
                ],
                "title": "LiveStar: Live Streaming Assistant for Real-World Online Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveStar: Live Streaming Assistant for Real-World Online Video\n  Understanding"
                },
                "summary": "Despite significant progress in Video Large Language Models (Video-LLMs) for\noffline video understanding, existing online Video-LLMs typically struggle to\nsimultaneously process continuous frame-by-frame inputs and determine optimal\nresponse timing, often compromising real-time responsiveness and narrative\ncoherence. To address these limitations, we introduce LiveStar, a pioneering\nlive streaming assistant that achieves always-on proactive responses through\nadaptive streaming decoding. Specifically, LiveStar incorporates: (1) a\ntraining strategy enabling incremental video-language alignment for\nvariable-length video streams, preserving temporal consistency across\ndynamically evolving frame sequences; (2) a response-silence decoding framework\nthat determines optimal proactive response timing via a single forward pass\nverification; (3) memory-aware acceleration via peak-end memory compression for\nonline inference on 10+ minute videos, combined with streaming key-value cache\nto achieve 1.53x faster inference. We also construct an OmniStar dataset, a\ncomprehensive dataset for training and benchmarking that encompasses 15 diverse\nreal-world scenarios and 5 evaluation tasks for online video understanding.\nExtensive experiments across three benchmarks demonstrate LiveStar's\nstate-of-the-art performance, achieving an average 19.5% improvement in\nsemantic correctness with 18.1% reduced timing difference compared to existing\nonline Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.\nOur model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress in Video Large Language Models (Video-LLMs) for\noffline video understanding, existing online Video-LLMs typically struggle to\nsimultaneously process continuous frame-by-frame inputs and determine optimal\nresponse timing, often compromising real-time responsiveness and narrative\ncoherence. To address these limitations, we introduce LiveStar, a pioneering\nlive streaming assistant that achieves always-on proactive responses through\nadaptive streaming decoding. Specifically, LiveStar incorporates: (1) a\ntraining strategy enabling incremental video-language alignment for\nvariable-length video streams, preserving temporal consistency across\ndynamically evolving frame sequences; (2) a response-silence decoding framework\nthat determines optimal proactive response timing via a single forward pass\nverification; (3) memory-aware acceleration via peak-end memory compression for\nonline inference on 10+ minute videos, combined with streaming key-value cache\nto achieve 1.53x faster inference. We also construct an OmniStar dataset, a\ncomprehensive dataset for training and benchmarking that encompasses 15 diverse\nreal-world scenarios and 5 evaluation tasks for online video understanding.\nExtensive experiments across three benchmarks demonstrate LiveStar's\nstate-of-the-art performance, achieving an average 19.5% improvement in\nsemantic correctness with 18.1% reduced timing difference compared to existing\nonline Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.\nOur model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar."
                },
                "authors": [
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Kairui Zhang"
                    },
                    {
                        "name": "Yuhang Hu"
                    },
                    {
                        "name": "Bing Wang"
                    },
                    {
                        "name": "Shengsheng Qian"
                    },
                    {
                        "name": "Bin Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Weiming Dong"
                    },
                    {
                        "name": "Changsheng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Changsheng Xu"
                },
                "author": "Changsheng Xu",
                "arxiv_comment": "NeurIPS 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05297v1",
                "updated": "2025-11-07T14:56:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    56,
                    45,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T14:56:45Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    56,
                    45,
                    4,
                    311,
                    0
                ],
                "title": "Building Specialized Software-Assistant ChatBot with Graph-Based\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Specialized Software-Assistant ChatBot with Graph-Based\n  Retrieval-Augmented Generation"
                },
                "summary": "Digital Adoption Platforms (DAPs) have become essential tools for helping\nemployees navigate complex enterprise software such as CRM, ERP, or HRMS\nsystems. Companies like LemonLearning have shown how digital guidance can\nreduce training costs and accelerate onboarding. However, building and\nmaintaining these interactive guides still requires extensive manual effort.\nLeveraging Large Language Models as virtual assistants is an appealing\nalternative, yet without a structured understanding of the target software,\nLLMs often hallucinate and produce unreliable answers. Moreover, most\nproduction-grade LLMs are black-box APIs, making fine-tuning impractical due to\nthe lack of access to model weights. In this work, we introduce a Graph-based\nRetrieval-Augmented Generation framework that automatically converts enterprise\nweb applications into state-action knowledge graphs, enabling LLMs to generate\ngrounded and context-aware assistance. The framework was co-developed with the\nAI enterprise RAKAM, in collaboration with Lemon Learning. We detail the\nengineering pipeline that extracts and structures software interfaces, the\ndesign of the graph-based retrieval process, and the integration of our\napproach into production DAP workflows. Finally, we discuss scalability,\nrobustness, and deployment lessons learned from industrial use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Adoption Platforms (DAPs) have become essential tools for helping\nemployees navigate complex enterprise software such as CRM, ERP, or HRMS\nsystems. Companies like LemonLearning have shown how digital guidance can\nreduce training costs and accelerate onboarding. However, building and\nmaintaining these interactive guides still requires extensive manual effort.\nLeveraging Large Language Models as virtual assistants is an appealing\nalternative, yet without a structured understanding of the target software,\nLLMs often hallucinate and produce unreliable answers. Moreover, most\nproduction-grade LLMs are black-box APIs, making fine-tuning impractical due to\nthe lack of access to model weights. In this work, we introduce a Graph-based\nRetrieval-Augmented Generation framework that automatically converts enterprise\nweb applications into state-action knowledge graphs, enabling LLMs to generate\ngrounded and context-aware assistance. The framework was co-developed with the\nAI enterprise RAKAM, in collaboration with Lemon Learning. We detail the\nengineering pipeline that extracts and structures software interfaces, the\ndesign of the graph-based retrieval process, and the integration of our\napproach into production DAP workflows. Finally, we discuss scalability,\nrobustness, and deployment lessons learned from industrial use cases."
                },
                "authors": [
                    {
                        "name": "Mohammed Hilel"
                    },
                    {
                        "name": "Yannis Karmim"
                    },
                    {
                        "name": "Jean De Bodinat"
                    },
                    {
                        "name": "Reda Sarehane"
                    },
                    {
                        "name": "Antoine Gillon"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Gillon"
                },
                "author": "Antoine Gillon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05295v1",
                "updated": "2025-11-07T14:56:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    56,
                    4,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T14:56:04Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    56,
                    4,
                    4,
                    311,
                    0
                ],
                "title": "Language Generation and Identification From Partial Enumeration: Tight\n  Density Bounds and Topological Characterizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Generation and Identification From Partial Enumeration: Tight\n  Density Bounds and Topological Characterizations"
                },
                "summary": "The success of large language models (LLMs) has motivated formal theories of\nlanguage generation and learning. We study the framework of \\emph{language\ngeneration in the limit}, where an adversary enumerates strings from an unknown\nlanguage $K$ drawn from a countable class, and an algorithm must generate\nunseen strings from $K$. Prior work showed that generation is always possible,\nand that some algorithms achieve positive lower density, revealing a\n\\emph{validity--breadth} trade-off between correctness and coverage. We resolve\na main open question in this line, proving a tight bound of $1/2$ on the best\nachievable lower density. We then strengthen the model to allow \\emph{partial\nenumeration}, where the adversary reveals only an infinite subset $C \\subseteq\nK$. We show that generation in the limit remains achievable, and if $C$ has\nlower density $\\alpha$ in $K$, the algorithm's output achieves density at least\n$\\alpha/2$, matching the upper bound. This generalizes the $1/2$ bound to the\npartial-information setting, where the generator must recover within a factor\n$1/2$ of the revealed subset's density. We further revisit the classical\nGold--Angluin model of \\emph{language identification} under partial\nenumeration. We characterize when identification in the limit is possible --\nwhen hypotheses $M_t$ eventually satisfy $C \\subseteq M \\subseteq K$ -- and in\nthe process give a new topological formulation of Angluin's characterization,\nshowing that her condition is precisely equivalent to an appropriate\ntopological space having the $T_D$ separation property.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of large language models (LLMs) has motivated formal theories of\nlanguage generation and learning. We study the framework of \\emph{language\ngeneration in the limit}, where an adversary enumerates strings from an unknown\nlanguage $K$ drawn from a countable class, and an algorithm must generate\nunseen strings from $K$. Prior work showed that generation is always possible,\nand that some algorithms achieve positive lower density, revealing a\n\\emph{validity--breadth} trade-off between correctness and coverage. We resolve\na main open question in this line, proving a tight bound of $1/2$ on the best\nachievable lower density. We then strengthen the model to allow \\emph{partial\nenumeration}, where the adversary reveals only an infinite subset $C \\subseteq\nK$. We show that generation in the limit remains achievable, and if $C$ has\nlower density $\\alpha$ in $K$, the algorithm's output achieves density at least\n$\\alpha/2$, matching the upper bound. This generalizes the $1/2$ bound to the\npartial-information setting, where the generator must recover within a factor\n$1/2$ of the revealed subset's density. We further revisit the classical\nGold--Angluin model of \\emph{language identification} under partial\nenumeration. We characterize when identification in the limit is possible --\nwhen hypotheses $M_t$ eventually satisfy $C \\subseteq M \\subseteq K$ -- and in\nthe process give a new topological formulation of Angluin's characterization,\nshowing that her condition is precisely equivalent to an appropriate\ntopological space having the $T_D$ separation property."
                },
                "authors": [
                    {
                        "name": "Jon Kleinberg"
                    },
                    {
                        "name": "Fan Wei"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wei"
                },
                "author": "Fan Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05292v1",
                "updated": "2025-11-07T14:54:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    54,
                    37,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T14:54:37Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    54,
                    37,
                    4,
                    311,
                    0
                ],
                "title": "What's on Your Plate? Inferring Chinese Cuisine Intake from Wearable\n  IMUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What's on Your Plate? Inferring Chinese Cuisine Intake from Wearable\n  IMUs"
                },
                "summary": "Accurate food intake detection is vital for dietary monitoring and chronic\ndisease prevention. Traditional self-report methods are prone to recall bias,\nwhile camera-based approaches raise concerns about privacy. Furthermore,\nexisting wearable-based methods primarily focus on a limited number of food\ntypes, such as hamburgers and pizza, failing to address the vast diversity of\nChinese cuisine. To bridge this gap, we propose CuisineSense, a system that\nclassifies Chinese food types by integrating hand motion cues from a smartwatch\nwith head dynamics from smart glasses. To filter out irrelevant daily\nactivities, we design a two-stage detection pipeline. The first stage\nidentifies eating states by distinguishing characteristic temporal patterns\nfrom non-eating behaviors. The second stage then conducts fine-grained food\ntype recognition based on the motions captured during food intake. To evaluate\nCuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings\nacross 11 food categories and 10 participants. Experiments demonstrate that\nCuisineSense achieves high accuracy in both eating state detection and food\nclassification, offering a practical solution for unobtrusive, wearable-based\ndietary monitoring.The system code is publicly available at\nhttps://github.com/joeeeeyin/CuisineSense.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate food intake detection is vital for dietary monitoring and chronic\ndisease prevention. Traditional self-report methods are prone to recall bias,\nwhile camera-based approaches raise concerns about privacy. Furthermore,\nexisting wearable-based methods primarily focus on a limited number of food\ntypes, such as hamburgers and pizza, failing to address the vast diversity of\nChinese cuisine. To bridge this gap, we propose CuisineSense, a system that\nclassifies Chinese food types by integrating hand motion cues from a smartwatch\nwith head dynamics from smart glasses. To filter out irrelevant daily\nactivities, we design a two-stage detection pipeline. The first stage\nidentifies eating states by distinguishing characteristic temporal patterns\nfrom non-eating behaviors. The second stage then conducts fine-grained food\ntype recognition based on the motions captured during food intake. To evaluate\nCuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings\nacross 11 food categories and 10 participants. Experiments demonstrate that\nCuisineSense achieves high accuracy in both eating state detection and food\nclassification, offering a practical solution for unobtrusive, wearable-based\ndietary monitoring.The system code is publicly available at\nhttps://github.com/joeeeeyin/CuisineSense.git."
                },
                "authors": [
                    {
                        "name": "Jiaxi Yin"
                    },
                    {
                        "name": "Pengcheng Wang"
                    },
                    {
                        "name": "Han Ding"
                    },
                    {
                        "name": "Fei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wang"
                },
                "author": "Fei Wang",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16470v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16470v2",
                "updated": "2025-11-07T14:52:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    52,
                    6,
                    4,
                    311,
                    0
                ],
                "published": "2025-05-22T09:52:57Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    9,
                    52,
                    57,
                    3,
                    142,
                    0
                ],
                "title": "Benchmarking Retrieval-Augmented Multimodal Generation for Document\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Retrieval-Augmented Multimodal Generation for Document\n  Question Answering"
                },
                "summary": "Document Visual Question Answering (DocVQA) faces dual challenges in\nprocessing lengthy multimodal documents (text, images, tables) and performing\ncross-modal reasoning. Current document retrieval-augmented generation (DocRAG)\nmethods remain limited by their text-centric approaches, frequently missing\ncritical visual information. The field also lacks robust benchmarks for\nassessing multimodal evidence selection and integration. We introduce MMDocRAG,\na comprehensive benchmark featuring 4,055 expert-annotated QA pairs with\nmulti-page, cross-modal evidence chains. Our framework introduces innovative\nmetrics for evaluating multimodal quote selection and enables answers that\ninterleave text with relevant visual elements. Through large-scale experiments\nwith 60 VLM/LLM models and 14 retrieval systems, we identify persistent\nchallenges in multimodal evidence retrieval, selection, and integration.Key\nfindings reveal advanced proprietary LVMs show superior performance than\nopen-sourced alternatives. Also, they show moderate advantages using multimodal\ninputs over text-only inputs, while open-source alternatives show significant\nperformance degradation. Notably, fine-tuned LLMs achieve substantial\nimprovements when using detailed image descriptions. MMDocRAG establishes a\nrigorous testing ground and provides actionable insights for developing more\nrobust multimodal DocVQA systems. Our benchmark and code are available at\nhttps://mmdocrag.github.io/MMDocRAG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document Visual Question Answering (DocVQA) faces dual challenges in\nprocessing lengthy multimodal documents (text, images, tables) and performing\ncross-modal reasoning. Current document retrieval-augmented generation (DocRAG)\nmethods remain limited by their text-centric approaches, frequently missing\ncritical visual information. The field also lacks robust benchmarks for\nassessing multimodal evidence selection and integration. We introduce MMDocRAG,\na comprehensive benchmark featuring 4,055 expert-annotated QA pairs with\nmulti-page, cross-modal evidence chains. Our framework introduces innovative\nmetrics for evaluating multimodal quote selection and enables answers that\ninterleave text with relevant visual elements. Through large-scale experiments\nwith 60 VLM/LLM models and 14 retrieval systems, we identify persistent\nchallenges in multimodal evidence retrieval, selection, and integration.Key\nfindings reveal advanced proprietary LVMs show superior performance than\nopen-sourced alternatives. Also, they show moderate advantages using multimodal\ninputs over text-only inputs, while open-source alternatives show significant\nperformance degradation. Notably, fine-tuned LLMs achieve substantial\nimprovements when using detailed image descriptions. MMDocRAG establishes a\nrigorous testing ground and provides actionable insights for developing more\nrobust multimodal DocVQA systems. Our benchmark and code are available at\nhttps://mmdocrag.github.io/MMDocRAG/."
                },
                "authors": [
                    {
                        "name": "Kuicai Dong"
                    },
                    {
                        "name": "Yujing Chang"
                    },
                    {
                        "name": "Shijie Huang"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Paper accepted to NeurIPS 2025 DB",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16470v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16470v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05289v1",
                "updated": "2025-11-07T14:49:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    49,
                    18,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T14:49:18Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    49,
                    18,
                    4,
                    311,
                    0
                ],
                "title": "Embedding-Space Data Augmentation to Prevent Membership Inference\n  Attacks in Clinical Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding-Space Data Augmentation to Prevent Membership Inference\n  Attacks in Clinical Time Series Forecasting"
                },
                "summary": "Balancing strong privacy guarantees with high predictive performance is\ncritical for time series forecasting (TSF) tasks involving Electronic Health\nRecords (EHR). In this study, we explore how data augmentation can mitigate\nMembership Inference Attacks (MIA) on TSF models. We show that retraining with\nsynthetic data can substantially reduce the effectiveness of loss-based MIAs by\nreducing the attacker's true-positive to false-positive ratio. The key\nchallenge is generating synthetic samples that closely resemble the original\ntraining data to confuse the attacker, while also introducing enough novelty to\nenhance the model's ability to generalize to unseen data. We examine multiple\naugmentation strategies - Zeroth-Order Optimization (ZOO), a variant of ZOO\nconstrained by Principal Component Analysis (ZOO-PCA), and MixUp - to\nstrengthen model resilience without sacrificing accuracy. Our experimental\nresults show that ZOO-PCA yields the best reductions in TPR/FPR ratio for MIA\nattacks without sacrificing performance on test data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing strong privacy guarantees with high predictive performance is\ncritical for time series forecasting (TSF) tasks involving Electronic Health\nRecords (EHR). In this study, we explore how data augmentation can mitigate\nMembership Inference Attacks (MIA) on TSF models. We show that retraining with\nsynthetic data can substantially reduce the effectiveness of loss-based MIAs by\nreducing the attacker's true-positive to false-positive ratio. The key\nchallenge is generating synthetic samples that closely resemble the original\ntraining data to confuse the attacker, while also introducing enough novelty to\nenhance the model's ability to generalize to unseen data. We examine multiple\naugmentation strategies - Zeroth-Order Optimization (ZOO), a variant of ZOO\nconstrained by Principal Component Analysis (ZOO-PCA), and MixUp - to\nstrengthen model resilience without sacrificing accuracy. Our experimental\nresults show that ZOO-PCA yields the best reductions in TPR/FPR ratio for MIA\nattacks without sacrificing performance on test data."
                },
                "authors": [
                    {
                        "name": "Marius Fracarolli"
                    },
                    {
                        "name": "Michael Staniek"
                    },
                    {
                        "name": "Stefan Riezler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Riezler"
                },
                "author": "Stefan Riezler",
                "arxiv_comment": "Accepted as a proceedings paper at Machine Learning for Health (ML4H)\n  symposium 2025, December 1-2, 2025, San Diego, United States, 15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05286v1",
                "updated": "2025-11-07T14:48:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    48,
                    49,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T14:48:49Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    48,
                    49,
                    4,
                    311,
                    0
                ],
                "title": "Reflective Personalization Optimization: A Post-hoc Rewriting Framework\n  for Black-Box Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflective Personalization Optimization: A Post-hoc Rewriting Framework\n  for Black-Box Large Language Models"
                },
                "summary": "The personalization of black-box large language models (LLMs) is a critical\nyet challenging task. Existing approaches predominantly rely on context\ninjection, where user history is embedded into the prompt to directly guide the\ngeneration process. However, this single-step paradigm imposes a dual burden on\nthe model: generating accurate content while simultaneously aligning with\nuser-specific styles. This often results in a trade-off that compromises output\nquality and limits precise control. To address this fundamental tension, we\npropose Reflective Personalization Optimization (RPO), a novel framework that\nredefines the personalization paradigm by decoupling content generation from\nalignment. RPO operates in two distinct stages: first, a base model generates a\nhigh-quality, generic response; then, an external reflection module explicitly\nrewrites this output to align with the user's preferences. This reflection\nmodule is trained using a two-stage process. Initially, supervised fine-tuning\nis employed on structured rewriting trajectories to establish a core\npersonalized reasoning policy that models the transformation from generic to\nuser-aligned responses. Subsequently, reinforcement learning is applied to\nfurther refine and enhance the quality of the personalized outputs.\nComprehensive experiments on the LaMP benchmark demonstrate that RPO, by\ndecoupling content generation from personalization, significantly outperforms\nstate-of-the-art baselines. These findings underscore the superiority of\nexplicit response shaping over implicit context injection. Moreover, RPO\nintroduces an efficient, model-agnostic personalization layer that can be\nseamlessly integrated with any underlying base model, paving the way for a new\nand effective direction in user-centric generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The personalization of black-box large language models (LLMs) is a critical\nyet challenging task. Existing approaches predominantly rely on context\ninjection, where user history is embedded into the prompt to directly guide the\ngeneration process. However, this single-step paradigm imposes a dual burden on\nthe model: generating accurate content while simultaneously aligning with\nuser-specific styles. This often results in a trade-off that compromises output\nquality and limits precise control. To address this fundamental tension, we\npropose Reflective Personalization Optimization (RPO), a novel framework that\nredefines the personalization paradigm by decoupling content generation from\nalignment. RPO operates in two distinct stages: first, a base model generates a\nhigh-quality, generic response; then, an external reflection module explicitly\nrewrites this output to align with the user's preferences. This reflection\nmodule is trained using a two-stage process. Initially, supervised fine-tuning\nis employed on structured rewriting trajectories to establish a core\npersonalized reasoning policy that models the transformation from generic to\nuser-aligned responses. Subsequently, reinforcement learning is applied to\nfurther refine and enhance the quality of the personalized outputs.\nComprehensive experiments on the LaMP benchmark demonstrate that RPO, by\ndecoupling content generation from personalization, significantly outperforms\nstate-of-the-art baselines. These findings underscore the superiority of\nexplicit response shaping over implicit context injection. Moreover, RPO\nintroduces an efficient, model-agnostic personalization layer that can be\nseamlessly integrated with any underlying base model, paving the way for a new\nand effective direction in user-centric generation scenarios."
                },
                "authors": [
                    {
                        "name": "Teqi Hao"
                    },
                    {
                        "name": "Xioayu Tan"
                    },
                    {
                        "name": "Shaojie Shi"
                    },
                    {
                        "name": "Yinghui Xu"
                    },
                    {
                        "name": "Xihe Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xihe Qiu"
                },
                "author": "Xihe Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02615v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02615v4",
                "updated": "2025-11-07T14:48:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    48,
                    44,
                    4,
                    311,
                    0
                ],
                "published": "2024-10-03T15:52:03Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    52,
                    3,
                    3,
                    277,
                    0
                ],
                "title": "ExGra-Med: Extended Context Graph Alignment for Medical Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExGra-Med: Extended Context Graph Alignment for Medical Vision-Language\n  Models"
                },
                "summary": "State-of-the-art medical multi-modal LLMs (med-MLLMs), such as LLaVA-Med and\nBioMedGPT, primarily depend on scaling model size and data volume, with\ntraining driven largely by autoregressive objectives. However, we reveal that\nthis approach can lead to weak vision-language alignment, making these models\noverly dependent on costly instruction-following data. To address this, we\nintroduce ExGra-Med, a novel multi-graph alignment framework that jointly\naligns images, instruction responses, and extended captions in the latent\nspace, advancing semantic grounding and cross-modal coherence. To scale to\nlarge LLMs (e.g., LLaMA-7B), we develop an efficient end-to-end training scheme\nusing black-box gradient estimation, enabling fast and scalable optimization.\nEmpirically, ExGra-Med matches LLaVA-Med's performance using just 10% of the\npre-training data, achieving a 20.13% gain on VQA-RAD and approaching full-data\nperformance. It also outperforms strong baselines like BioMedGPT and RadFM on\nvisual chatbot and zero-shot classification tasks, demonstrating its promise\nfor efficient, high-quality vision-language integration in medical AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art medical multi-modal LLMs (med-MLLMs), such as LLaVA-Med and\nBioMedGPT, primarily depend on scaling model size and data volume, with\ntraining driven largely by autoregressive objectives. However, we reveal that\nthis approach can lead to weak vision-language alignment, making these models\noverly dependent on costly instruction-following data. To address this, we\nintroduce ExGra-Med, a novel multi-graph alignment framework that jointly\naligns images, instruction responses, and extended captions in the latent\nspace, advancing semantic grounding and cross-modal coherence. To scale to\nlarge LLMs (e.g., LLaMA-7B), we develop an efficient end-to-end training scheme\nusing black-box gradient estimation, enabling fast and scalable optimization.\nEmpirically, ExGra-Med matches LLaVA-Med's performance using just 10% of the\npre-training data, achieving a 20.13% gain on VQA-RAD and approaching full-data\nperformance. It also outperforms strong baselines like BioMedGPT and RadFM on\nvisual chatbot and zero-shot classification tasks, demonstrating its promise\nfor efficient, high-quality vision-language integration in medical AI."
                },
                "authors": [
                    {
                        "name": "Duy M. H. Nguyen"
                    },
                    {
                        "name": "Nghiem T. Diep"
                    },
                    {
                        "name": "Trung Q. Nguyen"
                    },
                    {
                        "name": "Hoang-Bao Le"
                    },
                    {
                        "name": "Tai Nguyen"
                    },
                    {
                        "name": "Tien Nguyen"
                    },
                    {
                        "name": "TrungTin Nguyen"
                    },
                    {
                        "name": "Nhat Ho"
                    },
                    {
                        "name": "Pengtao Xie"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Daniel Sonntag"
                    },
                    {
                        "name": "Mathias Niepert"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Niepert"
                },
                "author": "Mathias Niepert",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02615v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02615v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09338v2",
                "updated": "2025-11-07T14:35:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    35,
                    34,
                    4,
                    311,
                    0
                ],
                "published": "2025-06-11T02:39:26Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    39,
                    26,
                    2,
                    162,
                    0
                ],
                "title": "Know What You Don't Know: Uncertainty Calibration of Process Reward\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know What You Don't Know: Uncertainty Calibration of Process Reward\n  Models"
                },
                "summary": "Process reward models (PRMs) play a central role in guiding inference-time\nscaling algorithms for large language models (LLMs). However, we observe that\neven state-of-the-art PRMs can be poorly calibrated. Specifically, they tend to\noverestimate the success probability that a partial reasoning step will lead to\na correct final answer, particularly when smaller LLMs are used to complete the\nreasoning trajectory. To address this, we present a calibration approach --\nperformed via quantile regression -- that adjusts PRM outputs to better align\nwith true success probabilities. Leveraging these calibrated success estimates\nand their associated confidence bounds, we introduce an \\emph{instance-adaptive\nscaling} (IAS) framework that dynamically adjusts the compute budget based on\nthe estimated likelihood that a partial reasoning trajectory will yield a\ncorrect final answer. Unlike conventional methods that allocate a fixed number\nof reasoning trajectories per query, this approach adapts to each instance and\nreasoning step when using our calibrated PRMs. Experiments on mathematical\nreasoning benchmarks show that (i) our PRM calibration method achieves small\ncalibration error, outperforming the baseline methods, (ii) calibration is\ncrucial for enabling effective IAS, and (iii) the proposed IAS strategy reduces\ninference costs while maintaining final answer accuracy, utilizing less compute\non more confident problems as desired.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process reward models (PRMs) play a central role in guiding inference-time\nscaling algorithms for large language models (LLMs). However, we observe that\neven state-of-the-art PRMs can be poorly calibrated. Specifically, they tend to\noverestimate the success probability that a partial reasoning step will lead to\na correct final answer, particularly when smaller LLMs are used to complete the\nreasoning trajectory. To address this, we present a calibration approach --\nperformed via quantile regression -- that adjusts PRM outputs to better align\nwith true success probabilities. Leveraging these calibrated success estimates\nand their associated confidence bounds, we introduce an \\emph{instance-adaptive\nscaling} (IAS) framework that dynamically adjusts the compute budget based on\nthe estimated likelihood that a partial reasoning trajectory will yield a\ncorrect final answer. Unlike conventional methods that allocate a fixed number\nof reasoning trajectories per query, this approach adapts to each instance and\nreasoning step when using our calibrated PRMs. Experiments on mathematical\nreasoning benchmarks show that (i) our PRM calibration method achieves small\ncalibration error, outperforming the baseline methods, (ii) calibration is\ncrucial for enabling effective IAS, and (iii) the proposed IAS strategy reduces\ninference costs while maintaining final answer accuracy, utilizing less compute\non more confident problems as desired."
                },
                "authors": [
                    {
                        "name": "Young-Jin Park"
                    },
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Kaveh Alim"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Navid Azizan"
                    }
                ],
                "author_detail": {
                    "name": "Navid Azizan"
                },
                "author": "Navid Azizan",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05269v1",
                "updated": "2025-11-07T14:30:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    30,
                    26,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T14:30:26Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    30,
                    26,
                    4,
                    311,
                    0
                ],
                "title": "TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities as\nautonomous agents through tool use, planning, and decision-making abilities,\nleading to their widespread adoption across diverse tasks. As task complexity\ngrows, multi-agent LLM systems are increasingly used to solve problems\ncollaboratively. However, safety and security of these systems remains largely\nunder-explored. Existing benchmarks and datasets predominantly focus on\nsingle-agent settings, failing to capture the unique vulnerabilities of\nmulti-agent dynamics and co-ordination. To address this gap, we introduce\n$\\textbf{T}$hreats and $\\textbf{A}$ttacks in $\\textbf{M}$ulti-$\\textbf{A}$gent\n$\\textbf{S}$ystems ($\\textbf{TAMAS}$), a benchmark designed to evaluate the\nrobustness and safety of multi-agent LLM systems. TAMAS includes five distinct\nscenarios comprising 300 adversarial instances across six attack types and 211\ntools, along with 100 harmless tasks. We assess system performance across ten\nbackbone LLMs and three agent interaction configurations from Autogen and\nCrewAI frameworks, highlighting critical challenges and failure modes in\ncurrent multi-agent deployments. Furthermore, we introduce Effective Robustness\nScore (ERS) to assess the tradeoff between safety and task effectiveness of\nthese frameworks. Our findings show that multi-agent systems are highly\nvulnerable to adversarial attacks, underscoring the urgent need for stronger\ndefenses. TAMAS provides a foundation for systematically studying and improving\nthe safety of multi-agent LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities as\nautonomous agents through tool use, planning, and decision-making abilities,\nleading to their widespread adoption across diverse tasks. As task complexity\ngrows, multi-agent LLM systems are increasingly used to solve problems\ncollaboratively. However, safety and security of these systems remains largely\nunder-explored. Existing benchmarks and datasets predominantly focus on\nsingle-agent settings, failing to capture the unique vulnerabilities of\nmulti-agent dynamics and co-ordination. To address this gap, we introduce\n$\\textbf{T}$hreats and $\\textbf{A}$ttacks in $\\textbf{M}$ulti-$\\textbf{A}$gent\n$\\textbf{S}$ystems ($\\textbf{TAMAS}$), a benchmark designed to evaluate the\nrobustness and safety of multi-agent LLM systems. TAMAS includes five distinct\nscenarios comprising 300 adversarial instances across six attack types and 211\ntools, along with 100 harmless tasks. We assess system performance across ten\nbackbone LLMs and three agent interaction configurations from Autogen and\nCrewAI frameworks, highlighting critical challenges and failure modes in\ncurrent multi-agent deployments. Furthermore, we introduce Effective Robustness\nScore (ERS) to assess the tradeoff between safety and task effectiveness of\nthese frameworks. Our findings show that multi-agent systems are highly\nvulnerable to adversarial attacks, underscoring the urgent need for stronger\ndefenses. TAMAS provides a foundation for systematically studying and improving\nthe safety of multi-agent LLM systems."
                },
                "authors": [
                    {
                        "name": "Ishan Kavathekar"
                    },
                    {
                        "name": "Hemang Jain"
                    },
                    {
                        "name": "Ameya Rathod"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    },
                    {
                        "name": "Tanuja Ganu"
                    }
                ],
                "author_detail": {
                    "name": "Tanuja Ganu"
                },
                "author": "Tanuja Ganu",
                "arxiv_comment": "Accepted at ICML 2025 MAS Workshop. This version includes additional\n  experiments and analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14450v2",
                "updated": "2025-11-07T14:25:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    25,
                    54,
                    4,
                    311,
                    0
                ],
                "published": "2025-02-20T11:05:10Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    11,
                    5,
                    10,
                    3,
                    51,
                    0
                ],
                "title": "LLM4FaaS: No-Code Application Development using LLMs and FaaS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4FaaS: No-Code Application Development using LLMs and FaaS"
                },
                "summary": "Large language models (LLMs) show great capabilities in generating code from\nnatural language descriptions, bringing programming power closer to\nnon-technical users. However, their lack of expertise in operating the\ngenerated code remains a key barrier to realizing customized applications.\nFunction-as-a-Service (FaaS) platforms offer a high level of abstraction for\ncode execution and deployment, allowing users to run LLM-generated code without\nrequiring technical expertise or incurring operational overhead.\n  In this paper, we present LLM4FaaS, a no-code application development\napproach that integrates LLMs and FaaS platforms to enable non-technical users\nto build and run customized applications using only natural language. By\ndeploying LLM-generated code through FaaS, LLM4FaaS abstracts away\ninfrastructure management and boilerplate code generation. We implement a\nproof-of-concept prototype based on an open-source FaaS platform, and evaluate\nit using real prompts from non-technical users. Experiments with GPT-4o show\nthat LLM4FaaS can automatically build and deploy code in 71.47% of cases,\noutperforming a non-FaaS baseline at 43.48% and an existing LLM-based platform\nat 14.55%, narrowing the gap to human performance at 88.99%. Further analysis\nof code quality, programming language diversity, latency, and consistency\ndemonstrates a balanced performance in terms of efficiency, maintainability and\navailability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show great capabilities in generating code from\nnatural language descriptions, bringing programming power closer to\nnon-technical users. However, their lack of expertise in operating the\ngenerated code remains a key barrier to realizing customized applications.\nFunction-as-a-Service (FaaS) platforms offer a high level of abstraction for\ncode execution and deployment, allowing users to run LLM-generated code without\nrequiring technical expertise or incurring operational overhead.\n  In this paper, we present LLM4FaaS, a no-code application development\napproach that integrates LLMs and FaaS platforms to enable non-technical users\nto build and run customized applications using only natural language. By\ndeploying LLM-generated code through FaaS, LLM4FaaS abstracts away\ninfrastructure management and boilerplate code generation. We implement a\nproof-of-concept prototype based on an open-source FaaS platform, and evaluate\nit using real prompts from non-technical users. Experiments with GPT-4o show\nthat LLM4FaaS can automatically build and deploy code in 71.47% of cases,\noutperforming a non-FaaS baseline at 43.48% and an existing LLM-based platform\nat 14.55%, narrowing the gap to human performance at 88.99%. Further analysis\nof code quality, programming language diversity, latency, and consistency\ndemonstrates a balanced performance in terms of efficiency, maintainability and\navailability."
                },
                "authors": [
                    {
                        "name": "Minghe Wang"
                    },
                    {
                        "name": "Tobias Pfandzelter"
                    },
                    {
                        "name": "Trever Schirmer"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "arxiv_comment": "Accepted for publication in 2025 IEEE/ACM 18th International\n  Conference on Utility and Cloud Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26573v2",
                "updated": "2025-11-07T14:25:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    25,
                    50,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-30T17:33:58Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    33,
                    58,
                    1,
                    273,
                    0
                ],
                "title": "Statistical Inference for Extended Target Detection in mmWave Automotive\n  Radar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for Extended Target Detection in mmWave Automotive\n  Radar"
                },
                "summary": "Millimeter-wave (mmWave) automotive radar systems offer high range resolution\ndue to their wide bandwidth, enabling the detection of multiple spatially\ndistributed scatterers from a single extended target, such as a vehicle.\nTraditional CFAR-based detection methods often treat these scatterers as\nindependent point targets, thereby neglecting the inherent spatial structure of\nextended objects. To address this limitation, we propose a novel Range-Doppler\n(RD) segment-based statistical inference framework that captures the\ncharacteristic scattering profile of extended automotive targets. The framework\nemploys Maximum Likelihood Estimation (MLE) for statistical parameter\nextraction and utilizes Gibbs sampling within a Markov Chain Monte Carlo (MCMC)\nscheme to model the posterior distribution of the segment features. A\nskewness-based test statistic, derived from the estimated distribution, is\nintroduced for binary hypothesis testing to distinguish extended targets.\nFurthermore, we develop a detection pipeline incorporating Intersection over\nUnion (IoU) metrics and peak-centric segment alignment, optimized for\nsingle-dwell radar operations. Comprehensive evaluations on both simulated and\nreal-world datasets demonstrate the effectiveness of the proposed method,\nachieving enhanced detection accuracy and robustness in automotive radar\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millimeter-wave (mmWave) automotive radar systems offer high range resolution\ndue to their wide bandwidth, enabling the detection of multiple spatially\ndistributed scatterers from a single extended target, such as a vehicle.\nTraditional CFAR-based detection methods often treat these scatterers as\nindependent point targets, thereby neglecting the inherent spatial structure of\nextended objects. To address this limitation, we propose a novel Range-Doppler\n(RD) segment-based statistical inference framework that captures the\ncharacteristic scattering profile of extended automotive targets. The framework\nemploys Maximum Likelihood Estimation (MLE) for statistical parameter\nextraction and utilizes Gibbs sampling within a Markov Chain Monte Carlo (MCMC)\nscheme to model the posterior distribution of the segment features. A\nskewness-based test statistic, derived from the estimated distribution, is\nintroduced for binary hypothesis testing to distinguish extended targets.\nFurthermore, we develop a detection pipeline incorporating Intersection over\nUnion (IoU) metrics and peak-centric segment alignment, optimized for\nsingle-dwell radar operations. Comprehensive evaluations on both simulated and\nreal-world datasets demonstrate the effectiveness of the proposed method,\nachieving enhanced detection accuracy and robustness in automotive radar\nscenarios."
                },
                "authors": [
                    {
                        "name": "Vinay Kulkarni"
                    },
                    {
                        "name": "V. V. Reddy"
                    }
                ],
                "author_detail": {
                    "name": "V. V. Reddy"
                },
                "author": "V. V. Reddy",
                "arxiv_comment": "12 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05261v1",
                "updated": "2025-11-07T14:24:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    24,
                    49,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T14:24:49Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    24,
                    49,
                    4,
                    311,
                    0
                ],
                "title": "Fuzzy Neural Network Performance and Interpretability of Quantum\n  Wavefunction Probability Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fuzzy Neural Network Performance and Interpretability of Quantum\n  Wavefunction Probability Predictions"
                },
                "summary": "Predicting quantum wavefunction probability distributions is crucial for\ncomputational chemistry and materials science, yet machine learning (ML) models\noften face a trade-off between accuracy and interpretability. This study\ncompares Artificial Neural Networks (ANNs) and Adaptive Neuro-Fuzzy Inference\nSystems (ANFIS) in modeling quantum probability distributions for the H$_{2}^+$\nion, leveraging data generated via Physics-Informed Neural Networks (PINNs).\nWhile ANN achieved superior accuracy (R$^2$ = 0.99 vs ANFIS's 0.95 with\nGaussian membership functions), it required over 50x more parameters (2,305 vs\n39-45). ANFIS, however, provided unique interpretability: its Gaussian\nmembership functions encoded spatial electron localization near proton\npositions ($\\mu = 1.2 A$), mirroring Born probability densities, while fuzzy\nrules reflected quantum superposition principles. Rules prioritizing the\ninternuclear direction revealed the system's 1D symmetry, aligning with Linear\nCombination of Atomic Orbitals theory--a novel data-driven perspective on\norbital hybridization. Membership function variances ($\\sigma$) further\nquantified electron delocalization trends, and peak prediction errors\nhighlighted unresolved quantum cusps. The choice of functions critically\nimpacted performance: Gaussian/Generalized Bell outperformed Sigmoid, with\nerrors improving as training data increased, showing scalability. This study\nunderscores the context-dependent value of ML: ANN for precision and ANFIS for\ninterpretable, parameter-efficient approximations that link inputs to physical\nbehavior. These findings advocate hybrid approaches in quantum simulations,\nbalancing accuracy with explainability to accelerate discovery. Future work\nshould extend ANFIS to multi-electron systems and integrate domain-specific\nconstraints (e.g., kinetic energy terms), bridging data-driven models and\nfundamental physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting quantum wavefunction probability distributions is crucial for\ncomputational chemistry and materials science, yet machine learning (ML) models\noften face a trade-off between accuracy and interpretability. This study\ncompares Artificial Neural Networks (ANNs) and Adaptive Neuro-Fuzzy Inference\nSystems (ANFIS) in modeling quantum probability distributions for the H$_{2}^+$\nion, leveraging data generated via Physics-Informed Neural Networks (PINNs).\nWhile ANN achieved superior accuracy (R$^2$ = 0.99 vs ANFIS's 0.95 with\nGaussian membership functions), it required over 50x more parameters (2,305 vs\n39-45). ANFIS, however, provided unique interpretability: its Gaussian\nmembership functions encoded spatial electron localization near proton\npositions ($\\mu = 1.2 A$), mirroring Born probability densities, while fuzzy\nrules reflected quantum superposition principles. Rules prioritizing the\ninternuclear direction revealed the system's 1D symmetry, aligning with Linear\nCombination of Atomic Orbitals theory--a novel data-driven perspective on\norbital hybridization. Membership function variances ($\\sigma$) further\nquantified electron delocalization trends, and peak prediction errors\nhighlighted unresolved quantum cusps. The choice of functions critically\nimpacted performance: Gaussian/Generalized Bell outperformed Sigmoid, with\nerrors improving as training data increased, showing scalability. This study\nunderscores the context-dependent value of ML: ANN for precision and ANFIS for\ninterpretable, parameter-efficient approximations that link inputs to physical\nbehavior. These findings advocate hybrid approaches in quantum simulations,\nbalancing accuracy with explainability to accelerate discovery. Future work\nshould extend ANFIS to multi-electron systems and integrate domain-specific\nconstraints (e.g., kinetic energy terms), bridging data-driven models and\nfundamental physics."
                },
                "authors": [
                    {
                        "name": "Pedro H. M. Zanineli"
                    },
                    {
                        "name": "Matheus Zaia Monteiro"
                    },
                    {
                        "name": "Vinicius Francisco Wasques"
                    },
                    {
                        "name": "Francielle Santo Pedro Simões"
                    },
                    {
                        "name": "Gabriel R. Schleder"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel R. Schleder"
                },
                "author": "Gabriel R. Schleder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23262v2",
                "updated": "2025-11-07T14:09:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    9,
                    42,
                    4,
                    311,
                    0
                ],
                "published": "2025-05-29T09:11:58Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    11,
                    58,
                    3,
                    149,
                    0
                ],
                "title": "Applying Large Language Models to Travel Satisfaction Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying Large Language Models to Travel Satisfaction Analysis"
                },
                "summary": "As a specific domain of subjective well-being, travel satisfaction has\nrecently attracted much research attention. Previous studies primarily relied\non statistical models and, more recently, machine learning models to explore\nits determinants. Both approaches,however, depend on sufficiently large sample\nsizes and appropriate statistical assumptions. The emergence of Large Language\nModels (LLMs) offers a new modeling approach that can address these\nlimitations. Pre-trained on extensive datasets, LLMs have strongcapabilities in\ncontextual understanding and generalization, significantly reducing their\ndependence on task-specific data and stringent statistical assumptions. The\nmain challenge in applying LLMs lies in the behavioral misalignment between\nLLMs and humans. Using household survey data collected in Shanghai, this study\nidentifies the existence and source of misalignment, and applies a few-shot\nlearning method to address the misalignment issue. We find that the zero-shot\nLLM exhibits behavioral misalignment, leading to low prediction accuracy. With\njust a few samples, few-shot learning can align LLMs and enable them to\noutperform baseline models. Discrepancies in variable importance among machine\nlearning model, zero-shot LLM, and few-shot LLM reveal that the misalignment\narises from the gap between the general knowledge embedded in pre-trained LLMs\nand the specific, unique characteristics of the dataset. On these bases, we\npropose an LLM-based modeling approach that can be applied to model travel\nbehavior with small sample sizes. This study highlights the potential of LLMs\nfor modeling not only travel satisfaction but also broader aspects of travel\nbehavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a specific domain of subjective well-being, travel satisfaction has\nrecently attracted much research attention. Previous studies primarily relied\non statistical models and, more recently, machine learning models to explore\nits determinants. Both approaches,however, depend on sufficiently large sample\nsizes and appropriate statistical assumptions. The emergence of Large Language\nModels (LLMs) offers a new modeling approach that can address these\nlimitations. Pre-trained on extensive datasets, LLMs have strongcapabilities in\ncontextual understanding and generalization, significantly reducing their\ndependence on task-specific data and stringent statistical assumptions. The\nmain challenge in applying LLMs lies in the behavioral misalignment between\nLLMs and humans. Using household survey data collected in Shanghai, this study\nidentifies the existence and source of misalignment, and applies a few-shot\nlearning method to address the misalignment issue. We find that the zero-shot\nLLM exhibits behavioral misalignment, leading to low prediction accuracy. With\njust a few samples, few-shot learning can align LLMs and enable them to\noutperform baseline models. Discrepancies in variable importance among machine\nlearning model, zero-shot LLM, and few-shot LLM reveal that the misalignment\narises from the gap between the general knowledge embedded in pre-trained LLMs\nand the specific, unique characteristics of the dataset. On these bases, we\npropose an LLM-based modeling approach that can be applied to model travel\nbehavior with small sample sizes. This study highlights the potential of LLMs\nfor modeling not only travel satisfaction but also broader aspects of travel\nbehavior."
                },
                "authors": [
                    {
                        "name": "Pengfei Xu"
                    },
                    {
                        "name": "Donggen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donggen Wang"
                },
                "author": "Donggen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04173v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04173v4",
                "updated": "2025-11-07T14:02:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    2,
                    33,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-05T12:26:42Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    12,
                    26,
                    42,
                    6,
                    278,
                    0
                ],
                "title": "Open Agent Specification (Agent Spec): A Unified Representation for AI\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Agent Specification (Agent Spec): A Unified Representation for AI\n  Agents"
                },
                "summary": "The proliferation of agent frameworks has led to fragmentation in how agents\nare defined, executed, and evaluated. Existing systems differ in their\nabstractions, data flow semantics, and tool integrations, making it difficult\nto share or reproduce workflows. We introduce Open Agent Specification (Agent\nSpec), a declarative language that defines AI agents and agentic workflows in a\nway that is compatible across frameworks, promoting reusability, portability\nand interoperability of AI agents. Agent Spec defines a common set of\ncomponents, control and data flow semantics, and schemas that allow an agent to\nbe defined once and executed across different runtimes. Agent Spec also\nintroduces a standardized Evaluation harness to assess agent behavior and\nagentic workflows across runtimes - analogous to how HELM and related harnesses\nstandardized LLM evaluation - so that performance, robustness, and efficiency\ncan be compared consistently across frameworks. We demonstrate this using four\ndistinct runtimes (LangGraph, CrewAI, AutoGen, and WayFlow) evaluated over\nthree different benchmarks (SimpleQA Verified, $\\tau^2$-Bench and BIRD-SQL). We\nprovide accompanying toolsets: a Python SDK (PyAgentSpec), a reference runtime\n(WayFlow), and adapters for popular frameworks (e.g., LangGraph, AutoGen,\nCrewAI). Agent Spec bridges the gap between model-centric and agent-centric\nstandardization & evaluation, laying the groundwork for reliable, reusable, and\nportable agentic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of agent frameworks has led to fragmentation in how agents\nare defined, executed, and evaluated. Existing systems differ in their\nabstractions, data flow semantics, and tool integrations, making it difficult\nto share or reproduce workflows. We introduce Open Agent Specification (Agent\nSpec), a declarative language that defines AI agents and agentic workflows in a\nway that is compatible across frameworks, promoting reusability, portability\nand interoperability of AI agents. Agent Spec defines a common set of\ncomponents, control and data flow semantics, and schemas that allow an agent to\nbe defined once and executed across different runtimes. Agent Spec also\nintroduces a standardized Evaluation harness to assess agent behavior and\nagentic workflows across runtimes - analogous to how HELM and related harnesses\nstandardized LLM evaluation - so that performance, robustness, and efficiency\ncan be compared consistently across frameworks. We demonstrate this using four\ndistinct runtimes (LangGraph, CrewAI, AutoGen, and WayFlow) evaluated over\nthree different benchmarks (SimpleQA Verified, $\\tau^2$-Bench and BIRD-SQL). We\nprovide accompanying toolsets: a Python SDK (PyAgentSpec), a reference runtime\n(WayFlow), and adapters for popular frameworks (e.g., LangGraph, AutoGen,\nCrewAI). Agent Spec bridges the gap between model-centric and agent-centric\nstandardization & evaluation, laying the groundwork for reliable, reusable, and\nportable agentic systems."
                },
                "authors": [
                    {
                        "name": "Soufiane Amini"
                    },
                    {
                        "name": "Yassine Benajiba"
                    },
                    {
                        "name": "Cesare Bernardis"
                    },
                    {
                        "name": "Paul Cayet"
                    },
                    {
                        "name": "Hassan Chafi"
                    },
                    {
                        "name": "Abderrahim Fathan"
                    },
                    {
                        "name": "Louis Faucon"
                    },
                    {
                        "name": "Damien Hilloulin"
                    },
                    {
                        "name": "Sungpack Hong"
                    },
                    {
                        "name": "Ingo Kossyk"
                    },
                    {
                        "name": "Tran Minh Son Le"
                    },
                    {
                        "name": "Rhicheek Patra"
                    },
                    {
                        "name": "Sujith Ravi"
                    },
                    {
                        "name": "Jonas Schweizer"
                    },
                    {
                        "name": "Jyotika Singh"
                    },
                    {
                        "name": "Shailender Singh"
                    },
                    {
                        "name": "Weiyi Sun"
                    },
                    {
                        "name": "Kartik Talamadupula"
                    },
                    {
                        "name": "Jerry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jerry Xu"
                },
                "author": "Jerry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04173v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04173v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03265v2",
                "updated": "2025-11-07T13:51:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    51,
                    28,
                    4,
                    311,
                    0
                ],
                "published": "2025-01-04T06:17:48Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    6,
                    17,
                    48,
                    5,
                    4,
                    0
                ],
                "title": "Cognitive Edge Computing: A Comprehensive Survey on Optimizing Large\n  Models and AI Agents for Pervasive Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Edge Computing: A Comprehensive Survey on Optimizing Large\n  Models and AI Agents for Pervasive Deployment"
                },
                "summary": "This article surveys Cognitive Edge Computing as a practical and methodical\npathway for deploying reasoning-capable Large Language Models (LLMs) and\nautonomous AI agents on resource-constrained devices at the network edge. We\npresent a unified, cognition-preserving framework spanning: (1) model\noptimization (quantization, sparsity, low-rank adaptation, distillation) aimed\nat retaining multi-step reasoning under tight memory/compute budgets; (2)\nsystem architecture (on-device inference, elastic offloading, cloud-edge\ncollaboration) that trades off latency, energy, privacy, and capacity; and (3)\nadaptive intelligence (context compression, dynamic routing, federated\npersonalization) that tailors computation to task difficulty and device\nconstraints. We synthesize advances in efficient Transformer design, multimodal\nintegration, hardware-aware compilation, privacy-preserving learning, and\nagentic tool use, and map them to edge-specific operating envelopes. We further\noutline a standardized evaluation protocol covering latency, throughput, energy\nper token, accuracy, robustness, privacy, and sustainability, with explicit\nmeasurement assumptions to enhance comparability. Remaining challenges include\nmodality-aware reasoning benchmarks, transparent and reproducible energy\nreporting, edge-oriented safety/alignment evaluation, and multi-agent testbeds.\nWe conclude with practitioner guidelines for cross-layer co-design of\nalgorithms, runtime, and hardware to deliver reliable, efficient, and\nprivacy-preserving cognitive capabilities on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article surveys Cognitive Edge Computing as a practical and methodical\npathway for deploying reasoning-capable Large Language Models (LLMs) and\nautonomous AI agents on resource-constrained devices at the network edge. We\npresent a unified, cognition-preserving framework spanning: (1) model\noptimization (quantization, sparsity, low-rank adaptation, distillation) aimed\nat retaining multi-step reasoning under tight memory/compute budgets; (2)\nsystem architecture (on-device inference, elastic offloading, cloud-edge\ncollaboration) that trades off latency, energy, privacy, and capacity; and (3)\nadaptive intelligence (context compression, dynamic routing, federated\npersonalization) that tailors computation to task difficulty and device\nconstraints. We synthesize advances in efficient Transformer design, multimodal\nintegration, hardware-aware compilation, privacy-preserving learning, and\nagentic tool use, and map them to edge-specific operating envelopes. We further\noutline a standardized evaluation protocol covering latency, throughput, energy\nper token, accuracy, robustness, privacy, and sustainability, with explicit\nmeasurement assumptions to enhance comparability. Remaining challenges include\nmodality-aware reasoning benchmarks, transparent and reproducible energy\nreporting, edge-oriented safety/alignment evaluation, and multi-agent testbeds.\nWe conclude with practitioner guidelines for cross-layer co-design of\nalgorithms, runtime, and hardware to deliver reliable, efficient, and\nprivacy-preserving cognitive capabilities on edge devices."
                },
                "authors": [
                    {
                        "name": "Xubin Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01708v2",
                "updated": "2025-11-07T13:49:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    49,
                    52,
                    4,
                    311,
                    0
                ],
                "published": "2025-03-03T16:21:04Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    16,
                    21,
                    4,
                    0,
                    62,
                    0
                ],
                "title": "Pseudo-Maximum Likelihood Theory for High-Dimensional Rank One Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pseudo-Maximum Likelihood Theory for High-Dimensional Rank One Inference"
                },
                "summary": "We develop a pseudo-likelihood theory for rank one matrix estimation problems\nin the high dimensional limit. We prove a variational principle for the\nlimiting pseudo-maximum likelihood which also characterizes the performance of\nthe corresponding pseudo-maximum likelihood estimator. We show that this\nvariational principle is universal and depends only on four parameters\ndetermined by the corresponding null model. Through this universality, we\nintroduce a notion of equivalence for estimation problems of this type and, in\nparticular, show that a broad class of estimation tasks, including community\ndetection, sparse submatrix detection, and non-linear spiked matrix models, are\nequivalent to spiked matrix models. As an application, we obtain a complete\ndescription of the performance of the least-squares (or ``best rank one'')\nestimator for any rank one matrix estimation problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a pseudo-likelihood theory for rank one matrix estimation problems\nin the high dimensional limit. We prove a variational principle for the\nlimiting pseudo-maximum likelihood which also characterizes the performance of\nthe corresponding pseudo-maximum likelihood estimator. We show that this\nvariational principle is universal and depends only on four parameters\ndetermined by the corresponding null model. Through this universality, we\nintroduce a notion of equivalence for estimation problems of this type and, in\nparticular, show that a broad class of estimation tasks, including community\ndetection, sparse submatrix detection, and non-linear spiked matrix models, are\nequivalent to spiked matrix models. As an application, we obtain a complete\ndescription of the performance of the least-squares (or ``best rank one'')\nestimator for any rank one matrix estimation problem."
                },
                "authors": [
                    {
                        "name": "Curtis Grant"
                    },
                    {
                        "name": "Aukosh Jagannath"
                    },
                    {
                        "name": "Justin Ko"
                    }
                ],
                "author_detail": {
                    "name": "Justin Ko"
                },
                "author": "Justin Ko",
                "arxiv_comment": "58 pages, 3 figures; added more examples and figures, modified the\n  orgainization of the article",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21993v2",
                "updated": "2025-11-07T13:49:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    49,
                    40,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-26T07:19:39Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    7,
                    19,
                    39,
                    4,
                    269,
                    0
                ],
                "title": "Bilinear relational structure fixes reversal curse and enables\n  consistent model editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bilinear relational structure fixes reversal curse and enables\n  consistent model editing"
                },
                "summary": "The reversal curse -- a language model's (LM) inability to infer an unseen\nfact ``B is A'' from a learned fact ``A is B'' -- is widely considered a\nfundamental limitation. We show that this is not an inherent failure but an\nartifact of how models encode knowledge. By training LMs from scratch on a\nsynthetic dataset of relational knowledge graphs, we demonstrate that bilinear\nrelational structure emerges in their hidden representations. This structure\nsubstantially alleviates the reversal curse, enabling LMs to infer unseen\nreverse facts. Crucially, we also find that this bilinear structure plays a key\nrole in consistent model editing. When a fact is updated in a LM with this\nstructure, the edit correctly propagates to its reverse and other logically\ndependent facts. In contrast, models lacking this representation not only\nsuffer from the reversal curse but also fail to generalize edits, further\nintroducing logical inconsistencies. Our results establish that training on a\nrelational knowledge dataset induces the emergence of bilinear internal\nrepresentations, which in turn enable LMs to behave in a logically consistent\nmanner after editing. This implies that the success of model editing depends\ncritically not just on editing algorithms but on the underlying\nrepresentational geometry of the knowledge being modified.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reversal curse -- a language model's (LM) inability to infer an unseen\nfact ``B is A'' from a learned fact ``A is B'' -- is widely considered a\nfundamental limitation. We show that this is not an inherent failure but an\nartifact of how models encode knowledge. By training LMs from scratch on a\nsynthetic dataset of relational knowledge graphs, we demonstrate that bilinear\nrelational structure emerges in their hidden representations. This structure\nsubstantially alleviates the reversal curse, enabling LMs to infer unseen\nreverse facts. Crucially, we also find that this bilinear structure plays a key\nrole in consistent model editing. When a fact is updated in a LM with this\nstructure, the edit correctly propagates to its reverse and other logically\ndependent facts. In contrast, models lacking this representation not only\nsuffer from the reversal curse but also fail to generalize edits, further\nintroducing logical inconsistencies. Our results establish that training on a\nrelational knowledge dataset induces the emergence of bilinear internal\nrepresentations, which in turn enable LMs to behave in a logically consistent\nmanner after editing. This implies that the success of model editing depends\ncritically not just on editing algorithms but on the underlying\nrepresentational geometry of the knowledge being modified."
                },
                "authors": [
                    {
                        "name": "Dong-Kyum Kim"
                    },
                    {
                        "name": "Minsung Kim"
                    },
                    {
                        "name": "Jea Kwon"
                    },
                    {
                        "name": "Nakyeong Yang"
                    },
                    {
                        "name": "Meeyoung Cha"
                    }
                ],
                "author_detail": {
                    "name": "Meeyoung Cha"
                },
                "author": "Meeyoung Cha",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05239v1",
                "updated": "2025-11-07T13:46:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    46,
                    16,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T13:46:16Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    46,
                    16,
                    4,
                    311,
                    0
                ],
                "title": "Translation via Annotation: A Computational Study of Translating\n  Classical Chinese into Japanese",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translation via Annotation: A Computational Study of Translating\n  Classical Chinese into Japanese"
                },
                "summary": "Ancient people translated classical Chinese into Japanese by annotating\naround each character. We abstract this process as sequence tagging tasks and\nfit them into modern language technologies. The research of this annotation and\ntranslation system is a facing low-resource problem. We release this problem by\nintroducing a LLM-based annotation pipeline and construct a new dataset from\ndigitalized open-source translation data. We show that under the low-resource\nsetting, introducing auxiliary Chinese NLP tasks has a promoting effect on the\ntraining of sequence tagging tasks. We also evaluate the performance of large\nlanguage models. They achieve high scores in direct machine translation, but\nthey are confused when being asked to annotate characters. Our method could\nwork as a supplement of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ancient people translated classical Chinese into Japanese by annotating\naround each character. We abstract this process as sequence tagging tasks and\nfit them into modern language technologies. The research of this annotation and\ntranslation system is a facing low-resource problem. We release this problem by\nintroducing a LLM-based annotation pipeline and construct a new dataset from\ndigitalized open-source translation data. We show that under the low-resource\nsetting, introducing auxiliary Chinese NLP tasks has a promoting effect on the\ntraining of sequence tagging tasks. We also evaluate the performance of large\nlanguage models. They achieve high scores in direct machine translation, but\nthey are confused when being asked to annotate characters. Our method could\nwork as a supplement of LLMs."
                },
                "authors": [
                    {
                        "name": "Zilong Li"
                    },
                    {
                        "name": "Jie Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jie Cao"
                },
                "author": "Jie Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17459v2",
                "updated": "2025-11-07T13:42:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    42,
                    16,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-20T11:46:55Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    46,
                    55,
                    0,
                    293,
                    0
                ],
                "title": "Estimating Orbital Parameters of Direct Imaging Exoplanet Using Neural\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Orbital Parameters of Direct Imaging Exoplanet Using Neural\n  Network"
                },
                "summary": "In this work, we propose a new flow-matching Markov chain Monte Carlo\n(FM-MCMC) algorithm for estimating the orbital parameters of exoplanetary\nsystems, especially for those only one exoplanet is involved. Compared to\ntraditional methods that rely on random sampling within the Bayesian framework,\nour approach first leverages flow matching posterior estimation (FMPE) to\nefficiently constrain the prior range of physical parameters, and then employs\nMCMC to accurately infer the posterior distribution. For example, in the\norbital parameter inference of beta Pictoris b, our model achieved a\nsubstantial speed-up while maintaining comparable accuracy-running 77.8 times\nfaster than Parallel Tempered MCMC (PTMCMC) and 365.4 times faster than nested\nsampling. Moreover, our FM-MCMC method also attained the highest average\nlog-likelihood among all approaches, demonstrating its superior sampling\nefficiency and accuracy. This highlights the scalability and efficiency of our\napproach, making it well-suited for processing the massive datasets expected\nfrom future exoplanet surveys. Beyond astrophysics, our methodology establishes\na versatile paradigm for synergizing deep generative models with traditional\nsampling, which can be adopted to tackle complex inference problems in other\nfields, such as cosmology, biomedical imaging, and particle physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a new flow-matching Markov chain Monte Carlo\n(FM-MCMC) algorithm for estimating the orbital parameters of exoplanetary\nsystems, especially for those only one exoplanet is involved. Compared to\ntraditional methods that rely on random sampling within the Bayesian framework,\nour approach first leverages flow matching posterior estimation (FMPE) to\nefficiently constrain the prior range of physical parameters, and then employs\nMCMC to accurately infer the posterior distribution. For example, in the\norbital parameter inference of beta Pictoris b, our model achieved a\nsubstantial speed-up while maintaining comparable accuracy-running 77.8 times\nfaster than Parallel Tempered MCMC (PTMCMC) and 365.4 times faster than nested\nsampling. Moreover, our FM-MCMC method also attained the highest average\nlog-likelihood among all approaches, demonstrating its superior sampling\nefficiency and accuracy. This highlights the scalability and efficiency of our\napproach, making it well-suited for processing the massive datasets expected\nfrom future exoplanet surveys. Beyond astrophysics, our methodology establishes\na versatile paradigm for synergizing deep generative models with traditional\nsampling, which can be adopted to tackle complex inference problems in other\nfields, such as cosmology, biomedical imaging, and particle physics."
                },
                "authors": [
                    {
                        "name": "Bo Liang"
                    },
                    {
                        "name": "Hanlin Song"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Tianyu Zhao"
                    },
                    {
                        "name": "Yuxiang Xu"
                    },
                    {
                        "name": "Zihao Xiao"
                    },
                    {
                        "name": "Manjia Liang"
                    },
                    {
                        "name": "Minghui Du"
                    },
                    {
                        "name": "Wei-Liang Qian"
                    },
                    {
                        "name": "Li-e Qiang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Ziren Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ziren Luo"
                },
                "author": "Ziren Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05236v1",
                "updated": "2025-11-07T13:37:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    37,
                    23,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T13:37:23Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    37,
                    23,
                    4,
                    311,
                    0
                ],
                "title": "The Causal Round Trip: Generating Authentic Counterfactuals by\n  Eliminating Information Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Causal Round Trip: Generating Authentic Counterfactuals by\n  Eliminating Information Loss"
                },
                "summary": "Judea Pearl's vision of Structural Causal Models (SCMs) as engines for\ncounterfactual reasoning hinges on faithful abduction: the precise inference of\nlatent exogenous noise. For decades, operationalizing this step for complex,\nnon-linear mechanisms has remained a significant computational challenge. The\nadvent of diffusion models, powerful universal function approximators, offers a\npromising solution. However, we argue that their standard design, optimized for\nperceptual generation over logical inference, introduces a fundamental flaw for\nthis classical problem: an inherent information loss we term the Structural\nReconstruction Error (SRE). To address this challenge, we formalize the\nprinciple of Causal Information Conservation (CIC) as the necessary condition\nfor faithful abduction. We then introduce BELM-MDCM, the first diffusion-based\nframework engineered to be causally sound by eliminating SRE by construction\nthrough an analytically invertible mechanism. To operationalize this framework,\na Targeted Modeling strategy provides structural regularization, while a Hybrid\nTraining Objective instills a strong causal inductive bias. Rigorous\nexperiments demonstrate that our Zero-SRE framework not only achieves\nstate-of-the-art accuracy but, more importantly, enables the high-fidelity,\nindividual-level counterfactuals required for deep causal inquiries. Our work\nprovides a foundational blueprint that reconciles the power of modern\ngenerative models with the rigor of classical causal theory, establishing a new\nand more rigorous standard for this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judea Pearl's vision of Structural Causal Models (SCMs) as engines for\ncounterfactual reasoning hinges on faithful abduction: the precise inference of\nlatent exogenous noise. For decades, operationalizing this step for complex,\nnon-linear mechanisms has remained a significant computational challenge. The\nadvent of diffusion models, powerful universal function approximators, offers a\npromising solution. However, we argue that their standard design, optimized for\nperceptual generation over logical inference, introduces a fundamental flaw for\nthis classical problem: an inherent information loss we term the Structural\nReconstruction Error (SRE). To address this challenge, we formalize the\nprinciple of Causal Information Conservation (CIC) as the necessary condition\nfor faithful abduction. We then introduce BELM-MDCM, the first diffusion-based\nframework engineered to be causally sound by eliminating SRE by construction\nthrough an analytically invertible mechanism. To operationalize this framework,\na Targeted Modeling strategy provides structural regularization, while a Hybrid\nTraining Objective instills a strong causal inductive bias. Rigorous\nexperiments demonstrate that our Zero-SRE framework not only achieves\nstate-of-the-art accuracy but, more importantly, enables the high-fidelity,\nindividual-level counterfactuals required for deep causal inquiries. Our work\nprovides a foundational blueprint that reconciles the power of modern\ngenerative models with the rigor of classical causal theory, establishing a new\nand more rigorous standard for this emerging field."
                },
                "authors": [
                    {
                        "name": "Rui Wu"
                    },
                    {
                        "name": "Lizheng Wang"
                    },
                    {
                        "name": "Yongjun Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongjun Li"
                },
                "author": "Yongjun Li",
                "arxiv_comment": "50 pages, 10 figures. Submitted to the Journal of Machine Learning\n  Research (JMLR). Keywords: Causal Inference, Diffusion Models, Causal\n  Information Conservation, Structural Causal Models, Counterfactual\n  Generation, BELM, Structural Reconstruction Error",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05234v1",
                "updated": "2025-11-07T13:34:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    34,
                    2,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T13:34:02Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    34,
                    2,
                    4,
                    311,
                    0
                ],
                "title": "Context-aware Learned Mesh-based Simulation via Trajectory-Level\n  Meta-Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-aware Learned Mesh-based Simulation via Trajectory-Level\n  Meta-Learning"
                },
                "summary": "Simulating object deformations is a critical challenge across many scientific\ndomains, including robotics, manufacturing, and structural mechanics. Learned\nGraph Network Simulators (GNSs) offer a promising alternative to traditional\nmesh-based physics simulators. Their speed and inherent differentiability make\nthem particularly well suited for applications that require fast and accurate\nsimulations, such as robotic manipulation or manufacturing optimization.\nHowever, existing learned simulators typically rely on single-step\nobservations, which limits their ability to exploit temporal context. Without\nthis information, these models fail to infer, e.g., material properties.\nFurther, they rely on auto-regressive rollouts, which quickly accumulate error\nfor long trajectories. We instead frame mesh-based simulation as a\ntrajectory-level meta-learning problem. Using Conditional Neural Processes, our\nmethod enables rapid adaptation to new simulation scenarios from limited\ninitial data while capturing their latent simulation properties. We utilize\nmovement primitives to directly predict fast, stable and accurate simulations\nfrom a single model call. The resulting approach, Movement-primitive\nMeta-MeshGraphNet (M3GN), provides higher simulation accuracy at a fraction of\nthe runtime cost compared to state-of-the-art GNSs across several tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating object deformations is a critical challenge across many scientific\ndomains, including robotics, manufacturing, and structural mechanics. Learned\nGraph Network Simulators (GNSs) offer a promising alternative to traditional\nmesh-based physics simulators. Their speed and inherent differentiability make\nthem particularly well suited for applications that require fast and accurate\nsimulations, such as robotic manipulation or manufacturing optimization.\nHowever, existing learned simulators typically rely on single-step\nobservations, which limits their ability to exploit temporal context. Without\nthis information, these models fail to infer, e.g., material properties.\nFurther, they rely on auto-regressive rollouts, which quickly accumulate error\nfor long trajectories. We instead frame mesh-based simulation as a\ntrajectory-level meta-learning problem. Using Conditional Neural Processes, our\nmethod enables rapid adaptation to new simulation scenarios from limited\ninitial data while capturing their latent simulation properties. We utilize\nmovement primitives to directly predict fast, stable and accurate simulations\nfrom a single model call. The resulting approach, Movement-primitive\nMeta-MeshGraphNet (M3GN), provides higher simulation accuracy at a fraction of\nthe runtime cost compared to state-of-the-art GNSs across several tasks."
                },
                "authors": [
                    {
                        "name": "Philipp Dahlinger"
                    },
                    {
                        "name": "Niklas Freymuth"
                    },
                    {
                        "name": "Tai Hoang"
                    },
                    {
                        "name": "Tobias Würth"
                    },
                    {
                        "name": "Michael Volpp"
                    },
                    {
                        "name": "Luise Kärger"
                    },
                    {
                        "name": "Gerhard Neumann"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Neumann"
                },
                "author": "Gerhard Neumann",
                "arxiv_comment": "35 pages. Submitted to Transactions on Machine Learning Research\n  (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18116v2",
                "updated": "2025-11-07T13:28:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    28,
                    17,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-10T07:03:35Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    3,
                    35,
                    2,
                    253,
                    0
                ],
                "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortized Latent Steering: Low-Cost Alternative to Test-Time\n  Optimization"
                },
                "summary": "Test-time optimization remains impractical at scale due to prohibitive\ninference costs--techniques like iterative refinement and multi-step\nverification can require $10-100\\times$ more compute per query than standard\ndecoding. Latent space test-time optimization methods like LatentSeek offer a\nmore direct approach by steering hidden representations, but still demand\nexpensive per-query optimization loops with multiple backward passes. We\npropose Amortized Latent Steering (ALS), which collapses this iterative\noptimization into a single offline-computed vector applied at constant cost\nduring inference. ALS computes the mean difference between hidden states from\nsuccessful versus unsuccessful generations, then uses this direction to\ncalibrate the model's hidden representations: when decoding drifts away from\nthe success manifold, ALS nudges activations back toward it. Across GSM8K and\nMATH-500 benchmarks, ALS achieves $2-5\\times$ speedup over iterative methods\nwhile matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency\nbaselines, yielding up to 101% improvement in efficiency--accuracy trade-off.\nThese results show that much of latent optimization's benefit can be captured\noffline, making sophisticated reasoning techniques viable for production\ndeployment. Code is available at https://github.com/negbuna/ALS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time optimization remains impractical at scale due to prohibitive\ninference costs--techniques like iterative refinement and multi-step\nverification can require $10-100\\times$ more compute per query than standard\ndecoding. Latent space test-time optimization methods like LatentSeek offer a\nmore direct approach by steering hidden representations, but still demand\nexpensive per-query optimization loops with multiple backward passes. We\npropose Amortized Latent Steering (ALS), which collapses this iterative\noptimization into a single offline-computed vector applied at constant cost\nduring inference. ALS computes the mean difference between hidden states from\nsuccessful versus unsuccessful generations, then uses this direction to\ncalibrate the model's hidden representations: when decoding drifts away from\nthe success manifold, ALS nudges activations back toward it. Across GSM8K and\nMATH-500 benchmarks, ALS achieves $2-5\\times$ speedup over iterative methods\nwhile matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency\nbaselines, yielding up to 101% improvement in efficiency--accuracy trade-off.\nThese results show that much of latent optimization's benefit can be captured\noffline, making sophisticated reasoning techniques viable for production\ndeployment. Code is available at https://github.com/negbuna/ALS."
                },
                "authors": [
                    {
                        "name": "Nathan Egbuna"
                    },
                    {
                        "name": "Saatvik Gaur"
                    },
                    {
                        "name": "Sunishchal Dev"
                    },
                    {
                        "name": "Ashwinee Panda"
                    },
                    {
                        "name": "Maheep Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Maheep Chaudhary"
                },
                "author": "Maheep Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17086v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17086v4",
                "updated": "2025-11-07T13:27:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    27,
                    39,
                    4,
                    311,
                    0
                ],
                "published": "2025-02-24T12:05:27Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    5,
                    27,
                    0,
                    55,
                    0
                ],
                "title": "Mind the Blind Spots: A Focus-Level Evaluation Framework for LLM Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Blind Spots: A Focus-Level Evaluation Framework for LLM Reviews"
                },
                "summary": "Peer review underpins scientific progress, but it is increasingly strained by\nreviewer shortages and growing workloads. Large Language Models (LLMs) can\nautomatically draft reviews now, but determining whether LLM-generated reviews\nare trustworthy requires systematic evaluation. Researchers have evaluated LLM\nreviews at either surface-level (e.g., BLEU and ROUGE) or content-level (e.g.,\nspecificity and factual accuracy). Yet it remains uncertain whether\nLLM-generated reviews attend to the same critical facets that human experts\nweigh -- the strengths and weaknesses that ultimately drive an accept-or-reject\ndecision. We introduce a focus-level evaluation framework that operationalizes\nthe focus as a normalized distribution of attention across predefined facets in\npaper reviews. Based on the framework, we developed an automatic focus-level\nevaluation pipeline based on two sets of facets: target (e.g., problem, method,\nand experiment) and aspect (e.g., validity, clarity, and novelty), leveraging\n676 paper reviews (https://figshare.com/s/d5adf26c802527dd0f62) from OpenReview\nthat consists of 3,657 strengths and weaknesses identified from human experts.\nThe comparison of focus distributions between LLMs and human experts showed\nthat the off-the-shelf LLMs consistently have a more biased focus towards\nexamining technical validity while significantly overlooking novelty assessment\nwhen criticizing papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review underpins scientific progress, but it is increasingly strained by\nreviewer shortages and growing workloads. Large Language Models (LLMs) can\nautomatically draft reviews now, but determining whether LLM-generated reviews\nare trustworthy requires systematic evaluation. Researchers have evaluated LLM\nreviews at either surface-level (e.g., BLEU and ROUGE) or content-level (e.g.,\nspecificity and factual accuracy). Yet it remains uncertain whether\nLLM-generated reviews attend to the same critical facets that human experts\nweigh -- the strengths and weaknesses that ultimately drive an accept-or-reject\ndecision. We introduce a focus-level evaluation framework that operationalizes\nthe focus as a normalized distribution of attention across predefined facets in\npaper reviews. Based on the framework, we developed an automatic focus-level\nevaluation pipeline based on two sets of facets: target (e.g., problem, method,\nand experiment) and aspect (e.g., validity, clarity, and novelty), leveraging\n676 paper reviews (https://figshare.com/s/d5adf26c802527dd0f62) from OpenReview\nthat consists of 3,657 strengths and weaknesses identified from human experts.\nThe comparison of focus distributions between LLMs and human experts showed\nthat the off-the-shelf LLMs consistently have a more biased focus towards\nexamining technical validity while significantly overlooking novelty assessment\nwhen criticizing papers."
                },
                "authors": [
                    {
                        "name": "Hyungyu Shin"
                    },
                    {
                        "name": "Jingyu Tang"
                    },
                    {
                        "name": "Yoonjoo Lee"
                    },
                    {
                        "name": "Nayoung Kim"
                    },
                    {
                        "name": "Hyunseung Lim"
                    },
                    {
                        "name": "Ji Yong Cho"
                    },
                    {
                        "name": "Hwajung Hong"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "arxiv_comment": "EMNLP 2025 Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17086v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17086v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05219v1",
                "updated": "2025-11-07T13:17:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    17,
                    46,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T13:17:46Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    17,
                    46,
                    4,
                    311,
                    0
                ],
                "title": "FreeControl: Efficient, Training-Free Structural Control via One-Step\n  Attention Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeControl: Efficient, Training-Free Structural Control via One-Step\n  Attention Extraction"
                },
                "summary": "Controlling the spatial and semantic structure of diffusion-generated images\nremains a challenge. Existing methods like ControlNet rely on handcrafted\ncondition maps and retraining, limiting flexibility and generalization.\nInversion-based approaches offer stronger alignment but incur high inference\ncost due to dual-path denoising. We present FreeControl, a training-free\nframework for semantic structural control in diffusion models. Unlike prior\nmethods that extract attention across multiple timesteps, FreeControl performs\none-step attention extraction from a single, optimally chosen key timestep and\nreuses it throughout denoising. This enables efficient structural guidance\nwithout inversion or retraining. To further improve quality and stability, we\nintroduce Latent-Condition Decoupling (LCD): a principled separation of the key\ntimestep and the noised latent used in attention extraction. LCD provides finer\ncontrol over attention quality and eliminates structural artifacts. FreeControl\nalso supports compositional control via reference images assembled from\nmultiple sources - enabling intuitive scene layout design and stronger prompt\nalignment. FreeControl introduces a new paradigm for test-time control,\nenabling structurally and semantically aligned, visually coherent generation\ndirectly from raw images, with the flexibility for intuitive compositional\ndesign and compatibility with modern diffusion models at approximately 5\npercent additional cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling the spatial and semantic structure of diffusion-generated images\nremains a challenge. Existing methods like ControlNet rely on handcrafted\ncondition maps and retraining, limiting flexibility and generalization.\nInversion-based approaches offer stronger alignment but incur high inference\ncost due to dual-path denoising. We present FreeControl, a training-free\nframework for semantic structural control in diffusion models. Unlike prior\nmethods that extract attention across multiple timesteps, FreeControl performs\none-step attention extraction from a single, optimally chosen key timestep and\nreuses it throughout denoising. This enables efficient structural guidance\nwithout inversion or retraining. To further improve quality and stability, we\nintroduce Latent-Condition Decoupling (LCD): a principled separation of the key\ntimestep and the noised latent used in attention extraction. LCD provides finer\ncontrol over attention quality and eliminates structural artifacts. FreeControl\nalso supports compositional control via reference images assembled from\nmultiple sources - enabling intuitive scene layout design and stronger prompt\nalignment. FreeControl introduces a new paradigm for test-time control,\nenabling structurally and semantically aligned, visually coherent generation\ndirectly from raw images, with the flexibility for intuitive compositional\ndesign and compatibility with modern diffusion models at approximately 5\npercent additional cost."
                },
                "authors": [
                    {
                        "name": "Jiang Lin"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Song Wu"
                    },
                    {
                        "name": "Zhiqiu Zhang"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Qiang Tang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Zili Yi"
                    }
                ],
                "author_detail": {
                    "name": "Zili Yi"
                },
                "author": "Zili Yi",
                "arxiv_comment": "Accepted by NIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13142v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13142v3",
                "updated": "2025-11-07T13:12:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    12,
                    3,
                    4,
                    311,
                    0
                ],
                "published": "2025-08-18T17:55:17Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    17,
                    55,
                    17,
                    0,
                    230,
                    0
                ],
                "title": "Holistic Evaluation of Multimodal LLMs on Spatial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holistic Evaluation of Multimodal LLMs on Spatial Intelligence"
                },
                "summary": "Multimodal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, the very capability that anchors artificial\ngeneral intelligence in the physical world. With the recent release of GPT-5,\nallegedly the most powerful AI model to date, it is timely to examine where the\nleading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path\ntoward spatial intelligence. We thus propose EASI for holistic Evaluation of\nmultimodAl LLMs on Spatial Intelligence. EASI conceptualizes a comprehensive\ntaxonomy of spatial tasks that unifies existing benchmarks and a standardized\nprotocol for the fair evaluation of state-of-the-art proprietary and\nopen-source models. In this report, we conduct the study across eight key\nbenchmarks, at a cost exceeding ten billion total tokens. Our empirical study\nthen reveals that (1) GPT-5 demonstrates unprecedented strength in spatial\nintelligence (SI), yet (2) still falls short of human performance significantly\nacross a broad spectrum of SI-tasks. Moreover, we (3) show that SI-tasks expose\ngreater model capability deficiency than non-SI tasks, to the extent that (4)\nproprietary models do not exhibit a decisive advantage when facing the most\ndifficult ones. In addition, we conduct a qualitative evaluation across a\ndiverse set of scenarios that are intuitive for humans, yet fail even the most\nadvanced multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, the very capability that anchors artificial\ngeneral intelligence in the physical world. With the recent release of GPT-5,\nallegedly the most powerful AI model to date, it is timely to examine where the\nleading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path\ntoward spatial intelligence. We thus propose EASI for holistic Evaluation of\nmultimodAl LLMs on Spatial Intelligence. EASI conceptualizes a comprehensive\ntaxonomy of spatial tasks that unifies existing benchmarks and a standardized\nprotocol for the fair evaluation of state-of-the-art proprietary and\nopen-source models. In this report, we conduct the study across eight key\nbenchmarks, at a cost exceeding ten billion total tokens. Our empirical study\nthen reveals that (1) GPT-5 demonstrates unprecedented strength in spatial\nintelligence (SI), yet (2) still falls short of human performance significantly\nacross a broad spectrum of SI-tasks. Moreover, we (3) show that SI-tasks expose\ngreater model capability deficiency than non-SI tasks, to the extent that (4)\nproprietary models do not exhibit a decisive advantage when facing the most\ndifficult ones. In addition, we conduct a qualitative evaluation across a\ndiverse set of scenarios that are intuitive for humans, yet fail even the most\nadvanced multimodal models."
                },
                "authors": [
                    {
                        "name": "Zhongang Cai"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Qingping Sun"
                    },
                    {
                        "name": "Ruisi Wang"
                    },
                    {
                        "name": "Chenyang Gu"
                    },
                    {
                        "name": "Wanqi Yin"
                    },
                    {
                        "name": "Zhiqian Lin"
                    },
                    {
                        "name": "Zhitao Yang"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Oscar Qian"
                    },
                    {
                        "name": "Hui En Pang"
                    },
                    {
                        "name": "Xuanke Shi"
                    },
                    {
                        "name": "Kewang Deng"
                    },
                    {
                        "name": "Xiaoyang Han"
                    },
                    {
                        "name": "Zukai Chen"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Xiangyu Fan"
                    },
                    {
                        "name": "Hanming Deng"
                    },
                    {
                        "name": "Lewei Lu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Quan Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Yang"
                },
                "author": "Lei Yang",
                "arxiv_comment": "Codebase: https://github.com/EvolvingLMMs-Lab/EASI/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13142v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13142v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05721v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05721v2",
                "updated": "2025-11-07T13:05:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    5,
                    57,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-06T14:03:28Z",
                "published_parsed": [
                    2025,
                    9,
                    6,
                    14,
                    3,
                    28,
                    5,
                    249,
                    0
                ],
                "title": "A Composable Agentic System for Automated Visual Data Reporting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Composable Agentic System for Automated Visual Data Reporting"
                },
                "summary": "To address the brittleness of monolithic AI agents, our prototype for\nautomated visual data reporting explores a Human-AI Partnership model. Its\nhybrid, multi-agent architecture strategically externalizes logic from LLMs to\ndeterministic modules, leveraging the rule-based system Draco for principled\nvisualization design. The system delivers a dual-output: an interactive\nObservable report with Mosaic for reader exploration, and executable Marimo\nnotebooks for deep, analyst-facing traceability. This granular architecture\nyields a fully automatic yet auditable and steerable system, charting a path\ntoward a more synergistic partnership between human experts and AI. For\nreproducibility, our implementation and examples are available at\nhttps://peter-gy.github.io/VISxGenAI-2025/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the brittleness of monolithic AI agents, our prototype for\nautomated visual data reporting explores a Human-AI Partnership model. Its\nhybrid, multi-agent architecture strategically externalizes logic from LLMs to\ndeterministic modules, leveraging the rule-based system Draco for principled\nvisualization design. The system delivers a dual-output: an interactive\nObservable report with Mosaic for reader exploration, and executable Marimo\nnotebooks for deep, analyst-facing traceability. This granular architecture\nyields a fully automatic yet auditable and steerable system, charting a path\ntoward a more synergistic partnership between human experts and AI. For\nreproducibility, our implementation and examples are available at\nhttps://peter-gy.github.io/VISxGenAI-2025/."
                },
                "authors": [
                    {
                        "name": "Péter Ferenc Gyarmati"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Torsten Möller"
                    },
                    {
                        "name": "Laura Koesten"
                    }
                ],
                "author_detail": {
                    "name": "Laura Koesten"
                },
                "author": "Laura Koesten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05721v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20217v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20217v3",
                "updated": "2025-11-07T12:33:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    12,
                    33,
                    20,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-23T05:06:24Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    6,
                    24,
                    3,
                    296,
                    0
                ],
                "title": "EditInfinity: Image Editing with Binary-Quantized Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EditInfinity: Image Editing with Binary-Quantized Generative Models"
                },
                "summary": "Adapting pretrained diffusion-based generative models for text-driven image\nediting with negligible tuning overhead has demonstrated remarkable potential.\nA classical adaptation paradigm, as followed by these methods, first infers the\ngenerative trajectory inversely for a given source image by image inversion,\nthen performs image editing along the inferred trajectory guided by the target\ntext prompts. However, the performance of image editing is heavily limited by\nthe approximation errors introduced during image inversion by diffusion models,\nwhich arise from the absence of exact supervision in the intermediate\ngenerative steps. To circumvent this issue, we investigate the\nparameter-efficient adaptation of binary-quantized generative models for image\nediting, and leverage their inherent characteristic that the exact intermediate\nquantized representations of a source image are attainable, enabling more\neffective supervision for precise image inversion. Specifically, we propose\nEditInfinity, which adapts \\emph{Infinity}, a binary-quantized generative\nmodel, for image editing. We propose an efficient yet effective image inversion\nmechanism that integrates text prompting rectification and image style\npreservation, enabling precise image inversion. Furthermore, we devise a\nholistic smoothing strategy which allows our EditInfinity to perform image\nediting with high fidelity to source images and precise semantic alignment to\nthe text prompts. Extensive experiments on the PIE-Bench benchmark across\n`add', `change', and `delete' editing operations, demonstrate the superior\nperformance of our model compared to state-of-the-art diffusion-based\nbaselines. Code available at: https://github.com/yx-chen-ust/EditInfinity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting pretrained diffusion-based generative models for text-driven image\nediting with negligible tuning overhead has demonstrated remarkable potential.\nA classical adaptation paradigm, as followed by these methods, first infers the\ngenerative trajectory inversely for a given source image by image inversion,\nthen performs image editing along the inferred trajectory guided by the target\ntext prompts. However, the performance of image editing is heavily limited by\nthe approximation errors introduced during image inversion by diffusion models,\nwhich arise from the absence of exact supervision in the intermediate\ngenerative steps. To circumvent this issue, we investigate the\nparameter-efficient adaptation of binary-quantized generative models for image\nediting, and leverage their inherent characteristic that the exact intermediate\nquantized representations of a source image are attainable, enabling more\neffective supervision for precise image inversion. Specifically, we propose\nEditInfinity, which adapts \\emph{Infinity}, a binary-quantized generative\nmodel, for image editing. We propose an efficient yet effective image inversion\nmechanism that integrates text prompting rectification and image style\npreservation, enabling precise image inversion. Furthermore, we devise a\nholistic smoothing strategy which allows our EditInfinity to perform image\nediting with high fidelity to source images and precise semantic alignment to\nthe text prompts. Extensive experiments on the PIE-Bench benchmark across\n`add', `change', and `delete' editing operations, demonstrate the superior\nperformance of our model compared to state-of-the-art diffusion-based\nbaselines. Code available at: https://github.com/yx-chen-ust/EditInfinity."
                },
                "authors": [
                    {
                        "name": "Jiahuan Wang"
                    },
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Jun Yu"
                    },
                    {
                        "name": "Guangming Lu"
                    },
                    {
                        "name": "Wenjie Pei"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Pei"
                },
                "author": "Wenjie Pei",
                "arxiv_comment": "28 pages, 13 figures, accepted by The Thirty-ninth Annual Conference\n  on Neural Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20217v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20217v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03366v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03366v3",
                "updated": "2025-11-07T12:06:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    12,
                    6,
                    33,
                    4,
                    311,
                    0
                ],
                "published": "2025-02-05T17:03:49Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    3,
                    49,
                    2,
                    36,
                    0
                ],
                "title": "Rethinking Approximate Gaussian Inference in Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Approximate Gaussian Inference in Classification"
                },
                "summary": "In classification tasks, softmax functions are ubiquitously used as output\nactivations to produce predictive probabilities. Such outputs only capture\naleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian\ninference methods have been proposed. We develop a common formalism to describe\nsuch methods, which we view as outputting Gaussian distributions over the logit\nspace. Predictives are then obtained as the expectations of the Gaussian\ndistributions pushed forward through the softmax. However, such softmax\nGaussian integrals cannot be solved analytically, and Monte Carlo (MC)\napproximations can be costly and noisy. We propose to replace the softmax\nactivation by element-wise normCDF or sigmoid, which allows for the accurate\nsampling-free approximation of predictives. This also enables the approximation\nof the Gaussian pushforwards by Dirichlet distributions with moment matching.\nThis approach entirely eliminates the runtime and memory overhead associated\nwith MC sampling. We evaluate it combined with several approximate Gaussian\ninference methods (Laplace, HET, SNGP) on large- and small-scale datasets\n(ImageNet, CIFAR-100, CIFAR-10), demonstrating improved uncertainty\nquantification capabilities compared to softmax MC sampling. Our code is\navailable at https://github.com/bmucsanyi/probit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In classification tasks, softmax functions are ubiquitously used as output\nactivations to produce predictive probabilities. Such outputs only capture\naleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian\ninference methods have been proposed. We develop a common formalism to describe\nsuch methods, which we view as outputting Gaussian distributions over the logit\nspace. Predictives are then obtained as the expectations of the Gaussian\ndistributions pushed forward through the softmax. However, such softmax\nGaussian integrals cannot be solved analytically, and Monte Carlo (MC)\napproximations can be costly and noisy. We propose to replace the softmax\nactivation by element-wise normCDF or sigmoid, which allows for the accurate\nsampling-free approximation of predictives. This also enables the approximation\nof the Gaussian pushforwards by Dirichlet distributions with moment matching.\nThis approach entirely eliminates the runtime and memory overhead associated\nwith MC sampling. We evaluate it combined with several approximate Gaussian\ninference methods (Laplace, HET, SNGP) on large- and small-scale datasets\n(ImageNet, CIFAR-100, CIFAR-10), demonstrating improved uncertainty\nquantification capabilities compared to softmax MC sampling. Our code is\navailable at https://github.com/bmucsanyi/probit."
                },
                "authors": [
                    {
                        "name": "Bálint Mucsányi"
                    },
                    {
                        "name": "Nathaël Da Costa"
                    },
                    {
                        "name": "Philipp Hennig"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Hennig"
                },
                "author": "Philipp Hennig",
                "arxiv_comment": "46 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03366v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03366v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05184v1",
                "updated": "2025-11-07T12:05:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    12,
                    5,
                    39,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T12:05:39Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    12,
                    5,
                    39,
                    4,
                    311,
                    0
                ],
                "title": "Effectiveness of Chain-of-Thought in Distilling Reasoning Capability\n  from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectiveness of Chain-of-Thought in Distilling Reasoning Capability\n  from Large Language Models"
                },
                "summary": "Chain-of-Thought (CoT) prompting is a widely used method to improve the\nreasoning capability of Large Language Models (LLMs). More recently, CoT has\nbeen leveraged in Knowledge Distillation (KD) to transfer reasoning capability\nfrom a larger LLM to a smaller one. This paper examines the role of CoT in\ndistilling the reasoning capability from larger LLMs to smaller LLMs using\nwhite-box KD, analysing its effectiveness in improving the performance of the\ndistilled models for various natural language reasoning and understanding\ntasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2\nfamilies, employing CoT data from the CoT-Collection dataset. The distilled\nmodels are then evaluated on natural language reasoning and understanding tasks\nfrom the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for\nsmaller LLMs. Experimental results demonstrate the role of CoT in improving\nwhite-box KD effectiveness, enabling the distilled models to achieve better\naverage performance in natural language reasoning and understanding tasks from\nBBH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting is a widely used method to improve the\nreasoning capability of Large Language Models (LLMs). More recently, CoT has\nbeen leveraged in Knowledge Distillation (KD) to transfer reasoning capability\nfrom a larger LLM to a smaller one. This paper examines the role of CoT in\ndistilling the reasoning capability from larger LLMs to smaller LLMs using\nwhite-box KD, analysing its effectiveness in improving the performance of the\ndistilled models for various natural language reasoning and understanding\ntasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2\nfamilies, employing CoT data from the CoT-Collection dataset. The distilled\nmodels are then evaluated on natural language reasoning and understanding tasks\nfrom the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for\nsmaller LLMs. Experimental results demonstrate the role of CoT in improving\nwhite-box KD effectiveness, enabling the distilled models to achieve better\naverage performance in natural language reasoning and understanding tasks from\nBBH."
                },
                "authors": [
                    {
                        "name": "Cong-Thanh Do"
                    },
                    {
                        "name": "Rama Doddipatla"
                    },
                    {
                        "name": "Kate Knill"
                    }
                ],
                "author_detail": {
                    "name": "Kate Knill"
                },
                "author": "Kate Knill",
                "arxiv_comment": "In proceedings of the 18th International Natural Language Generation\n  Conference (INLG 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18156v2",
                "updated": "2025-11-07T11:59:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    59,
                    6,
                    4,
                    311,
                    0
                ],
                "published": "2025-06-22T19:58:19Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    19,
                    58,
                    19,
                    6,
                    173,
                    0
                ],
                "title": "AI Through the Human Lens: Investigating Cognitive Theories in Machine\n  Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Through the Human Lens: Investigating Cognitive Theories in Machine\n  Psychology"
                },
                "summary": "We investigate whether Large Language Models (LLMs) exhibit human-like\ncognitive patterns under four established frameworks from psychology: Thematic\nApperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and\nCognitive Dissonance. We evaluated several proprietary and open-source models\nusing structured prompts and automated scoring. Our findings reveal that these\nmodels often produce coherent narratives, show susceptibility to positive\nframing, exhibit moral judgments aligned with Liberty/Oppression concerns, and\ndemonstrate self-contradictions tempered by extensive rationalization. Such\nbehaviors mirror human cognitive tendencies yet are shaped by their training\ndata and alignment methods. We discuss the implications for AI transparency,\nethical deployment, and future work that bridges cognitive psychology and AI\nsafety",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether Large Language Models (LLMs) exhibit human-like\ncognitive patterns under four established frameworks from psychology: Thematic\nApperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and\nCognitive Dissonance. We evaluated several proprietary and open-source models\nusing structured prompts and automated scoring. Our findings reveal that these\nmodels often produce coherent narratives, show susceptibility to positive\nframing, exhibit moral judgments aligned with Liberty/Oppression concerns, and\ndemonstrate self-contradictions tempered by extensive rationalization. Such\nbehaviors mirror human cognitive tendencies yet are shaped by their training\ndata and alignment methods. We discuss the implications for AI transparency,\nethical deployment, and future work that bridges cognitive psychology and AI\nsafety"
                },
                "authors": [
                    {
                        "name": "Akash Kundu"
                    },
                    {
                        "name": "Rishika Goswami"
                    }
                ],
                "author_detail": {
                    "name": "Rishika Goswami"
                },
                "author": "Rishika Goswami",
                "arxiv_comment": "Accepted to IJCNLP-AACL 2025 Student Research Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05165v1",
                "updated": "2025-11-07T11:35:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    35,
                    46,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T11:35:46Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    35,
                    46,
                    4,
                    311,
                    0
                ],
                "title": "Generating Software Architecture Description from Source Code using\n  Reverse Engineering and Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Software Architecture Description from Source Code using\n  Reverse Engineering and Large Language Model"
                },
                "summary": "Software Architecture Descriptions (SADs) are essential for managing the\ninherent complexity of modern software systems. They enable high-level\narchitectural reasoning, guide design decisions, and facilitate effective\ncommunication among diverse stakeholders. However, in practice, SADs are often\nmissing, outdated, or poorly aligned with the system's actual implementation.\nConsequently, developers are compelled to derive architectural insights\ndirectly from source code-a time-intensive process that increases cognitive\nload, slows new developer onboarding, and contributes to the gradual\ndegradation of clarity over the system's lifetime. To address these issues, we\npropose a semi-automated generation of SADs from source code by integrating\nreverse engineering (RE) techniques with a Large Language Model (LLM). Our\napproach recovers both static and behavioral architectural views by extracting\na comprehensive component diagram, filtering architecturally significant\nelements (core components) via prompt engineering, and generating state machine\ndiagrams to model component behavior based on underlying code logic with\nfew-shots prompting. This resulting views representation offer a scalable and\nmaintainable alternative to traditional manual architectural documentation.\nThis methodology, demonstrated using C++ examples, highlights the potent\ncapability of LLMs to: 1) abstract the component diagram, thereby reducing the\nreliance on human expert involvement, and 2) accurately represent complex\nsoftware behaviors, especially when enriched with domain-specific knowledge\nthrough few-shot prompting. These findings suggest a viable path toward\nsignificantly reducing manual effort while enhancing system understanding and\nlong-term maintainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Architecture Descriptions (SADs) are essential for managing the\ninherent complexity of modern software systems. They enable high-level\narchitectural reasoning, guide design decisions, and facilitate effective\ncommunication among diverse stakeholders. However, in practice, SADs are often\nmissing, outdated, or poorly aligned with the system's actual implementation.\nConsequently, developers are compelled to derive architectural insights\ndirectly from source code-a time-intensive process that increases cognitive\nload, slows new developer onboarding, and contributes to the gradual\ndegradation of clarity over the system's lifetime. To address these issues, we\npropose a semi-automated generation of SADs from source code by integrating\nreverse engineering (RE) techniques with a Large Language Model (LLM). Our\napproach recovers both static and behavioral architectural views by extracting\na comprehensive component diagram, filtering architecturally significant\nelements (core components) via prompt engineering, and generating state machine\ndiagrams to model component behavior based on underlying code logic with\nfew-shots prompting. This resulting views representation offer a scalable and\nmaintainable alternative to traditional manual architectural documentation.\nThis methodology, demonstrated using C++ examples, highlights the potent\ncapability of LLMs to: 1) abstract the component diagram, thereby reducing the\nreliance on human expert involvement, and 2) accurately represent complex\nsoftware behaviors, especially when enriched with domain-specific knowledge\nthrough few-shot prompting. These findings suggest a viable path toward\nsignificantly reducing manual effort while enhancing system understanding and\nlong-term maintainability."
                },
                "authors": [
                    {
                        "name": "Ahmad Hatahet"
                    },
                    {
                        "name": "Christoph Knieke"
                    },
                    {
                        "name": "Andreas Rausch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Rausch"
                },
                "author": "Andreas Rausch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05162v1",
                "updated": "2025-11-07T11:30:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    30,
                    10,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T11:30:10Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    30,
                    10,
                    4,
                    311,
                    0
                ],
                "title": "Mind the Gap... or Not? How Translation Errors and Evaluation Details\n  Skew Multilingual Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap... or Not? How Translation Errors and Evaluation Details\n  Skew Multilingual Results"
                },
                "summary": "Most current large language models (LLMs) support a wide variety of languages\nin addition to English, including high-resource languages (e.g. German,\nChinese, French), as well as low-resource ones (e.g. Swahili, Telugu). In\naddition they have also shown impressive capabilities in different domains,\nlike coding, science and math. In this short paper, taking math as an example\ndomain, we study the performance of different LLMs across languages.\nExperimental results show that there exists a non-negligible and consistent gap\nin the performance of the models across languages. Interestingly, and somewhat\nagainst expectations, the gap exists for both high- and low-resource languages.\nWe hope that these results influence further research into cross-lingual\ncapability generalization for next generation LLMs. If it weren't for the fact\nthat they are false! By analyzing one of the standard multilingual math\nbenchmarks (MGSM), we determine that several translation errors are present in\nthe data. Furthermore, the lack of standardized answer extraction from LLM\noutputs further influences the final results. We propose a method for automatic\nquality assurance to address the first issue at scale, and give recommendations\nto address the second one. Combining these two approaches we show that the\naforementioned language gap mostly disappears, leading to completely different\nconclusions from our research. We additionally release the corrected dataset to\nthe community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most current large language models (LLMs) support a wide variety of languages\nin addition to English, including high-resource languages (e.g. German,\nChinese, French), as well as low-resource ones (e.g. Swahili, Telugu). In\naddition they have also shown impressive capabilities in different domains,\nlike coding, science and math. In this short paper, taking math as an example\ndomain, we study the performance of different LLMs across languages.\nExperimental results show that there exists a non-negligible and consistent gap\nin the performance of the models across languages. Interestingly, and somewhat\nagainst expectations, the gap exists for both high- and low-resource languages.\nWe hope that these results influence further research into cross-lingual\ncapability generalization for next generation LLMs. If it weren't for the fact\nthat they are false! By analyzing one of the standard multilingual math\nbenchmarks (MGSM), we determine that several translation errors are present in\nthe data. Furthermore, the lack of standardized answer extraction from LLM\noutputs further influences the final results. We propose a method for automatic\nquality assurance to address the first issue at scale, and give recommendations\nto address the second one. Combining these two approaches we show that the\naforementioned language gap mostly disappears, leading to completely different\nconclusions from our research. We additionally release the corrected dataset to\nthe community."
                },
                "authors": [
                    {
                        "name": "Jan-Thorsten Peter"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Tobias Domhan"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Markus Freitag"
                    }
                ],
                "author_detail": {
                    "name": "Markus Freitag"
                },
                "author": "Markus Freitag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26854v2",
                "updated": "2025-11-07T11:11:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    11,
                    26,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-30T15:38:50Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    38,
                    50,
                    3,
                    303,
                    0
                ],
                "title": "Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a\n  Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a\n  Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base"
                },
                "summary": "Most scientific materials compress reasoning, presenting conclusions while\nomitting the derivational chains that justify them. This compression hinders\nverification by lacking explicit, step-wise justifications and inhibits\ncross-domain links by collapsing the very pathways that establish the logical\nand causal connections between concepts. We introduce a scalable framework that\ndecompresses scientific reasoning, constructing a verifiable Long\nChain-of-Thought (LCoT) knowledge base and projecting it into an emergent\nencyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven,\nreductionist strategy: a Socratic agent, guided by a curriculum of around 200\ncourses, generates approximately 3 million first-principles questions. To\nensure high fidelity, multiple independent solver models generate LCoTs, which\nare then rigorously filtered by prompt sanitization and cross-model answer\nconsensus, retaining only those with verifiable endpoints. This verified corpus\npowers the Brainstorm Search Engine, which performs inverse knowledge search --\nretrieving diverse, first-principles derivations that culminate in a target\nconcept. This engine, in turn, feeds the Plato synthesizer, which narrates\nthese verified chains into coherent articles. The initial SciencePedia\ncomprises approximately 200,000 fine-grained entries spanning mathematics,\nphysics, chemistry, biology, engineering, and computation. In evaluations\nacross six disciplines, Plato-synthesized articles (conditioned on retrieved\nLCoTs) exhibit substantially higher knowledge-point density and significantly\nlower factual error rates than an equally-prompted baseline without retrieval\n(as judged by an external LLM). Built on this verifiable LCoT knowledge base,\nthis reasoning-centric approach enables trustworthy, cross-domain scientific\nsynthesis at scale and establishes the foundation for an ever-expanding\nencyclopedia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most scientific materials compress reasoning, presenting conclusions while\nomitting the derivational chains that justify them. This compression hinders\nverification by lacking explicit, step-wise justifications and inhibits\ncross-domain links by collapsing the very pathways that establish the logical\nand causal connections between concepts. We introduce a scalable framework that\ndecompresses scientific reasoning, constructing a verifiable Long\nChain-of-Thought (LCoT) knowledge base and projecting it into an emergent\nencyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven,\nreductionist strategy: a Socratic agent, guided by a curriculum of around 200\ncourses, generates approximately 3 million first-principles questions. To\nensure high fidelity, multiple independent solver models generate LCoTs, which\nare then rigorously filtered by prompt sanitization and cross-model answer\nconsensus, retaining only those with verifiable endpoints. This verified corpus\npowers the Brainstorm Search Engine, which performs inverse knowledge search --\nretrieving diverse, first-principles derivations that culminate in a target\nconcept. This engine, in turn, feeds the Plato synthesizer, which narrates\nthese verified chains into coherent articles. The initial SciencePedia\ncomprises approximately 200,000 fine-grained entries spanning mathematics,\nphysics, chemistry, biology, engineering, and computation. In evaluations\nacross six disciplines, Plato-synthesized articles (conditioned on retrieved\nLCoTs) exhibit substantially higher knowledge-point density and significantly\nlower factual error rates than an equally-prompted baseline without retrieval\n(as judged by an external LLM). Built on this verifiable LCoT knowledge base,\nthis reasoning-centric approach enables trustworthy, cross-domain scientific\nsynthesis at scale and establishes the foundation for an ever-expanding\nencyclopedia."
                },
                "authors": [
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Yuan Huang"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Caiyu Fan"
                    },
                    {
                        "name": "Xiansheng Cai"
                    },
                    {
                        "name": "Sihan Hu"
                    },
                    {
                        "name": "Xinzijian Liu"
                    },
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Mingjun Xu"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Tianhan Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Youjin Deng"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Weijie Sun"
                    },
                    {
                        "name": "Xingyu Li"
                    },
                    {
                        "name": "Weinan E"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Zhiyuan Yao"
                    },
                    {
                        "name": "Kun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kun Chen"
                },
                "author": "Kun Chen",
                "arxiv_comment": "43 pages, 4 figures. This work is part of the SciencePedia project\n  (sciencepedia.bohrium.com)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05150v1",
                "updated": "2025-11-07T11:05:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    5,
                    36,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T11:05:36Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    5,
                    36,
                    4,
                    311,
                    0
                ],
                "title": "From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation\n  Model Bridging Global and Cellular Representations in Biomarker Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation\n  Model Bridging Global and Cellular Representations in Biomarker Detection"
                },
                "summary": "AI-based biomarkers can infer molecular features directly from hematoxylin &\neosin (H&E) slides, yet most pathology foundation models (PFMs) rely on global\npatch-level embeddings and overlook cell-level morphology. We present a PFM\nmodel, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale\nself-supervised pretraining with cell-centric post-tuning and attention pooling\nto fuse local and global tokens. Across four tasks involving four biomarkers\nand eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2%\naverage improvement over prior PFMs, advancing interpretable and robust\nAI-based biomarker detection in digital pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-based biomarkers can infer molecular features directly from hematoxylin &\neosin (H&E) slides, yet most pathology foundation models (PFMs) rely on global\npatch-level embeddings and overlook cell-level morphology. We present a PFM\nmodel, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale\nself-supervised pretraining with cell-centric post-tuning and attention pooling\nto fuse local and global tokens. Across four tasks involving four biomarkers\nand eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2%\naverage improvement over prior PFMs, advancing interpretable and robust\nAI-based biomarker detection in digital pathology."
                },
                "authors": [
                    {
                        "name": "Jingsong Liu"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Peter J. Schüffler"
                    }
                ],
                "author_detail": {
                    "name": "Peter J. Schüffler"
                },
                "author": "Peter J. Schüffler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09702v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09702v4",
                "updated": "2025-11-07T10:20:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    10,
                    20,
                    39,
                    4,
                    311,
                    0
                ],
                "published": "2023-10-15T01:41:42Z",
                "published_parsed": [
                    2023,
                    10,
                    15,
                    1,
                    41,
                    42,
                    6,
                    288,
                    0
                ],
                "title": "Inference with Mondrian Random Forests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Mondrian Random Forests"
                },
                "summary": "Random forests are popular methods for regression and classification\nanalysis, and many different variants have been proposed in recent years. One\ninteresting example is the Mondrian random forest, in which the underlying\nconstituent trees are constructed via a Mondrian process. We give precise bias\nand variance characterizations, along with a Berry-Esseen-type central limit\ntheorem, for the Mondrian random forest regression estimator. By combining\nthese results with a carefully crafted debiasing approach and an accurate\nvariance estimator, we present valid statistical inference methods for the\nunknown regression function. These methods come with explicitly characterized\nerror bounds in terms of the sample size, tree complexity parameter, and number\nof trees in the forest, and include coverage error rates for feasible\nconfidence interval estimators. Our debiasing procedure for the Mondrian random\nforest also allows it to achieve the minimax-optimal point estimation\nconvergence rate in mean squared error for multivariate $\\beta$-H\\\"older\nregression functions, for all $\\beta > 0$, provided that the underlying tuning\nparameters are chosen appropriately. Efficient and implementable algorithms are\ndevised for both batch and online learning settings, and we study the\ncomputational complexity of different Mondrian random forest implementations.\nFinally, simulations with synthetic data validate our theory and methodology,\ndemonstrating their excellent finite-sample properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random forests are popular methods for regression and classification\nanalysis, and many different variants have been proposed in recent years. One\ninteresting example is the Mondrian random forest, in which the underlying\nconstituent trees are constructed via a Mondrian process. We give precise bias\nand variance characterizations, along with a Berry-Esseen-type central limit\ntheorem, for the Mondrian random forest regression estimator. By combining\nthese results with a carefully crafted debiasing approach and an accurate\nvariance estimator, we present valid statistical inference methods for the\nunknown regression function. These methods come with explicitly characterized\nerror bounds in terms of the sample size, tree complexity parameter, and number\nof trees in the forest, and include coverage error rates for feasible\nconfidence interval estimators. Our debiasing procedure for the Mondrian random\nforest also allows it to achieve the minimax-optimal point estimation\nconvergence rate in mean squared error for multivariate $\\beta$-H\\\"older\nregression functions, for all $\\beta > 0$, provided that the underlying tuning\nparameters are chosen appropriately. Efficient and implementable algorithms are\ndevised for both batch and online learning settings, and we study the\ncomputational complexity of different Mondrian random forest implementations.\nFinally, simulations with synthetic data validate our theory and methodology,\ndemonstrating their excellent finite-sample properties."
                },
                "authors": [
                    {
                        "name": "Matias D. Cattaneo"
                    },
                    {
                        "name": "Jason M. Klusowski"
                    },
                    {
                        "name": "William G. Underwood"
                    }
                ],
                "author_detail": {
                    "name": "William G. Underwood"
                },
                "author": "William G. Underwood",
                "arxiv_comment": "64 pages, 1 figure, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09702v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09702v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G08 (Primary), 62G05, 62G20 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18469v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18469v6",
                "updated": "2025-11-07T10:17:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    10,
                    17,
                    59,
                    4,
                    311,
                    0
                ],
                "published": "2024-10-24T06:36:12Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    6,
                    36,
                    12,
                    3,
                    298,
                    0
                ],
                "title": "Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities"
                },
                "summary": "Recent research has shown that Large Language Models (LLMs) are vulnerable to\nautomated jailbreak attacks, where adversarial suffixes crafted by algorithms\nappended to harmful queries bypass safety alignment and trigger unintended\nresponses. Current methods for generating these suffixes are computationally\nexpensive and have low Attack Success Rates (ASR), especially against\nwell-aligned models like Llama2 and Llama3. To overcome these limitations, we\nintroduce ADV-LLM, an iterative self-tuning process that crafts adversarial\nLLMs with enhanced jailbreak ability. Our framework significantly reduces the\ncomputational cost of generating adversarial suffixes while achieving nearly\n100\\% ASR on various open-source LLMs. Moreover, it exhibits strong attack\ntransferability to closed-source models, achieving 99\\% ASR on GPT-3.5 and 49\\%\nASR on GPT-4, despite being optimized solely on Llama3. Beyond improving\njailbreak ability, ADV-LLM provides valuable insights for future safety\nalignment research through its ability to generate large datasets for studying\nLLM safety. Our code is available at: https://github.com/SunChungEn/ADV-LLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that Large Language Models (LLMs) are vulnerable to\nautomated jailbreak attacks, where adversarial suffixes crafted by algorithms\nappended to harmful queries bypass safety alignment and trigger unintended\nresponses. Current methods for generating these suffixes are computationally\nexpensive and have low Attack Success Rates (ASR), especially against\nwell-aligned models like Llama2 and Llama3. To overcome these limitations, we\nintroduce ADV-LLM, an iterative self-tuning process that crafts adversarial\nLLMs with enhanced jailbreak ability. Our framework significantly reduces the\ncomputational cost of generating adversarial suffixes while achieving nearly\n100\\% ASR on various open-source LLMs. Moreover, it exhibits strong attack\ntransferability to closed-source models, achieving 99\\% ASR on GPT-3.5 and 49\\%\nASR on GPT-4, despite being optimized solely on Llama3. Beyond improving\njailbreak ability, ADV-LLM provides valuable insights for future safety\nalignment research through its ability to generate large datasets for studying\nLLM safety. Our code is available at: https://github.com/SunChungEn/ADV-LLM"
                },
                "authors": [
                    {
                        "name": "Chung-En Sun"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Weiwei Yang"
                    },
                    {
                        "name": "Tsui-Wei Weng"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Aidan San"
                    },
                    {
                        "name": "Michel Galley"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "Accepted to NAACL 2025 Main (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18469v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18469v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25441v2",
                "updated": "2025-11-07T10:05:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    10,
                    5,
                    6,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-29T12:08:07Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    8,
                    7,
                    2,
                    302,
                    0
                ],
                "title": "Grounded in Reality: Learning and Deploying Proactive LLM from Offline\n  Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded in Reality: Learning and Deploying Proactive LLM from Offline\n  Logs"
                },
                "summary": "Large Language Models (LLMs) excel as passive responders, but teaching them\nto be proactive, goal-oriented partners, a critical capability in high-stakes\ndomains, remains a major challenge. Current paradigms either myopically\noptimize single-turn attributes or rely on brittle, high-cost user simulators,\ncreating a persistent ``reality gap''. To bridge this gap, we introduce\n\\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and\ndeploying proactive dialogue agents \\textit{directly from offline expert data},\nbypassing the need to model complex user dynamics. Our key insight is to\nreframe the offline policy learning problem by leveraging the \\textbf{observed\nfuture} of each expert trajectory. This allows us to infer a dense,\nturn-by-turn reward signal grounded in the expert's revealed strategy,\ndecomposing the intractable long-horizon problem into a series of supervised\nlearning tasks, and training a policy to output a structured \\texttt{(action,\nstate_assessment)} tuple, governing both \\textbf{what to ask} and, crucially,\n\\textbf{when to stop}. To ensure reward fidelity, our Automated Grader\nCalibration pipeline systematically purges noise from the LLM-based reward\nmodel with minimal human supervision. Empirically, we demonstrate the efficacy\nof \\texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying\nsizes up to 32B. Our approach culminates in the successful deployment of LLMs\ninto a live, large-scale online AI service. In rigorous in-house evaluations,\nour model was launched and achieved performance even superior to human experts,\nproving our framework's ability to translate offline data into tangible,\nreal-world impact. We hope this work provides a practical and economically\nviable blueprint for transforming passive LLMs into proactive, goal-oriented\nLLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel as passive responders, but teaching them\nto be proactive, goal-oriented partners, a critical capability in high-stakes\ndomains, remains a major challenge. Current paradigms either myopically\noptimize single-turn attributes or rely on brittle, high-cost user simulators,\ncreating a persistent ``reality gap''. To bridge this gap, we introduce\n\\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and\ndeploying proactive dialogue agents \\textit{directly from offline expert data},\nbypassing the need to model complex user dynamics. Our key insight is to\nreframe the offline policy learning problem by leveraging the \\textbf{observed\nfuture} of each expert trajectory. This allows us to infer a dense,\nturn-by-turn reward signal grounded in the expert's revealed strategy,\ndecomposing the intractable long-horizon problem into a series of supervised\nlearning tasks, and training a policy to output a structured \\texttt{(action,\nstate_assessment)} tuple, governing both \\textbf{what to ask} and, crucially,\n\\textbf{when to stop}. To ensure reward fidelity, our Automated Grader\nCalibration pipeline systematically purges noise from the LLM-based reward\nmodel with minimal human supervision. Empirically, we demonstrate the efficacy\nof \\texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying\nsizes up to 32B. Our approach culminates in the successful deployment of LLMs\ninto a live, large-scale online AI service. In rigorous in-house evaluations,\nour model was launched and achieved performance even superior to human experts,\nproving our framework's ability to translate offline data into tangible,\nreal-world impact. We hope this work provides a practical and economically\nviable blueprint for transforming passive LLMs into proactive, goal-oriented\nLLM applications."
                },
                "authors": [
                    {
                        "name": "Fei Wei"
                    },
                    {
                        "name": "Daoyuan Chen"
                    },
                    {
                        "name": "Ce Wang"
                    },
                    {
                        "name": "Yilun Huang"
                    },
                    {
                        "name": "Yushuo Chen"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "arxiv_comment": "27 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05120v1",
                "updated": "2025-11-07T10:04:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    10,
                    4,
                    41,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T10:04:41Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    10,
                    4,
                    41,
                    4,
                    311,
                    0
                ],
                "title": "A Toolbox for Improving Evolutionary Prompt Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Toolbox for Improving Evolutionary Prompt Search"
                },
                "summary": "Evolutionary prompt optimization has demonstrated effectiveness in refining\nprompts for LLMs. However, existing approaches lack robust operators and\nefficient evaluation mechanisms. In this work, we propose several key\nimprovements to evolutionary prompt optimization that can partially generalize\nto prompt optimization in general: 1) decomposing evolution into distinct steps\nto enhance the evolution and its control, 2) introducing an LLM-based judge to\nverify the evolutions, 3) integrating human feedback to refine the evolutionary\noperator, and 4) developing more efficient evaluation strategies that maintain\nperformance while reducing computational overhead. Our approach improves both\noptimization quality and efficiency. We release our code, enabling prompt\noptimization on new tasks and facilitating further research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary prompt optimization has demonstrated effectiveness in refining\nprompts for LLMs. However, existing approaches lack robust operators and\nefficient evaluation mechanisms. In this work, we propose several key\nimprovements to evolutionary prompt optimization that can partially generalize\nto prompt optimization in general: 1) decomposing evolution into distinct steps\nto enhance the evolution and its control, 2) introducing an LLM-based judge to\nverify the evolutions, 3) integrating human feedback to refine the evolutionary\noperator, and 4) developing more efficient evaluation strategies that maintain\nperformance while reducing computational overhead. Our approach improves both\noptimization quality and efficiency. We release our code, enabling prompt\noptimization on new tasks and facilitating further research in this area."
                },
                "authors": [
                    {
                        "name": "Daniel Grießhaber"
                    },
                    {
                        "name": "Maximilian Kimmich"
                    },
                    {
                        "name": "Johannes Maucher"
                    },
                    {
                        "name": "Ngoc Thang Vu"
                    }
                ],
                "author_detail": {
                    "name": "Ngoc Thang Vu"
                },
                "author": "Ngoc Thang Vu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16447v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16447v2",
                "updated": "2025-11-07T10:02:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    10,
                    2,
                    55,
                    4,
                    311,
                    0
                ],
                "published": "2025-08-22T15:02:07Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    2,
                    7,
                    4,
                    234,
                    0
                ],
                "title": "Boardwalk: Towards a Framework for Creating Board Games with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boardwalk: Towards a Framework for Creating Board Games with LLMs"
                },
                "summary": "Implementing board games in code can be a time-consuming task. However, Large\nLanguage Models (LLMs) have been proven effective at generating code for\ndomain-specific tasks with simple contextual information. We aim to investigate\nwhether LLMs can implement digital versions of board games from rules described\nin natural language. This would be a step towards an LLM-assisted framework for\nquick board game code generation. We expect to determine the main challenges\nfor LLMs to implement the board games, and how different approaches and models\ncompare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek\nand ChatGPT) with coding a selection of 12 popular and obscure games in\nfree-form and within Boardwalk, our proposed General Game Playing API. We\nanonymize the games and components to avoid evoking pre-trained LLM knowledge.\nThe implementations are tested for playability and rule compliance. We evaluate\nsuccess rate and common errors across LLMs and game popularity. Our approach\nproves viable, with the best performing model, Claude 3.7 Sonnet, yielding\n55.6\\% of games without any errors. While compliance with the API increases\nerror frequency, the severity of errors is more significantly dependent on the\nLLM. We outline future steps for creating a framework to integrate this\nprocess, making the elaboration of board games more accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing board games in code can be a time-consuming task. However, Large\nLanguage Models (LLMs) have been proven effective at generating code for\ndomain-specific tasks with simple contextual information. We aim to investigate\nwhether LLMs can implement digital versions of board games from rules described\nin natural language. This would be a step towards an LLM-assisted framework for\nquick board game code generation. We expect to determine the main challenges\nfor LLMs to implement the board games, and how different approaches and models\ncompare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek\nand ChatGPT) with coding a selection of 12 popular and obscure games in\nfree-form and within Boardwalk, our proposed General Game Playing API. We\nanonymize the games and components to avoid evoking pre-trained LLM knowledge.\nThe implementations are tested for playability and rule compliance. We evaluate\nsuccess rate and common errors across LLMs and game popularity. Our approach\nproves viable, with the best performing model, Claude 3.7 Sonnet, yielding\n55.6\\% of games without any errors. While compliance with the API increases\nerror frequency, the severity of errors is more significantly dependent on the\nLLM. We outline future steps for creating a framework to integrate this\nprocess, making the elaboration of board games more accessible."
                },
                "authors": [
                    {
                        "name": "Álvaro Guglielmin Becker"
                    },
                    {
                        "name": "Gabriel Bauer de Oliveira"
                    },
                    {
                        "name": "Lana Bertoldo Rossato"
                    },
                    {
                        "name": "Anderson Rocha Tavares"
                    }
                ],
                "author_detail": {
                    "name": "Anderson Rocha Tavares"
                },
                "author": "Anderson Rocha Tavares",
                "arxiv_doi": "10.5753/sbgames.2025.10222",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5753/sbgames.2025.10222",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.16447v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16447v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Presented at SBGames 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05114v1",
                "updated": "2025-11-07T09:58:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    58,
                    1,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T09:58:01Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    58,
                    1,
                    4,
                    311,
                    0
                ],
                "title": "Usando LLMs para Programar Jogos de Tabuleiro e Variações",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Usando LLMs para Programar Jogos de Tabuleiro e Variações"
                },
                "summary": "Creating programs to represent board games can be a time-consuming task.\nLarge Language Models (LLMs) arise as appealing tools to expedite this process,\ngiven their capacity to efficiently generate code from simple contextual\ninformation. In this work, we propose a method to test how capable three LLMs\n(Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as\nnew variants of existing games.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating programs to represent board games can be a time-consuming task.\nLarge Language Models (LLMs) arise as appealing tools to expedite this process,\ngiven their capacity to efficiently generate code from simple contextual\ninformation. In this work, we propose a method to test how capable three LLMs\n(Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as\nnew variants of existing games."
                },
                "authors": [
                    {
                        "name": "Álvaro Guglielmin Becker"
                    },
                    {
                        "name": "Lana Bertoldo Rossato"
                    },
                    {
                        "name": "Anderson Rocha Tavares"
                    }
                ],
                "author_detail": {
                    "name": "Anderson Rocha Tavares"
                },
                "author": "Anderson Rocha Tavares",
                "arxiv_comment": "Accepted for presentation at the I Escola Regional de Aprendizado de\n  M\\'aquina e Intelig\\^encia Artificial da Regi\\~ao Sul, 2025, in Portuguese\n  language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05113v1",
                "updated": "2025-11-07T09:52:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    52,
                    49,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T09:52:49Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    52,
                    49,
                    4,
                    311,
                    0
                ],
                "title": "Spatially-resolved PAH_3.3 emission and stellar ages in ram pressure\n  stripped clumps at z~0.3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatially-resolved PAH_3.3 emission and stellar ages in ram pressure\n  stripped clumps at z~0.3"
                },
                "summary": "Ram pressure stripping (RPS) plays a crucial role in shaping galaxy evolution\nin dense environments, yet its impact on the molecular and dusty phases of the\ninterstellar medium remains poorly understood. We present JWST/NIRCam 3.3\nmicrometres PAH emission maps for the nine most striking RPS galaxies in the\nAbell 2744 cluster at redshift z_cl = 0.306, tracing the effects of\nenvironmental processes on small dust grains. Exploiting multi-band JWST/NIRCam\nand HST photometry, we perform spatially-resolved UV to mid-infrared spectral\nenergy distribution (SED) fitting, characterising stellar populations in both\ngalactic disks and clumps detected in the stripped tails. We detect PAH_3.3\nmission in eight of the nine galaxies at 5 sigma, with morphologies revealing\ndisk truncation and elongation along the RPS direction. In three galaxies,\nPAH_3.3 emission is also found in star-forming clumps embedded in the stripped\ntails up to a distance of 40 kpc. Star formation rates inferred from PAH_3.3\nemission agree with those derived from SED fitting averaged over the past 100\nMyr within an intrinsic scatter of 0.4 dex, but the relation appears to be age\ndependent. The spatial correlation between PAH strength, stellar age, and SFR -\nconsistent across disks and tails - demonstrates that PAH-carrying molecules\ncan survive and be stripped by ram pressure. Finally, age gradients revealed by\nthe SED fitting provide the first observational evidence outside the Local\nUniverse for the fireball model of star formation in stripped clumps. This work\nrepresents the first detailed study of PAH emission in cluster galaxies,\noffering new insights into the fate of dust and star formation in extreme\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ram pressure stripping (RPS) plays a crucial role in shaping galaxy evolution\nin dense environments, yet its impact on the molecular and dusty phases of the\ninterstellar medium remains poorly understood. We present JWST/NIRCam 3.3\nmicrometres PAH emission maps for the nine most striking RPS galaxies in the\nAbell 2744 cluster at redshift z_cl = 0.306, tracing the effects of\nenvironmental processes on small dust grains. Exploiting multi-band JWST/NIRCam\nand HST photometry, we perform spatially-resolved UV to mid-infrared spectral\nenergy distribution (SED) fitting, characterising stellar populations in both\ngalactic disks and clumps detected in the stripped tails. We detect PAH_3.3\nmission in eight of the nine galaxies at 5 sigma, with morphologies revealing\ndisk truncation and elongation along the RPS direction. In three galaxies,\nPAH_3.3 emission is also found in star-forming clumps embedded in the stripped\ntails up to a distance of 40 kpc. Star formation rates inferred from PAH_3.3\nemission agree with those derived from SED fitting averaged over the past 100\nMyr within an intrinsic scatter of 0.4 dex, but the relation appears to be age\ndependent. The spatial correlation between PAH strength, stellar age, and SFR -\nconsistent across disks and tails - demonstrates that PAH-carrying molecules\ncan survive and be stripped by ram pressure. Finally, age gradients revealed by\nthe SED fitting provide the first observational evidence outside the Local\nUniverse for the fireball model of star formation in stripped clumps. This work\nrepresents the first detailed study of PAH emission in cluster galaxies,\noffering new insights into the fate of dust and star formation in extreme\nenvironments."
                },
                "authors": [
                    {
                        "name": "Pietro Benotto"
                    },
                    {
                        "name": "Benedetta Vulcani"
                    },
                    {
                        "name": "Peter J. Watson"
                    },
                    {
                        "name": "Giulia Rodighiero"
                    },
                    {
                        "name": "Bianca M. Poggianti"
                    },
                    {
                        "name": "Marco Gullieuszik"
                    },
                    {
                        "name": "Jacopo Fritz"
                    },
                    {
                        "name": "Thomas S. -Y. Lai"
                    },
                    {
                        "name": "Augusto E. Lassen"
                    },
                    {
                        "name": "Matthew A. Malkan"
                    },
                    {
                        "name": "Alessia Moretti"
                    }
                ],
                "author_detail": {
                    "name": "Alessia Moretti"
                },
                "arxiv_affiliation": "INAF OAPd - Unipd",
                "author": "Alessia Moretti",
                "arxiv_comment": "Submitted to A&A, 17 pages plus appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09766v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09766v5",
                "updated": "2025-11-07T09:32:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    32,
                    5,
                    4,
                    311,
                    0
                ],
                "published": "2025-01-15T04:52:34Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    52,
                    34,
                    2,
                    15,
                    0
                ],
                "title": "iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for\n  Advanced Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for\n  Advanced Tool Use"
                },
                "summary": "Augmenting large language models (LLMs) with external tools is a promising\napproach to enhance their capabilities, especially for complex tasks.\nSynthesizing tool-use data through real-world simulations is an effective way\nto achieve this. However, our investigation reveals that training gains\nsignificantly decay as synthetic data increases. The model struggles to benefit\nfrom additional synthetic data, which fails to endow it with advanced tool-use\ncapabilities in complex scenarios Moreover, we discovered that the above\nlimitation usually manifests as a fragment deficiency (i.e., parameter errors)\nin response. To this end, we propose an iterative reinforced fine-tuning\nstrategy designed to alleviate this limitation. This strategy involves: (1)\nenhancing the diversity of response for synthetic data through path exploration\nof Monte Carlo Tree Search. (2) iteratively pinpointing the model's deficiency\nby constructing fine-grained preference pairs, and then improving it by\npreference optimization algorithms for targeted improvement. The experiments\nshow that our method achieves 13.11% better performance than the same-size base\nmodel. It achieves an improvement of 6.5% in complex scenarios compared to the\nbaseline, and it also outperforms larger open-source and closed-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting large language models (LLMs) with external tools is a promising\napproach to enhance their capabilities, especially for complex tasks.\nSynthesizing tool-use data through real-world simulations is an effective way\nto achieve this. However, our investigation reveals that training gains\nsignificantly decay as synthetic data increases. The model struggles to benefit\nfrom additional synthetic data, which fails to endow it with advanced tool-use\ncapabilities in complex scenarios Moreover, we discovered that the above\nlimitation usually manifests as a fragment deficiency (i.e., parameter errors)\nin response. To this end, we propose an iterative reinforced fine-tuning\nstrategy designed to alleviate this limitation. This strategy involves: (1)\nenhancing the diversity of response for synthetic data through path exploration\nof Monte Carlo Tree Search. (2) iteratively pinpointing the model's deficiency\nby constructing fine-grained preference pairs, and then improving it by\npreference optimization algorithms for targeted improvement. The experiments\nshow that our method achieves 13.11% better performance than the same-size base\nmodel. It achieves an improvement of 6.5% in complex scenarios compared to the\nbaseline, and it also outperforms larger open-source and closed-source models."
                },
                "authors": [
                    {
                        "name": "Yirong Zeng"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Yuxian Wang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Wu Ning"
                    },
                    {
                        "name": "Yutai Hou"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09766v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09766v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01083v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01083v5",
                "updated": "2025-11-07T09:27:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    27,
                    1,
                    4,
                    311,
                    0
                ],
                "published": "2024-09-02T09:11:28Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    9,
                    11,
                    28,
                    0,
                    246,
                    0
                ],
                "title": "Affordance-based Robot Manipulation with Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordance-based Robot Manipulation with Flow Matching"
                },
                "summary": "We present a framework for assistive robot manipulation, which focuses on two\nfundamental challenges: first, efficiently adapting large-scale models to\ndownstream scene affordance understanding tasks, especially in daily living\nscenarios where gathering multi-task data involving humans requires strenuous\neffort; second, effectively learning robot action trajectories by grounding the\nvisual affordance model. We tackle the first challenge by employing a\nparameter-efficient prompt tuning method that prepends learnable text prompts\nto the frozen vision model to predict manipulation affordances in multi-task\nscenarios. Then we propose to learn robot action trajectories guided by\naffordances in a supervised flow matching method. Flow matching represents a\nrobot visuomotor policy as a conditional process of flowing random waypoints to\ndesired robot action trajectories. Finally, we introduce a real-world dataset\nwith 10 tasks across Activities of Daily Living to test our framework. Our\nextensive evaluation highlights that the proposed prompt tuning method for\nlearning manipulation affordance achieves competitive performance and even\noutperforms some other finetuning protocols across data scales, while\nsatisfying parameter efficiency. Learning multi-task robot action trajectories\nwith flow matching leads to consistently favorable results in several robot\nmanipulation benchmarks than some alternative behavior cloning methods. This\nincludes more stable training and evaluation, and noticeably faster inference,\nwhile maintaining comparable generalization performance to diffusion policy,\nwhere flow matching performs marginally better in most cases. Our framework\nseamlessly unifies affordance learning and action generation with flow matching\nfor robot manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a framework for assistive robot manipulation, which focuses on two\nfundamental challenges: first, efficiently adapting large-scale models to\ndownstream scene affordance understanding tasks, especially in daily living\nscenarios where gathering multi-task data involving humans requires strenuous\neffort; second, effectively learning robot action trajectories by grounding the\nvisual affordance model. We tackle the first challenge by employing a\nparameter-efficient prompt tuning method that prepends learnable text prompts\nto the frozen vision model to predict manipulation affordances in multi-task\nscenarios. Then we propose to learn robot action trajectories guided by\naffordances in a supervised flow matching method. Flow matching represents a\nrobot visuomotor policy as a conditional process of flowing random waypoints to\ndesired robot action trajectories. Finally, we introduce a real-world dataset\nwith 10 tasks across Activities of Daily Living to test our framework. Our\nextensive evaluation highlights that the proposed prompt tuning method for\nlearning manipulation affordance achieves competitive performance and even\noutperforms some other finetuning protocols across data scales, while\nsatisfying parameter efficiency. Learning multi-task robot action trajectories\nwith flow matching leads to consistently favorable results in several robot\nmanipulation benchmarks than some alternative behavior cloning methods. This\nincludes more stable training and evaluation, and noticeably faster inference,\nwhile maintaining comparable generalization performance to diffusion policy,\nwhere flow matching performs marginally better in most cases. Our framework\nseamlessly unifies affordance learning and action generation with flow matching\nfor robot manipulation."
                },
                "authors": [
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Michael Gienger"
                    }
                ],
                "author_detail": {
                    "name": "Michael Gienger"
                },
                "author": "Michael Gienger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01083v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01083v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05094v1",
                "updated": "2025-11-07T09:21:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    21,
                    22,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T09:21:22Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    21,
                    22,
                    4,
                    311,
                    0
                ],
                "title": "FM4Com: Foundation Model for Scene-Adaptive Communication Strategy\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FM4Com: Foundation Model for Scene-Adaptive Communication Strategy\n  Optimization"
                },
                "summary": "The emergence of sixth-generation (6G) networks heralds an intelligent\ncommunication ecosystem driven by AI-native air interfaces. However, current\nphysical-layer designs-typically following modular and isolated optimization\nparadigms-fail to achieve global end-to-end optimality due to neglected\ninter-module dependencies. Although large language models (LLMs) have recently\nbeen applied to communication tasks such as beam prediction and resource\nallocation, existing studies remain limited to single-task or single-modality\nscenarios and lack the ability to jointly reason over communication states and\nuser intents for personalized strategy adaptation. To address these\nlimitations, this paper proposes a novel multimodal communication\ndecision-making model based on reinforcement learning. The proposed model\nsemantically aligns channel state information (CSI) and textual user\ninstructions, enabling comprehensive understanding of both physical-layer\nconditions and communication intents. It then generates physically realizable,\nuser-customized link construction strategies that dynamically adapt to changing\nenvironments and preference tendencies. A two-stage reinforcement learning\nframework is employed: the first stage expands the experience pool via\nheuristic exploration and behavior cloning to obtain a near-optimal\ninitialization, while the second stage fine-tunes the model through\nmulti-objective reinforcement learning considering bit error rate, throughput,\nand complexity. Experimental results demonstrate that the proposed model\nsignificantly outperforms conventional planning-based algorithms under\nchallenging channel conditions, achieving robust, efficient, and personalized\n6G link construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of sixth-generation (6G) networks heralds an intelligent\ncommunication ecosystem driven by AI-native air interfaces. However, current\nphysical-layer designs-typically following modular and isolated optimization\nparadigms-fail to achieve global end-to-end optimality due to neglected\ninter-module dependencies. Although large language models (LLMs) have recently\nbeen applied to communication tasks such as beam prediction and resource\nallocation, existing studies remain limited to single-task or single-modality\nscenarios and lack the ability to jointly reason over communication states and\nuser intents for personalized strategy adaptation. To address these\nlimitations, this paper proposes a novel multimodal communication\ndecision-making model based on reinforcement learning. The proposed model\nsemantically aligns channel state information (CSI) and textual user\ninstructions, enabling comprehensive understanding of both physical-layer\nconditions and communication intents. It then generates physically realizable,\nuser-customized link construction strategies that dynamically adapt to changing\nenvironments and preference tendencies. A two-stage reinforcement learning\nframework is employed: the first stage expands the experience pool via\nheuristic exploration and behavior cloning to obtain a near-optimal\ninitialization, while the second stage fine-tunes the model through\nmulti-objective reinforcement learning considering bit error rate, throughput,\nand complexity. Experimental results demonstrate that the proposed model\nsignificantly outperforms conventional planning-based algorithms under\nchallenging channel conditions, achieving robust, efficient, and personalized\n6G link construction."
                },
                "authors": [
                    {
                        "name": "Zhaoyang Li"
                    },
                    {
                        "name": "Shangzhuo Xie"
                    },
                    {
                        "name": "Qianqian Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qianqian Yang"
                },
                "author": "Qianqian Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16821v3",
                "updated": "2025-11-07T09:20:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    20,
                    34,
                    4,
                    311,
                    0
                ],
                "published": "2025-05-22T15:55:56Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    55,
                    56,
                    3,
                    142,
                    0
                ],
                "title": "LLM-Based Emulation of the Radio Resource Control Layer: Towards\n  AI-Native RAN Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Emulation of the Radio Resource Control Layer: Towards\n  AI-Native RAN Protocols"
                },
                "summary": "Integrating Large AI Models (LAMs) into 6G mobile networks is a key enabler\nof the AI-Native Air Interface (AI-AI), where protocol intelligence must scale\nbeyond handcrafted logic. This paper presents, to our knowledge, the first\nstandards-compliant emulation of the Radio Resource Control (RRC) layer using a\ndecoder-only LAM (LLAMA-class) fine-tuned with Low-Rank Adaptation (LoRA) on a\nmulti-vendor corpus of real-world traces spanning both 5G and 4G systems. We\ntreat RRC as a domain-specific language and construct a segmentation-safe,\nquestion--answer (Question-and-Answer (QA)) dataset that preserves Abstract\nSyntax Notation (ASN.1) structure through linearization prior to Byte Pair\nEncoding (BPE) tokenization. The proposed approach combines parameter-efficient\nadaptation with schema-bounded prompting to ensure syntactic and procedural\nfidelity. Evaluation introduces a standards-aware triad -- ASN.1 conformance,\nfield-level coverage analysis, and uplink-to-downlink state-machine checks --\nalongside semantic similarity and latency profiling across 120 configurations.\nOn 30k 5G request--response pairs plus an additional 4.8k QA turns from 4G\nsessions, our 8B model achieves a median cosine similarity of 0.97, a 61%\nrelative gain over a zero-shot baseline, while sustaining high conformance\nrates. These results demonstrate that LAMs, when augmented with protocol-aware\nreasoning, can directly orchestrate control-plane procedures, laying the\nfoundation for the future Artificial Intelligence (AI)-native Radio Access\nNetwork (RAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large AI Models (LAMs) into 6G mobile networks is a key enabler\nof the AI-Native Air Interface (AI-AI), where protocol intelligence must scale\nbeyond handcrafted logic. This paper presents, to our knowledge, the first\nstandards-compliant emulation of the Radio Resource Control (RRC) layer using a\ndecoder-only LAM (LLAMA-class) fine-tuned with Low-Rank Adaptation (LoRA) on a\nmulti-vendor corpus of real-world traces spanning both 5G and 4G systems. We\ntreat RRC as a domain-specific language and construct a segmentation-safe,\nquestion--answer (Question-and-Answer (QA)) dataset that preserves Abstract\nSyntax Notation (ASN.1) structure through linearization prior to Byte Pair\nEncoding (BPE) tokenization. The proposed approach combines parameter-efficient\nadaptation with schema-bounded prompting to ensure syntactic and procedural\nfidelity. Evaluation introduces a standards-aware triad -- ASN.1 conformance,\nfield-level coverage analysis, and uplink-to-downlink state-machine checks --\nalongside semantic similarity and latency profiling across 120 configurations.\nOn 30k 5G request--response pairs plus an additional 4.8k QA turns from 4G\nsessions, our 8B model achieves a median cosine similarity of 0.97, a 61%\nrelative gain over a zero-shot baseline, while sustaining high conformance\nrates. These results demonstrate that LAMs, when augmented with protocol-aware\nreasoning, can directly orchestrate control-plane procedures, laying the\nfoundation for the future Artificial Intelligence (AI)-native Radio Access\nNetwork (RAN)."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Bryan Liu"
                    },
                    {
                        "name": "Alvaro Valcarce"
                    },
                    {
                        "name": "Xiaoli Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoli Chu"
                },
                "author": "Xiaoli Chu",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Focuses on applying LLMs to 5G RRC protocol generation; primary: cs.NI;\n  cross-list: eess.SP, cs.LG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19544v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19544v4",
                "updated": "2025-11-07T09:09:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    9,
                    43,
                    4,
                    311,
                    0
                ],
                "published": "2024-06-27T21:47:27Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    21,
                    47,
                    27,
                    3,
                    179,
                    0
                ],
                "title": "Where Is Self-admitted Code Generated by Large Language Models on\n  GitHub?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where Is Self-admitted Code Generated by Large Language Models on\n  GitHub?"
                },
                "summary": "The increasing use of Large Language Models (LLMs) in software development\nhas garnered significant attention from researchers evaluating the capabilities\nand limitations of LLMs for code generation. However, much of the research\nfocuses on controlled datasets such as HumanEval, which do not adequately\ncapture the characteristics of LLM-generated code in real-world development\nscenarios. To address this gap, our study investigates self-admitted code\ngenerated by LLMs on GitHub, specifically focusing on instances where\ndevelopers in projects with over five stars acknowledge the use of LLMs to\ngenerate code through code comments. Our findings reveal several key insights:\n(1) ChatGPT and Copilot dominate code generation, with minimal contributions\nfrom other LLMs. (2) Projects containing ChatGPT/Copilot-generated code appears\nin small/medium-sized projects led by small teams, which are continuously\nevolving. (3) ChatGPT/Copilot-generated code generally is a minor project\nportion, primarily generating short/moderate-length, low-complexity snippets\n(e.g., algorithms and data structures code; text processing code). (4)\nChatGPT/Copilot-generated code generally undergoes minimal modifications, with\nbug-related changes ranging from 4% to 12%. (5) Most code comments only state\nLLM use, while few include details like prompts, human edits, or code testing\nstatus. Based on these findings, we discuss the implications for researchers\nand practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Large Language Models (LLMs) in software development\nhas garnered significant attention from researchers evaluating the capabilities\nand limitations of LLMs for code generation. However, much of the research\nfocuses on controlled datasets such as HumanEval, which do not adequately\ncapture the characteristics of LLM-generated code in real-world development\nscenarios. To address this gap, our study investigates self-admitted code\ngenerated by LLMs on GitHub, specifically focusing on instances where\ndevelopers in projects with over five stars acknowledge the use of LLMs to\ngenerate code through code comments. Our findings reveal several key insights:\n(1) ChatGPT and Copilot dominate code generation, with minimal contributions\nfrom other LLMs. (2) Projects containing ChatGPT/Copilot-generated code appears\nin small/medium-sized projects led by small teams, which are continuously\nevolving. (3) ChatGPT/Copilot-generated code generally is a minor project\nportion, primarily generating short/moderate-length, low-complexity snippets\n(e.g., algorithms and data structures code; text processing code). (4)\nChatGPT/Copilot-generated code generally undergoes minimal modifications, with\nbug-related changes ranging from 4% to 12%. (5) Most code comments only state\nLLM use, while few include details like prompts, human edits, or code testing\nstatus. Based on these findings, we discuss the implications for researchers\nand practitioners."
                },
                "authors": [
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Jin Liu"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19544v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19544v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22963v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22963v2",
                "updated": "2025-11-07T09:01:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    1,
                    26,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-27T03:37:41Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    3,
                    37,
                    41,
                    0,
                    300,
                    0
                ],
                "title": "CompressionAttack: Exploiting Prompt Compression as a New Attack Surface\n  in LLM-Powered Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompressionAttack: Exploiting Prompt Compression as a New Attack Surface\n  in LLM-Powered Agents"
                },
                "summary": "LLM-powered agents often use prompt compression to reduce inference costs,\nbut this introduces a new security risk. Compression modules, which are\noptimized for efficiency rather than safety, can be manipulated by adversarial\ninputs, causing semantic drift and altering LLM behavior. This work identifies\nprompt compression as a novel attack surface and presents CompressionAttack,\nthe first framework to exploit it. CompressionAttack includes two strategies:\nHardCom, which uses discrete adversarial edits for hard compression, and\nSoftCom, which performs latent-space perturbations for soft compression.\nExperiments on multiple LLMs show up to 80% attack success and 98% preference\nflips, while remaining highly stealthy and transferable. Case studies in VSCode\nCline and Ollama confirm real-world impact, and current defenses prove\nineffective, highlighting the need for stronger protections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered agents often use prompt compression to reduce inference costs,\nbut this introduces a new security risk. Compression modules, which are\noptimized for efficiency rather than safety, can be manipulated by adversarial\ninputs, causing semantic drift and altering LLM behavior. This work identifies\nprompt compression as a novel attack surface and presents CompressionAttack,\nthe first framework to exploit it. CompressionAttack includes two strategies:\nHardCom, which uses discrete adversarial edits for hard compression, and\nSoftCom, which performs latent-space perturbations for soft compression.\nExperiments on multiple LLMs show up to 80% attack success and 98% preference\nflips, while remaining highly stealthy and transferable. Case studies in VSCode\nCline and Ollama confirm real-world impact, and current defenses prove\nineffective, highlighting the need for stronger protections."
                },
                "authors": [
                    {
                        "name": "Zesen Liu"
                    },
                    {
                        "name": "Zhixiang Zhang"
                    },
                    {
                        "name": "Yuchong Xie"
                    },
                    {
                        "name": "Dongdong She"
                    }
                ],
                "author_detail": {
                    "name": "Dongdong She"
                },
                "author": "Dongdong She",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22963v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22963v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.05476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05476v1",
                "updated": "2025-11-07T18:38:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    38,
                    54,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T18:38:54Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    38,
                    54,
                    4,
                    311,
                    0
                ],
                "title": "A Metamorphic Testing Perspective on Knowledge Distillation for Language\n  Models of Code: Does the Student Deeply Mimic the Teacher?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Metamorphic Testing Perspective on Knowledge Distillation for Language\n  Models of Code: Does the Student Deeply Mimic the Teacher?"
                },
                "summary": "Transformer-based language models of code have achieved state-of-the-art\nperformance across a wide range of software analytics tasks, but their\npractical deployment remains limited due to high computational costs, slow\ninference speeds, and significant environmental impact. To address these\nchallenges, recent research has increasingly explored knowledge distillation as\na method for compressing a large language model of code (the teacher) into a\nsmaller model (the student) while maintaining performance. However, the degree\nto which a student model deeply mimics the predictive behavior and internal\nrepresentations of its teacher remains largely unexplored, as current\naccuracy-based evaluation provides only a surface-level view of model quality\nand often fails to capture more profound discrepancies in behavioral fidelity\nbetween the teacher and student models. To address this gap, we empirically\nshow that the student model often fails to deeply mimic the teacher model,\nresulting in up to 285% greater performance drop under adversarial attacks,\nwhich is not captured by traditional accuracy-based evaluation. Therefore, we\npropose MetaCompress, a metamorphic testing framework that systematically\nevaluates behavioral fidelity by comparing the outputs of teacher and student\nmodels under a set of behavior-preserving metamorphic relations. We evaluate\nMetaCompress on two widely studied tasks, using compressed versions of popular\nlanguage models of code, obtained via three different knowledge distillation\ntechniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress\nidentifies up to 62% behavioral discrepancies in student models, underscoring\nthe need for behavioral fidelity evaluation within the knowledge distillation\npipeline and establishing MetaCompress as a practical framework for testing\ncompressed language models of code derived through knowledge distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based language models of code have achieved state-of-the-art\nperformance across a wide range of software analytics tasks, but their\npractical deployment remains limited due to high computational costs, slow\ninference speeds, and significant environmental impact. To address these\nchallenges, recent research has increasingly explored knowledge distillation as\na method for compressing a large language model of code (the teacher) into a\nsmaller model (the student) while maintaining performance. However, the degree\nto which a student model deeply mimics the predictive behavior and internal\nrepresentations of its teacher remains largely unexplored, as current\naccuracy-based evaluation provides only a surface-level view of model quality\nand often fails to capture more profound discrepancies in behavioral fidelity\nbetween the teacher and student models. To address this gap, we empirically\nshow that the student model often fails to deeply mimic the teacher model,\nresulting in up to 285% greater performance drop under adversarial attacks,\nwhich is not captured by traditional accuracy-based evaluation. Therefore, we\npropose MetaCompress, a metamorphic testing framework that systematically\nevaluates behavioral fidelity by comparing the outputs of teacher and student\nmodels under a set of behavior-preserving metamorphic relations. We evaluate\nMetaCompress on two widely studied tasks, using compressed versions of popular\nlanguage models of code, obtained via three different knowledge distillation\ntechniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress\nidentifies up to 62% behavioral discrepancies in student models, underscoring\nthe need for behavioral fidelity evaluation within the knowledge distillation\npipeline and establishing MetaCompress as a practical framework for testing\ncompressed language models of code derived through knowledge distillation."
                },
                "authors": [
                    {
                        "name": "Md. Abdul Awal"
                    },
                    {
                        "name": "Mrigank Rochan"
                    },
                    {
                        "name": "Chanchal K. Roy"
                    }
                ],
                "author_detail": {
                    "name": "Chanchal K. Roy"
                },
                "author": "Chanchal K. Roy",
                "arxiv_comment": "The paper is currently under review at a peer-reviewed journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17797v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17797v2",
                "updated": "2025-11-07T18:10:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    10,
                    23,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-20T17:55:11Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    55,
                    11,
                    0,
                    293,
                    0
                ],
                "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for\n  Enterprise Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for\n  Enterprise Analytics"
                },
                "summary": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200"
                },
                "authors": [
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Roshan Ram"
                    },
                    {
                        "name": "Zixiang Chen"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Frank Wang"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Weiran Yao"
                    }
                ],
                "author_detail": {
                    "name": "Weiran Yao"
                },
                "author": "Weiran Yao",
                "arxiv_comment": "Technical report; 13 pages plus references and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17797v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17797v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00077v2",
                "updated": "2025-11-07T18:05:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    5,
                    11,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-29T15:37:44Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    37,
                    44,
                    2,
                    302,
                    0
                ],
                "title": "What is the Return on Investment of Digital Engineering for Complex\n  Systems Development? Findings from a Mixed-Methods Study on the\n  Post-production Design Change Process of Navy Assets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Return on Investment of Digital Engineering for Complex\n  Systems Development? Findings from a Mixed-Methods Study on the\n  Post-production Design Change Process of Navy Assets"
                },
                "summary": "Complex engineered systems routinely face schedule and cost overruns, along\nwith poor post-deployment performance. Championed by both INCOSE and the U.S.\nDepartment of Defense (DoD), the systems engineering (SE) community has\nincreasingly looked to Digital Engineering (DE) as a potential remedy. Despite\nthis growing advocacy, most of DE's purported benefits remain anecdotal, and\nits return on investment (ROI) remains poorly understood. This research\npresents findings from a case study on a Navy SE team responsible for the\npreliminary design phase of post-production design change projects for Navy\nassets. Using a mixed-methods approach, we document why complex system\nsustainment projects are routinely late, where and to what extent schedule\nslips arise, and how a DE transformation could improve schedule adherence. This\nstudy makes three contributions. First, it identifies four archetypical\ninefficiency modes that drive schedule overruns and explains how these\nmechanisms unfold in their organizational context. Second, it quantifies the\nmagnitude and variation of schedule slips. Third, it creates a hypothetical\ndigitally transformed version of the current process, aligned with DoD DE\npolicy, and compares it to the current state to estimate potential schedule\ngains. Our findings suggest that a DE transformation could reduce the median\nproject duration by 50.1% and reduce the standard deviation by 41.5%, leading\nto faster and more predictable timelines. However, the observed gains are not\nuniform across task categories. Overall, this study provides initial\nquantitative evidence of DE's potential ROI and its value in improving the\nefficiency and predictability of complex system sustainment projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex engineered systems routinely face schedule and cost overruns, along\nwith poor post-deployment performance. Championed by both INCOSE and the U.S.\nDepartment of Defense (DoD), the systems engineering (SE) community has\nincreasingly looked to Digital Engineering (DE) as a potential remedy. Despite\nthis growing advocacy, most of DE's purported benefits remain anecdotal, and\nits return on investment (ROI) remains poorly understood. This research\npresents findings from a case study on a Navy SE team responsible for the\npreliminary design phase of post-production design change projects for Navy\nassets. Using a mixed-methods approach, we document why complex system\nsustainment projects are routinely late, where and to what extent schedule\nslips arise, and how a DE transformation could improve schedule adherence. This\nstudy makes three contributions. First, it identifies four archetypical\ninefficiency modes that drive schedule overruns and explains how these\nmechanisms unfold in their organizational context. Second, it quantifies the\nmagnitude and variation of schedule slips. Third, it creates a hypothetical\ndigitally transformed version of the current process, aligned with DoD DE\npolicy, and compares it to the current state to estimate potential schedule\ngains. Our findings suggest that a DE transformation could reduce the median\nproject duration by 50.1% and reduce the standard deviation by 41.5%, leading\nto faster and more predictable timelines. However, the observed gains are not\nuniform across task categories. Overall, this study provides initial\nquantitative evidence of DE's potential ROI and its value in improving the\nefficiency and predictability of complex system sustainment projects."
                },
                "authors": [
                    {
                        "name": "Jannatul Shefa"
                    },
                    {
                        "name": "Taylan G. Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Taylan G. Topcu"
                },
                "author": "Taylan G. Topcu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05459v1",
                "updated": "2025-11-07T18:01:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    1,
                    32,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T18:01:32Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    1,
                    32,
                    4,
                    311,
                    0
                ],
                "title": "SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for\n  Large Language Models"
                },
                "summary": "Evaluating large language models (LLMs) for software engineering has been\nlimited by narrow task coverage, language bias, and insufficient alignment with\nreal-world developer workflows. Existing benchmarks often focus on algorithmic\nproblems or Python-centric bug fixing, leaving critical dimensions of software\nengineering underexplored. To address these gaps, we introduce SWE-Compass1, a\ncomprehensive benchmark that unifies heterogeneous code-related evaluations\ninto a structured and production-aligned framework. SWE-Compass spans 8 task\ntypes, 8 programming scenarios, and 10 programming languages, with 2000\nhigh-quality instances curated from authentic GitHub pull requests and refined\nthrough systematic filtering and validation. We benchmark ten state-of-the-art\nLLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear\nhierarchy of difficulty across task types, languages, and scenarios. Moreover,\nby aligning evaluation with real-world developer practices, SWE-Compass\nprovides a rigorous and reproducible foundation for diagnosing and advancing\nagentic coding capabilities in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) for software engineering has been\nlimited by narrow task coverage, language bias, and insufficient alignment with\nreal-world developer workflows. Existing benchmarks often focus on algorithmic\nproblems or Python-centric bug fixing, leaving critical dimensions of software\nengineering underexplored. To address these gaps, we introduce SWE-Compass1, a\ncomprehensive benchmark that unifies heterogeneous code-related evaluations\ninto a structured and production-aligned framework. SWE-Compass spans 8 task\ntypes, 8 programming scenarios, and 10 programming languages, with 2000\nhigh-quality instances curated from authentic GitHub pull requests and refined\nthrough systematic filtering and validation. We benchmark ten state-of-the-art\nLLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear\nhierarchy of difficulty across task types, languages, and scenarios. Moreover,\nby aligning evaluation with real-world developer practices, SWE-Compass\nprovides a rigorous and reproducible foundation for diagnosing and advancing\nagentic coding capabilities in large language models."
                },
                "authors": [
                    {
                        "name": "Jingxuan Xu"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Weihao Li"
                    },
                    {
                        "name": "Songwei Yu"
                    },
                    {
                        "name": "Huaixi Tang"
                    },
                    {
                        "name": "Haoyang Huang"
                    },
                    {
                        "name": "Zhiyi Lai"
                    },
                    {
                        "name": "Zizheng Zhan"
                    },
                    {
                        "name": "Yanan Wu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Kepeng Lei"
                    },
                    {
                        "name": "Yifan Yao"
                    },
                    {
                        "name": "Xinping Lei"
                    },
                    {
                        "name": "Wenqiang Zhu"
                    },
                    {
                        "name": "Zongxian Feng"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Junqi Xiong"
                    },
                    {
                        "name": "Dailin Li"
                    },
                    {
                        "name": "Zuchen Gao"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Wen Xiang"
                    },
                    {
                        "name": "Ziqi Zhan"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Wuxuan Gong"
                    },
                    {
                        "name": "Ziyuan Gao"
                    },
                    {
                        "name": "Guanxiang Wang"
                    },
                    {
                        "name": "Yirong Xue"
                    },
                    {
                        "name": "Xiaojiang Zhang"
                    },
                    {
                        "name": "Jinghui Wang"
                    },
                    {
                        "name": "Huiming Wang"
                    },
                    {
                        "name": "Wenhao Zhuang"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Yuqun Zhang"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Jiaheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Liu"
                },
                "author": "Jiaheng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23247v2",
                "updated": "2025-11-07T17:49:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    17,
                    49,
                    33,
                    4,
                    311,
                    0
                ],
                "published": "2025-07-31T05:10:38Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    5,
                    10,
                    38,
                    3,
                    212,
                    0
                ],
                "title": "P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication"
                },
                "summary": "Although explainability and interpretability have received significant\nattention in artificial intelligence (AI) and natural language processing (NLP)\nfor mental health, reasoning has not been examined in the same depth.\nAddressing this gap is essential to bridge NLP and mental health through\ninterpretable and reasoning-capable AI systems. To this end, we investigate the\npragmatic reasoning capability of large-language models (LLMs) in the mental\nhealth domain. We introduce PRiMH dataset, and propose pragmatic reasoning\ntasks in mental health with pragmatic implicature and presupposition phenomena.\nIn particular, we formulate two tasks in implicature and one task in\npresupposition. To benchmark the dataset and the tasks presented, we consider\nfour models: Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the\nexperiments suggest that Mistral and Qwen show substantial reasoning abilities\nin the domain. Subsequently, we study the behavior of MentaLLaMA on the\nproposed reasoning tasks with the rollout attention mechanism. In addition, we\nalso propose three StiPRompts to study the stigma around mental health with the\nstate-of-the-art LLMs, GPT4o-mini, Deepseek-chat, and Claude-3.5-haiku. Our\nevaluated findings show that Claude-3.5-haiku deals with stigma more\nresponsibly compared to the other two LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although explainability and interpretability have received significant\nattention in artificial intelligence (AI) and natural language processing (NLP)\nfor mental health, reasoning has not been examined in the same depth.\nAddressing this gap is essential to bridge NLP and mental health through\ninterpretable and reasoning-capable AI systems. To this end, we investigate the\npragmatic reasoning capability of large-language models (LLMs) in the mental\nhealth domain. We introduce PRiMH dataset, and propose pragmatic reasoning\ntasks in mental health with pragmatic implicature and presupposition phenomena.\nIn particular, we formulate two tasks in implicature and one task in\npresupposition. To benchmark the dataset and the tasks presented, we consider\nfour models: Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the\nexperiments suggest that Mistral and Qwen show substantial reasoning abilities\nin the domain. Subsequently, we study the behavior of MentaLLaMA on the\nproposed reasoning tasks with the rollout attention mechanism. In addition, we\nalso propose three StiPRompts to study the stigma around mental health with the\nstate-of-the-art LLMs, GPT4o-mini, Deepseek-chat, and Claude-3.5-haiku. Our\nevaluated findings show that Claude-3.5-haiku deals with stigma more\nresponsibly compared to the other two LLMs."
                },
                "authors": [
                    {
                        "name": "Sneha Oram"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02043v3",
                "updated": "2025-11-07T17:26:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    17,
                    26,
                    2,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-03T20:25:19Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    20,
                    25,
                    19,
                    0,
                    307,
                    0
                ],
                "title": "Flashlight: PyTorch Compiler Extensions to Accelerate Attention Variants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flashlight: PyTorch Compiler Extensions to Accelerate Attention Variants"
                },
                "summary": "Attention is a fundamental building block of large language models (LLMs), so\nthere have been many efforts to implement it efficiently. For example,\nFlashAttention leverages tiling and kernel fusion to optimize attention.\nRecently, a number of variants of attention have been introduced to enhance\nmodel quality or efficiency. Supporting them efficiently remains difficult\nsince they usually require specialized kernels or hand-tuned implementations.\nFlexAttention recently addressed part of this gap by using static programming\ntemplates to support FlashAttention-like kernels for a subset of attention\nvariants.\n  In this paper, we introduce Flashlight, a compiler-native framework within\nthe PyTorch ecosystem that automatically generates fused, FlashAttention-style\nkernels for arbitrary attention-based programs, without relying on static\ntemplates or predefined kernel specializations. Flashlight leverages PyTorch's\ncompilation workflow to fuse and tile attention computations transparently,\nenabling efficient execution for diverse attention patterns. Not only does it\nsupport all variants expressible in the FlexAttention model but it also handles\nmore general, data-dependent attention formulations that are beyond the\ncapabilities of FlexAttention.\n  Our results show that Flashlight produces kernels with competitive or\nsuperior performance to FlexAttention, while offering the flexibility of native\nPyTorch code, enabling developers to rapidly explore new attention models\nwithout sacrificing performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention is a fundamental building block of large language models (LLMs), so\nthere have been many efforts to implement it efficiently. For example,\nFlashAttention leverages tiling and kernel fusion to optimize attention.\nRecently, a number of variants of attention have been introduced to enhance\nmodel quality or efficiency. Supporting them efficiently remains difficult\nsince they usually require specialized kernels or hand-tuned implementations.\nFlexAttention recently addressed part of this gap by using static programming\ntemplates to support FlashAttention-like kernels for a subset of attention\nvariants.\n  In this paper, we introduce Flashlight, a compiler-native framework within\nthe PyTorch ecosystem that automatically generates fused, FlashAttention-style\nkernels for arbitrary attention-based programs, without relying on static\ntemplates or predefined kernel specializations. Flashlight leverages PyTorch's\ncompilation workflow to fuse and tile attention computations transparently,\nenabling efficient execution for diverse attention patterns. Not only does it\nsupport all variants expressible in the FlexAttention model but it also handles\nmore general, data-dependent attention formulations that are beyond the\ncapabilities of FlexAttention.\n  Our results show that Flashlight produces kernels with competitive or\nsuperior performance to FlexAttention, while offering the flexibility of native\nPyTorch code, enabling developers to rapidly explore new attention models\nwithout sacrificing performance."
                },
                "authors": [
                    {
                        "name": "Bozhi You"
                    },
                    {
                        "name": "Irene Wang"
                    },
                    {
                        "name": "Zelal Su Mustafaoglu"
                    },
                    {
                        "name": "Abhinav Jangda"
                    },
                    {
                        "name": "Angélica Moreira"
                    },
                    {
                        "name": "Roshan Dathathri"
                    },
                    {
                        "name": "Divya Mahajan"
                    },
                    {
                        "name": "Keshav Pingali"
                    }
                ],
                "author_detail": {
                    "name": "Keshav Pingali"
                },
                "author": "Keshav Pingali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13557v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13557v5",
                "updated": "2025-11-07T17:21:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    17,
                    21,
                    8,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-16T21:52:04Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    21,
                    52,
                    4,
                    1,
                    259,
                    0
                ],
                "title": "MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for\n  CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for\n  CGRAs"
                },
                "summary": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO-- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO-- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design."
                },
                "authors": [
                    {
                        "name": "Zesong Jiang"
                    },
                    {
                        "name": "Yuqi Sun"
                    },
                    {
                        "name": "Qing Zhong"
                    },
                    {
                        "name": "Mahathi Krishna"
                    },
                    {
                        "name": "Deepak Patil"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    },
                    {
                        "name": "Jeff Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Zhang"
                },
                "author": "Jeff Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13557v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13557v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06265v3",
                "updated": "2025-11-07T17:11:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    17,
                    11,
                    12,
                    4,
                    311,
                    0
                ],
                "published": "2025-04-08T17:59:57Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    57,
                    1,
                    98,
                    0
                ],
                "title": "Large language models as uncertainty-calibrated optimizers for\n  experimental discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models as uncertainty-calibrated optimizers for\n  experimental discovery"
                },
                "summary": "Scientific discovery increasingly depends on efficient experimental\noptimization to navigate vast design spaces under time and resource\nconstraints. Traditional approaches often require extensive domain expertise\nand feature engineering. While large language models, with their vast\nscientific knowledge, circumvent the feature engineering limitations, they lack\nthe calibrated uncertainty estimates required for high-stakes decision making.\nHence, current optimization methods force a choice between domain knowledge and\nreliability, with no principled approach that affords both. In this work, we\nshow that training language models through the uncertainty-aware objectives of\ntraditional optimization methods enables their use as reliable optimizers\nguided by natural language. By teaching LLMs from experimental outcomes under\nuncertainty, we transform their overconfidence from a fundamental limitation\ninto a precise calibration mechanism. Applied to Buchwald-Hartwig reactions, a\ncornerstone of pharmaceutical synthesis, our method nearly doubles the\ndiscovery rate of high-yielding reaction conditions, from 24% to 43% in 50\nexperimental iterations starting from 10 unsuccessful conditions. Across 19\ndiverse optimization problems spanning organic synthesis, materials science and\ncatalysis, process chemistry, and molecular design, our approach ranks first on\naverage, establishing a new paradigm for reliable, uncertainty-guided\noptimization with LLMs. Our approach can accelerate discovery by lowering the\nbarrier to using powerful optimization methods, replacing the need for\ndomain-specific feature engineering with more accessible natural language\ninterfaces. These findings highlight that ensuring reliability through\nprincipled uncertainty quantification is critical for realizing the full\npotential of AI-guided experimentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific discovery increasingly depends on efficient experimental\noptimization to navigate vast design spaces under time and resource\nconstraints. Traditional approaches often require extensive domain expertise\nand feature engineering. While large language models, with their vast\nscientific knowledge, circumvent the feature engineering limitations, they lack\nthe calibrated uncertainty estimates required for high-stakes decision making.\nHence, current optimization methods force a choice between domain knowledge and\nreliability, with no principled approach that affords both. In this work, we\nshow that training language models through the uncertainty-aware objectives of\ntraditional optimization methods enables their use as reliable optimizers\nguided by natural language. By teaching LLMs from experimental outcomes under\nuncertainty, we transform their overconfidence from a fundamental limitation\ninto a precise calibration mechanism. Applied to Buchwald-Hartwig reactions, a\ncornerstone of pharmaceutical synthesis, our method nearly doubles the\ndiscovery rate of high-yielding reaction conditions, from 24% to 43% in 50\nexperimental iterations starting from 10 unsuccessful conditions. Across 19\ndiverse optimization problems spanning organic synthesis, materials science and\ncatalysis, process chemistry, and molecular design, our approach ranks first on\naverage, establishing a new paradigm for reliable, uncertainty-guided\noptimization with LLMs. Our approach can accelerate discovery by lowering the\nbarrier to using powerful optimization methods, replacing the need for\ndomain-specific feature engineering with more accessible natural language\ninterfaces. These findings highlight that ensuring reliability through\nprincipled uncertainty quantification is critical for realizing the full\npotential of AI-guided experimentation."
                },
                "authors": [
                    {
                        "name": "Bojana Ranković"
                    },
                    {
                        "name": "Ryan-Rhys Griffiths"
                    },
                    {
                        "name": "Philippe Schwaller"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Schwaller"
                },
                "author": "Philippe Schwaller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05423v1",
                "updated": "2025-11-07T16:56:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    56,
                    37,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T16:56:37Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    56,
                    37,
                    4,
                    311,
                    0
                ],
                "title": "Scanning the IPv6 Internet Using Subnet-Router Anycast Probing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning the IPv6 Internet Using Subnet-Router Anycast Probing"
                },
                "summary": "Identifying active IPv6 addresses is challenging. Various methods emerged to\nmaster the measurement challenge in this huge address space, including\nhitlists, new probing techniques, and AI-generated target lists. In this paper,\nwe apply active Subnet-Router anycast (SRA) probing, a commonly unused method\nto explore the IPv6 address space. We compare our results with lists of active\nIPv6 nodes obtained from prior methods and with random probing. Our findings\nindicate that probing an SRA address reveals on average 10% more router IP\naddresses than random probing and is far less affected by ICMP rate limiting.\nCompared to targeting router addresses directly, SRA probing discovers 80% more\naddresses. We conclude that SRA probing is an important addition to the IPv6\nmeasurement toolbox and may improve the stability of results significantly. We\nalso find evidence that some active scans can cause harmful conditions in\ncurrent IPv6 deployments, which we started to fix in collaboration with network\noperators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying active IPv6 addresses is challenging. Various methods emerged to\nmaster the measurement challenge in this huge address space, including\nhitlists, new probing techniques, and AI-generated target lists. In this paper,\nwe apply active Subnet-Router anycast (SRA) probing, a commonly unused method\nto explore the IPv6 address space. We compare our results with lists of active\nIPv6 nodes obtained from prior methods and with random probing. Our findings\nindicate that probing an SRA address reveals on average 10% more router IP\naddresses than random probing and is far less affected by ICMP rate limiting.\nCompared to targeting router addresses directly, SRA probing discovers 80% more\naddresses. We conclude that SRA probing is an important addition to the IPv6\nmeasurement toolbox and may improve the stability of results significantly. We\nalso find evidence that some active scans can cause harmful conditions in\ncurrent IPv6 deployments, which we started to fix in collaboration with network\noperators."
                },
                "authors": [
                    {
                        "name": "Maynard Koch"
                    },
                    {
                        "name": "Raphael Hiesgen"
                    },
                    {
                        "name": "Marcin Nawrocki"
                    },
                    {
                        "name": "Thomas C. Schmidt"
                    },
                    {
                        "name": "Matthias Wählisch"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Wählisch"
                },
                "author": "Matthias Wählisch",
                "arxiv_doi": "10.1145/3768997",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3768997",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2511.05423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the ACM on Networking (PACMNET) 3, CoNEXT4, Article\n  50 (December 2025)",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00801v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00801v3",
                "updated": "2025-11-07T16:53:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    53,
                    2,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-02T04:46:43Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    4,
                    46,
                    43,
                    6,
                    306,
                    0
                ],
                "title": "Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided\n  Medical Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided\n  Medical Image Editing"
                },
                "summary": "Medical image editing has emerged as a pivotal technology with broad\napplications in data augmentation, model interpretability, medical education,\nand treatment simulation. However, the lack of large-scale, high-quality, and\nopenly accessible datasets tailored for medical contexts with strict anatomical\nand clinical constraints has significantly hindered progress in this domain. To\nbridge this gap, we introduce Med-Banana-50K, a comprehensive dataset of over\n50k medically curated image edits spanning chest X-ray, brain MRI, and fundus\nphotography across 23 diseases. Each sample supports bidirectional lesion\nediting (addition and removal) and is constructed using Gemini-2.5-Flash-Image\nbased on real clinical images. A key differentiator of our dataset is the\nmedically grounded quality control protocol: we employ an LLM-as-Judge\nevaluation framework with criteria such as instruction compliance, structural\nplausibility, image realism, and fidelity preservation, alongside iterative\nrefinement over up to five rounds. Additionally, Med-Banana-50K includes around\n37,000 failed editing attempts with full evaluation logs to support preference\nlearning and alignment research. By offering a large-scale, medically rigorous,\nand fully documented resource, Med-Banana-50K establishes a critical foundation\nfor developing and evaluating reliable medical image editing systems. Our\ndataset and code are publicly available.\n[https://github.com/richardChenzhihui/med-banana-50k].",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image editing has emerged as a pivotal technology with broad\napplications in data augmentation, model interpretability, medical education,\nand treatment simulation. However, the lack of large-scale, high-quality, and\nopenly accessible datasets tailored for medical contexts with strict anatomical\nand clinical constraints has significantly hindered progress in this domain. To\nbridge this gap, we introduce Med-Banana-50K, a comprehensive dataset of over\n50k medically curated image edits spanning chest X-ray, brain MRI, and fundus\nphotography across 23 diseases. Each sample supports bidirectional lesion\nediting (addition and removal) and is constructed using Gemini-2.5-Flash-Image\nbased on real clinical images. A key differentiator of our dataset is the\nmedically grounded quality control protocol: we employ an LLM-as-Judge\nevaluation framework with criteria such as instruction compliance, structural\nplausibility, image realism, and fidelity preservation, alongside iterative\nrefinement over up to five rounds. Additionally, Med-Banana-50K includes around\n37,000 failed editing attempts with full evaluation logs to support preference\nlearning and alignment research. By offering a large-scale, medically rigorous,\nand fully documented resource, Med-Banana-50K establishes a critical foundation\nfor developing and evaluating reliable medical image editing systems. Our\ndataset and code are publicly available.\n[https://github.com/richardChenzhihui/med-banana-50k]."
                },
                "authors": [
                    {
                        "name": "Zhihui Chen"
                    },
                    {
                        "name": "Mengling Feng"
                    }
                ],
                "author_detail": {
                    "name": "Mengling Feng"
                },
                "author": "Mengling Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00801v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00801v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05345v2",
                "updated": "2025-11-07T16:42:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    42,
                    30,
                    4,
                    311,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Hyper-Scaling with KV Cache Compression"
                },
                "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference latency and memory load. For\ninstance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and\n9.7 on LiveCodeBench on average for an equivalent number of memory reads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference latency and memory load. For\ninstance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and\n9.7 on LiveCodeBench on average for an equivalent number of memory reads."
                },
                "authors": [
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03299v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03299v6",
                "updated": "2025-11-07T16:36:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    36,
                    35,
                    4,
                    311,
                    0
                ],
                "published": "2024-02-05T18:54:43Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    18,
                    54,
                    43,
                    0,
                    36,
                    0
                ],
                "title": "GUARD: Role-playing to Generate Natural-language Jailbreakings to Test\n  Guideline Adherence of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUARD: Role-playing to Generate Natural-language Jailbreakings to Test\n  Guideline Adherence of Large Language Models"
                },
                "summary": "The discovery of \"jailbreaks\" to bypass safety filters of Large Language\nModels (LLMs) and harmful responses have encouraged the community to implement\nsafety measures. One major safety measure is to proactively test the LLMs with\njailbreaks prior to the release. Therefore, such testing will require a method\nthat can generate jailbreaks massively and efficiently. In this paper, we\nfollow a novel yet intuitive strategy to generate jailbreaks in the style of\nthe human generation. We propose a role-playing system that assigns four\ndifferent roles to the user LLMs to collaborate on new jailbreaks. Furthermore,\nwe collect existing jailbreaks and split them into different independent\ncharacteristics using clustering frequency and semantic patterns sentence by\nsentence. We organize these characteristics into a knowledge graph, making them\nmore accessible and easier to retrieve. Our system of different roles will\nleverage this knowledge graph to generate new jailbreaks, which have proved\neffective in inducing LLMs to generate unethical or guideline-violating\nresponses. In addition, we also pioneer a setting in our system that will\nautomatically follow the government-issued guidelines to generate jailbreaks to\ntest whether LLMs follow the guidelines accordingly. We refer to our system as\nGUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have\nempirically validated the effectiveness of GUARD on three cutting-edge\nopen-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a\nwidely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the\nrealm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing\nGUARD's versatility and contributing valuable insights for the development of\nsafer, more reliable LLM-based applications across diverse modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of \"jailbreaks\" to bypass safety filters of Large Language\nModels (LLMs) and harmful responses have encouraged the community to implement\nsafety measures. One major safety measure is to proactively test the LLMs with\njailbreaks prior to the release. Therefore, such testing will require a method\nthat can generate jailbreaks massively and efficiently. In this paper, we\nfollow a novel yet intuitive strategy to generate jailbreaks in the style of\nthe human generation. We propose a role-playing system that assigns four\ndifferent roles to the user LLMs to collaborate on new jailbreaks. Furthermore,\nwe collect existing jailbreaks and split them into different independent\ncharacteristics using clustering frequency and semantic patterns sentence by\nsentence. We organize these characteristics into a knowledge graph, making them\nmore accessible and easier to retrieve. Our system of different roles will\nleverage this knowledge graph to generate new jailbreaks, which have proved\neffective in inducing LLMs to generate unethical or guideline-violating\nresponses. In addition, we also pioneer a setting in our system that will\nautomatically follow the government-issued guidelines to generate jailbreaks to\ntest whether LLMs follow the guidelines accordingly. We refer to our system as\nGUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have\nempirically validated the effectiveness of GUARD on three cutting-edge\nopen-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a\nwidely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the\nrealm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing\nGUARD's versatility and contributing valuable insights for the development of\nsafer, more reliable LLM-based applications across diverse modalities."
                },
                "authors": [
                    {
                        "name": "Haibo Jin"
                    },
                    {
                        "name": "Ruoxi Chen"
                    },
                    {
                        "name": "Peiyan Zhang"
                    },
                    {
                        "name": "Andy Zhou"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "28 papges",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03299v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03299v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05408v1",
                "updated": "2025-11-07T16:34:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    34,
                    16,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T16:34:16Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    34,
                    16,
                    4,
                    311,
                    0
                ],
                "title": "Steering Language Models with Weight Arithmetic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Language Models with Weight Arithmetic"
                },
                "summary": "Providing high-quality feedback to Large Language Models (LLMs) on a diverse\ntraining distribution can be difficult and expensive, and providing feedback\nonly on a narrow distribution can result in unintended generalizations. To\nbetter leverage narrow training data, we propose contrastive weight steering, a\nsimple post-training method that edits the model parameters using weight\narithmetic. We isolate a behavior direction in weight-space by subtracting the\nweight deltas from two small fine-tunes -- one that induces the desired\nbehavior and another that induces its opposite -- and then add or remove this\ndirection to modify the model's weights. We apply this technique to mitigate\nsycophancy and induce misalignment, and find that weight steering often\ngeneralizes further than activation steering, achieving stronger\nout-of-distribution behavioral control before degrading general capabilities.\nWe also show that, in the context of task-specific fine-tuning, weight steering\ncan partially mitigate undesired behavioral drift: it can reduce sycophancy and\nunder-refusals introduced during fine-tuning while preserving task performance\ngains. Finally, we provide preliminary evidence that emergent misalignment can\nbe detected by measuring the similarity between fine-tuning updates and an\n\"evil\" weight direction, suggesting that it may be possible to monitor the\nevolution of weights during training and detect rare misaligned behaviors that\nnever manifest during training or evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing high-quality feedback to Large Language Models (LLMs) on a diverse\ntraining distribution can be difficult and expensive, and providing feedback\nonly on a narrow distribution can result in unintended generalizations. To\nbetter leverage narrow training data, we propose contrastive weight steering, a\nsimple post-training method that edits the model parameters using weight\narithmetic. We isolate a behavior direction in weight-space by subtracting the\nweight deltas from two small fine-tunes -- one that induces the desired\nbehavior and another that induces its opposite -- and then add or remove this\ndirection to modify the model's weights. We apply this technique to mitigate\nsycophancy and induce misalignment, and find that weight steering often\ngeneralizes further than activation steering, achieving stronger\nout-of-distribution behavioral control before degrading general capabilities.\nWe also show that, in the context of task-specific fine-tuning, weight steering\ncan partially mitigate undesired behavioral drift: it can reduce sycophancy and\nunder-refusals introduced during fine-tuning while preserving task performance\ngains. Finally, we provide preliminary evidence that emergent misalignment can\nbe detected by measuring the similarity between fine-tuning updates and an\n\"evil\" weight direction, suggesting that it may be possible to monitor the\nevolution of weights during training and detect rare misaligned behaviors that\nnever manifest during training or evaluations."
                },
                "authors": [
                    {
                        "name": "Constanza Fierro"
                    },
                    {
                        "name": "Fabien Roger"
                    }
                ],
                "author_detail": {
                    "name": "Fabien Roger"
                },
                "author": "Fabien Roger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05406v1",
                "updated": "2025-11-07T16:32:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    32,
                    41,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T16:32:41Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    32,
                    41,
                    4,
                    311,
                    0
                ],
                "title": "Large Language Models for Explainable Threat Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Explainable Threat Intelligence"
                },
                "summary": "As cyber threats continue to grow in complexity, traditional security\nmechanisms struggle to keep up. Large language models (LLMs) offer significant\npotential in cybersecurity due to their advanced capabilities in text\nprocessing and generation. This paper explores the use of LLMs with\nretrieval-augmented generation (RAG) to obtain threat intelligence by combining\nreal-time information retrieval with domain-specific data. The proposed system,\nRAGRecon, uses a LLM with RAG to answer questions about cybersecurity threats.\nMoreover, it makes this form of Artificial Intelligence (AI) explainable by\ngenerating and visually presenting to the user a knowledge graph for every\nreply. This increases the transparency and interpretability of the reasoning of\nthe model, allowing analysts to better understand the connections made by the\nsystem based on the context recovered by the RAG system. We evaluated RAGRecon\nexperimentally with two datasets and seven different LLMs and the responses\nmatched the reference responses more than 91% of the time for the best\ncombinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As cyber threats continue to grow in complexity, traditional security\nmechanisms struggle to keep up. Large language models (LLMs) offer significant\npotential in cybersecurity due to their advanced capabilities in text\nprocessing and generation. This paper explores the use of LLMs with\nretrieval-augmented generation (RAG) to obtain threat intelligence by combining\nreal-time information retrieval with domain-specific data. The proposed system,\nRAGRecon, uses a LLM with RAG to answer questions about cybersecurity threats.\nMoreover, it makes this form of Artificial Intelligence (AI) explainable by\ngenerating and visually presenting to the user a knowledge graph for every\nreply. This increases the transparency and interpretability of the reasoning of\nthe model, allowing analysts to better understand the connections made by the\nsystem based on the context recovered by the RAG system. We evaluated RAGRecon\nexperimentally with two datasets and seven different LLMs and the responses\nmatched the reference responses more than 91% of the time for the best\ncombinations."
                },
                "authors": [
                    {
                        "name": "Tiago Dinis"
                    },
                    {
                        "name": "Miguel Correia"
                    },
                    {
                        "name": "Roger Tavares"
                    }
                ],
                "author_detail": {
                    "name": "Roger Tavares"
                },
                "author": "Roger Tavares",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05396v1",
                "updated": "2025-11-07T16:24:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    24,
                    22,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T16:24:22Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    24,
                    22,
                    4,
                    311,
                    0
                ],
                "title": "Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement\n  Learning with Online Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement\n  Learning with Online Interaction"
                },
                "summary": "Off-dynamics reinforcement learning (RL), where training and deployment\ntransition dynamics are different, can be formulated as learning in a robust\nMarkov decision process (RMDP) where uncertainties in transition dynamics are\nimposed. Existing literature mostly assumes access to generative models\nallowing arbitrary state-action queries or pre-collected datasets with a good\nstate coverage of the deployment environment, bypassing the challenge of\nexploration. In this work, we study a more realistic and challenging setting\nwhere the agent is limited to online interaction with the training environment.\nTo capture the intrinsic difficulty of exploration in online RMDPs, we\nintroduce the supremal visitation ratio, a novel quantity that measures the\nmismatch between the training dynamics and the deployment dynamics. We show\nthat if this ratio is unbounded, online learning becomes exponentially hard. We\npropose the first computationally efficient algorithm that achieves sublinear\nregret in online RMDPs with $f$-divergence based transition uncertainties. We\nalso establish matching regret lower bounds, demonstrating that our algorithm\nachieves optimal dependence on both the supremal visitation ratio and the\nnumber of interaction episodes. Finally, we validate our theoretical results\nthrough comprehensive numerical experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-dynamics reinforcement learning (RL), where training and deployment\ntransition dynamics are different, can be formulated as learning in a robust\nMarkov decision process (RMDP) where uncertainties in transition dynamics are\nimposed. Existing literature mostly assumes access to generative models\nallowing arbitrary state-action queries or pre-collected datasets with a good\nstate coverage of the deployment environment, bypassing the challenge of\nexploration. In this work, we study a more realistic and challenging setting\nwhere the agent is limited to online interaction with the training environment.\nTo capture the intrinsic difficulty of exploration in online RMDPs, we\nintroduce the supremal visitation ratio, a novel quantity that measures the\nmismatch between the training dynamics and the deployment dynamics. We show\nthat if this ratio is unbounded, online learning becomes exponentially hard. We\npropose the first computationally efficient algorithm that achieves sublinear\nregret in online RMDPs with $f$-divergence based transition uncertainties. We\nalso establish matching regret lower bounds, demonstrating that our algorithm\nachieves optimal dependence on both the supremal visitation ratio and the\nnumber of interaction episodes. Finally, we validate our theoretical results\nthrough comprehensive numerical experiments."
                },
                "authors": [
                    {
                        "name": "Yiting He"
                    },
                    {
                        "name": "Zhishuai Liu"
                    },
                    {
                        "name": "Weixin Wang"
                    },
                    {
                        "name": "Pan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Pan Xu"
                },
                "author": "Pan Xu",
                "arxiv_comment": "53 pages, 6 figures, 3 tables. Published in Proceedings of the 42nd\n  International Conference on Machine Learning (ICML 2025)",
                "arxiv_journal_ref": "Proceedings of the 42nd International Conference on Machine\n  Learning, PMLR 267:22595-22646, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07869v2",
                "updated": "2025-11-07T16:21:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    21,
                    31,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-09T15:56:51Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    56,
                    51,
                    1,
                    252,
                    0
                ],
                "title": "Are Humans as Brittle as Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Humans as Brittle as Large Language Models?"
                },
                "summary": "The output of large language models (LLMs) is unstable, due both to\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\nprompt changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The output of large language models (LLMs) is unstable, due both to\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\nprompt changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Sean Papay"
                    },
                    {
                        "name": "Roman Klinger"
                    }
                ],
                "author_detail": {
                    "name": "Roman Klinger"
                },
                "author": "Roman Klinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21700v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21700v3",
                "updated": "2025-11-07T16:21:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    21,
                    6,
                    4,
                    311,
                    0
                ],
                "published": "2025-04-30T14:44:24Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    44,
                    24,
                    2,
                    120,
                    0
                ],
                "title": "XBreaking: Understanding how LLMs security alignment can be broken",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBreaking: Understanding how LLMs security alignment can be broken"
                },
                "summary": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. These mechanisms maintain the\nintegrity of LLM alignment by guaranteeing that the models respond safely and\nethically. In response to this, attacks on LLMs are a significant threat to\nsuch protections, and many previous approaches have already demonstrated their\neffectiveness across diverse domains. Existing LLM attacks mostly adopt a\ngenerate-and-test strategy to craft malicious input. To improve the\ncomprehension of censoring mechanisms and design a targeted attack, we propose\nan Explainable-AI solution that comparatively analyzes the behavior of censored\nand uncensored models to derive unique exploitable alignment patterns. Then, we\npropose XBreaking, a novel approach that exploits these unique patterns to\nbreak the security and alignment constraints of LLMs by targeted noise\ninjection. Our thorough experimental campaign returns important insights about\nthe censoring mechanisms and demonstrates the effectiveness and performance of\nour approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. These mechanisms maintain the\nintegrity of LLM alignment by guaranteeing that the models respond safely and\nethically. In response to this, attacks on LLMs are a significant threat to\nsuch protections, and many previous approaches have already demonstrated their\neffectiveness across diverse domains. Existing LLM attacks mostly adopt a\ngenerate-and-test strategy to craft malicious input. To improve the\ncomprehension of censoring mechanisms and design a targeted attack, we propose\nan Explainable-AI solution that comparatively analyzes the behavior of censored\nand uncensored models to derive unique exploitable alignment patterns. Then, we\npropose XBreaking, a novel approach that exploits these unique patterns to\nbreak the security and alignment constraints of LLMs by targeted noise\ninjection. Our thorough experimental campaign returns important insights about\nthe censoring mechanisms and demonstrates the effectiveness and performance of\nour approach."
                },
                "authors": [
                    {
                        "name": "Marco Arazzi"
                    },
                    {
                        "name": "Vignesh Kumar Kembu"
                    },
                    {
                        "name": "Antonino Nocera"
                    },
                    {
                        "name": "Vinod P"
                    }
                ],
                "author_detail": {
                    "name": "Vinod P"
                },
                "author": "Vinod P",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21700v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21700v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03724v2",
                "updated": "2025-11-07T16:11:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    11,
                    8,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-05T18:58:18Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    18,
                    58,
                    18,
                    2,
                    309,
                    0
                ],
                "title": "Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via\n  Self-Play and Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via\n  Self-Play and Reinforcement Learning"
                },
                "summary": "AI researchers have long focused on poker-like games as a testbed for\nenvironments characterized by multi-player dynamics, imperfect information, and\nreasoning under uncertainty. While recent breakthroughs have matched elite\nhuman play at no-limit Texas hold'em, the multi-player dynamics are subdued:\nmost hands converge quickly with only two players engaged through multiple\nrounds of bidding. In this paper, we present Solly, the first AI agent to\nachieve elite human play in reduced-format Liar's Poker, a game characterized\nby extensive multi-player engagement. We trained Solly using self-play with a\nmodel-free, actor-critic, deep reinforcement learning algorithm. Solly played\nat an elite human level as measured by win rate (won over 50% of hands) and\nequity (money won) in heads-up and multi-player Liar's Poker. Solly also\noutperformed large language models (LLMs), including those with reasoning\nabilities, on the same metrics. Solly developed novel bidding strategies,\nrandomized play effectively, and was not easily exploitable by world-class\nhuman players.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI researchers have long focused on poker-like games as a testbed for\nenvironments characterized by multi-player dynamics, imperfect information, and\nreasoning under uncertainty. While recent breakthroughs have matched elite\nhuman play at no-limit Texas hold'em, the multi-player dynamics are subdued:\nmost hands converge quickly with only two players engaged through multiple\nrounds of bidding. In this paper, we present Solly, the first AI agent to\nachieve elite human play in reduced-format Liar's Poker, a game characterized\nby extensive multi-player engagement. We trained Solly using self-play with a\nmodel-free, actor-critic, deep reinforcement learning algorithm. Solly played\nat an elite human level as measured by win rate (won over 50% of hands) and\nequity (money won) in heads-up and multi-player Liar's Poker. Solly also\noutperformed large language models (LLMs), including those with reasoning\nabilities, on the same metrics. Solly developed novel bidding strategies,\nrandomized play effectively, and was not easily exploitable by world-class\nhuman players."
                },
                "authors": [
                    {
                        "name": "Richard Dewey"
                    },
                    {
                        "name": "Janos Botyanszki"
                    },
                    {
                        "name": "Ciamac C. Moallemi"
                    },
                    {
                        "name": "Andrew T. Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Andrew T. Zheng"
                },
                "author": "Andrew T. Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23994v2",
                "updated": "2025-11-07T16:08:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    8,
                    47,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-28T17:36:52Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    36,
                    52,
                    6,
                    271,
                    0
                ],
                "title": "Policy-as-Prompt: Turning AI Governance Rules into Guardrails for AI\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy-as-Prompt: Turning AI Governance Rules into Guardrails for AI\n  Agents"
                },
                "summary": "As autonomous AI agents are used in regulated and safety-critical settings,\norganizations need effective ways to turn policy into enforceable controls. We\nintroduce a regulatory machine learning framework that converts unstructured\ndesign artifacts (like PRDs, TDDs, and code) into verifiable runtime\nguardrails. Our Policy as Prompt method reads these documents and risk controls\nto build a source-linked policy tree. This tree is then compiled into\nlightweight, prompt-based classifiers for real-time runtime monitoring. The\nsystem is built to enforce least privilege and data minimization. For\nconformity assessment, it provides complete provenance, traceability, and audit\nlogging, all integrated with a human-in-the-loop review process. Evaluations\nshow our system reduces prompt-injection risk, blocks out-of-scope requests,\nand limits toxic outputs. It also generates auditable rationales aligned with\nAI governance frameworks. By treating policies as executable prompts (a\npolicy-as-code for agents), this approach enables secure-by-design deployment,\ncontinuous compliance, and scalable AI safety and AI security assurance for\nregulatable ML.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As autonomous AI agents are used in regulated and safety-critical settings,\norganizations need effective ways to turn policy into enforceable controls. We\nintroduce a regulatory machine learning framework that converts unstructured\ndesign artifacts (like PRDs, TDDs, and code) into verifiable runtime\nguardrails. Our Policy as Prompt method reads these documents and risk controls\nto build a source-linked policy tree. This tree is then compiled into\nlightweight, prompt-based classifiers for real-time runtime monitoring. The\nsystem is built to enforce least privilege and data minimization. For\nconformity assessment, it provides complete provenance, traceability, and audit\nlogging, all integrated with a human-in-the-loop review process. Evaluations\nshow our system reduces prompt-injection risk, blocks out-of-scope requests,\nand limits toxic outputs. It also generates auditable rationales aligned with\nAI governance frameworks. By treating policies as executable prompts (a\npolicy-as-code for agents), this approach enables secure-by-design deployment,\ncontinuous compliance, and scalable AI safety and AI security assurance for\nregulatable ML."
                },
                "authors": [
                    {
                        "name": "Gauri Kholkar"
                    },
                    {
                        "name": "Ratinder Ahuja"
                    }
                ],
                "author_detail": {
                    "name": "Ratinder Ahuja"
                },
                "author": "Ratinder Ahuja",
                "arxiv_comment": "Accepted at 3rd Regulatable ML Workshop at NEURIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05385v1",
                "updated": "2025-11-07T16:08:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    8,
                    34,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T16:08:34Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    8,
                    34,
                    4,
                    311,
                    0
                ],
                "title": "TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation\n  Framework"
                },
                "summary": "Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment\nLarge Language Models' (LLMs) reliability. For flexibility, agentic RAG employs\nautonomous, multi-round retrieval and reasoning to resolve queries. Although\nrecent agentic RAG has improved via reinforcement learning, they often incur\nsubstantial token overhead from search and reasoning processes. This trade-off\nprioritizes accuracy over efficiency. To address this issue, this work proposes\nTeaRAG, a token-efficient agentic RAG framework capable of compressing both\nretrieval content and reasoning steps. 1) First, the retrieved content is\ncompressed by augmenting chunk-based semantic retrieval with a graph retrieval\nusing concise triplets. A knowledge association graph is then built from\nsemantic similarity and co-occurrence. Finally, Personalized PageRank is\nleveraged to highlight key knowledge within this graph, reducing the number of\ntokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative\nProcess-aware Direct Preference Optimization (IP-DPO) is proposed.\nSpecifically, our reward function evaluates the knowledge sufficiency by a\nknowledge matching mechanism, while penalizing excessive reasoning steps. This\ndesign can produce high-quality preference-pair datasets, supporting iterative\nDPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the\naverage Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on\nLlama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/TeaRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment\nLarge Language Models' (LLMs) reliability. For flexibility, agentic RAG employs\nautonomous, multi-round retrieval and reasoning to resolve queries. Although\nrecent agentic RAG has improved via reinforcement learning, they often incur\nsubstantial token overhead from search and reasoning processes. This trade-off\nprioritizes accuracy over efficiency. To address this issue, this work proposes\nTeaRAG, a token-efficient agentic RAG framework capable of compressing both\nretrieval content and reasoning steps. 1) First, the retrieved content is\ncompressed by augmenting chunk-based semantic retrieval with a graph retrieval\nusing concise triplets. A knowledge association graph is then built from\nsemantic similarity and co-occurrence. Finally, Personalized PageRank is\nleveraged to highlight key knowledge within this graph, reducing the number of\ntokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative\nProcess-aware Direct Preference Optimization (IP-DPO) is proposed.\nSpecifically, our reward function evaluates the knowledge sufficiency by a\nknowledge matching mechanism, while penalizing excessive reasoning steps. This\ndesign can produce high-quality preference-pair datasets, supporting iterative\nDPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the\naverage Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on\nLlama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/TeaRAG."
                },
                "authors": [
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Derong Xu"
                    },
                    {
                        "name": "Haoxin Zhang"
                    },
                    {
                        "name": "Yuanjie Lyu"
                    },
                    {
                        "name": "Yuhao Chen"
                    },
                    {
                        "name": "Shuochen Liu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05383v1",
                "updated": "2025-11-07T16:05:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    5,
                    21,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T16:05:21Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    5,
                    21,
                    4,
                    311,
                    0
                ],
                "title": "Connectomics Informed by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connectomics Informed by Large Language Models"
                },
                "summary": "Tractography is a unique method for mapping white matter connections in the\nbrain, but tractography algorithms suffer from an inherent trade-off between\nsensitivity and specificity that limits accuracy. Incorporating prior knowledge\nof white matter anatomy is an effective strategy for improving accuracy and has\nbeen successful for reducing false positives and false negatives in\nbundle-mapping protocols. However, it is challenging to scale this approach for\nconnectomics due to the difficulty in synthesising information relating to many\nthousands of possible connections. In this work, we develop and evaluate a\npipeline using large language models (LLMs) to generate quantitative priors for\nconnectomics, based on their knowledge of neuroanatomy. We benchmark our\napproach against an evaluation set derived from a gold-standard tractography\natlas, identifying prompting techniques to elicit accurate connectivity\ninformation from the LLMs. We further identify strategies for incorporating\nexternal knowledge sources into the pipeline, which can provide grounding for\nthe LLM and improve accuracy. Finally, we demonstrate how the LLM-derived\npriors can augment existing tractography filtering approaches by identifying\ntrue-positive connections to retain during the filtering process. We show that\nthese additional connections can improve the accuracy of a connectome-based\nmodel of pathology spread, which provides supporting evidence that the\nconnections preserved by the LLM are valid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tractography is a unique method for mapping white matter connections in the\nbrain, but tractography algorithms suffer from an inherent trade-off between\nsensitivity and specificity that limits accuracy. Incorporating prior knowledge\nof white matter anatomy is an effective strategy for improving accuracy and has\nbeen successful for reducing false positives and false negatives in\nbundle-mapping protocols. However, it is challenging to scale this approach for\nconnectomics due to the difficulty in synthesising information relating to many\nthousands of possible connections. In this work, we develop and evaluate a\npipeline using large language models (LLMs) to generate quantitative priors for\nconnectomics, based on their knowledge of neuroanatomy. We benchmark our\napproach against an evaluation set derived from a gold-standard tractography\natlas, identifying prompting techniques to elicit accurate connectivity\ninformation from the LLMs. We further identify strategies for incorporating\nexternal knowledge sources into the pipeline, which can provide grounding for\nthe LLM and improve accuracy. Finally, we demonstrate how the LLM-derived\npriors can augment existing tractography filtering approaches by identifying\ntrue-positive connections to retain during the filtering process. We show that\nthese additional connections can improve the accuracy of a connectome-based\nmodel of pathology spread, which provides supporting evidence that the\nconnections preserved by the LLM are valid."
                },
                "authors": [
                    {
                        "name": "Elinor Thompson"
                    },
                    {
                        "name": "Tiantian He"
                    },
                    {
                        "name": "Anna Schroder"
                    },
                    {
                        "name": "Ahmed Abdulaal"
                    },
                    {
                        "name": "Alec Sargood"
                    },
                    {
                        "name": "Sonja Soskic"
                    },
                    {
                        "name": "Henry F. J. Tregidgo"
                    },
                    {
                        "name": "Daniel C. Alexander"
                    }
                ],
                "author_detail": {
                    "name": "Daniel C. Alexander"
                },
                "author": "Daniel C. Alexander",
                "arxiv_comment": "35 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04576v2",
                "updated": "2025-11-07T15:58:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    58,
                    37,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-04T18:01:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    18,
                    1,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "Communication-Efficient Collaborative LLM Inference via Distributed\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication-Efficient Collaborative LLM Inference via Distributed\n  Speculative Decoding"
                },
                "summary": "Speculative decoding is an emerging technique that accelerates large language\nmodel (LLM) inference by allowing a smaller draft model to predict multiple\ntokens in advance, which are then verified or corrected by a larger target\nmodel. In AI-native radio access networks (AI-RAN), this paradigm is\nwell-suited for collaborative inference between resource-constrained end\ndevices and more capable edge servers or base stations (BSs). However, existing\ndistributed speculative decoding requires transmitting the full vocabulary\nprobability distribution from the draft model on the device to the target model\nat the BS, which leads to prohibitive uplink communication overhead. To address\nthis issue, we propose a ``Top-K Sparse Logits Transmission (TK-SLT)`` scheme,\nwhere the draft model transmits only the top-K token raw probabilities and the\ncorresponding token indices instead of the entire distribution. This approach\nsignificantly reduces bandwidth consumption while maintaining inference\nperformance. We further derive an analytical expression for the optimal draft\nlength that maximizes inference throughput, and provide a theoretical analysis\nof the achievable speedup ratio under TK-SLT. Experimental results validate\nboth the efficiency and effectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an emerging technique that accelerates large language\nmodel (LLM) inference by allowing a smaller draft model to predict multiple\ntokens in advance, which are then verified or corrected by a larger target\nmodel. In AI-native radio access networks (AI-RAN), this paradigm is\nwell-suited for collaborative inference between resource-constrained end\ndevices and more capable edge servers or base stations (BSs). However, existing\ndistributed speculative decoding requires transmitting the full vocabulary\nprobability distribution from the draft model on the device to the target model\nat the BS, which leads to prohibitive uplink communication overhead. To address\nthis issue, we propose a ``Top-K Sparse Logits Transmission (TK-SLT)`` scheme,\nwhere the draft model transmits only the top-K token raw probabilities and the\ncorresponding token indices instead of the entire distribution. This approach\nsignificantly reduces bandwidth consumption while maintaining inference\nperformance. We further derive an analytical expression for the optimal draft\nlength that maximizes inference throughput, and provide a theoretical analysis\nof the achievable speedup ratio under TK-SLT. Experimental results validate\nboth the efficiency and effectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "arxiv_comment": "Accepted in the Seventeenth International Conference on Wireless\n  Communications and Signal Processing Oct. 23-25, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10335v2",
                "updated": "2025-11-07T15:53:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    53,
                    49,
                    4,
                    311,
                    0
                ],
                "published": "2025-04-14T15:44:45Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    44,
                    45,
                    0,
                    104,
                    0
                ],
                "title": "MorphTok: Morphologically Grounded Tokenization for Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MorphTok: Morphologically Grounded Tokenization for Indian Languages"
                },
                "summary": "Tokenization is a crucial step in NLP, especially with the rise of large\nlanguage models (LLMs), impacting downstream performance, computational cost,\nand efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE)\nalgorithm for subword tokenization that greedily merges frequent character\nbigrams, often leading to segmentation that does not align with linguistically\nmeaningful units. To address this, we propose morphology-aware segmentation as\na pre-tokenization step before applying BPE. To facilitate morphology-aware\nsegmentation, we create a novel dataset for Hindi and Marathi, incorporating\nsandhi splitting to enhance the subword tokenization. Experiments on downstream\ntasks show that morphologically grounded tokenization improves machine\ntranslation and language modeling performance. Additionally, to handle the\ndependent vowels common in syllable-based writing systems used by Indic\nlanguages, we propose Constrained BPE (CBPE), an extension to the standard BPE\nalgorithm incorporating script-specific constraints. In particular, CBPE\nhandles dependent vowels to form a cohesive unit with other characters instead\nof occurring as a single unit. Our results show that CBPE achieves a 1.68\\%\nreduction in fertility scores while maintaining comparable or improved\ndownstream performance in machine translation and language modeling, offering a\ncomputationally efficient alternative to standard BPE. Moreover, to evaluate\nsegmentation across different tokenization algorithms, we introduce a new human\nevaluation metric, \\textit{EvalTok}, enabling more human-grounded assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a crucial step in NLP, especially with the rise of large\nlanguage models (LLMs), impacting downstream performance, computational cost,\nand efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE)\nalgorithm for subword tokenization that greedily merges frequent character\nbigrams, often leading to segmentation that does not align with linguistically\nmeaningful units. To address this, we propose morphology-aware segmentation as\na pre-tokenization step before applying BPE. To facilitate morphology-aware\nsegmentation, we create a novel dataset for Hindi and Marathi, incorporating\nsandhi splitting to enhance the subword tokenization. Experiments on downstream\ntasks show that morphologically grounded tokenization improves machine\ntranslation and language modeling performance. Additionally, to handle the\ndependent vowels common in syllable-based writing systems used by Indic\nlanguages, we propose Constrained BPE (CBPE), an extension to the standard BPE\nalgorithm incorporating script-specific constraints. In particular, CBPE\nhandles dependent vowels to form a cohesive unit with other characters instead\nof occurring as a single unit. Our results show that CBPE achieves a 1.68\\%\nreduction in fertility scores while maintaining comparable or improved\ndownstream performance in machine translation and language modeling, offering a\ncomputationally efficient alternative to standard BPE. Moreover, to evaluate\nsegmentation across different tokenization algorithms, we introduce a new human\nevaluation metric, \\textit{EvalTok}, enabling more human-grounded assessment."
                },
                "authors": [
                    {
                        "name": "Maharaj Brahma"
                    },
                    {
                        "name": "N J Karthika"
                    },
                    {
                        "name": "Atul Singh"
                    },
                    {
                        "name": "Devaraj Adiga"
                    },
                    {
                        "name": "Smruti Bhate"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    },
                    {
                        "name": "Rohit Saluja"
                    },
                    {
                        "name": "Maunendra Sankar Desarkar"
                    }
                ],
                "author_detail": {
                    "name": "Maunendra Sankar Desarkar"
                },
                "author": "Maunendra Sankar Desarkar",
                "arxiv_comment": "Accepted at Tokenization Workshop (TokShop), ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18312v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18312v3",
                "updated": "2025-11-07T15:28:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    28,
                    43,
                    4,
                    311,
                    0
                ],
                "published": "2025-08-23T16:00:30Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    16,
                    0,
                    30,
                    5,
                    235,
                    0
                ],
                "title": "What Matters in Data for DPO?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Matters in Data for DPO?"
                },
                "summary": "Direct Preference Optimization (DPO) has emerged as a simple and effective\napproach for aligning large language models (LLMs) with human preferences,\nbypassing the need for a learned reward model. Despite its growing adoption, a\nfundamental question remains open: what characteristics of preference data are\nmost critical for DPO performance? In this work, we provide a systematic study\nof how preference data distribution influences DPO, from both theoretical and\nempirical perspectives. We show that the quality of chosen responses plays a\ndominant role in optimizing the DPO objective, while the quality of rejected\nresponses may have relatively limited impact. Our theoretical analysis\ncharacterizes the optimal response distribution under DPO and reveals how\ncontrastiveness between responses helps primarily by improving the chosen\nsamples. We further study an online DPO setting and show it effectively reduces\nto supervised fine-tuning on the chosen responses. Extensive experiments across\ndiverse tasks confirm our findings: improving the quality of chosen responses\nconsistently boosts performance regardless of the quality of the rejected\nresponses. We also investigate the benefit of mixing the on-policy data. Our\nresults interpret the mechanism behind some widely adopted strategies and offer\npractical insights for constructing high-impact preference datasets for LLM\nalignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has emerged as a simple and effective\napproach for aligning large language models (LLMs) with human preferences,\nbypassing the need for a learned reward model. Despite its growing adoption, a\nfundamental question remains open: what characteristics of preference data are\nmost critical for DPO performance? In this work, we provide a systematic study\nof how preference data distribution influences DPO, from both theoretical and\nempirical perspectives. We show that the quality of chosen responses plays a\ndominant role in optimizing the DPO objective, while the quality of rejected\nresponses may have relatively limited impact. Our theoretical analysis\ncharacterizes the optimal response distribution under DPO and reveals how\ncontrastiveness between responses helps primarily by improving the chosen\nsamples. We further study an online DPO setting and show it effectively reduces\nto supervised fine-tuning on the chosen responses. Extensive experiments across\ndiverse tasks confirm our findings: improving the quality of chosen responses\nconsistently boosts performance regardless of the quality of the rejected\nresponses. We also investigate the benefit of mixing the on-policy data. Our\nresults interpret the mechanism behind some widely adopted strategies and offer\npractical insights for constructing high-impact preference datasets for LLM\nalignment."
                },
                "authors": [
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Zhongze Cai"
                    },
                    {
                        "name": "Guanting Chen"
                    },
                    {
                        "name": "Huaiyang Zhong"
                    },
                    {
                        "name": "Chonghuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chonghuan Wang"
                },
                "author": "Chonghuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18312v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18312v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05320v1",
                "updated": "2025-11-07T15:17:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    17,
                    45,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:17:45Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    17,
                    45,
                    4,
                    311,
                    0
                ],
                "title": "What Are the Facts? Automated Extraction of Court-Established Facts from\n  Criminal-Court Opinions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Are the Facts? Automated Extraction of Court-Established Facts from\n  Criminal-Court Opinions"
                },
                "summary": "Criminal justice administrative data contain only a limited amount of\ninformation about the committed offense. However, there is an unused source of\nextensive information in continental European courts' decisions: descriptions\nof criminal behaviors in verdicts by which offenders are found guilty. In this\npaper, we study the feasibility of extracting these descriptions from publicly\navailable court decisions from Slovakia. We use two different approaches for\nretrieval: regular expressions and large language models (LLMs). Our baseline\nwas a simple method employing regular expressions to identify typical words\noccurring before and after the description. The advanced regular expression\napproach further focused on \"sparing\" and its normalization (insertion of\nspaces between individual letters), typical for delineating the description.\nThe LLM approach involved prompting the Gemini Flash 2.0 model to extract the\ndescriptions using predefined instructions. Although the baseline identified\ndescriptions in only 40.5% of verdicts, both methods significantly outperformed\nit, achieving 97% with advanced regular expressions and 98.75% with LLMs, and\n99.5% when combined. Evaluation by law students showed that both advanced\nmethods matched human annotations in about 90% of cases, compared to just 34.5%\nfor the baseline. LLMs fully matched human-labeled descriptions in 91.75% of\ninstances, and a combination of advanced regular expressions with LLMs reached\n92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Criminal justice administrative data contain only a limited amount of\ninformation about the committed offense. However, there is an unused source of\nextensive information in continental European courts' decisions: descriptions\nof criminal behaviors in verdicts by which offenders are found guilty. In this\npaper, we study the feasibility of extracting these descriptions from publicly\navailable court decisions from Slovakia. We use two different approaches for\nretrieval: regular expressions and large language models (LLMs). Our baseline\nwas a simple method employing regular expressions to identify typical words\noccurring before and after the description. The advanced regular expression\napproach further focused on \"sparing\" and its normalization (insertion of\nspaces between individual letters), typical for delineating the description.\nThe LLM approach involved prompting the Gemini Flash 2.0 model to extract the\ndescriptions using predefined instructions. Although the baseline identified\ndescriptions in only 40.5% of verdicts, both methods significantly outperformed\nit, achieving 97% with advanced regular expressions and 98.75% with LLMs, and\n99.5% when combined. Evaluation by law students showed that both advanced\nmethods matched human annotations in about 90% of cases, compared to just 34.5%\nfor the baseline. LLMs fully matched human-labeled descriptions in 91.75% of\ninstances, and a combination of advanced regular expressions with LLMs reached\n92%."
                },
                "authors": [
                    {
                        "name": "Klára Bendová"
                    },
                    {
                        "name": "Tomáš Knap"
                    },
                    {
                        "name": "Jan Černý"
                    },
                    {
                        "name": "Vojtěch Pour"
                    },
                    {
                        "name": "Jaromir Savelka"
                    },
                    {
                        "name": "Ivana Kvapilíková"
                    },
                    {
                        "name": "Jakub Drápal"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Drápal"
                },
                "author": "Jakub Drápal",
                "arxiv_comment": "Paper accepted to the proceedings of ASAIL 2025 Workshop under ICAIL\n  conference for publication. Paper contains 6 pages (references included) and\n  2 appendices. It contains 8 tables, no figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05319v1",
                "updated": "2025-11-07T15:17:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    17,
                    40,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:17:40Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    17,
                    40,
                    4,
                    311,
                    0
                ],
                "title": "$\\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language\n  Models"
                },
                "summary": "Although steganography has made significant advancements in recent years, it\nstill struggles to embed semantically rich, sentence-level information into\ncarriers. However, in the era of AIGC, the capacity of steganography is more\ncritical than ever. In this work, we present Sentence-to-Image Steganography,\nan instance of Semantic Steganography, a novel task that enables the hiding of\narbitrary sentence-level messages within a cover image. Furthermore, we\nestablish a benchmark named Invisible Text (IVT), comprising a diverse set of\nsentence-level texts as secret messages for evaluation. Finally, we present\n$\\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large\nlanguage models (LLMs) to embed high-level textual information, such as\nsentences or even paragraphs, into images. Unlike traditional bit-level\ncounterparts, $\\mathrm{S^2LM}$ enables the integration of semantically rich\ncontent through a newly designed pipeline in which the LLM is involved\nthroughout the entire process. Both quantitative and qualitative experiments\ndemonstrate that our method effectively unlocks new semantic steganographic\ncapabilities for LLMs. The source code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although steganography has made significant advancements in recent years, it\nstill struggles to embed semantically rich, sentence-level information into\ncarriers. However, in the era of AIGC, the capacity of steganography is more\ncritical than ever. In this work, we present Sentence-to-Image Steganography,\nan instance of Semantic Steganography, a novel task that enables the hiding of\narbitrary sentence-level messages within a cover image. Furthermore, we\nestablish a benchmark named Invisible Text (IVT), comprising a diverse set of\nsentence-level texts as secret messages for evaluation. Finally, we present\n$\\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large\nlanguage models (LLMs) to embed high-level textual information, such as\nsentences or even paragraphs, into images. Unlike traditional bit-level\ncounterparts, $\\mathrm{S^2LM}$ enables the integration of semantically rich\ncontent through a newly designed pipeline in which the LLM is involved\nthroughout the entire process. Both quantitative and qualitative experiments\ndemonstrate that our method effectively unlocks new semantic steganographic\ncapabilities for LLMs. The source code will be released soon."
                },
                "authors": [
                    {
                        "name": "Huanqi Wu"
                    },
                    {
                        "name": "Huangbiao Xu"
                    },
                    {
                        "name": "Runfeng Xie"
                    },
                    {
                        "name": "Jiaxin Cai"
                    },
                    {
                        "name": "Kaixin Zhang"
                    },
                    {
                        "name": "Xiao Ke"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Ke"
                },
                "author": "Xiao Ke",
                "arxiv_comment": "35 Pages, 20 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05311v1",
                "updated": "2025-11-07T15:12:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    12,
                    49,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:12:49Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    12,
                    49,
                    4,
                    311,
                    0
                ],
                "title": "Cleaning Maintenance Logs with LLM Agents for Improved Predictive\n  Maintenance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cleaning Maintenance Logs with LLM Agents for Improved Predictive\n  Maintenance"
                },
                "summary": "Economic constraints, limited availability of datasets for reproducibility\nand shortages of specialized expertise have long been recognized as key\nchallenges to the adoption and advancement of predictive maintenance (PdM) in\nthe automotive sector. Recent progress in large language models (LLMs) presents\nan opportunity to overcome these barriers and speed up the transition of PdM\nfrom research to industrial practice. Under these conditions, we explore the\npotential of LLM-based agents to support PdM cleaning pipelines. Specifically,\nwe focus on maintenance logs, a critical data source for training\nwell-performing machine learning (ML) models, but one often affected by errors\nsuch as typos, missing fields, near-duplicate entries, and incorrect dates. We\nevaluate LLM agents on cleaning tasks involving six distinct types of noise.\nOur findings show that LLMs are effective at handling generic cleaning tasks\nand offer a promising foundation for future industrial applications. While\ndomain-specific errors remain challenging, these results highlight the\npotential for further improvements through specialized training and enhanced\nagentic capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Economic constraints, limited availability of datasets for reproducibility\nand shortages of specialized expertise have long been recognized as key\nchallenges to the adoption and advancement of predictive maintenance (PdM) in\nthe automotive sector. Recent progress in large language models (LLMs) presents\nan opportunity to overcome these barriers and speed up the transition of PdM\nfrom research to industrial practice. Under these conditions, we explore the\npotential of LLM-based agents to support PdM cleaning pipelines. Specifically,\nwe focus on maintenance logs, a critical data source for training\nwell-performing machine learning (ML) models, but one often affected by errors\nsuch as typos, missing fields, near-duplicate entries, and incorrect dates. We\nevaluate LLM agents on cleaning tasks involving six distinct types of noise.\nOur findings show that LLMs are effective at handling generic cleaning tasks\nand offer a promising foundation for future industrial applications. While\ndomain-specific errors remain challenging, these results highlight the\npotential for further improvements through specialized training and enhanced\nagentic capabilities."
                },
                "authors": [
                    {
                        "name": "Valeriu Dimidov"
                    },
                    {
                        "name": "Faisal Hawlader"
                    },
                    {
                        "name": "Sasan Jafarnejad"
                    },
                    {
                        "name": "Raphaël Frank"
                    }
                ],
                "author_detail": {
                    "name": "Raphaël Frank"
                },
                "author": "Raphaël Frank",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05307v1",
                "updated": "2025-11-07T15:07:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    7,
                    1,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:07:01Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    7,
                    1,
                    4,
                    311,
                    0
                ],
                "title": "Force-Safe Environment Maps and Real-Time Detection for Soft Robot\n  Manipulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Force-Safe Environment Maps and Real-Time Detection for Soft Robot\n  Manipulators"
                },
                "summary": "Soft robot manipulators have the potential for deployment in delicate\nenvironments to perform complex manipulation tasks. However, existing obstacle\ndetection and avoidance methods do not consider limits on the forces that\nmanipulators may exert upon contact with delicate obstacles. This work\nintroduces a framework that maps force safety criteria from task space (i.e.\npositions along the robot's body) to configuration space (i.e. the robot's\njoint angles) and enables real-time force safety detection. We incorporate\nlimits on allowable environmental contact forces for given task-space\nobstacles, and map them into configuration space (C-space) through the\nmanipulator's forward kinematics. This formulation ensures that configurations\nclassified as safe are provably below the maximum force thresholds, thereby\nallowing us to determine force-safe configurations of the soft robot\nmanipulator in real-time. We validate our approach in simulation and hardware\nexperiments on a two-segment pneumatic soft robot manipulator. Results\ndemonstrate that the proposed method accurately detects force safety during\ninteractions with deformable obstacles, thereby laying the foundation for\nreal-time safe planning of soft manipulators in delicate, cluttered\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft robot manipulators have the potential for deployment in delicate\nenvironments to perform complex manipulation tasks. However, existing obstacle\ndetection and avoidance methods do not consider limits on the forces that\nmanipulators may exert upon contact with delicate obstacles. This work\nintroduces a framework that maps force safety criteria from task space (i.e.\npositions along the robot's body) to configuration space (i.e. the robot's\njoint angles) and enables real-time force safety detection. We incorporate\nlimits on allowable environmental contact forces for given task-space\nobstacles, and map them into configuration space (C-space) through the\nmanipulator's forward kinematics. This formulation ensures that configurations\nclassified as safe are provably below the maximum force thresholds, thereby\nallowing us to determine force-safe configurations of the soft robot\nmanipulator in real-time. We validate our approach in simulation and hardware\nexperiments on a two-segment pneumatic soft robot manipulator. Results\ndemonstrate that the proposed method accurately detects force safety during\ninteractions with deformable obstacles, thereby laying the foundation for\nreal-time safe planning of soft manipulators in delicate, cluttered\nenvironments."
                },
                "authors": [
                    {
                        "name": "Akua K. Dickson"
                    },
                    {
                        "name": "Juan C. Pacheco Garcia"
                    },
                    {
                        "name": "Andrew P. Sabelhaus"
                    }
                ],
                "author_detail": {
                    "name": "Andrew P. Sabelhaus"
                },
                "author": "Andrew P. Sabelhaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05302v1",
                "updated": "2025-11-07T15:02:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    2,
                    42,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:02:42Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    2,
                    42,
                    4,
                    311,
                    0
                ],
                "title": "Code Review Automation using Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Review Automation using Retrieval Augmented Generation"
                },
                "summary": "Code review is essential for maintaining software quality but is\nlabor-intensive. Automated code review generation offers a promising solution\nto this challenge. Both deep learning-based generative techniques and\nretrieval-based methods have demonstrated strong performance in this task.\nHowever, despite these advancements, there are still some limitations where\ngenerated reviews can be either off-point or overly general. To address these\nissues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages\nRetrieval-Augmented Generation (RAG) to combine retrieval-based and generative\nmethods, explicitly incorporating external domain knowledge into the code\nreview process. RARe uses a dense retriever to select the most relevant reviews\nfrom the codebase, which then enrich the input for a neural generator,\nutilizing the contextual learning capacity of large language models (LLMs), to\nproduce the final review. RARe outperforms state-of-the-art methods on two\nbenchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively.\nIts effectiveness is further validated through a detailed human evaluation and\na case study using an interpretability tool, demonstrating its practical\nutility and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code review is essential for maintaining software quality but is\nlabor-intensive. Automated code review generation offers a promising solution\nto this challenge. Both deep learning-based generative techniques and\nretrieval-based methods have demonstrated strong performance in this task.\nHowever, despite these advancements, there are still some limitations where\ngenerated reviews can be either off-point or overly general. To address these\nissues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages\nRetrieval-Augmented Generation (RAG) to combine retrieval-based and generative\nmethods, explicitly incorporating external domain knowledge into the code\nreview process. RARe uses a dense retriever to select the most relevant reviews\nfrom the codebase, which then enrich the input for a neural generator,\nutilizing the contextual learning capacity of large language models (LLMs), to\nproduce the final review. RARe outperforms state-of-the-art methods on two\nbenchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively.\nIts effectiveness is further validated through a detailed human evaluation and\na case study using an interpretability tool, demonstrating its practical\nutility and reliability."
                },
                "authors": [
                    {
                        "name": "Qianru Meng"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Zhaochen Ren"
                    },
                    {
                        "name": "Joost Visser"
                    }
                ],
                "author_detail": {
                    "name": "Joost Visser"
                },
                "author": "Joost Visser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05301v1",
                "updated": "2025-11-07T15:01:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    1,
                    38,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:01:38Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    1,
                    38,
                    4,
                    311,
                    0
                ],
                "title": "QUESTER: Query Specification for Generative Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUESTER: Query Specification for Generative Retrieval"
                },
                "summary": "Generative Retrieval (GR) differs from the traditional index-then-retrieve\npipeline by storing relevance in model parameters and directly generating\ndocument identifiers. However, GR often struggles to generalize and is costly\nto scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval),\nwhich reframes GR as query specification generation - in this work, a simple\nkeyword query handled by BM25 - using a (small) LLM. The policy is trained\nusing reinforcement learning techniques (GRPO). Across in- and out-of-domain\nevaluations, we show that our model is more effective than BM25, and\ncompetitive with neural IR models, while maintaining a good efficiency",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Retrieval (GR) differs from the traditional index-then-retrieve\npipeline by storing relevance in model parameters and directly generating\ndocument identifiers. However, GR often struggles to generalize and is costly\nto scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval),\nwhich reframes GR as query specification generation - in this work, a simple\nkeyword query handled by BM25 - using a (small) LLM. The policy is trained\nusing reinforcement learning techniques (GRPO). Across in- and out-of-domain\nevaluations, we show that our model is more effective than BM25, and\ncompetitive with neural IR models, while maintaining a good efficiency"
                },
                "authors": [
                    {
                        "name": "Arthur Satouf"
                    },
                    {
                        "name": "Yuxuan Zong"
                    },
                    {
                        "name": "Habiboulaye Amadou-Boubacar"
                    },
                    {
                        "name": "Pablo Piantanida"
                    },
                    {
                        "name": "Benjamin Piwowarski"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Piwowarski"
                },
                "author": "Benjamin Piwowarski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68P20, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05299v1",
                "updated": "2025-11-07T15:00:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    0,
                    37,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T15:00:37Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    0,
                    37,
                    4,
                    311,
                    0
                ],
                "title": "LiveStar: Live Streaming Assistant for Real-World Online Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveStar: Live Streaming Assistant for Real-World Online Video\n  Understanding"
                },
                "summary": "Despite significant progress in Video Large Language Models (Video-LLMs) for\noffline video understanding, existing online Video-LLMs typically struggle to\nsimultaneously process continuous frame-by-frame inputs and determine optimal\nresponse timing, often compromising real-time responsiveness and narrative\ncoherence. To address these limitations, we introduce LiveStar, a pioneering\nlive streaming assistant that achieves always-on proactive responses through\nadaptive streaming decoding. Specifically, LiveStar incorporates: (1) a\ntraining strategy enabling incremental video-language alignment for\nvariable-length video streams, preserving temporal consistency across\ndynamically evolving frame sequences; (2) a response-silence decoding framework\nthat determines optimal proactive response timing via a single forward pass\nverification; (3) memory-aware acceleration via peak-end memory compression for\nonline inference on 10+ minute videos, combined with streaming key-value cache\nto achieve 1.53x faster inference. We also construct an OmniStar dataset, a\ncomprehensive dataset for training and benchmarking that encompasses 15 diverse\nreal-world scenarios and 5 evaluation tasks for online video understanding.\nExtensive experiments across three benchmarks demonstrate LiveStar's\nstate-of-the-art performance, achieving an average 19.5% improvement in\nsemantic correctness with 18.1% reduced timing difference compared to existing\nonline Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.\nOur model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress in Video Large Language Models (Video-LLMs) for\noffline video understanding, existing online Video-LLMs typically struggle to\nsimultaneously process continuous frame-by-frame inputs and determine optimal\nresponse timing, often compromising real-time responsiveness and narrative\ncoherence. To address these limitations, we introduce LiveStar, a pioneering\nlive streaming assistant that achieves always-on proactive responses through\nadaptive streaming decoding. Specifically, LiveStar incorporates: (1) a\ntraining strategy enabling incremental video-language alignment for\nvariable-length video streams, preserving temporal consistency across\ndynamically evolving frame sequences; (2) a response-silence decoding framework\nthat determines optimal proactive response timing via a single forward pass\nverification; (3) memory-aware acceleration via peak-end memory compression for\nonline inference on 10+ minute videos, combined with streaming key-value cache\nto achieve 1.53x faster inference. We also construct an OmniStar dataset, a\ncomprehensive dataset for training and benchmarking that encompasses 15 diverse\nreal-world scenarios and 5 evaluation tasks for online video understanding.\nExtensive experiments across three benchmarks demonstrate LiveStar's\nstate-of-the-art performance, achieving an average 19.5% improvement in\nsemantic correctness with 18.1% reduced timing difference compared to existing\nonline Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.\nOur model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar."
                },
                "authors": [
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Kairui Zhang"
                    },
                    {
                        "name": "Yuhang Hu"
                    },
                    {
                        "name": "Bing Wang"
                    },
                    {
                        "name": "Shengsheng Qian"
                    },
                    {
                        "name": "Bin Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Weiming Dong"
                    },
                    {
                        "name": "Changsheng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Changsheng Xu"
                },
                "author": "Changsheng Xu",
                "arxiv_comment": "NeurIPS 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05298v1",
                "updated": "2025-11-07T14:57:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    57,
                    13,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T14:57:13Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    57,
                    13,
                    4,
                    311,
                    0
                ],
                "title": "Location-Informed Interference Suppression Precoding Methods for\n  Distributed Massive MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Location-Informed Interference Suppression Precoding Methods for\n  Distributed Massive MIMO Systems"
                },
                "summary": "The evolution of mobile networks towards user-centric cell-free distributed\nMassive MIMO configurations requires the development of novel signal processing\ntechniques. More specifically, digital precoding algorithms have to be designed\nor adopted to enable distributed operation. Future deployments are expected to\nimprove coexistence between cellular generations, and between mobile networks\nand incumbent services such as radar. In dense cell-free deployments, it might\nalso not be possible to have full channel state information for all users at\nall antennas. To leverage location information in a dense deployment area, we\nsuggest and investigate several algorithmic alterations on existing precoding\nmethods, aimed at location-informed interference suppression, for usage in\nexisting and emerging systems where user locations are known. The proposed\nalgorithms are derived using a theoretical channel model and validated and\nnumerically evaluated using an empirical dataset containing channel\nmeasurements from an indoor distributed Massive MIMO testbed. When dealing with\nmeasured CSI, the impact of the hardware, in addition to the location-based\nchannel, needs to be compensated for. We propose a method to calibrate the\nhardware and achieve measurement-based evaluation of our location-based\ninterference suppression algorithms. The results demonstrate that the proposed\nmethods allow location-based interference suppression without explicit CSI\nknowledge at the transmitter, under certain realistic network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of mobile networks towards user-centric cell-free distributed\nMassive MIMO configurations requires the development of novel signal processing\ntechniques. More specifically, digital precoding algorithms have to be designed\nor adopted to enable distributed operation. Future deployments are expected to\nimprove coexistence between cellular generations, and between mobile networks\nand incumbent services such as radar. In dense cell-free deployments, it might\nalso not be possible to have full channel state information for all users at\nall antennas. To leverage location information in a dense deployment area, we\nsuggest and investigate several algorithmic alterations on existing precoding\nmethods, aimed at location-informed interference suppression, for usage in\nexisting and emerging systems where user locations are known. The proposed\nalgorithms are derived using a theoretical channel model and validated and\nnumerically evaluated using an empirical dataset containing channel\nmeasurements from an indoor distributed Massive MIMO testbed. When dealing with\nmeasured CSI, the impact of the hardware, in addition to the location-based\nchannel, needs to be compensated for. We propose a method to calibrate the\nhardware and achieve measurement-based evaluation of our location-based\ninterference suppression algorithms. The results demonstrate that the proposed\nmethods allow location-based interference suppression without explicit CSI\nknowledge at the transmitter, under certain realistic network conditions."
                },
                "authors": [
                    {
                        "name": "Emiel Vanspranghels"
                    },
                    {
                        "name": "Raquel Marina Noguera Oishi"
                    },
                    {
                        "name": "Franco Minucci"
                    },
                    {
                        "name": "Sofie Pollin"
                    }
                ],
                "author_detail": {
                    "name": "Sofie Pollin"
                },
                "author": "Sofie Pollin",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05297v1",
                "updated": "2025-11-07T14:56:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    56,
                    45,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T14:56:45Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    56,
                    45,
                    4,
                    311,
                    0
                ],
                "title": "Building Specialized Software-Assistant ChatBot with Graph-Based\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Specialized Software-Assistant ChatBot with Graph-Based\n  Retrieval-Augmented Generation"
                },
                "summary": "Digital Adoption Platforms (DAPs) have become essential tools for helping\nemployees navigate complex enterprise software such as CRM, ERP, or HRMS\nsystems. Companies like LemonLearning have shown how digital guidance can\nreduce training costs and accelerate onboarding. However, building and\nmaintaining these interactive guides still requires extensive manual effort.\nLeveraging Large Language Models as virtual assistants is an appealing\nalternative, yet without a structured understanding of the target software,\nLLMs often hallucinate and produce unreliable answers. Moreover, most\nproduction-grade LLMs are black-box APIs, making fine-tuning impractical due to\nthe lack of access to model weights. In this work, we introduce a Graph-based\nRetrieval-Augmented Generation framework that automatically converts enterprise\nweb applications into state-action knowledge graphs, enabling LLMs to generate\ngrounded and context-aware assistance. The framework was co-developed with the\nAI enterprise RAKAM, in collaboration with Lemon Learning. We detail the\nengineering pipeline that extracts and structures software interfaces, the\ndesign of the graph-based retrieval process, and the integration of our\napproach into production DAP workflows. Finally, we discuss scalability,\nrobustness, and deployment lessons learned from industrial use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Adoption Platforms (DAPs) have become essential tools for helping\nemployees navigate complex enterprise software such as CRM, ERP, or HRMS\nsystems. Companies like LemonLearning have shown how digital guidance can\nreduce training costs and accelerate onboarding. However, building and\nmaintaining these interactive guides still requires extensive manual effort.\nLeveraging Large Language Models as virtual assistants is an appealing\nalternative, yet without a structured understanding of the target software,\nLLMs often hallucinate and produce unreliable answers. Moreover, most\nproduction-grade LLMs are black-box APIs, making fine-tuning impractical due to\nthe lack of access to model weights. In this work, we introduce a Graph-based\nRetrieval-Augmented Generation framework that automatically converts enterprise\nweb applications into state-action knowledge graphs, enabling LLMs to generate\ngrounded and context-aware assistance. The framework was co-developed with the\nAI enterprise RAKAM, in collaboration with Lemon Learning. We detail the\nengineering pipeline that extracts and structures software interfaces, the\ndesign of the graph-based retrieval process, and the integration of our\napproach into production DAP workflows. Finally, we discuss scalability,\nrobustness, and deployment lessons learned from industrial use cases."
                },
                "authors": [
                    {
                        "name": "Mohammed Hilel"
                    },
                    {
                        "name": "Yannis Karmim"
                    },
                    {
                        "name": "Jean De Bodinat"
                    },
                    {
                        "name": "Reda Sarehane"
                    },
                    {
                        "name": "Antoine Gillon"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Gillon"
                },
                "author": "Antoine Gillon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05295v1",
                "updated": "2025-11-07T14:56:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    56,
                    4,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T14:56:04Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    56,
                    4,
                    4,
                    311,
                    0
                ],
                "title": "Language Generation and Identification From Partial Enumeration: Tight\n  Density Bounds and Topological Characterizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Generation and Identification From Partial Enumeration: Tight\n  Density Bounds and Topological Characterizations"
                },
                "summary": "The success of large language models (LLMs) has motivated formal theories of\nlanguage generation and learning. We study the framework of \\emph{language\ngeneration in the limit}, where an adversary enumerates strings from an unknown\nlanguage $K$ drawn from a countable class, and an algorithm must generate\nunseen strings from $K$. Prior work showed that generation is always possible,\nand that some algorithms achieve positive lower density, revealing a\n\\emph{validity--breadth} trade-off between correctness and coverage. We resolve\na main open question in this line, proving a tight bound of $1/2$ on the best\nachievable lower density. We then strengthen the model to allow \\emph{partial\nenumeration}, where the adversary reveals only an infinite subset $C \\subseteq\nK$. We show that generation in the limit remains achievable, and if $C$ has\nlower density $\\alpha$ in $K$, the algorithm's output achieves density at least\n$\\alpha/2$, matching the upper bound. This generalizes the $1/2$ bound to the\npartial-information setting, where the generator must recover within a factor\n$1/2$ of the revealed subset's density. We further revisit the classical\nGold--Angluin model of \\emph{language identification} under partial\nenumeration. We characterize when identification in the limit is possible --\nwhen hypotheses $M_t$ eventually satisfy $C \\subseteq M \\subseteq K$ -- and in\nthe process give a new topological formulation of Angluin's characterization,\nshowing that her condition is precisely equivalent to an appropriate\ntopological space having the $T_D$ separation property.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of large language models (LLMs) has motivated formal theories of\nlanguage generation and learning. We study the framework of \\emph{language\ngeneration in the limit}, where an adversary enumerates strings from an unknown\nlanguage $K$ drawn from a countable class, and an algorithm must generate\nunseen strings from $K$. Prior work showed that generation is always possible,\nand that some algorithms achieve positive lower density, revealing a\n\\emph{validity--breadth} trade-off between correctness and coverage. We resolve\na main open question in this line, proving a tight bound of $1/2$ on the best\nachievable lower density. We then strengthen the model to allow \\emph{partial\nenumeration}, where the adversary reveals only an infinite subset $C \\subseteq\nK$. We show that generation in the limit remains achievable, and if $C$ has\nlower density $\\alpha$ in $K$, the algorithm's output achieves density at least\n$\\alpha/2$, matching the upper bound. This generalizes the $1/2$ bound to the\npartial-information setting, where the generator must recover within a factor\n$1/2$ of the revealed subset's density. We further revisit the classical\nGold--Angluin model of \\emph{language identification} under partial\nenumeration. We characterize when identification in the limit is possible --\nwhen hypotheses $M_t$ eventually satisfy $C \\subseteq M \\subseteq K$ -- and in\nthe process give a new topological formulation of Angluin's characterization,\nshowing that her condition is precisely equivalent to an appropriate\ntopological space having the $T_D$ separation property."
                },
                "authors": [
                    {
                        "name": "Jon Kleinberg"
                    },
                    {
                        "name": "Fan Wei"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wei"
                },
                "author": "Fan Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16470v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16470v2",
                "updated": "2025-11-07T14:52:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    52,
                    6,
                    4,
                    311,
                    0
                ],
                "published": "2025-05-22T09:52:57Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    9,
                    52,
                    57,
                    3,
                    142,
                    0
                ],
                "title": "Benchmarking Retrieval-Augmented Multimodal Generation for Document\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Retrieval-Augmented Multimodal Generation for Document\n  Question Answering"
                },
                "summary": "Document Visual Question Answering (DocVQA) faces dual challenges in\nprocessing lengthy multimodal documents (text, images, tables) and performing\ncross-modal reasoning. Current document retrieval-augmented generation (DocRAG)\nmethods remain limited by their text-centric approaches, frequently missing\ncritical visual information. The field also lacks robust benchmarks for\nassessing multimodal evidence selection and integration. We introduce MMDocRAG,\na comprehensive benchmark featuring 4,055 expert-annotated QA pairs with\nmulti-page, cross-modal evidence chains. Our framework introduces innovative\nmetrics for evaluating multimodal quote selection and enables answers that\ninterleave text with relevant visual elements. Through large-scale experiments\nwith 60 VLM/LLM models and 14 retrieval systems, we identify persistent\nchallenges in multimodal evidence retrieval, selection, and integration.Key\nfindings reveal advanced proprietary LVMs show superior performance than\nopen-sourced alternatives. Also, they show moderate advantages using multimodal\ninputs over text-only inputs, while open-source alternatives show significant\nperformance degradation. Notably, fine-tuned LLMs achieve substantial\nimprovements when using detailed image descriptions. MMDocRAG establishes a\nrigorous testing ground and provides actionable insights for developing more\nrobust multimodal DocVQA systems. Our benchmark and code are available at\nhttps://mmdocrag.github.io/MMDocRAG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document Visual Question Answering (DocVQA) faces dual challenges in\nprocessing lengthy multimodal documents (text, images, tables) and performing\ncross-modal reasoning. Current document retrieval-augmented generation (DocRAG)\nmethods remain limited by their text-centric approaches, frequently missing\ncritical visual information. The field also lacks robust benchmarks for\nassessing multimodal evidence selection and integration. We introduce MMDocRAG,\na comprehensive benchmark featuring 4,055 expert-annotated QA pairs with\nmulti-page, cross-modal evidence chains. Our framework introduces innovative\nmetrics for evaluating multimodal quote selection and enables answers that\ninterleave text with relevant visual elements. Through large-scale experiments\nwith 60 VLM/LLM models and 14 retrieval systems, we identify persistent\nchallenges in multimodal evidence retrieval, selection, and integration.Key\nfindings reveal advanced proprietary LVMs show superior performance than\nopen-sourced alternatives. Also, they show moderate advantages using multimodal\ninputs over text-only inputs, while open-source alternatives show significant\nperformance degradation. Notably, fine-tuned LLMs achieve substantial\nimprovements when using detailed image descriptions. MMDocRAG establishes a\nrigorous testing ground and provides actionable insights for developing more\nrobust multimodal DocVQA systems. Our benchmark and code are available at\nhttps://mmdocrag.github.io/MMDocRAG/."
                },
                "authors": [
                    {
                        "name": "Kuicai Dong"
                    },
                    {
                        "name": "Yujing Chang"
                    },
                    {
                        "name": "Shijie Huang"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Paper accepted to NeurIPS 2025 DB",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16470v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16470v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05286v1",
                "updated": "2025-11-07T14:48:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    48,
                    49,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T14:48:49Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    48,
                    49,
                    4,
                    311,
                    0
                ],
                "title": "Reflective Personalization Optimization: A Post-hoc Rewriting Framework\n  for Black-Box Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflective Personalization Optimization: A Post-hoc Rewriting Framework\n  for Black-Box Large Language Models"
                },
                "summary": "The personalization of black-box large language models (LLMs) is a critical\nyet challenging task. Existing approaches predominantly rely on context\ninjection, where user history is embedded into the prompt to directly guide the\ngeneration process. However, this single-step paradigm imposes a dual burden on\nthe model: generating accurate content while simultaneously aligning with\nuser-specific styles. This often results in a trade-off that compromises output\nquality and limits precise control. To address this fundamental tension, we\npropose Reflective Personalization Optimization (RPO), a novel framework that\nredefines the personalization paradigm by decoupling content generation from\nalignment. RPO operates in two distinct stages: first, a base model generates a\nhigh-quality, generic response; then, an external reflection module explicitly\nrewrites this output to align with the user's preferences. This reflection\nmodule is trained using a two-stage process. Initially, supervised fine-tuning\nis employed on structured rewriting trajectories to establish a core\npersonalized reasoning policy that models the transformation from generic to\nuser-aligned responses. Subsequently, reinforcement learning is applied to\nfurther refine and enhance the quality of the personalized outputs.\nComprehensive experiments on the LaMP benchmark demonstrate that RPO, by\ndecoupling content generation from personalization, significantly outperforms\nstate-of-the-art baselines. These findings underscore the superiority of\nexplicit response shaping over implicit context injection. Moreover, RPO\nintroduces an efficient, model-agnostic personalization layer that can be\nseamlessly integrated with any underlying base model, paving the way for a new\nand effective direction in user-centric generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The personalization of black-box large language models (LLMs) is a critical\nyet challenging task. Existing approaches predominantly rely on context\ninjection, where user history is embedded into the prompt to directly guide the\ngeneration process. However, this single-step paradigm imposes a dual burden on\nthe model: generating accurate content while simultaneously aligning with\nuser-specific styles. This often results in a trade-off that compromises output\nquality and limits precise control. To address this fundamental tension, we\npropose Reflective Personalization Optimization (RPO), a novel framework that\nredefines the personalization paradigm by decoupling content generation from\nalignment. RPO operates in two distinct stages: first, a base model generates a\nhigh-quality, generic response; then, an external reflection module explicitly\nrewrites this output to align with the user's preferences. This reflection\nmodule is trained using a two-stage process. Initially, supervised fine-tuning\nis employed on structured rewriting trajectories to establish a core\npersonalized reasoning policy that models the transformation from generic to\nuser-aligned responses. Subsequently, reinforcement learning is applied to\nfurther refine and enhance the quality of the personalized outputs.\nComprehensive experiments on the LaMP benchmark demonstrate that RPO, by\ndecoupling content generation from personalization, significantly outperforms\nstate-of-the-art baselines. These findings underscore the superiority of\nexplicit response shaping over implicit context injection. Moreover, RPO\nintroduces an efficient, model-agnostic personalization layer that can be\nseamlessly integrated with any underlying base model, paving the way for a new\nand effective direction in user-centric generation scenarios."
                },
                "authors": [
                    {
                        "name": "Teqi Hao"
                    },
                    {
                        "name": "Xioayu Tan"
                    },
                    {
                        "name": "Shaojie Shi"
                    },
                    {
                        "name": "Yinghui Xu"
                    },
                    {
                        "name": "Xihe Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xihe Qiu"
                },
                "author": "Xihe Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02615v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02615v4",
                "updated": "2025-11-07T14:48:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    48,
                    44,
                    4,
                    311,
                    0
                ],
                "published": "2024-10-03T15:52:03Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    52,
                    3,
                    3,
                    277,
                    0
                ],
                "title": "ExGra-Med: Extended Context Graph Alignment for Medical Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExGra-Med: Extended Context Graph Alignment for Medical Vision-Language\n  Models"
                },
                "summary": "State-of-the-art medical multi-modal LLMs (med-MLLMs), such as LLaVA-Med and\nBioMedGPT, primarily depend on scaling model size and data volume, with\ntraining driven largely by autoregressive objectives. However, we reveal that\nthis approach can lead to weak vision-language alignment, making these models\noverly dependent on costly instruction-following data. To address this, we\nintroduce ExGra-Med, a novel multi-graph alignment framework that jointly\naligns images, instruction responses, and extended captions in the latent\nspace, advancing semantic grounding and cross-modal coherence. To scale to\nlarge LLMs (e.g., LLaMA-7B), we develop an efficient end-to-end training scheme\nusing black-box gradient estimation, enabling fast and scalable optimization.\nEmpirically, ExGra-Med matches LLaVA-Med's performance using just 10% of the\npre-training data, achieving a 20.13% gain on VQA-RAD and approaching full-data\nperformance. It also outperforms strong baselines like BioMedGPT and RadFM on\nvisual chatbot and zero-shot classification tasks, demonstrating its promise\nfor efficient, high-quality vision-language integration in medical AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art medical multi-modal LLMs (med-MLLMs), such as LLaVA-Med and\nBioMedGPT, primarily depend on scaling model size and data volume, with\ntraining driven largely by autoregressive objectives. However, we reveal that\nthis approach can lead to weak vision-language alignment, making these models\noverly dependent on costly instruction-following data. To address this, we\nintroduce ExGra-Med, a novel multi-graph alignment framework that jointly\naligns images, instruction responses, and extended captions in the latent\nspace, advancing semantic grounding and cross-modal coherence. To scale to\nlarge LLMs (e.g., LLaMA-7B), we develop an efficient end-to-end training scheme\nusing black-box gradient estimation, enabling fast and scalable optimization.\nEmpirically, ExGra-Med matches LLaVA-Med's performance using just 10% of the\npre-training data, achieving a 20.13% gain on VQA-RAD and approaching full-data\nperformance. It also outperforms strong baselines like BioMedGPT and RadFM on\nvisual chatbot and zero-shot classification tasks, demonstrating its promise\nfor efficient, high-quality vision-language integration in medical AI."
                },
                "authors": [
                    {
                        "name": "Duy M. H. Nguyen"
                    },
                    {
                        "name": "Nghiem T. Diep"
                    },
                    {
                        "name": "Trung Q. Nguyen"
                    },
                    {
                        "name": "Hoang-Bao Le"
                    },
                    {
                        "name": "Tai Nguyen"
                    },
                    {
                        "name": "Tien Nguyen"
                    },
                    {
                        "name": "TrungTin Nguyen"
                    },
                    {
                        "name": "Nhat Ho"
                    },
                    {
                        "name": "Pengtao Xie"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Daniel Sonntag"
                    },
                    {
                        "name": "Mathias Niepert"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Niepert"
                },
                "author": "Mathias Niepert",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02615v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02615v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09338v2",
                "updated": "2025-11-07T14:35:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    35,
                    34,
                    4,
                    311,
                    0
                ],
                "published": "2025-06-11T02:39:26Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    39,
                    26,
                    2,
                    162,
                    0
                ],
                "title": "Know What You Don't Know: Uncertainty Calibration of Process Reward\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know What You Don't Know: Uncertainty Calibration of Process Reward\n  Models"
                },
                "summary": "Process reward models (PRMs) play a central role in guiding inference-time\nscaling algorithms for large language models (LLMs). However, we observe that\neven state-of-the-art PRMs can be poorly calibrated. Specifically, they tend to\noverestimate the success probability that a partial reasoning step will lead to\na correct final answer, particularly when smaller LLMs are used to complete the\nreasoning trajectory. To address this, we present a calibration approach --\nperformed via quantile regression -- that adjusts PRM outputs to better align\nwith true success probabilities. Leveraging these calibrated success estimates\nand their associated confidence bounds, we introduce an \\emph{instance-adaptive\nscaling} (IAS) framework that dynamically adjusts the compute budget based on\nthe estimated likelihood that a partial reasoning trajectory will yield a\ncorrect final answer. Unlike conventional methods that allocate a fixed number\nof reasoning trajectories per query, this approach adapts to each instance and\nreasoning step when using our calibrated PRMs. Experiments on mathematical\nreasoning benchmarks show that (i) our PRM calibration method achieves small\ncalibration error, outperforming the baseline methods, (ii) calibration is\ncrucial for enabling effective IAS, and (iii) the proposed IAS strategy reduces\ninference costs while maintaining final answer accuracy, utilizing less compute\non more confident problems as desired.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process reward models (PRMs) play a central role in guiding inference-time\nscaling algorithms for large language models (LLMs). However, we observe that\neven state-of-the-art PRMs can be poorly calibrated. Specifically, they tend to\noverestimate the success probability that a partial reasoning step will lead to\na correct final answer, particularly when smaller LLMs are used to complete the\nreasoning trajectory. To address this, we present a calibration approach --\nperformed via quantile regression -- that adjusts PRM outputs to better align\nwith true success probabilities. Leveraging these calibrated success estimates\nand their associated confidence bounds, we introduce an \\emph{instance-adaptive\nscaling} (IAS) framework that dynamically adjusts the compute budget based on\nthe estimated likelihood that a partial reasoning trajectory will yield a\ncorrect final answer. Unlike conventional methods that allocate a fixed number\nof reasoning trajectories per query, this approach adapts to each instance and\nreasoning step when using our calibrated PRMs. Experiments on mathematical\nreasoning benchmarks show that (i) our PRM calibration method achieves small\ncalibration error, outperforming the baseline methods, (ii) calibration is\ncrucial for enabling effective IAS, and (iii) the proposed IAS strategy reduces\ninference costs while maintaining final answer accuracy, utilizing less compute\non more confident problems as desired."
                },
                "authors": [
                    {
                        "name": "Young-Jin Park"
                    },
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Kaveh Alim"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Navid Azizan"
                    }
                ],
                "author_detail": {
                    "name": "Navid Azizan"
                },
                "author": "Navid Azizan",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05269v1",
                "updated": "2025-11-07T14:30:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    30,
                    26,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T14:30:26Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    30,
                    26,
                    4,
                    311,
                    0
                ],
                "title": "TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities as\nautonomous agents through tool use, planning, and decision-making abilities,\nleading to their widespread adoption across diverse tasks. As task complexity\ngrows, multi-agent LLM systems are increasingly used to solve problems\ncollaboratively. However, safety and security of these systems remains largely\nunder-explored. Existing benchmarks and datasets predominantly focus on\nsingle-agent settings, failing to capture the unique vulnerabilities of\nmulti-agent dynamics and co-ordination. To address this gap, we introduce\n$\\textbf{T}$hreats and $\\textbf{A}$ttacks in $\\textbf{M}$ulti-$\\textbf{A}$gent\n$\\textbf{S}$ystems ($\\textbf{TAMAS}$), a benchmark designed to evaluate the\nrobustness and safety of multi-agent LLM systems. TAMAS includes five distinct\nscenarios comprising 300 adversarial instances across six attack types and 211\ntools, along with 100 harmless tasks. We assess system performance across ten\nbackbone LLMs and three agent interaction configurations from Autogen and\nCrewAI frameworks, highlighting critical challenges and failure modes in\ncurrent multi-agent deployments. Furthermore, we introduce Effective Robustness\nScore (ERS) to assess the tradeoff between safety and task effectiveness of\nthese frameworks. Our findings show that multi-agent systems are highly\nvulnerable to adversarial attacks, underscoring the urgent need for stronger\ndefenses. TAMAS provides a foundation for systematically studying and improving\nthe safety of multi-agent LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities as\nautonomous agents through tool use, planning, and decision-making abilities,\nleading to their widespread adoption across diverse tasks. As task complexity\ngrows, multi-agent LLM systems are increasingly used to solve problems\ncollaboratively. However, safety and security of these systems remains largely\nunder-explored. Existing benchmarks and datasets predominantly focus on\nsingle-agent settings, failing to capture the unique vulnerabilities of\nmulti-agent dynamics and co-ordination. To address this gap, we introduce\n$\\textbf{T}$hreats and $\\textbf{A}$ttacks in $\\textbf{M}$ulti-$\\textbf{A}$gent\n$\\textbf{S}$ystems ($\\textbf{TAMAS}$), a benchmark designed to evaluate the\nrobustness and safety of multi-agent LLM systems. TAMAS includes five distinct\nscenarios comprising 300 adversarial instances across six attack types and 211\ntools, along with 100 harmless tasks. We assess system performance across ten\nbackbone LLMs and three agent interaction configurations from Autogen and\nCrewAI frameworks, highlighting critical challenges and failure modes in\ncurrent multi-agent deployments. Furthermore, we introduce Effective Robustness\nScore (ERS) to assess the tradeoff between safety and task effectiveness of\nthese frameworks. Our findings show that multi-agent systems are highly\nvulnerable to adversarial attacks, underscoring the urgent need for stronger\ndefenses. TAMAS provides a foundation for systematically studying and improving\nthe safety of multi-agent LLM systems."
                },
                "authors": [
                    {
                        "name": "Ishan Kavathekar"
                    },
                    {
                        "name": "Hemang Jain"
                    },
                    {
                        "name": "Ameya Rathod"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    },
                    {
                        "name": "Tanuja Ganu"
                    }
                ],
                "author_detail": {
                    "name": "Tanuja Ganu"
                },
                "author": "Tanuja Ganu",
                "arxiv_comment": "Accepted at ICML 2025 MAS Workshop. This version includes additional\n  experiments and analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14450v2",
                "updated": "2025-11-07T14:25:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    25,
                    54,
                    4,
                    311,
                    0
                ],
                "published": "2025-02-20T11:05:10Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    11,
                    5,
                    10,
                    3,
                    51,
                    0
                ],
                "title": "LLM4FaaS: No-Code Application Development using LLMs and FaaS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4FaaS: No-Code Application Development using LLMs and FaaS"
                },
                "summary": "Large language models (LLMs) show great capabilities in generating code from\nnatural language descriptions, bringing programming power closer to\nnon-technical users. However, their lack of expertise in operating the\ngenerated code remains a key barrier to realizing customized applications.\nFunction-as-a-Service (FaaS) platforms offer a high level of abstraction for\ncode execution and deployment, allowing users to run LLM-generated code without\nrequiring technical expertise or incurring operational overhead.\n  In this paper, we present LLM4FaaS, a no-code application development\napproach that integrates LLMs and FaaS platforms to enable non-technical users\nto build and run customized applications using only natural language. By\ndeploying LLM-generated code through FaaS, LLM4FaaS abstracts away\ninfrastructure management and boilerplate code generation. We implement a\nproof-of-concept prototype based on an open-source FaaS platform, and evaluate\nit using real prompts from non-technical users. Experiments with GPT-4o show\nthat LLM4FaaS can automatically build and deploy code in 71.47% of cases,\noutperforming a non-FaaS baseline at 43.48% and an existing LLM-based platform\nat 14.55%, narrowing the gap to human performance at 88.99%. Further analysis\nof code quality, programming language diversity, latency, and consistency\ndemonstrates a balanced performance in terms of efficiency, maintainability and\navailability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show great capabilities in generating code from\nnatural language descriptions, bringing programming power closer to\nnon-technical users. However, their lack of expertise in operating the\ngenerated code remains a key barrier to realizing customized applications.\nFunction-as-a-Service (FaaS) platforms offer a high level of abstraction for\ncode execution and deployment, allowing users to run LLM-generated code without\nrequiring technical expertise or incurring operational overhead.\n  In this paper, we present LLM4FaaS, a no-code application development\napproach that integrates LLMs and FaaS platforms to enable non-technical users\nto build and run customized applications using only natural language. By\ndeploying LLM-generated code through FaaS, LLM4FaaS abstracts away\ninfrastructure management and boilerplate code generation. We implement a\nproof-of-concept prototype based on an open-source FaaS platform, and evaluate\nit using real prompts from non-technical users. Experiments with GPT-4o show\nthat LLM4FaaS can automatically build and deploy code in 71.47% of cases,\noutperforming a non-FaaS baseline at 43.48% and an existing LLM-based platform\nat 14.55%, narrowing the gap to human performance at 88.99%. Further analysis\nof code quality, programming language diversity, latency, and consistency\ndemonstrates a balanced performance in terms of efficiency, maintainability and\navailability."
                },
                "authors": [
                    {
                        "name": "Minghe Wang"
                    },
                    {
                        "name": "Tobias Pfandzelter"
                    },
                    {
                        "name": "Trever Schirmer"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "arxiv_comment": "Accepted for publication in 2025 IEEE/ACM 18th International\n  Conference on Utility and Cloud Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23262v2",
                "updated": "2025-11-07T14:09:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    9,
                    42,
                    4,
                    311,
                    0
                ],
                "published": "2025-05-29T09:11:58Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    11,
                    58,
                    3,
                    149,
                    0
                ],
                "title": "Applying Large Language Models to Travel Satisfaction Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying Large Language Models to Travel Satisfaction Analysis"
                },
                "summary": "As a specific domain of subjective well-being, travel satisfaction has\nrecently attracted much research attention. Previous studies primarily relied\non statistical models and, more recently, machine learning models to explore\nits determinants. Both approaches,however, depend on sufficiently large sample\nsizes and appropriate statistical assumptions. The emergence of Large Language\nModels (LLMs) offers a new modeling approach that can address these\nlimitations. Pre-trained on extensive datasets, LLMs have strongcapabilities in\ncontextual understanding and generalization, significantly reducing their\ndependence on task-specific data and stringent statistical assumptions. The\nmain challenge in applying LLMs lies in the behavioral misalignment between\nLLMs and humans. Using household survey data collected in Shanghai, this study\nidentifies the existence and source of misalignment, and applies a few-shot\nlearning method to address the misalignment issue. We find that the zero-shot\nLLM exhibits behavioral misalignment, leading to low prediction accuracy. With\njust a few samples, few-shot learning can align LLMs and enable them to\noutperform baseline models. Discrepancies in variable importance among machine\nlearning model, zero-shot LLM, and few-shot LLM reveal that the misalignment\narises from the gap between the general knowledge embedded in pre-trained LLMs\nand the specific, unique characteristics of the dataset. On these bases, we\npropose an LLM-based modeling approach that can be applied to model travel\nbehavior with small sample sizes. This study highlights the potential of LLMs\nfor modeling not only travel satisfaction but also broader aspects of travel\nbehavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a specific domain of subjective well-being, travel satisfaction has\nrecently attracted much research attention. Previous studies primarily relied\non statistical models and, more recently, machine learning models to explore\nits determinants. Both approaches,however, depend on sufficiently large sample\nsizes and appropriate statistical assumptions. The emergence of Large Language\nModels (LLMs) offers a new modeling approach that can address these\nlimitations. Pre-trained on extensive datasets, LLMs have strongcapabilities in\ncontextual understanding and generalization, significantly reducing their\ndependence on task-specific data and stringent statistical assumptions. The\nmain challenge in applying LLMs lies in the behavioral misalignment between\nLLMs and humans. Using household survey data collected in Shanghai, this study\nidentifies the existence and source of misalignment, and applies a few-shot\nlearning method to address the misalignment issue. We find that the zero-shot\nLLM exhibits behavioral misalignment, leading to low prediction accuracy. With\njust a few samples, few-shot learning can align LLMs and enable them to\noutperform baseline models. Discrepancies in variable importance among machine\nlearning model, zero-shot LLM, and few-shot LLM reveal that the misalignment\narises from the gap between the general knowledge embedded in pre-trained LLMs\nand the specific, unique characteristics of the dataset. On these bases, we\npropose an LLM-based modeling approach that can be applied to model travel\nbehavior with small sample sizes. This study highlights the potential of LLMs\nfor modeling not only travel satisfaction but also broader aspects of travel\nbehavior."
                },
                "authors": [
                    {
                        "name": "Pengfei Xu"
                    },
                    {
                        "name": "Donggen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donggen Wang"
                },
                "author": "Donggen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04173v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04173v4",
                "updated": "2025-11-07T14:02:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    14,
                    2,
                    33,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-05T12:26:42Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    12,
                    26,
                    42,
                    6,
                    278,
                    0
                ],
                "title": "Open Agent Specification (Agent Spec): A Unified Representation for AI\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Agent Specification (Agent Spec): A Unified Representation for AI\n  Agents"
                },
                "summary": "The proliferation of agent frameworks has led to fragmentation in how agents\nare defined, executed, and evaluated. Existing systems differ in their\nabstractions, data flow semantics, and tool integrations, making it difficult\nto share or reproduce workflows. We introduce Open Agent Specification (Agent\nSpec), a declarative language that defines AI agents and agentic workflows in a\nway that is compatible across frameworks, promoting reusability, portability\nand interoperability of AI agents. Agent Spec defines a common set of\ncomponents, control and data flow semantics, and schemas that allow an agent to\nbe defined once and executed across different runtimes. Agent Spec also\nintroduces a standardized Evaluation harness to assess agent behavior and\nagentic workflows across runtimes - analogous to how HELM and related harnesses\nstandardized LLM evaluation - so that performance, robustness, and efficiency\ncan be compared consistently across frameworks. We demonstrate this using four\ndistinct runtimes (LangGraph, CrewAI, AutoGen, and WayFlow) evaluated over\nthree different benchmarks (SimpleQA Verified, $\\tau^2$-Bench and BIRD-SQL). We\nprovide accompanying toolsets: a Python SDK (PyAgentSpec), a reference runtime\n(WayFlow), and adapters for popular frameworks (e.g., LangGraph, AutoGen,\nCrewAI). Agent Spec bridges the gap between model-centric and agent-centric\nstandardization & evaluation, laying the groundwork for reliable, reusable, and\nportable agentic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of agent frameworks has led to fragmentation in how agents\nare defined, executed, and evaluated. Existing systems differ in their\nabstractions, data flow semantics, and tool integrations, making it difficult\nto share or reproduce workflows. We introduce Open Agent Specification (Agent\nSpec), a declarative language that defines AI agents and agentic workflows in a\nway that is compatible across frameworks, promoting reusability, portability\nand interoperability of AI agents. Agent Spec defines a common set of\ncomponents, control and data flow semantics, and schemas that allow an agent to\nbe defined once and executed across different runtimes. Agent Spec also\nintroduces a standardized Evaluation harness to assess agent behavior and\nagentic workflows across runtimes - analogous to how HELM and related harnesses\nstandardized LLM evaluation - so that performance, robustness, and efficiency\ncan be compared consistently across frameworks. We demonstrate this using four\ndistinct runtimes (LangGraph, CrewAI, AutoGen, and WayFlow) evaluated over\nthree different benchmarks (SimpleQA Verified, $\\tau^2$-Bench and BIRD-SQL). We\nprovide accompanying toolsets: a Python SDK (PyAgentSpec), a reference runtime\n(WayFlow), and adapters for popular frameworks (e.g., LangGraph, AutoGen,\nCrewAI). Agent Spec bridges the gap between model-centric and agent-centric\nstandardization & evaluation, laying the groundwork for reliable, reusable, and\nportable agentic systems."
                },
                "authors": [
                    {
                        "name": "Soufiane Amini"
                    },
                    {
                        "name": "Yassine Benajiba"
                    },
                    {
                        "name": "Cesare Bernardis"
                    },
                    {
                        "name": "Paul Cayet"
                    },
                    {
                        "name": "Hassan Chafi"
                    },
                    {
                        "name": "Abderrahim Fathan"
                    },
                    {
                        "name": "Louis Faucon"
                    },
                    {
                        "name": "Damien Hilloulin"
                    },
                    {
                        "name": "Sungpack Hong"
                    },
                    {
                        "name": "Ingo Kossyk"
                    },
                    {
                        "name": "Tran Minh Son Le"
                    },
                    {
                        "name": "Rhicheek Patra"
                    },
                    {
                        "name": "Sujith Ravi"
                    },
                    {
                        "name": "Jonas Schweizer"
                    },
                    {
                        "name": "Jyotika Singh"
                    },
                    {
                        "name": "Shailender Singh"
                    },
                    {
                        "name": "Weiyi Sun"
                    },
                    {
                        "name": "Kartik Talamadupula"
                    },
                    {
                        "name": "Jerry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jerry Xu"
                },
                "author": "Jerry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04173v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04173v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03265v2",
                "updated": "2025-11-07T13:51:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    51,
                    28,
                    4,
                    311,
                    0
                ],
                "published": "2025-01-04T06:17:48Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    6,
                    17,
                    48,
                    5,
                    4,
                    0
                ],
                "title": "Cognitive Edge Computing: A Comprehensive Survey on Optimizing Large\n  Models and AI Agents for Pervasive Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Edge Computing: A Comprehensive Survey on Optimizing Large\n  Models and AI Agents for Pervasive Deployment"
                },
                "summary": "This article surveys Cognitive Edge Computing as a practical and methodical\npathway for deploying reasoning-capable Large Language Models (LLMs) and\nautonomous AI agents on resource-constrained devices at the network edge. We\npresent a unified, cognition-preserving framework spanning: (1) model\noptimization (quantization, sparsity, low-rank adaptation, distillation) aimed\nat retaining multi-step reasoning under tight memory/compute budgets; (2)\nsystem architecture (on-device inference, elastic offloading, cloud-edge\ncollaboration) that trades off latency, energy, privacy, and capacity; and (3)\nadaptive intelligence (context compression, dynamic routing, federated\npersonalization) that tailors computation to task difficulty and device\nconstraints. We synthesize advances in efficient Transformer design, multimodal\nintegration, hardware-aware compilation, privacy-preserving learning, and\nagentic tool use, and map them to edge-specific operating envelopes. We further\noutline a standardized evaluation protocol covering latency, throughput, energy\nper token, accuracy, robustness, privacy, and sustainability, with explicit\nmeasurement assumptions to enhance comparability. Remaining challenges include\nmodality-aware reasoning benchmarks, transparent and reproducible energy\nreporting, edge-oriented safety/alignment evaluation, and multi-agent testbeds.\nWe conclude with practitioner guidelines for cross-layer co-design of\nalgorithms, runtime, and hardware to deliver reliable, efficient, and\nprivacy-preserving cognitive capabilities on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article surveys Cognitive Edge Computing as a practical and methodical\npathway for deploying reasoning-capable Large Language Models (LLMs) and\nautonomous AI agents on resource-constrained devices at the network edge. We\npresent a unified, cognition-preserving framework spanning: (1) model\noptimization (quantization, sparsity, low-rank adaptation, distillation) aimed\nat retaining multi-step reasoning under tight memory/compute budgets; (2)\nsystem architecture (on-device inference, elastic offloading, cloud-edge\ncollaboration) that trades off latency, energy, privacy, and capacity; and (3)\nadaptive intelligence (context compression, dynamic routing, federated\npersonalization) that tailors computation to task difficulty and device\nconstraints. We synthesize advances in efficient Transformer design, multimodal\nintegration, hardware-aware compilation, privacy-preserving learning, and\nagentic tool use, and map them to edge-specific operating envelopes. We further\noutline a standardized evaluation protocol covering latency, throughput, energy\nper token, accuracy, robustness, privacy, and sustainability, with explicit\nmeasurement assumptions to enhance comparability. Remaining challenges include\nmodality-aware reasoning benchmarks, transparent and reproducible energy\nreporting, edge-oriented safety/alignment evaluation, and multi-agent testbeds.\nWe conclude with practitioner guidelines for cross-layer co-design of\nalgorithms, runtime, and hardware to deliver reliable, efficient, and\nprivacy-preserving cognitive capabilities on edge devices."
                },
                "authors": [
                    {
                        "name": "Xubin Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05239v1",
                "updated": "2025-11-07T13:46:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    46,
                    16,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T13:46:16Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    46,
                    16,
                    4,
                    311,
                    0
                ],
                "title": "Translation via Annotation: A Computational Study of Translating\n  Classical Chinese into Japanese",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translation via Annotation: A Computational Study of Translating\n  Classical Chinese into Japanese"
                },
                "summary": "Ancient people translated classical Chinese into Japanese by annotating\naround each character. We abstract this process as sequence tagging tasks and\nfit them into modern language technologies. The research of this annotation and\ntranslation system is a facing low-resource problem. We release this problem by\nintroducing a LLM-based annotation pipeline and construct a new dataset from\ndigitalized open-source translation data. We show that under the low-resource\nsetting, introducing auxiliary Chinese NLP tasks has a promoting effect on the\ntraining of sequence tagging tasks. We also evaluate the performance of large\nlanguage models. They achieve high scores in direct machine translation, but\nthey are confused when being asked to annotate characters. Our method could\nwork as a supplement of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ancient people translated classical Chinese into Japanese by annotating\naround each character. We abstract this process as sequence tagging tasks and\nfit them into modern language technologies. The research of this annotation and\ntranslation system is a facing low-resource problem. We release this problem by\nintroducing a LLM-based annotation pipeline and construct a new dataset from\ndigitalized open-source translation data. We show that under the low-resource\nsetting, introducing auxiliary Chinese NLP tasks has a promoting effect on the\ntraining of sequence tagging tasks. We also evaluate the performance of large\nlanguage models. They achieve high scores in direct machine translation, but\nthey are confused when being asked to annotate characters. Our method could\nwork as a supplement of LLMs."
                },
                "authors": [
                    {
                        "name": "Zilong Li"
                    },
                    {
                        "name": "Jie Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jie Cao"
                },
                "author": "Jie Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18116v2",
                "updated": "2025-11-07T13:28:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    28,
                    17,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-10T07:03:35Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    3,
                    35,
                    2,
                    253,
                    0
                ],
                "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortized Latent Steering: Low-Cost Alternative to Test-Time\n  Optimization"
                },
                "summary": "Test-time optimization remains impractical at scale due to prohibitive\ninference costs--techniques like iterative refinement and multi-step\nverification can require $10-100\\times$ more compute per query than standard\ndecoding. Latent space test-time optimization methods like LatentSeek offer a\nmore direct approach by steering hidden representations, but still demand\nexpensive per-query optimization loops with multiple backward passes. We\npropose Amortized Latent Steering (ALS), which collapses this iterative\noptimization into a single offline-computed vector applied at constant cost\nduring inference. ALS computes the mean difference between hidden states from\nsuccessful versus unsuccessful generations, then uses this direction to\ncalibrate the model's hidden representations: when decoding drifts away from\nthe success manifold, ALS nudges activations back toward it. Across GSM8K and\nMATH-500 benchmarks, ALS achieves $2-5\\times$ speedup over iterative methods\nwhile matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency\nbaselines, yielding up to 101% improvement in efficiency--accuracy trade-off.\nThese results show that much of latent optimization's benefit can be captured\noffline, making sophisticated reasoning techniques viable for production\ndeployment. Code is available at https://github.com/negbuna/ALS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time optimization remains impractical at scale due to prohibitive\ninference costs--techniques like iterative refinement and multi-step\nverification can require $10-100\\times$ more compute per query than standard\ndecoding. Latent space test-time optimization methods like LatentSeek offer a\nmore direct approach by steering hidden representations, but still demand\nexpensive per-query optimization loops with multiple backward passes. We\npropose Amortized Latent Steering (ALS), which collapses this iterative\noptimization into a single offline-computed vector applied at constant cost\nduring inference. ALS computes the mean difference between hidden states from\nsuccessful versus unsuccessful generations, then uses this direction to\ncalibrate the model's hidden representations: when decoding drifts away from\nthe success manifold, ALS nudges activations back toward it. Across GSM8K and\nMATH-500 benchmarks, ALS achieves $2-5\\times$ speedup over iterative methods\nwhile matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency\nbaselines, yielding up to 101% improvement in efficiency--accuracy trade-off.\nThese results show that much of latent optimization's benefit can be captured\noffline, making sophisticated reasoning techniques viable for production\ndeployment. Code is available at https://github.com/negbuna/ALS."
                },
                "authors": [
                    {
                        "name": "Nathan Egbuna"
                    },
                    {
                        "name": "Saatvik Gaur"
                    },
                    {
                        "name": "Sunishchal Dev"
                    },
                    {
                        "name": "Ashwinee Panda"
                    },
                    {
                        "name": "Maheep Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Maheep Chaudhary"
                },
                "author": "Maheep Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17086v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17086v4",
                "updated": "2025-11-07T13:27:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    27,
                    39,
                    4,
                    311,
                    0
                ],
                "published": "2025-02-24T12:05:27Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    5,
                    27,
                    0,
                    55,
                    0
                ],
                "title": "Mind the Blind Spots: A Focus-Level Evaluation Framework for LLM Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Blind Spots: A Focus-Level Evaluation Framework for LLM Reviews"
                },
                "summary": "Peer review underpins scientific progress, but it is increasingly strained by\nreviewer shortages and growing workloads. Large Language Models (LLMs) can\nautomatically draft reviews now, but determining whether LLM-generated reviews\nare trustworthy requires systematic evaluation. Researchers have evaluated LLM\nreviews at either surface-level (e.g., BLEU and ROUGE) or content-level (e.g.,\nspecificity and factual accuracy). Yet it remains uncertain whether\nLLM-generated reviews attend to the same critical facets that human experts\nweigh -- the strengths and weaknesses that ultimately drive an accept-or-reject\ndecision. We introduce a focus-level evaluation framework that operationalizes\nthe focus as a normalized distribution of attention across predefined facets in\npaper reviews. Based on the framework, we developed an automatic focus-level\nevaluation pipeline based on two sets of facets: target (e.g., problem, method,\nand experiment) and aspect (e.g., validity, clarity, and novelty), leveraging\n676 paper reviews (https://figshare.com/s/d5adf26c802527dd0f62) from OpenReview\nthat consists of 3,657 strengths and weaknesses identified from human experts.\nThe comparison of focus distributions between LLMs and human experts showed\nthat the off-the-shelf LLMs consistently have a more biased focus towards\nexamining technical validity while significantly overlooking novelty assessment\nwhen criticizing papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review underpins scientific progress, but it is increasingly strained by\nreviewer shortages and growing workloads. Large Language Models (LLMs) can\nautomatically draft reviews now, but determining whether LLM-generated reviews\nare trustworthy requires systematic evaluation. Researchers have evaluated LLM\nreviews at either surface-level (e.g., BLEU and ROUGE) or content-level (e.g.,\nspecificity and factual accuracy). Yet it remains uncertain whether\nLLM-generated reviews attend to the same critical facets that human experts\nweigh -- the strengths and weaknesses that ultimately drive an accept-or-reject\ndecision. We introduce a focus-level evaluation framework that operationalizes\nthe focus as a normalized distribution of attention across predefined facets in\npaper reviews. Based on the framework, we developed an automatic focus-level\nevaluation pipeline based on two sets of facets: target (e.g., problem, method,\nand experiment) and aspect (e.g., validity, clarity, and novelty), leveraging\n676 paper reviews (https://figshare.com/s/d5adf26c802527dd0f62) from OpenReview\nthat consists of 3,657 strengths and weaknesses identified from human experts.\nThe comparison of focus distributions between LLMs and human experts showed\nthat the off-the-shelf LLMs consistently have a more biased focus towards\nexamining technical validity while significantly overlooking novelty assessment\nwhen criticizing papers."
                },
                "authors": [
                    {
                        "name": "Hyungyu Shin"
                    },
                    {
                        "name": "Jingyu Tang"
                    },
                    {
                        "name": "Yoonjoo Lee"
                    },
                    {
                        "name": "Nayoung Kim"
                    },
                    {
                        "name": "Hyunseung Lim"
                    },
                    {
                        "name": "Ji Yong Cho"
                    },
                    {
                        "name": "Hwajung Hong"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "arxiv_comment": "EMNLP 2025 Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17086v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17086v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05221v1",
                "updated": "2025-11-07T13:18:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    18,
                    20,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T13:18:20Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    18,
                    20,
                    4,
                    311,
                    0
                ],
                "title": "ActiTect: A Generalizable Machine Learning Pipeline for REM Sleep\n  Behavior Disorder Screening through Standardized Actigraphy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ActiTect: A Generalizable Machine Learning Pipeline for REM Sleep\n  Behavior Disorder Screening through Standardized Actigraphy"
                },
                "summary": "Isolated rapid eye movement sleep behavior disorder (iRBD) is a major\nprodromal marker of $\\alpha$-synucleinopathies, often preceding the clinical\nonset of Parkinson's disease, dementia with Lewy bodies, or multiple system\natrophy. While wrist-worn actimeters hold significant potential for detecting\nRBD in large-scale screening efforts by capturing abnormal nocturnal movements,\nthey become inoperable without a reliable and efficient analysis pipeline. This\nstudy presents ActiTect, a fully automated, open-source machine learning tool\nto identify RBD from actigraphy recordings. To ensure generalizability across\nheterogeneous acquisition settings, our pipeline includes robust preprocessing\nand automated sleep-wake detection to harmonize multi-device data and extract\nphysiologically interpretable motion features characterizing activity patterns.\nModel development was conducted on a cohort of 78 individuals, yielding strong\ndiscrimination under nested cross-validation (AUROC = 0.95). Generalization was\nconfirmed on a blinded local test set (n = 31, AUROC = 0.86) and on two\nindependent external cohorts (n = 113, AUROC = 0.84; n = 57, AUROC = 0.94). To\nassess real-world robustness, leave-one-dataset-out cross-validation across the\ninternal and external cohorts demonstrated consistent performance (AUROC range\n= 0.84-0.89). A complementary stability analysis showed that key predictive\nfeatures remained reproducible across datasets, supporting the final pooled\nmulti-center model as a robust pre-trained resource for broader deployment. By\nbeing open-source and easy to use, our tool promotes widespread adoption and\nfacilitates independent validation and collaborative improvements, thereby\nadvancing the field toward a unified and generalizable RBD detection model\nusing wearable devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isolated rapid eye movement sleep behavior disorder (iRBD) is a major\nprodromal marker of $\\alpha$-synucleinopathies, often preceding the clinical\nonset of Parkinson's disease, dementia with Lewy bodies, or multiple system\natrophy. While wrist-worn actimeters hold significant potential for detecting\nRBD in large-scale screening efforts by capturing abnormal nocturnal movements,\nthey become inoperable without a reliable and efficient analysis pipeline. This\nstudy presents ActiTect, a fully automated, open-source machine learning tool\nto identify RBD from actigraphy recordings. To ensure generalizability across\nheterogeneous acquisition settings, our pipeline includes robust preprocessing\nand automated sleep-wake detection to harmonize multi-device data and extract\nphysiologically interpretable motion features characterizing activity patterns.\nModel development was conducted on a cohort of 78 individuals, yielding strong\ndiscrimination under nested cross-validation (AUROC = 0.95). Generalization was\nconfirmed on a blinded local test set (n = 31, AUROC = 0.86) and on two\nindependent external cohorts (n = 113, AUROC = 0.84; n = 57, AUROC = 0.94). To\nassess real-world robustness, leave-one-dataset-out cross-validation across the\ninternal and external cohorts demonstrated consistent performance (AUROC range\n= 0.84-0.89). A complementary stability analysis showed that key predictive\nfeatures remained reproducible across datasets, supporting the final pooled\nmulti-center model as a robust pre-trained resource for broader deployment. By\nbeing open-source and easy to use, our tool promotes widespread adoption and\nfacilitates independent validation and collaborative improvements, thereby\nadvancing the field toward a unified and generalizable RBD detection model\nusing wearable devices."
                },
                "authors": [
                    {
                        "name": "David Bertram"
                    },
                    {
                        "name": "Anja Ophey"
                    },
                    {
                        "name": "Sinah Röttgen"
                    },
                    {
                        "name": "Konstantin Kuffer"
                    },
                    {
                        "name": "Gereon R. Fink"
                    },
                    {
                        "name": "Elke Kalbe"
                    },
                    {
                        "name": "Clint Hansen"
                    },
                    {
                        "name": "Walter Maetzler"
                    },
                    {
                        "name": "Maximilian Kapsecker"
                    },
                    {
                        "name": "Lara M. Reimer"
                    },
                    {
                        "name": "Stephan Jonas"
                    },
                    {
                        "name": "Andreas T. Damgaard"
                    },
                    {
                        "name": "Natasha B. Bertelsen"
                    },
                    {
                        "name": "Casper Skjaerbaek"
                    },
                    {
                        "name": "Per Borghammer"
                    },
                    {
                        "name": "Karolien Groenewald"
                    },
                    {
                        "name": "Pietro-Luca Ratti"
                    },
                    {
                        "name": "Michele T. Hu"
                    },
                    {
                        "name": "Noémie Moreau"
                    },
                    {
                        "name": "Michael Sommerauer"
                    },
                    {
                        "name": "Katarzyna Bozek"
                    }
                ],
                "author_detail": {
                    "name": "Katarzyna Bozek"
                },
                "author": "Katarzyna Bozek",
                "arxiv_comment": "30 pages including supplement, 4 core figures, 1 supplement figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13142v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13142v3",
                "updated": "2025-11-07T13:12:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    12,
                    3,
                    4,
                    311,
                    0
                ],
                "published": "2025-08-18T17:55:17Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    17,
                    55,
                    17,
                    0,
                    230,
                    0
                ],
                "title": "Holistic Evaluation of Multimodal LLMs on Spatial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holistic Evaluation of Multimodal LLMs on Spatial Intelligence"
                },
                "summary": "Multimodal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, the very capability that anchors artificial\ngeneral intelligence in the physical world. With the recent release of GPT-5,\nallegedly the most powerful AI model to date, it is timely to examine where the\nleading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path\ntoward spatial intelligence. We thus propose EASI for holistic Evaluation of\nmultimodAl LLMs on Spatial Intelligence. EASI conceptualizes a comprehensive\ntaxonomy of spatial tasks that unifies existing benchmarks and a standardized\nprotocol for the fair evaluation of state-of-the-art proprietary and\nopen-source models. In this report, we conduct the study across eight key\nbenchmarks, at a cost exceeding ten billion total tokens. Our empirical study\nthen reveals that (1) GPT-5 demonstrates unprecedented strength in spatial\nintelligence (SI), yet (2) still falls short of human performance significantly\nacross a broad spectrum of SI-tasks. Moreover, we (3) show that SI-tasks expose\ngreater model capability deficiency than non-SI tasks, to the extent that (4)\nproprietary models do not exhibit a decisive advantage when facing the most\ndifficult ones. In addition, we conduct a qualitative evaluation across a\ndiverse set of scenarios that are intuitive for humans, yet fail even the most\nadvanced multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, the very capability that anchors artificial\ngeneral intelligence in the physical world. With the recent release of GPT-5,\nallegedly the most powerful AI model to date, it is timely to examine where the\nleading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path\ntoward spatial intelligence. We thus propose EASI for holistic Evaluation of\nmultimodAl LLMs on Spatial Intelligence. EASI conceptualizes a comprehensive\ntaxonomy of spatial tasks that unifies existing benchmarks and a standardized\nprotocol for the fair evaluation of state-of-the-art proprietary and\nopen-source models. In this report, we conduct the study across eight key\nbenchmarks, at a cost exceeding ten billion total tokens. Our empirical study\nthen reveals that (1) GPT-5 demonstrates unprecedented strength in spatial\nintelligence (SI), yet (2) still falls short of human performance significantly\nacross a broad spectrum of SI-tasks. Moreover, we (3) show that SI-tasks expose\ngreater model capability deficiency than non-SI tasks, to the extent that (4)\nproprietary models do not exhibit a decisive advantage when facing the most\ndifficult ones. In addition, we conduct a qualitative evaluation across a\ndiverse set of scenarios that are intuitive for humans, yet fail even the most\nadvanced multimodal models."
                },
                "authors": [
                    {
                        "name": "Zhongang Cai"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Qingping Sun"
                    },
                    {
                        "name": "Ruisi Wang"
                    },
                    {
                        "name": "Chenyang Gu"
                    },
                    {
                        "name": "Wanqi Yin"
                    },
                    {
                        "name": "Zhiqian Lin"
                    },
                    {
                        "name": "Zhitao Yang"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Oscar Qian"
                    },
                    {
                        "name": "Hui En Pang"
                    },
                    {
                        "name": "Xuanke Shi"
                    },
                    {
                        "name": "Kewang Deng"
                    },
                    {
                        "name": "Xiaoyang Han"
                    },
                    {
                        "name": "Zukai Chen"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Xiangyu Fan"
                    },
                    {
                        "name": "Hanming Deng"
                    },
                    {
                        "name": "Lewei Lu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Quan Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Yang"
                },
                "author": "Lei Yang",
                "arxiv_comment": "Codebase: https://github.com/EvolvingLMMs-Lab/EASI/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13142v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13142v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05721v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05721v2",
                "updated": "2025-11-07T13:05:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    5,
                    57,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-06T14:03:28Z",
                "published_parsed": [
                    2025,
                    9,
                    6,
                    14,
                    3,
                    28,
                    5,
                    249,
                    0
                ],
                "title": "A Composable Agentic System for Automated Visual Data Reporting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Composable Agentic System for Automated Visual Data Reporting"
                },
                "summary": "To address the brittleness of monolithic AI agents, our prototype for\nautomated visual data reporting explores a Human-AI Partnership model. Its\nhybrid, multi-agent architecture strategically externalizes logic from LLMs to\ndeterministic modules, leveraging the rule-based system Draco for principled\nvisualization design. The system delivers a dual-output: an interactive\nObservable report with Mosaic for reader exploration, and executable Marimo\nnotebooks for deep, analyst-facing traceability. This granular architecture\nyields a fully automatic yet auditable and steerable system, charting a path\ntoward a more synergistic partnership between human experts and AI. For\nreproducibility, our implementation and examples are available at\nhttps://peter-gy.github.io/VISxGenAI-2025/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the brittleness of monolithic AI agents, our prototype for\nautomated visual data reporting explores a Human-AI Partnership model. Its\nhybrid, multi-agent architecture strategically externalizes logic from LLMs to\ndeterministic modules, leveraging the rule-based system Draco for principled\nvisualization design. The system delivers a dual-output: an interactive\nObservable report with Mosaic for reader exploration, and executable Marimo\nnotebooks for deep, analyst-facing traceability. This granular architecture\nyields a fully automatic yet auditable and steerable system, charting a path\ntoward a more synergistic partnership between human experts and AI. For\nreproducibility, our implementation and examples are available at\nhttps://peter-gy.github.io/VISxGenAI-2025/."
                },
                "authors": [
                    {
                        "name": "Péter Ferenc Gyarmati"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Torsten Möller"
                    },
                    {
                        "name": "Laura Koesten"
                    }
                ],
                "author_detail": {
                    "name": "Laura Koesten"
                },
                "author": "Laura Koesten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05721v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15428v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15428v2",
                "updated": "2025-11-07T13:04:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    13,
                    4,
                    52,
                    4,
                    311,
                    0
                ],
                "published": "2025-02-21T12:54:35Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    35,
                    4,
                    52,
                    0
                ],
                "title": "OptiLog: Assigning Roles in Byzantine Consensus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptiLog: Assigning Roles in Byzantine Consensus"
                },
                "summary": "Byzantine Fault-Tolerant (BFT) protocols play an important role in\nblockchains. As the deployment of such systems extends to wide-area networks,\nthe scalability of BFT protocols becomes a critical concern. Optimizations that\nassign specific roles to individual replicas can significantly improve the\nperformance of BFT systems. However, such role assignment is highly sensitive\nto faults, potentially undermining the optimizations' effectiveness. To address\nthese challenges, we present OptiLog, a logging framework for collecting and\nanalyzing measurements that help to assign roles in globally distributed\nsystems, despite the presence of faults. OptiLog presents local measurements in\nglobal data structures, to enable consistent decisions and hold replicas\naccountable if they do not perform according to their reported measurements. We\ndemonstrate OptiLog's flexibility by applying it to two BFT protocols: (1)\nAware, a highly optimized PBFT-like protocol, and (2) Kauri, a tree-based\nprotocol designed for large-scale deployments. OptiLog detects and excludes\nreplicas that misbehave during consensus and thus enables the system to operate\nin an optimized, low-latency configuration, even under adverse conditions.\nExperiments show that for tree overlays deployed across 73 worldwide cities,\ntrees found by OptiLog display 39% lower latency than Kauri.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Byzantine Fault-Tolerant (BFT) protocols play an important role in\nblockchains. As the deployment of such systems extends to wide-area networks,\nthe scalability of BFT protocols becomes a critical concern. Optimizations that\nassign specific roles to individual replicas can significantly improve the\nperformance of BFT systems. However, such role assignment is highly sensitive\nto faults, potentially undermining the optimizations' effectiveness. To address\nthese challenges, we present OptiLog, a logging framework for collecting and\nanalyzing measurements that help to assign roles in globally distributed\nsystems, despite the presence of faults. OptiLog presents local measurements in\nglobal data structures, to enable consistent decisions and hold replicas\naccountable if they do not perform according to their reported measurements. We\ndemonstrate OptiLog's flexibility by applying it to two BFT protocols: (1)\nAware, a highly optimized PBFT-like protocol, and (2) Kauri, a tree-based\nprotocol designed for large-scale deployments. OptiLog detects and excludes\nreplicas that misbehave during consensus and thus enables the system to operate\nin an optimized, low-latency configuration, even under adverse conditions.\nExperiments show that for tree overlays deployed across 73 worldwide cities,\ntrees found by OptiLog display 39% lower latency than Kauri."
                },
                "authors": [
                    {
                        "name": "Hanish Gogada"
                    },
                    {
                        "name": "Christian Berger"
                    },
                    {
                        "name": "Leander Jehl"
                    },
                    {
                        "name": "Hans P. Reiser"
                    },
                    {
                        "name": "Hein Meling"
                    }
                ],
                "author_detail": {
                    "name": "Hein Meling"
                },
                "author": "Hein Meling",
                "arxiv_doi": "10.1145/3767295.3769342",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3767295.3769342",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.15428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15428v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, accepted to appear at EuroSys 2026 conference. This work is\n  licensed under a Creative Commons Attribution 4.0 International License",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05204v1",
                "updated": "2025-11-07T12:43:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    12,
                    43,
                    54,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T12:43:54Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    12,
                    43,
                    54,
                    4,
                    311,
                    0
                ],
                "title": "Millimeter-Scale Absolute Carrier Phase-Based Localization in Multi-Band\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millimeter-Scale Absolute Carrier Phase-Based Localization in Multi-Band\n  Systems"
                },
                "summary": "Localization is a key feature of future Sixth Generation (6G) net-works with\nforeseen accuracy requirements down to the millimeter level, to enable novel\napplications in the fields of telesurgery, high-precision manufacturing, and\nothers. Currently, such accuracy requirements are only achievable with\nspecialized or highly resource-demanding systems, rendering them impractical\nfor more wide-spread deployment. In this paper, we present the first system\nthat enables low-complexity and low-bandwidth absolute 3D localization with\nmillimeter-level accuracy in generic wireless networks. It performs a carrier\nphase-based wireless localization refinement of an initial location estimate\nbased on successive location-likelihood optimization across multiple bands.\nUnlike previous phase unwrapping methods, our solution is one-shot. We evaluate\nits performance collecting ~350, 000 measurements, showing an improvement of\nmore than one order of magnitude over classical localization techniques.\nFinally, we will open-source the low-cost, modular FR3 front-end that we\ndeveloped for the experimental campaign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localization is a key feature of future Sixth Generation (6G) net-works with\nforeseen accuracy requirements down to the millimeter level, to enable novel\napplications in the fields of telesurgery, high-precision manufacturing, and\nothers. Currently, such accuracy requirements are only achievable with\nspecialized or highly resource-demanding systems, rendering them impractical\nfor more wide-spread deployment. In this paper, we present the first system\nthat enables low-complexity and low-bandwidth absolute 3D localization with\nmillimeter-level accuracy in generic wireless networks. It performs a carrier\nphase-based wireless localization refinement of an initial location estimate\nbased on successive location-likelihood optimization across multiple bands.\nUnlike previous phase unwrapping methods, our solution is one-shot. We evaluate\nits performance collecting ~350, 000 measurements, showing an improvement of\nmore than one order of magnitude over classical localization techniques.\nFinally, we will open-source the low-cost, modular FR3 front-end that we\ndeveloped for the experimental campaign."
                },
                "authors": [
                    {
                        "name": "Andrea Bedin"
                    },
                    {
                        "name": "Joerg Widmer"
                    },
                    {
                        "name": "Melanny Davila"
                    },
                    {
                        "name": "Marco Canil"
                    },
                    {
                        "name": "Rafael Ruiz"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Ruiz"
                },
                "author": "Rafael Ruiz",
                "arxiv_comment": "14 pages, 22 figures, Accepted for publication at SenSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05185v1",
                "updated": "2025-11-07T12:06:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    12,
                    6,
                    21,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T12:06:21Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    12,
                    6,
                    21,
                    4,
                    311,
                    0
                ],
                "title": "Procedimiento de auditoría de ciberseguridad para sistemas\n  autónomos: metodología, amenazas y mitigaciones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedimiento de auditoría de ciberseguridad para sistemas\n  autónomos: metodología, amenazas y mitigaciones"
                },
                "summary": "The deployment of autonomous systems has experienced remarkable growth in\nrecent years, driven by their integration into sectors such as industry,\nmedicine, logistics, and domestic environments. This expansion is accompanied\nby a series of security issues that entail significant risks due to the\ncritical nature of autonomous systems, especially those operating in\nhuman-interaction environments. Furthermore, technological advancement and the\nhigh operational and architectural complexity of autonomous systems have\nresulted in an increased attack surface. This article presents a specific\nsecurity auditing procedure for autonomous systems, based on a layer-structured\nmethodology, a threat taxonomy adapted to the robotic context, and a set of\nconcrete mitigation measures. The validity of the proposed approach is\ndemonstrated through four practical case studies applied to representative\nrobotic platforms: the Vision 60 military quadruped from Ghost Robotics, the A1\nrobot from Unitree Robotics, the UR3 collaborative arm from Universal Robots,\nand the Pepper social robot from Aldebaran Robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of autonomous systems has experienced remarkable growth in\nrecent years, driven by their integration into sectors such as industry,\nmedicine, logistics, and domestic environments. This expansion is accompanied\nby a series of security issues that entail significant risks due to the\ncritical nature of autonomous systems, especially those operating in\nhuman-interaction environments. Furthermore, technological advancement and the\nhigh operational and architectural complexity of autonomous systems have\nresulted in an increased attack surface. This article presents a specific\nsecurity auditing procedure for autonomous systems, based on a layer-structured\nmethodology, a threat taxonomy adapted to the robotic context, and a set of\nconcrete mitigation measures. The validity of the proposed approach is\ndemonstrated through four practical case studies applied to representative\nrobotic platforms: the Vision 60 military quadruped from Ghost Robotics, the A1\nrobot from Unitree Robotics, the UR3 collaborative arm from Universal Robots,\nand the Pepper social robot from Aldebaran Robotics."
                },
                "authors": [
                    {
                        "name": "Adrián Campazas-Vega"
                    },
                    {
                        "name": "Claudia Álvarez-Aparicio"
                    },
                    {
                        "name": "David Sobrín-Hidalgo"
                    },
                    {
                        "name": "Laura Inyesto-Alonso"
                    },
                    {
                        "name": "Francisco Javier Rodríguez-Lera"
                    },
                    {
                        "name": "Vicente Matellán-Olivera"
                    },
                    {
                        "name": "Ángel Manuel Guerrero-Higueras"
                    }
                ],
                "author_detail": {
                    "name": "Ángel Manuel Guerrero-Higueras"
                },
                "author": "Ángel Manuel Guerrero-Higueras",
                "arxiv_comment": "32 pages, in Spanish language, 7 tables, 12 Figures. White paper\n  under the TESCAC project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05184v1",
                "updated": "2025-11-07T12:05:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    12,
                    5,
                    39,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T12:05:39Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    12,
                    5,
                    39,
                    4,
                    311,
                    0
                ],
                "title": "Effectiveness of Chain-of-Thought in Distilling Reasoning Capability\n  from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectiveness of Chain-of-Thought in Distilling Reasoning Capability\n  from Large Language Models"
                },
                "summary": "Chain-of-Thought (CoT) prompting is a widely used method to improve the\nreasoning capability of Large Language Models (LLMs). More recently, CoT has\nbeen leveraged in Knowledge Distillation (KD) to transfer reasoning capability\nfrom a larger LLM to a smaller one. This paper examines the role of CoT in\ndistilling the reasoning capability from larger LLMs to smaller LLMs using\nwhite-box KD, analysing its effectiveness in improving the performance of the\ndistilled models for various natural language reasoning and understanding\ntasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2\nfamilies, employing CoT data from the CoT-Collection dataset. The distilled\nmodels are then evaluated on natural language reasoning and understanding tasks\nfrom the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for\nsmaller LLMs. Experimental results demonstrate the role of CoT in improving\nwhite-box KD effectiveness, enabling the distilled models to achieve better\naverage performance in natural language reasoning and understanding tasks from\nBBH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting is a widely used method to improve the\nreasoning capability of Large Language Models (LLMs). More recently, CoT has\nbeen leveraged in Knowledge Distillation (KD) to transfer reasoning capability\nfrom a larger LLM to a smaller one. This paper examines the role of CoT in\ndistilling the reasoning capability from larger LLMs to smaller LLMs using\nwhite-box KD, analysing its effectiveness in improving the performance of the\ndistilled models for various natural language reasoning and understanding\ntasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2\nfamilies, employing CoT data from the CoT-Collection dataset. The distilled\nmodels are then evaluated on natural language reasoning and understanding tasks\nfrom the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for\nsmaller LLMs. Experimental results demonstrate the role of CoT in improving\nwhite-box KD effectiveness, enabling the distilled models to achieve better\naverage performance in natural language reasoning and understanding tasks from\nBBH."
                },
                "authors": [
                    {
                        "name": "Cong-Thanh Do"
                    },
                    {
                        "name": "Rama Doddipatla"
                    },
                    {
                        "name": "Kate Knill"
                    }
                ],
                "author_detail": {
                    "name": "Kate Knill"
                },
                "author": "Kate Knill",
                "arxiv_comment": "In proceedings of the 18th International Natural Language Generation\n  Conference (INLG 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18156v2",
                "updated": "2025-11-07T11:59:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    59,
                    6,
                    4,
                    311,
                    0
                ],
                "published": "2025-06-22T19:58:19Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    19,
                    58,
                    19,
                    6,
                    173,
                    0
                ],
                "title": "AI Through the Human Lens: Investigating Cognitive Theories in Machine\n  Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Through the Human Lens: Investigating Cognitive Theories in Machine\n  Psychology"
                },
                "summary": "We investigate whether Large Language Models (LLMs) exhibit human-like\ncognitive patterns under four established frameworks from psychology: Thematic\nApperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and\nCognitive Dissonance. We evaluated several proprietary and open-source models\nusing structured prompts and automated scoring. Our findings reveal that these\nmodels often produce coherent narratives, show susceptibility to positive\nframing, exhibit moral judgments aligned with Liberty/Oppression concerns, and\ndemonstrate self-contradictions tempered by extensive rationalization. Such\nbehaviors mirror human cognitive tendencies yet are shaped by their training\ndata and alignment methods. We discuss the implications for AI transparency,\nethical deployment, and future work that bridges cognitive psychology and AI\nsafety",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether Large Language Models (LLMs) exhibit human-like\ncognitive patterns under four established frameworks from psychology: Thematic\nApperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and\nCognitive Dissonance. We evaluated several proprietary and open-source models\nusing structured prompts and automated scoring. Our findings reveal that these\nmodels often produce coherent narratives, show susceptibility to positive\nframing, exhibit moral judgments aligned with Liberty/Oppression concerns, and\ndemonstrate self-contradictions tempered by extensive rationalization. Such\nbehaviors mirror human cognitive tendencies yet are shaped by their training\ndata and alignment methods. We discuss the implications for AI transparency,\nethical deployment, and future work that bridges cognitive psychology and AI\nsafety"
                },
                "authors": [
                    {
                        "name": "Akash Kundu"
                    },
                    {
                        "name": "Rishika Goswami"
                    }
                ],
                "author_detail": {
                    "name": "Rishika Goswami"
                },
                "author": "Rishika Goswami",
                "arxiv_comment": "Accepted to IJCNLP-AACL 2025 Student Research Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05179v1",
                "updated": "2025-11-07T11:50:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    50,
                    39,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T11:50:39Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    50,
                    39,
                    4,
                    311,
                    0
                ],
                "title": "No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs\n  with Graph Neural Networks and Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs\n  with Graph Neural Networks and Foundation Models"
                },
                "summary": "Modern IoT deployments for environmental sensing produce high volume\nspatiotemporal data to support downstream tasks such as forecasting, typically\npowered by machine learning models. While existing filtering and strategic\ndeployment techniques optimize collected data volume at the edge, they overlook\nhow variations in sampling frequencies and spatial coverage affect downstream\nmodel performance. In many forecasting models, incorporating data from\nadditional sensors denoise predictions by providing broader spatial contexts.\nThis interplay between sampling frequency, spatial coverage and different\nforecasting model architectures remain underexplored. This work presents a\nsystematic study of forecasting models - classical models (VAR), neural\nnetworks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs),\nand time series foundation models (TSFMs: Chronos Moirai, TimesFM) under\nvarying spatial sensor nodes density and sampling intervals using real-world\ntemperature data in a wireless sensor network. Our results show that STGNNs are\neffective when sensor deployments are sparse and sampling rate is moderate,\nleveraging spatial correlations via encoded graph structure to compensate for\nlimited coverage. In contrast, TSFMs perform competitively at high frequencies\nbut degrade when spatial coverage from neighboring sensors is reduced.\nCrucially, the multivariate TSFM Moirai outperforms all models by natively\nlearning cross-sensor dependencies. These findings offer actionable insights\nfor building efficient forecasting pipelines in spatio-temporal systems. All\ncode for model configurations, training, dataset, and logs are open-sourced for\nreproducibility:\nhttps://github.com/UIUC-MONET-Projects/Benchmarking-Spatiotemporal-Forecast-Models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern IoT deployments for environmental sensing produce high volume\nspatiotemporal data to support downstream tasks such as forecasting, typically\npowered by machine learning models. While existing filtering and strategic\ndeployment techniques optimize collected data volume at the edge, they overlook\nhow variations in sampling frequencies and spatial coverage affect downstream\nmodel performance. In many forecasting models, incorporating data from\nadditional sensors denoise predictions by providing broader spatial contexts.\nThis interplay between sampling frequency, spatial coverage and different\nforecasting model architectures remain underexplored. This work presents a\nsystematic study of forecasting models - classical models (VAR), neural\nnetworks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs),\nand time series foundation models (TSFMs: Chronos Moirai, TimesFM) under\nvarying spatial sensor nodes density and sampling intervals using real-world\ntemperature data in a wireless sensor network. Our results show that STGNNs are\neffective when sensor deployments are sparse and sampling rate is moderate,\nleveraging spatial correlations via encoded graph structure to compensate for\nlimited coverage. In contrast, TSFMs perform competitively at high frequencies\nbut degrade when spatial coverage from neighboring sensors is reduced.\nCrucially, the multivariate TSFM Moirai outperforms all models by natively\nlearning cross-sensor dependencies. These findings offer actionable insights\nfor building efficient forecasting pipelines in spatio-temporal systems. All\ncode for model configurations, training, dataset, and logs are open-sourced for\nreproducibility:\nhttps://github.com/UIUC-MONET-Projects/Benchmarking-Spatiotemporal-Forecast-Models"
                },
                "authors": [
                    {
                        "name": "Ragini Gupta"
                    },
                    {
                        "name": "Naman Raina"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Claudiu Danilov"
                    },
                    {
                        "name": "Josh Eckhardt"
                    },
                    {
                        "name": "Keyshla Bernard"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05165v1",
                "updated": "2025-11-07T11:35:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    35,
                    46,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T11:35:46Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    35,
                    46,
                    4,
                    311,
                    0
                ],
                "title": "Generating Software Architecture Description from Source Code using\n  Reverse Engineering and Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Software Architecture Description from Source Code using\n  Reverse Engineering and Large Language Model"
                },
                "summary": "Software Architecture Descriptions (SADs) are essential for managing the\ninherent complexity of modern software systems. They enable high-level\narchitectural reasoning, guide design decisions, and facilitate effective\ncommunication among diverse stakeholders. However, in practice, SADs are often\nmissing, outdated, or poorly aligned with the system's actual implementation.\nConsequently, developers are compelled to derive architectural insights\ndirectly from source code-a time-intensive process that increases cognitive\nload, slows new developer onboarding, and contributes to the gradual\ndegradation of clarity over the system's lifetime. To address these issues, we\npropose a semi-automated generation of SADs from source code by integrating\nreverse engineering (RE) techniques with a Large Language Model (LLM). Our\napproach recovers both static and behavioral architectural views by extracting\na comprehensive component diagram, filtering architecturally significant\nelements (core components) via prompt engineering, and generating state machine\ndiagrams to model component behavior based on underlying code logic with\nfew-shots prompting. This resulting views representation offer a scalable and\nmaintainable alternative to traditional manual architectural documentation.\nThis methodology, demonstrated using C++ examples, highlights the potent\ncapability of LLMs to: 1) abstract the component diagram, thereby reducing the\nreliance on human expert involvement, and 2) accurately represent complex\nsoftware behaviors, especially when enriched with domain-specific knowledge\nthrough few-shot prompting. These findings suggest a viable path toward\nsignificantly reducing manual effort while enhancing system understanding and\nlong-term maintainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Architecture Descriptions (SADs) are essential for managing the\ninherent complexity of modern software systems. They enable high-level\narchitectural reasoning, guide design decisions, and facilitate effective\ncommunication among diverse stakeholders. However, in practice, SADs are often\nmissing, outdated, or poorly aligned with the system's actual implementation.\nConsequently, developers are compelled to derive architectural insights\ndirectly from source code-a time-intensive process that increases cognitive\nload, slows new developer onboarding, and contributes to the gradual\ndegradation of clarity over the system's lifetime. To address these issues, we\npropose a semi-automated generation of SADs from source code by integrating\nreverse engineering (RE) techniques with a Large Language Model (LLM). Our\napproach recovers both static and behavioral architectural views by extracting\na comprehensive component diagram, filtering architecturally significant\nelements (core components) via prompt engineering, and generating state machine\ndiagrams to model component behavior based on underlying code logic with\nfew-shots prompting. This resulting views representation offer a scalable and\nmaintainable alternative to traditional manual architectural documentation.\nThis methodology, demonstrated using C++ examples, highlights the potent\ncapability of LLMs to: 1) abstract the component diagram, thereby reducing the\nreliance on human expert involvement, and 2) accurately represent complex\nsoftware behaviors, especially when enriched with domain-specific knowledge\nthrough few-shot prompting. These findings suggest a viable path toward\nsignificantly reducing manual effort while enhancing system understanding and\nlong-term maintainability."
                },
                "authors": [
                    {
                        "name": "Ahmad Hatahet"
                    },
                    {
                        "name": "Christoph Knieke"
                    },
                    {
                        "name": "Andreas Rausch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Rausch"
                },
                "author": "Andreas Rausch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05162v1",
                "updated": "2025-11-07T11:30:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    30,
                    10,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T11:30:10Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    30,
                    10,
                    4,
                    311,
                    0
                ],
                "title": "Mind the Gap... or Not? How Translation Errors and Evaluation Details\n  Skew Multilingual Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap... or Not? How Translation Errors and Evaluation Details\n  Skew Multilingual Results"
                },
                "summary": "Most current large language models (LLMs) support a wide variety of languages\nin addition to English, including high-resource languages (e.g. German,\nChinese, French), as well as low-resource ones (e.g. Swahili, Telugu). In\naddition they have also shown impressive capabilities in different domains,\nlike coding, science and math. In this short paper, taking math as an example\ndomain, we study the performance of different LLMs across languages.\nExperimental results show that there exists a non-negligible and consistent gap\nin the performance of the models across languages. Interestingly, and somewhat\nagainst expectations, the gap exists for both high- and low-resource languages.\nWe hope that these results influence further research into cross-lingual\ncapability generalization for next generation LLMs. If it weren't for the fact\nthat they are false! By analyzing one of the standard multilingual math\nbenchmarks (MGSM), we determine that several translation errors are present in\nthe data. Furthermore, the lack of standardized answer extraction from LLM\noutputs further influences the final results. We propose a method for automatic\nquality assurance to address the first issue at scale, and give recommendations\nto address the second one. Combining these two approaches we show that the\naforementioned language gap mostly disappears, leading to completely different\nconclusions from our research. We additionally release the corrected dataset to\nthe community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most current large language models (LLMs) support a wide variety of languages\nin addition to English, including high-resource languages (e.g. German,\nChinese, French), as well as low-resource ones (e.g. Swahili, Telugu). In\naddition they have also shown impressive capabilities in different domains,\nlike coding, science and math. In this short paper, taking math as an example\ndomain, we study the performance of different LLMs across languages.\nExperimental results show that there exists a non-negligible and consistent gap\nin the performance of the models across languages. Interestingly, and somewhat\nagainst expectations, the gap exists for both high- and low-resource languages.\nWe hope that these results influence further research into cross-lingual\ncapability generalization for next generation LLMs. If it weren't for the fact\nthat they are false! By analyzing one of the standard multilingual math\nbenchmarks (MGSM), we determine that several translation errors are present in\nthe data. Furthermore, the lack of standardized answer extraction from LLM\noutputs further influences the final results. We propose a method for automatic\nquality assurance to address the first issue at scale, and give recommendations\nto address the second one. Combining these two approaches we show that the\naforementioned language gap mostly disappears, leading to completely different\nconclusions from our research. We additionally release the corrected dataset to\nthe community."
                },
                "authors": [
                    {
                        "name": "Jan-Thorsten Peter"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Tobias Domhan"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Markus Freitag"
                    }
                ],
                "author_detail": {
                    "name": "Markus Freitag"
                },
                "author": "Markus Freitag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05158v1",
                "updated": "2025-11-07T11:24:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    24,
                    20,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T11:24:20Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    24,
                    20,
                    4,
                    311,
                    0
                ],
                "title": "Follow-Me in Micro-Mobility with End-to-End Imitation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Me in Micro-Mobility with End-to-End Imitation Learning"
                },
                "summary": "Autonomous micro-mobility platforms face challenges from the perspective of\nthe typical deployment environment: large indoor spaces or urban areas that are\npotentially crowded and highly dynamic. While social navigation algorithms have\nprogressed significantly, optimizing user comfort and overall user experience\nover other typical metrics in robotics (e.g., time or distance traveled) is\nunderstudied. Specifically, these metrics are critical in commercial\napplications. In this paper, we show how imitation learning delivers smoother\nand overall better controllers, versus previously used manually-tuned\ncontrollers. We demonstrate how DAAV's autonomous wheelchair achieves\nstate-of-the-art comfort in follow-me mode, in which it follows a human\noperator assisting persons with reduced mobility (PRM). This paper analyzes\ndifferent neural network architectures for end-to-end control and demonstrates\ntheir usability in real-world production-level deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous micro-mobility platforms face challenges from the perspective of\nthe typical deployment environment: large indoor spaces or urban areas that are\npotentially crowded and highly dynamic. While social navigation algorithms have\nprogressed significantly, optimizing user comfort and overall user experience\nover other typical metrics in robotics (e.g., time or distance traveled) is\nunderstudied. Specifically, these metrics are critical in commercial\napplications. In this paper, we show how imitation learning delivers smoother\nand overall better controllers, versus previously used manually-tuned\ncontrollers. We demonstrate how DAAV's autonomous wheelchair achieves\nstate-of-the-art comfort in follow-me mode, in which it follows a human\noperator assisting persons with reduced mobility (PRM). This paper analyzes\ndifferent neural network architectures for end-to-end control and demonstrates\ntheir usability in real-world production-level deployments."
                },
                "authors": [
                    {
                        "name": "Sahar Salimpour"
                    },
                    {
                        "name": "Iacopo Catalano"
                    },
                    {
                        "name": "Tomi Westerlund"
                    },
                    {
                        "name": "Mohsen Falahi"
                    },
                    {
                        "name": "Jorge Peña Queralta"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Peña Queralta"
                },
                "author": "Jorge Peña Queralta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26854v2",
                "updated": "2025-11-07T11:11:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    11,
                    11,
                    26,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-30T15:38:50Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    38,
                    50,
                    3,
                    303,
                    0
                ],
                "title": "Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a\n  Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a\n  Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base"
                },
                "summary": "Most scientific materials compress reasoning, presenting conclusions while\nomitting the derivational chains that justify them. This compression hinders\nverification by lacking explicit, step-wise justifications and inhibits\ncross-domain links by collapsing the very pathways that establish the logical\nand causal connections between concepts. We introduce a scalable framework that\ndecompresses scientific reasoning, constructing a verifiable Long\nChain-of-Thought (LCoT) knowledge base and projecting it into an emergent\nencyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven,\nreductionist strategy: a Socratic agent, guided by a curriculum of around 200\ncourses, generates approximately 3 million first-principles questions. To\nensure high fidelity, multiple independent solver models generate LCoTs, which\nare then rigorously filtered by prompt sanitization and cross-model answer\nconsensus, retaining only those with verifiable endpoints. This verified corpus\npowers the Brainstorm Search Engine, which performs inverse knowledge search --\nretrieving diverse, first-principles derivations that culminate in a target\nconcept. This engine, in turn, feeds the Plato synthesizer, which narrates\nthese verified chains into coherent articles. The initial SciencePedia\ncomprises approximately 200,000 fine-grained entries spanning mathematics,\nphysics, chemistry, biology, engineering, and computation. In evaluations\nacross six disciplines, Plato-synthesized articles (conditioned on retrieved\nLCoTs) exhibit substantially higher knowledge-point density and significantly\nlower factual error rates than an equally-prompted baseline without retrieval\n(as judged by an external LLM). Built on this verifiable LCoT knowledge base,\nthis reasoning-centric approach enables trustworthy, cross-domain scientific\nsynthesis at scale and establishes the foundation for an ever-expanding\nencyclopedia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most scientific materials compress reasoning, presenting conclusions while\nomitting the derivational chains that justify them. This compression hinders\nverification by lacking explicit, step-wise justifications and inhibits\ncross-domain links by collapsing the very pathways that establish the logical\nand causal connections between concepts. We introduce a scalable framework that\ndecompresses scientific reasoning, constructing a verifiable Long\nChain-of-Thought (LCoT) knowledge base and projecting it into an emergent\nencyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven,\nreductionist strategy: a Socratic agent, guided by a curriculum of around 200\ncourses, generates approximately 3 million first-principles questions. To\nensure high fidelity, multiple independent solver models generate LCoTs, which\nare then rigorously filtered by prompt sanitization and cross-model answer\nconsensus, retaining only those with verifiable endpoints. This verified corpus\npowers the Brainstorm Search Engine, which performs inverse knowledge search --\nretrieving diverse, first-principles derivations that culminate in a target\nconcept. This engine, in turn, feeds the Plato synthesizer, which narrates\nthese verified chains into coherent articles. The initial SciencePedia\ncomprises approximately 200,000 fine-grained entries spanning mathematics,\nphysics, chemistry, biology, engineering, and computation. In evaluations\nacross six disciplines, Plato-synthesized articles (conditioned on retrieved\nLCoTs) exhibit substantially higher knowledge-point density and significantly\nlower factual error rates than an equally-prompted baseline without retrieval\n(as judged by an external LLM). Built on this verifiable LCoT knowledge base,\nthis reasoning-centric approach enables trustworthy, cross-domain scientific\nsynthesis at scale and establishes the foundation for an ever-expanding\nencyclopedia."
                },
                "authors": [
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Yuan Huang"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Caiyu Fan"
                    },
                    {
                        "name": "Xiansheng Cai"
                    },
                    {
                        "name": "Sihan Hu"
                    },
                    {
                        "name": "Xinzijian Liu"
                    },
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Mingjun Xu"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Tianhan Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Youjin Deng"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Weijie Sun"
                    },
                    {
                        "name": "Xingyu Li"
                    },
                    {
                        "name": "Weinan E"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Zhiyuan Yao"
                    },
                    {
                        "name": "Kun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kun Chen"
                },
                "author": "Kun Chen",
                "arxiv_comment": "43 pages, 4 figures. This work is part of the SciencePedia project\n  (sciencepedia.bohrium.com)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18469v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18469v6",
                "updated": "2025-11-07T10:17:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    10,
                    17,
                    59,
                    4,
                    311,
                    0
                ],
                "published": "2024-10-24T06:36:12Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    6,
                    36,
                    12,
                    3,
                    298,
                    0
                ],
                "title": "Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities"
                },
                "summary": "Recent research has shown that Large Language Models (LLMs) are vulnerable to\nautomated jailbreak attacks, where adversarial suffixes crafted by algorithms\nappended to harmful queries bypass safety alignment and trigger unintended\nresponses. Current methods for generating these suffixes are computationally\nexpensive and have low Attack Success Rates (ASR), especially against\nwell-aligned models like Llama2 and Llama3. To overcome these limitations, we\nintroduce ADV-LLM, an iterative self-tuning process that crafts adversarial\nLLMs with enhanced jailbreak ability. Our framework significantly reduces the\ncomputational cost of generating adversarial suffixes while achieving nearly\n100\\% ASR on various open-source LLMs. Moreover, it exhibits strong attack\ntransferability to closed-source models, achieving 99\\% ASR on GPT-3.5 and 49\\%\nASR on GPT-4, despite being optimized solely on Llama3. Beyond improving\njailbreak ability, ADV-LLM provides valuable insights for future safety\nalignment research through its ability to generate large datasets for studying\nLLM safety. Our code is available at: https://github.com/SunChungEn/ADV-LLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that Large Language Models (LLMs) are vulnerable to\nautomated jailbreak attacks, where adversarial suffixes crafted by algorithms\nappended to harmful queries bypass safety alignment and trigger unintended\nresponses. Current methods for generating these suffixes are computationally\nexpensive and have low Attack Success Rates (ASR), especially against\nwell-aligned models like Llama2 and Llama3. To overcome these limitations, we\nintroduce ADV-LLM, an iterative self-tuning process that crafts adversarial\nLLMs with enhanced jailbreak ability. Our framework significantly reduces the\ncomputational cost of generating adversarial suffixes while achieving nearly\n100\\% ASR on various open-source LLMs. Moreover, it exhibits strong attack\ntransferability to closed-source models, achieving 99\\% ASR on GPT-3.5 and 49\\%\nASR on GPT-4, despite being optimized solely on Llama3. Beyond improving\njailbreak ability, ADV-LLM provides valuable insights for future safety\nalignment research through its ability to generate large datasets for studying\nLLM safety. Our code is available at: https://github.com/SunChungEn/ADV-LLM"
                },
                "authors": [
                    {
                        "name": "Chung-En Sun"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Weiwei Yang"
                    },
                    {
                        "name": "Tsui-Wei Weng"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Aidan San"
                    },
                    {
                        "name": "Michel Galley"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "Accepted to NAACL 2025 Main (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18469v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18469v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25441v2",
                "updated": "2025-11-07T10:05:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    10,
                    5,
                    6,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-29T12:08:07Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    12,
                    8,
                    7,
                    2,
                    302,
                    0
                ],
                "title": "Grounded in Reality: Learning and Deploying Proactive LLM from Offline\n  Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded in Reality: Learning and Deploying Proactive LLM from Offline\n  Logs"
                },
                "summary": "Large Language Models (LLMs) excel as passive responders, but teaching them\nto be proactive, goal-oriented partners, a critical capability in high-stakes\ndomains, remains a major challenge. Current paradigms either myopically\noptimize single-turn attributes or rely on brittle, high-cost user simulators,\ncreating a persistent ``reality gap''. To bridge this gap, we introduce\n\\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and\ndeploying proactive dialogue agents \\textit{directly from offline expert data},\nbypassing the need to model complex user dynamics. Our key insight is to\nreframe the offline policy learning problem by leveraging the \\textbf{observed\nfuture} of each expert trajectory. This allows us to infer a dense,\nturn-by-turn reward signal grounded in the expert's revealed strategy,\ndecomposing the intractable long-horizon problem into a series of supervised\nlearning tasks, and training a policy to output a structured \\texttt{(action,\nstate_assessment)} tuple, governing both \\textbf{what to ask} and, crucially,\n\\textbf{when to stop}. To ensure reward fidelity, our Automated Grader\nCalibration pipeline systematically purges noise from the LLM-based reward\nmodel with minimal human supervision. Empirically, we demonstrate the efficacy\nof \\texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying\nsizes up to 32B. Our approach culminates in the successful deployment of LLMs\ninto a live, large-scale online AI service. In rigorous in-house evaluations,\nour model was launched and achieved performance even superior to human experts,\nproving our framework's ability to translate offline data into tangible,\nreal-world impact. We hope this work provides a practical and economically\nviable blueprint for transforming passive LLMs into proactive, goal-oriented\nLLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel as passive responders, but teaching them\nto be proactive, goal-oriented partners, a critical capability in high-stakes\ndomains, remains a major challenge. Current paradigms either myopically\noptimize single-turn attributes or rely on brittle, high-cost user simulators,\ncreating a persistent ``reality gap''. To bridge this gap, we introduce\n\\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and\ndeploying proactive dialogue agents \\textit{directly from offline expert data},\nbypassing the need to model complex user dynamics. Our key insight is to\nreframe the offline policy learning problem by leveraging the \\textbf{observed\nfuture} of each expert trajectory. This allows us to infer a dense,\nturn-by-turn reward signal grounded in the expert's revealed strategy,\ndecomposing the intractable long-horizon problem into a series of supervised\nlearning tasks, and training a policy to output a structured \\texttt{(action,\nstate_assessment)} tuple, governing both \\textbf{what to ask} and, crucially,\n\\textbf{when to stop}. To ensure reward fidelity, our Automated Grader\nCalibration pipeline systematically purges noise from the LLM-based reward\nmodel with minimal human supervision. Empirically, we demonstrate the efficacy\nof \\texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying\nsizes up to 32B. Our approach culminates in the successful deployment of LLMs\ninto a live, large-scale online AI service. In rigorous in-house evaluations,\nour model was launched and achieved performance even superior to human experts,\nproving our framework's ability to translate offline data into tangible,\nreal-world impact. We hope this work provides a practical and economically\nviable blueprint for transforming passive LLMs into proactive, goal-oriented\nLLM applications."
                },
                "authors": [
                    {
                        "name": "Fei Wei"
                    },
                    {
                        "name": "Daoyuan Chen"
                    },
                    {
                        "name": "Ce Wang"
                    },
                    {
                        "name": "Yilun Huang"
                    },
                    {
                        "name": "Yushuo Chen"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "arxiv_comment": "27 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05120v1",
                "updated": "2025-11-07T10:04:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    10,
                    4,
                    41,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T10:04:41Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    10,
                    4,
                    41,
                    4,
                    311,
                    0
                ],
                "title": "A Toolbox for Improving Evolutionary Prompt Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Toolbox for Improving Evolutionary Prompt Search"
                },
                "summary": "Evolutionary prompt optimization has demonstrated effectiveness in refining\nprompts for LLMs. However, existing approaches lack robust operators and\nefficient evaluation mechanisms. In this work, we propose several key\nimprovements to evolutionary prompt optimization that can partially generalize\nto prompt optimization in general: 1) decomposing evolution into distinct steps\nto enhance the evolution and its control, 2) introducing an LLM-based judge to\nverify the evolutions, 3) integrating human feedback to refine the evolutionary\noperator, and 4) developing more efficient evaluation strategies that maintain\nperformance while reducing computational overhead. Our approach improves both\noptimization quality and efficiency. We release our code, enabling prompt\noptimization on new tasks and facilitating further research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary prompt optimization has demonstrated effectiveness in refining\nprompts for LLMs. However, existing approaches lack robust operators and\nefficient evaluation mechanisms. In this work, we propose several key\nimprovements to evolutionary prompt optimization that can partially generalize\nto prompt optimization in general: 1) decomposing evolution into distinct steps\nto enhance the evolution and its control, 2) introducing an LLM-based judge to\nverify the evolutions, 3) integrating human feedback to refine the evolutionary\noperator, and 4) developing more efficient evaluation strategies that maintain\nperformance while reducing computational overhead. Our approach improves both\noptimization quality and efficiency. We release our code, enabling prompt\noptimization on new tasks and facilitating further research in this area."
                },
                "authors": [
                    {
                        "name": "Daniel Grießhaber"
                    },
                    {
                        "name": "Maximilian Kimmich"
                    },
                    {
                        "name": "Johannes Maucher"
                    },
                    {
                        "name": "Ngoc Thang Vu"
                    }
                ],
                "author_detail": {
                    "name": "Ngoc Thang Vu"
                },
                "author": "Ngoc Thang Vu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16447v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16447v2",
                "updated": "2025-11-07T10:02:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    10,
                    2,
                    55,
                    4,
                    311,
                    0
                ],
                "published": "2025-08-22T15:02:07Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    2,
                    7,
                    4,
                    234,
                    0
                ],
                "title": "Boardwalk: Towards a Framework for Creating Board Games with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boardwalk: Towards a Framework for Creating Board Games with LLMs"
                },
                "summary": "Implementing board games in code can be a time-consuming task. However, Large\nLanguage Models (LLMs) have been proven effective at generating code for\ndomain-specific tasks with simple contextual information. We aim to investigate\nwhether LLMs can implement digital versions of board games from rules described\nin natural language. This would be a step towards an LLM-assisted framework for\nquick board game code generation. We expect to determine the main challenges\nfor LLMs to implement the board games, and how different approaches and models\ncompare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek\nand ChatGPT) with coding a selection of 12 popular and obscure games in\nfree-form and within Boardwalk, our proposed General Game Playing API. We\nanonymize the games and components to avoid evoking pre-trained LLM knowledge.\nThe implementations are tested for playability and rule compliance. We evaluate\nsuccess rate and common errors across LLMs and game popularity. Our approach\nproves viable, with the best performing model, Claude 3.7 Sonnet, yielding\n55.6\\% of games without any errors. While compliance with the API increases\nerror frequency, the severity of errors is more significantly dependent on the\nLLM. We outline future steps for creating a framework to integrate this\nprocess, making the elaboration of board games more accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing board games in code can be a time-consuming task. However, Large\nLanguage Models (LLMs) have been proven effective at generating code for\ndomain-specific tasks with simple contextual information. We aim to investigate\nwhether LLMs can implement digital versions of board games from rules described\nin natural language. This would be a step towards an LLM-assisted framework for\nquick board game code generation. We expect to determine the main challenges\nfor LLMs to implement the board games, and how different approaches and models\ncompare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek\nand ChatGPT) with coding a selection of 12 popular and obscure games in\nfree-form and within Boardwalk, our proposed General Game Playing API. We\nanonymize the games and components to avoid evoking pre-trained LLM knowledge.\nThe implementations are tested for playability and rule compliance. We evaluate\nsuccess rate and common errors across LLMs and game popularity. Our approach\nproves viable, with the best performing model, Claude 3.7 Sonnet, yielding\n55.6\\% of games without any errors. While compliance with the API increases\nerror frequency, the severity of errors is more significantly dependent on the\nLLM. We outline future steps for creating a framework to integrate this\nprocess, making the elaboration of board games more accessible."
                },
                "authors": [
                    {
                        "name": "Álvaro Guglielmin Becker"
                    },
                    {
                        "name": "Gabriel Bauer de Oliveira"
                    },
                    {
                        "name": "Lana Bertoldo Rossato"
                    },
                    {
                        "name": "Anderson Rocha Tavares"
                    }
                ],
                "author_detail": {
                    "name": "Anderson Rocha Tavares"
                },
                "author": "Anderson Rocha Tavares",
                "arxiv_doi": "10.5753/sbgames.2025.10222",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5753/sbgames.2025.10222",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.16447v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16447v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Presented at SBGames 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05114v1",
                "updated": "2025-11-07T09:58:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    58,
                    1,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T09:58:01Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    58,
                    1,
                    4,
                    311,
                    0
                ],
                "title": "Usando LLMs para Programar Jogos de Tabuleiro e Variações",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Usando LLMs para Programar Jogos de Tabuleiro e Variações"
                },
                "summary": "Creating programs to represent board games can be a time-consuming task.\nLarge Language Models (LLMs) arise as appealing tools to expedite this process,\ngiven their capacity to efficiently generate code from simple contextual\ninformation. In this work, we propose a method to test how capable three LLMs\n(Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as\nnew variants of existing games.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating programs to represent board games can be a time-consuming task.\nLarge Language Models (LLMs) arise as appealing tools to expedite this process,\ngiven their capacity to efficiently generate code from simple contextual\ninformation. In this work, we propose a method to test how capable three LLMs\n(Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as\nnew variants of existing games."
                },
                "authors": [
                    {
                        "name": "Álvaro Guglielmin Becker"
                    },
                    {
                        "name": "Lana Bertoldo Rossato"
                    },
                    {
                        "name": "Anderson Rocha Tavares"
                    }
                ],
                "author_detail": {
                    "name": "Anderson Rocha Tavares"
                },
                "author": "Anderson Rocha Tavares",
                "arxiv_comment": "Accepted for presentation at the I Escola Regional de Aprendizado de\n  M\\'aquina e Intelig\\^encia Artificial da Regi\\~ao Sul, 2025, in Portuguese\n  language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09766v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09766v5",
                "updated": "2025-11-07T09:32:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    32,
                    5,
                    4,
                    311,
                    0
                ],
                "published": "2025-01-15T04:52:34Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    4,
                    52,
                    34,
                    2,
                    15,
                    0
                ],
                "title": "iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for\n  Advanced Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for\n  Advanced Tool Use"
                },
                "summary": "Augmenting large language models (LLMs) with external tools is a promising\napproach to enhance their capabilities, especially for complex tasks.\nSynthesizing tool-use data through real-world simulations is an effective way\nto achieve this. However, our investigation reveals that training gains\nsignificantly decay as synthetic data increases. The model struggles to benefit\nfrom additional synthetic data, which fails to endow it with advanced tool-use\ncapabilities in complex scenarios Moreover, we discovered that the above\nlimitation usually manifests as a fragment deficiency (i.e., parameter errors)\nin response. To this end, we propose an iterative reinforced fine-tuning\nstrategy designed to alleviate this limitation. This strategy involves: (1)\nenhancing the diversity of response for synthetic data through path exploration\nof Monte Carlo Tree Search. (2) iteratively pinpointing the model's deficiency\nby constructing fine-grained preference pairs, and then improving it by\npreference optimization algorithms for targeted improvement. The experiments\nshow that our method achieves 13.11% better performance than the same-size base\nmodel. It achieves an improvement of 6.5% in complex scenarios compared to the\nbaseline, and it also outperforms larger open-source and closed-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting large language models (LLMs) with external tools is a promising\napproach to enhance their capabilities, especially for complex tasks.\nSynthesizing tool-use data through real-world simulations is an effective way\nto achieve this. However, our investigation reveals that training gains\nsignificantly decay as synthetic data increases. The model struggles to benefit\nfrom additional synthetic data, which fails to endow it with advanced tool-use\ncapabilities in complex scenarios Moreover, we discovered that the above\nlimitation usually manifests as a fragment deficiency (i.e., parameter errors)\nin response. To this end, we propose an iterative reinforced fine-tuning\nstrategy designed to alleviate this limitation. This strategy involves: (1)\nenhancing the diversity of response for synthetic data through path exploration\nof Monte Carlo Tree Search. (2) iteratively pinpointing the model's deficiency\nby constructing fine-grained preference pairs, and then improving it by\npreference optimization algorithms for targeted improvement. The experiments\nshow that our method achieves 13.11% better performance than the same-size base\nmodel. It achieves an improvement of 6.5% in complex scenarios compared to the\nbaseline, and it also outperforms larger open-source and closed-source models."
                },
                "authors": [
                    {
                        "name": "Yirong Zeng"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Yuxian Wang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Wu Ning"
                    },
                    {
                        "name": "Yutai Hou"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09766v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09766v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05094v1",
                "updated": "2025-11-07T09:21:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    21,
                    22,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T09:21:22Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    21,
                    22,
                    4,
                    311,
                    0
                ],
                "title": "FM4Com: Foundation Model for Scene-Adaptive Communication Strategy\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FM4Com: Foundation Model for Scene-Adaptive Communication Strategy\n  Optimization"
                },
                "summary": "The emergence of sixth-generation (6G) networks heralds an intelligent\ncommunication ecosystem driven by AI-native air interfaces. However, current\nphysical-layer designs-typically following modular and isolated optimization\nparadigms-fail to achieve global end-to-end optimality due to neglected\ninter-module dependencies. Although large language models (LLMs) have recently\nbeen applied to communication tasks such as beam prediction and resource\nallocation, existing studies remain limited to single-task or single-modality\nscenarios and lack the ability to jointly reason over communication states and\nuser intents for personalized strategy adaptation. To address these\nlimitations, this paper proposes a novel multimodal communication\ndecision-making model based on reinforcement learning. The proposed model\nsemantically aligns channel state information (CSI) and textual user\ninstructions, enabling comprehensive understanding of both physical-layer\nconditions and communication intents. It then generates physically realizable,\nuser-customized link construction strategies that dynamically adapt to changing\nenvironments and preference tendencies. A two-stage reinforcement learning\nframework is employed: the first stage expands the experience pool via\nheuristic exploration and behavior cloning to obtain a near-optimal\ninitialization, while the second stage fine-tunes the model through\nmulti-objective reinforcement learning considering bit error rate, throughput,\nand complexity. Experimental results demonstrate that the proposed model\nsignificantly outperforms conventional planning-based algorithms under\nchallenging channel conditions, achieving robust, efficient, and personalized\n6G link construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of sixth-generation (6G) networks heralds an intelligent\ncommunication ecosystem driven by AI-native air interfaces. However, current\nphysical-layer designs-typically following modular and isolated optimization\nparadigms-fail to achieve global end-to-end optimality due to neglected\ninter-module dependencies. Although large language models (LLMs) have recently\nbeen applied to communication tasks such as beam prediction and resource\nallocation, existing studies remain limited to single-task or single-modality\nscenarios and lack the ability to jointly reason over communication states and\nuser intents for personalized strategy adaptation. To address these\nlimitations, this paper proposes a novel multimodal communication\ndecision-making model based on reinforcement learning. The proposed model\nsemantically aligns channel state information (CSI) and textual user\ninstructions, enabling comprehensive understanding of both physical-layer\nconditions and communication intents. It then generates physically realizable,\nuser-customized link construction strategies that dynamically adapt to changing\nenvironments and preference tendencies. A two-stage reinforcement learning\nframework is employed: the first stage expands the experience pool via\nheuristic exploration and behavior cloning to obtain a near-optimal\ninitialization, while the second stage fine-tunes the model through\nmulti-objective reinforcement learning considering bit error rate, throughput,\nand complexity. Experimental results demonstrate that the proposed model\nsignificantly outperforms conventional planning-based algorithms under\nchallenging channel conditions, achieving robust, efficient, and personalized\n6G link construction."
                },
                "authors": [
                    {
                        "name": "Zhaoyang Li"
                    },
                    {
                        "name": "Shangzhuo Xie"
                    },
                    {
                        "name": "Qianqian Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qianqian Yang"
                },
                "author": "Qianqian Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16821v3",
                "updated": "2025-11-07T09:20:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    20,
                    34,
                    4,
                    311,
                    0
                ],
                "published": "2025-05-22T15:55:56Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    55,
                    56,
                    3,
                    142,
                    0
                ],
                "title": "LLM-Based Emulation of the Radio Resource Control Layer: Towards\n  AI-Native RAN Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Emulation of the Radio Resource Control Layer: Towards\n  AI-Native RAN Protocols"
                },
                "summary": "Integrating Large AI Models (LAMs) into 6G mobile networks is a key enabler\nof the AI-Native Air Interface (AI-AI), where protocol intelligence must scale\nbeyond handcrafted logic. This paper presents, to our knowledge, the first\nstandards-compliant emulation of the Radio Resource Control (RRC) layer using a\ndecoder-only LAM (LLAMA-class) fine-tuned with Low-Rank Adaptation (LoRA) on a\nmulti-vendor corpus of real-world traces spanning both 5G and 4G systems. We\ntreat RRC as a domain-specific language and construct a segmentation-safe,\nquestion--answer (Question-and-Answer (QA)) dataset that preserves Abstract\nSyntax Notation (ASN.1) structure through linearization prior to Byte Pair\nEncoding (BPE) tokenization. The proposed approach combines parameter-efficient\nadaptation with schema-bounded prompting to ensure syntactic and procedural\nfidelity. Evaluation introduces a standards-aware triad -- ASN.1 conformance,\nfield-level coverage analysis, and uplink-to-downlink state-machine checks --\nalongside semantic similarity and latency profiling across 120 configurations.\nOn 30k 5G request--response pairs plus an additional 4.8k QA turns from 4G\nsessions, our 8B model achieves a median cosine similarity of 0.97, a 61%\nrelative gain over a zero-shot baseline, while sustaining high conformance\nrates. These results demonstrate that LAMs, when augmented with protocol-aware\nreasoning, can directly orchestrate control-plane procedures, laying the\nfoundation for the future Artificial Intelligence (AI)-native Radio Access\nNetwork (RAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large AI Models (LAMs) into 6G mobile networks is a key enabler\nof the AI-Native Air Interface (AI-AI), where protocol intelligence must scale\nbeyond handcrafted logic. This paper presents, to our knowledge, the first\nstandards-compliant emulation of the Radio Resource Control (RRC) layer using a\ndecoder-only LAM (LLAMA-class) fine-tuned with Low-Rank Adaptation (LoRA) on a\nmulti-vendor corpus of real-world traces spanning both 5G and 4G systems. We\ntreat RRC as a domain-specific language and construct a segmentation-safe,\nquestion--answer (Question-and-Answer (QA)) dataset that preserves Abstract\nSyntax Notation (ASN.1) structure through linearization prior to Byte Pair\nEncoding (BPE) tokenization. The proposed approach combines parameter-efficient\nadaptation with schema-bounded prompting to ensure syntactic and procedural\nfidelity. Evaluation introduces a standards-aware triad -- ASN.1 conformance,\nfield-level coverage analysis, and uplink-to-downlink state-machine checks --\nalongside semantic similarity and latency profiling across 120 configurations.\nOn 30k 5G request--response pairs plus an additional 4.8k QA turns from 4G\nsessions, our 8B model achieves a median cosine similarity of 0.97, a 61%\nrelative gain over a zero-shot baseline, while sustaining high conformance\nrates. These results demonstrate that LAMs, when augmented with protocol-aware\nreasoning, can directly orchestrate control-plane procedures, laying the\nfoundation for the future Artificial Intelligence (AI)-native Radio Access\nNetwork (RAN)."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Bryan Liu"
                    },
                    {
                        "name": "Alvaro Valcarce"
                    },
                    {
                        "name": "Xiaoli Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoli Chu"
                },
                "author": "Xiaoli Chu",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Focuses on applying LLMs to 5G RRC protocol generation; primary: cs.NI;\n  cross-list: eess.SP, cs.LG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19544v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19544v4",
                "updated": "2025-11-07T09:09:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    9,
                    43,
                    4,
                    311,
                    0
                ],
                "published": "2024-06-27T21:47:27Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    21,
                    47,
                    27,
                    3,
                    179,
                    0
                ],
                "title": "Where Is Self-admitted Code Generated by Large Language Models on\n  GitHub?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where Is Self-admitted Code Generated by Large Language Models on\n  GitHub?"
                },
                "summary": "The increasing use of Large Language Models (LLMs) in software development\nhas garnered significant attention from researchers evaluating the capabilities\nand limitations of LLMs for code generation. However, much of the research\nfocuses on controlled datasets such as HumanEval, which do not adequately\ncapture the characteristics of LLM-generated code in real-world development\nscenarios. To address this gap, our study investigates self-admitted code\ngenerated by LLMs on GitHub, specifically focusing on instances where\ndevelopers in projects with over five stars acknowledge the use of LLMs to\ngenerate code through code comments. Our findings reveal several key insights:\n(1) ChatGPT and Copilot dominate code generation, with minimal contributions\nfrom other LLMs. (2) Projects containing ChatGPT/Copilot-generated code appears\nin small/medium-sized projects led by small teams, which are continuously\nevolving. (3) ChatGPT/Copilot-generated code generally is a minor project\nportion, primarily generating short/moderate-length, low-complexity snippets\n(e.g., algorithms and data structures code; text processing code). (4)\nChatGPT/Copilot-generated code generally undergoes minimal modifications, with\nbug-related changes ranging from 4% to 12%. (5) Most code comments only state\nLLM use, while few include details like prompts, human edits, or code testing\nstatus. Based on these findings, we discuss the implications for researchers\nand practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Large Language Models (LLMs) in software development\nhas garnered significant attention from researchers evaluating the capabilities\nand limitations of LLMs for code generation. However, much of the research\nfocuses on controlled datasets such as HumanEval, which do not adequately\ncapture the characteristics of LLM-generated code in real-world development\nscenarios. To address this gap, our study investigates self-admitted code\ngenerated by LLMs on GitHub, specifically focusing on instances where\ndevelopers in projects with over five stars acknowledge the use of LLMs to\ngenerate code through code comments. Our findings reveal several key insights:\n(1) ChatGPT and Copilot dominate code generation, with minimal contributions\nfrom other LLMs. (2) Projects containing ChatGPT/Copilot-generated code appears\nin small/medium-sized projects led by small teams, which are continuously\nevolving. (3) ChatGPT/Copilot-generated code generally is a minor project\nportion, primarily generating short/moderate-length, low-complexity snippets\n(e.g., algorithms and data structures code; text processing code). (4)\nChatGPT/Copilot-generated code generally undergoes minimal modifications, with\nbug-related changes ranging from 4% to 12%. (5) Most code comments only state\nLLM use, while few include details like prompts, human edits, or code testing\nstatus. Based on these findings, we discuss the implications for researchers\nand practitioners."
                },
                "authors": [
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Jin Liu"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19544v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19544v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22963v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22963v2",
                "updated": "2025-11-07T09:01:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    1,
                    26,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-27T03:37:41Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    3,
                    37,
                    41,
                    0,
                    300,
                    0
                ],
                "title": "CompressionAttack: Exploiting Prompt Compression as a New Attack Surface\n  in LLM-Powered Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompressionAttack: Exploiting Prompt Compression as a New Attack Surface\n  in LLM-Powered Agents"
                },
                "summary": "LLM-powered agents often use prompt compression to reduce inference costs,\nbut this introduces a new security risk. Compression modules, which are\noptimized for efficiency rather than safety, can be manipulated by adversarial\ninputs, causing semantic drift and altering LLM behavior. This work identifies\nprompt compression as a novel attack surface and presents CompressionAttack,\nthe first framework to exploit it. CompressionAttack includes two strategies:\nHardCom, which uses discrete adversarial edits for hard compression, and\nSoftCom, which performs latent-space perturbations for soft compression.\nExperiments on multiple LLMs show up to 80% attack success and 98% preference\nflips, while remaining highly stealthy and transferable. Case studies in VSCode\nCline and Ollama confirm real-world impact, and current defenses prove\nineffective, highlighting the need for stronger protections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered agents often use prompt compression to reduce inference costs,\nbut this introduces a new security risk. Compression modules, which are\noptimized for efficiency rather than safety, can be manipulated by adversarial\ninputs, causing semantic drift and altering LLM behavior. This work identifies\nprompt compression as a novel attack surface and presents CompressionAttack,\nthe first framework to exploit it. CompressionAttack includes two strategies:\nHardCom, which uses discrete adversarial edits for hard compression, and\nSoftCom, which performs latent-space perturbations for soft compression.\nExperiments on multiple LLMs show up to 80% attack success and 98% preference\nflips, while remaining highly stealthy and transferable. Case studies in VSCode\nCline and Ollama confirm real-world impact, and current defenses prove\nineffective, highlighting the need for stronger protections."
                },
                "authors": [
                    {
                        "name": "Zesen Liu"
                    },
                    {
                        "name": "Zhixiang Zhang"
                    },
                    {
                        "name": "Yuchong Xie"
                    },
                    {
                        "name": "Dongdong She"
                    }
                ],
                "author_detail": {
                    "name": "Dongdong She"
                },
                "author": "Dongdong She",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22963v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22963v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05085v1",
                "updated": "2025-11-07T09:00:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    0,
                    26,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T09:00:26Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    0,
                    26,
                    4,
                    311,
                    0
                ],
                "title": "Iterative Layer-wise Distillation for Efficient Compression of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Layer-wise Distillation for Efficient Compression of Large\n  Language Models"
                },
                "summary": "This work investigates distillation methods for large language models (LLMs)\nwith the goal of developing compact models that preserve high performance.\nSeveral existing approaches are reviewed, with a discussion of their respective\nstrengths and limitations. An improved method based on the ShortGPT approach\nhas been developed, building upon the idea of incorporating iterative\nevaluation of layer importance. At each step, importance is assessed by\nmeasuring performance degradation when individual layers are removed, using a\nset of representative datasets. This process is combined with further training\nusing a joint loss function based on KL divergence and mean squared error.\nExperiments on the Qwen2.5-3B model show that the number of layers can be\nreduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a\n9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that\nthe middle transformer layers contribute less to inference, underscoring the\npotential of the proposed method for creating efficient models. The results\ndemonstrate the effectiveness of iterative distillation and fine-tuning, making\nthe approach suitable for deployment in resource-limited settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work investigates distillation methods for large language models (LLMs)\nwith the goal of developing compact models that preserve high performance.\nSeveral existing approaches are reviewed, with a discussion of their respective\nstrengths and limitations. An improved method based on the ShortGPT approach\nhas been developed, building upon the idea of incorporating iterative\nevaluation of layer importance. At each step, importance is assessed by\nmeasuring performance degradation when individual layers are removed, using a\nset of representative datasets. This process is combined with further training\nusing a joint loss function based on KL divergence and mean squared error.\nExperiments on the Qwen2.5-3B model show that the number of layers can be\nreduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a\n9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that\nthe middle transformer layers contribute less to inference, underscoring the\npotential of the proposed method for creating efficient models. The results\ndemonstrate the effectiveness of iterative distillation and fine-tuning, making\nthe approach suitable for deployment in resource-limited settings."
                },
                "authors": [
                    {
                        "name": "Grigory Kovalev"
                    },
                    {
                        "name": "Mikhail Tikhomirov"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Tikhomirov"
                },
                "author": "Mikhail Tikhomirov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09360v2",
                "updated": "2025-11-07T09:00:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    9,
                    0,
                    4,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-11T11:18:23Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    18,
                    23,
                    3,
                    254,
                    0
                ],
                "title": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in enterprise\napplications, yet their reliability remains limited by hallucinations, i.e.,\nconfident but factually incorrect information. Existing detection approaches,\nsuch as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not\naddress the unique challenges of Retrieval-Augmented Generation (RAG) systems,\nwhere responses must be consistent with retrieved evidence. We therefore\npresent MetaRAG, a metamorphic testing framework for hallucination detection in\nRetrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,\nunsupervised, black-box setting, requiring neither ground-truth references nor\naccess to model internals, making it suitable for proprietary and high-stakes\ndomains. The framework proceeds in four stages: (1) decompose answers into\natomic factoids, (2) generate controlled mutations of each factoid using\nsynonym and antonym substitutions, (3) verify each variant against the\nretrieved context (synonyms are expected to be entailed and antonyms\ncontradicted), and (4) aggregate penalties for inconsistencies into a\nresponse-level hallucination score. Crucially for identity-aware AI, MetaRAG\nlocalizes unsupported claims at the factoid span where they occur (e.g.,\npregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),\nallowing users to see flagged spans and enabling system designers to configure\nthresholds and guardrails for identity-sensitive queries. Experiments on a\nproprietary enterprise dataset illustrate the effectiveness of MetaRAG for\ndetecting hallucinations and enabling trustworthy deployment of RAG-based\nconversational agents. We also outline a topic-based deployment design that\ntranslates MetaRAG's span-level scores into identity-aware safeguards; this\ndesign is discussed but not evaluated in our experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in enterprise\napplications, yet their reliability remains limited by hallucinations, i.e.,\nconfident but factually incorrect information. Existing detection approaches,\nsuch as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not\naddress the unique challenges of Retrieval-Augmented Generation (RAG) systems,\nwhere responses must be consistent with retrieved evidence. We therefore\npresent MetaRAG, a metamorphic testing framework for hallucination detection in\nRetrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,\nunsupervised, black-box setting, requiring neither ground-truth references nor\naccess to model internals, making it suitable for proprietary and high-stakes\ndomains. The framework proceeds in four stages: (1) decompose answers into\natomic factoids, (2) generate controlled mutations of each factoid using\nsynonym and antonym substitutions, (3) verify each variant against the\nretrieved context (synonyms are expected to be entailed and antonyms\ncontradicted), and (4) aggregate penalties for inconsistencies into a\nresponse-level hallucination score. Crucially for identity-aware AI, MetaRAG\nlocalizes unsupported claims at the factoid span where they occur (e.g.,\npregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),\nallowing users to see flagged spans and enabling system designers to configure\nthresholds and guardrails for identity-sensitive queries. Experiments on a\nproprietary enterprise dataset illustrate the effectiveness of MetaRAG for\ndetecting hallucinations and enabling trustworthy deployment of RAG-based\nconversational agents. We also outline a topic-based deployment design that\ntranslates MetaRAG's span-level scores into identity-aware safeguards; this\ndesign is discussed but not evaluated in our experiments."
                },
                "authors": [
                    {
                        "name": "Channdeth Sok"
                    },
                    {
                        "name": "David Luz"
                    },
                    {
                        "name": "Yacine Haddam"
                    }
                ],
                "author_detail": {
                    "name": "Yacine Haddam"
                },
                "author": "Yacine Haddam",
                "arxiv_comment": "Identity-Aware AI workshop at 28th European Conference on Artificial\n  Intelligence, October 25, 2025, Bologna, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05080v1",
                "updated": "2025-11-07T08:53:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    8,
                    53,
                    39,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T08:53:39Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    8,
                    53,
                    39,
                    4,
                    311,
                    0
                ],
                "title": "On Text Simplification Metrics and General-Purpose LLMs for Accessible\n  Health Information, and A Potential Architectural Advantage of The\n  Instruction-Tuned LLM class",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Text Simplification Metrics and General-Purpose LLMs for Accessible\n  Health Information, and A Potential Architectural Advantage of The\n  Instruction-Tuned LLM class"
                },
                "summary": "The increasing health-seeking behavior and digital consumption of biomedical\ninformation by the general public necessitate scalable solutions for\nautomatically adapting complex scientific and technical documents into plain\nlanguage. Automatic text simplification solutions, including advanced large\nlanguage models, however, continue to face challenges in reliably arbitrating\nthe tension between optimizing readability performance and ensuring\npreservation of discourse fidelity. This report empirically assesses the\nperformance of two major classes of general-purpose LLMs, demonstrating their\nlinguistic capabilities and foundational readiness for the task compared to a\nhuman benchmark. Using a comparative analysis of the instruction-tuned Mistral\n24B and the reasoning-augmented QWen2.5 32B, we identify a potential\narchitectural advantage in the instruction-tuned LLM. Mistral exhibits a\ntempered lexical simplification strategy that enhances readability across a\nsuite of metrics and the simplification-specific formula SARI (mean 42.46),\nwhile preserving human-level discourse with a BERTScore of 0.91. QWen also\nattains enhanced readability performance, but its operational strategy shows a\ndisconnect in balancing between readability and accuracy, reaching a\nstatistically significantly lower BERTScore of 0.89. Additionally, a\ncomprehensive correlation analysis of 21 metrics spanning readability,\ndiscourse fidelity, content safety, and underlying distributional measures for\nmechanistic insights, confirms strong functional redundancies among five\nreadability indices. This empirical evidence tracks baseline performance of the\nevolving LLMs for the task of text simplification, identifies the\ninstruction-tuned Mistral 24B for simplification, provides necessary heuristics\nfor metric selection, and points to lexical support as a primary\ndomain-adaptation issue for simplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing health-seeking behavior and digital consumption of biomedical\ninformation by the general public necessitate scalable solutions for\nautomatically adapting complex scientific and technical documents into plain\nlanguage. Automatic text simplification solutions, including advanced large\nlanguage models, however, continue to face challenges in reliably arbitrating\nthe tension between optimizing readability performance and ensuring\npreservation of discourse fidelity. This report empirically assesses the\nperformance of two major classes of general-purpose LLMs, demonstrating their\nlinguistic capabilities and foundational readiness for the task compared to a\nhuman benchmark. Using a comparative analysis of the instruction-tuned Mistral\n24B and the reasoning-augmented QWen2.5 32B, we identify a potential\narchitectural advantage in the instruction-tuned LLM. Mistral exhibits a\ntempered lexical simplification strategy that enhances readability across a\nsuite of metrics and the simplification-specific formula SARI (mean 42.46),\nwhile preserving human-level discourse with a BERTScore of 0.91. QWen also\nattains enhanced readability performance, but its operational strategy shows a\ndisconnect in balancing between readability and accuracy, reaching a\nstatistically significantly lower BERTScore of 0.89. Additionally, a\ncomprehensive correlation analysis of 21 metrics spanning readability,\ndiscourse fidelity, content safety, and underlying distributional measures for\nmechanistic insights, confirms strong functional redundancies among five\nreadability indices. This empirical evidence tracks baseline performance of the\nevolving LLMs for the task of text simplification, identifies the\ninstruction-tuned Mistral 24B for simplification, provides necessary heuristics\nfor metric selection, and points to lexical support as a primary\ndomain-adaptation issue for simplification."
                },
                "authors": [
                    {
                        "name": "P. Bilha Githinji"
                    },
                    {
                        "name": "Aikaterini Meilliou"
                    },
                    {
                        "name": "Peiwu Qin"
                    }
                ],
                "author_detail": {
                    "name": "Peiwu Qin"
                },
                "author": "Peiwu Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03138v2",
                "updated": "2025-11-07T08:01:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    8,
                    1,
                    49,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-05T03:04:35Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    3,
                    4,
                    35,
                    2,
                    309,
                    0
                ],
                "title": "A Proprietary Model-Based Safety Response Framework for AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Proprietary Model-Based Safety Response Framework for AI Agents"
                },
                "summary": "With the widespread application of Large Language Models (LLMs), their\nassociated security issues have become increasingly prominent, severely\nconstraining their trustworthy deployment in critical domains. This paper\nproposes a novel safety response framework designed to systematically safeguard\nLLMs at both the input and output levels. At the input level, the framework\nemploys a supervised fine-tuning-based safety classification model. Through a\nfine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused\nAttention), it performs precise risk identification and differentiated handling\nof user queries, significantly enhancing risk coverage and business scenario\nadaptability, and achieving a risk recall rate of 99.3%. At the output level,\nthe framework integrates Retrieval-Augmented Generation (RAG) with a\nspecifically fine-tuned interpretation model, ensuring all responses are\ngrounded in a real-time, trustworthy knowledge base. This approach eliminates\ninformation fabrication and enables result traceability. Experimental results\ndemonstrate that our proposed safety control model achieves a significantly\nhigher safety score on public safety evaluation benchmarks compared to the\nbaseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk\ntest set, the framework's components attained a perfect 100% safety score,\nvalidating their exceptional protective capabilities in complex risk scenarios.\nThis research provides an effective engineering pathway for building\nhigh-security, high-trust LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of Large Language Models (LLMs), their\nassociated security issues have become increasingly prominent, severely\nconstraining their trustworthy deployment in critical domains. This paper\nproposes a novel safety response framework designed to systematically safeguard\nLLMs at both the input and output levels. At the input level, the framework\nemploys a supervised fine-tuning-based safety classification model. Through a\nfine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused\nAttention), it performs precise risk identification and differentiated handling\nof user queries, significantly enhancing risk coverage and business scenario\nadaptability, and achieving a risk recall rate of 99.3%. At the output level,\nthe framework integrates Retrieval-Augmented Generation (RAG) with a\nspecifically fine-tuned interpretation model, ensuring all responses are\ngrounded in a real-time, trustworthy knowledge base. This approach eliminates\ninformation fabrication and enables result traceability. Experimental results\ndemonstrate that our proposed safety control model achieves a significantly\nhigher safety score on public safety evaluation benchmarks compared to the\nbaseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk\ntest set, the framework's components attained a perfect 100% safety score,\nvalidating their exceptional protective capabilities in complex risk scenarios.\nThis research provides an effective engineering pathway for building\nhigh-security, high-trust LLM applications."
                },
                "authors": [
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Jianjun Xu"
                    },
                    {
                        "name": "Pingtao Wei"
                    },
                    {
                        "name": "Jiu Li"
                    },
                    {
                        "name": "Peiqiang Zhao"
                    },
                    {
                        "name": "Jiwei Shi"
                    },
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Yanhui Yang"
                    },
                    {
                        "name": "Xiaodong Hui"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Wenqin Shao"
                    }
                ],
                "author_detail": {
                    "name": "Wenqin Shao"
                },
                "author": "Wenqin Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03293v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03293v2",
                "updated": "2025-11-07T07:54:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    7,
                    54,
                    50,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-05T08:44:19Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    8,
                    44,
                    19,
                    2,
                    309,
                    0
                ],
                "title": "UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous\n  NPU-PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous\n  NPU-PIM"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed on edge devices with\nNeural Processing Units (NPUs), yet the decode phase remains memory-intensive,\nlimiting performance. Processing-in-Memory (PIM) offers a promising solution,\nbut co-executing NPU-PIM systems face challenges such as data layout\nmismatches, bandwidth loss, and redundant storage. To address these issues, we\npropose UMDAM, a unified memory-affinity data layout and DRAM address mapping\nscheme tailored for NPU-PIM co-execution. UMDAM employs a column-major,\ntile-based layout and a configurable DRAM mapping strategy to ensure\ncompatibility with NPU computation while maximizing PIM efficiency -- without\nintroducing extra memory overhead or bandwidth loss. Comprehensive evaluations\non OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up\nto 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving\nend-to-end LLM inference efficiency on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed on edge devices with\nNeural Processing Units (NPUs), yet the decode phase remains memory-intensive,\nlimiting performance. Processing-in-Memory (PIM) offers a promising solution,\nbut co-executing NPU-PIM systems face challenges such as data layout\nmismatches, bandwidth loss, and redundant storage. To address these issues, we\npropose UMDAM, a unified memory-affinity data layout and DRAM address mapping\nscheme tailored for NPU-PIM co-execution. UMDAM employs a column-major,\ntile-based layout and a configurable DRAM mapping strategy to ensure\ncompatibility with NPU computation while maximizing PIM efficiency -- without\nintroducing extra memory overhead or bandwidth loss. Comprehensive evaluations\non OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up\nto 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving\nend-to-end LLM inference efficiency on edge devices."
                },
                "authors": [
                    {
                        "name": "Hai Huang"
                    },
                    {
                        "name": "Xuhong Qiang"
                    },
                    {
                        "name": "Weisheng Zhao"
                    },
                    {
                        "name": "Chenchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chenchen Liu"
                },
                "author": "Chenchen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03293v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03293v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05040v1",
                "updated": "2025-11-07T07:24:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    7,
                    24,
                    56,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T07:24:56Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    7,
                    24,
                    56,
                    4,
                    311,
                    0
                ],
                "title": "UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM\n  Code Generation in Ukrainian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM\n  Code Generation in Ukrainian"
                },
                "summary": "Evaluating the real capabilities of large language models in low-resource\nlanguages still represents a challenge, as many existing benchmarks focus on\nwidespread tasks translated from English or evaluate only simple language\nunderstanding. This paper introduces UA-Code-Bench, a new open-source benchmark\nestablished for a thorough evaluation of language models' code generation and\ncompetitive programming problem-solving abilities in Ukrainian. The benchmark\ncomprises 500 problems from the Eolymp platform, evenly distributed across five\ncomplexity levels from very easy to very hard. A diverse set of 13 leading\nproprietary and open-source models, generating Python solutions based on a\none-shot prompt, was evaluated via the dedicated Eolymp environment against\nhidden tests, ensuring code correctness. The obtained results reveal that even\ntop-performing models, such as OpenAI o3 and GPT-5, solve only half of the\nproblems, highlighting the challenge of code generation in low-resource natural\nlanguage. Furthermore, this research presents a comprehensive analysis of\nperformance across various difficulty levels, as well as an assessment of\nsolution uniqueness and computational efficiency, measured by both elapsed time\nand memory consumption of the generated solutions. In conclusion, this work\ndemonstrates the value of competitive programming benchmarks in evaluating\nlarge language models, especially in underrepresented languages. It also paves\nthe way for future research on multilingual code generation and\nreasoning-enhanced models. The benchmark, data parsing, preparation, code\ngeneration, and evaluation scripts are available at\nhttps://huggingface.co/datasets/NLPForUA/ua-code-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the real capabilities of large language models in low-resource\nlanguages still represents a challenge, as many existing benchmarks focus on\nwidespread tasks translated from English or evaluate only simple language\nunderstanding. This paper introduces UA-Code-Bench, a new open-source benchmark\nestablished for a thorough evaluation of language models' code generation and\ncompetitive programming problem-solving abilities in Ukrainian. The benchmark\ncomprises 500 problems from the Eolymp platform, evenly distributed across five\ncomplexity levels from very easy to very hard. A diverse set of 13 leading\nproprietary and open-source models, generating Python solutions based on a\none-shot prompt, was evaluated via the dedicated Eolymp environment against\nhidden tests, ensuring code correctness. The obtained results reveal that even\ntop-performing models, such as OpenAI o3 and GPT-5, solve only half of the\nproblems, highlighting the challenge of code generation in low-resource natural\nlanguage. Furthermore, this research presents a comprehensive analysis of\nperformance across various difficulty levels, as well as an assessment of\nsolution uniqueness and computational efficiency, measured by both elapsed time\nand memory consumption of the generated solutions. In conclusion, this work\ndemonstrates the value of competitive programming benchmarks in evaluating\nlarge language models, especially in underrepresented languages. It also paves\nthe way for future research on multilingual code generation and\nreasoning-enhanced models. The benchmark, data parsing, preparation, code\ngeneration, and evaluation scripts are available at\nhttps://huggingface.co/datasets/NLPForUA/ua-code-bench."
                },
                "authors": [
                    {
                        "name": "Mykyta Syromiatnikov"
                    },
                    {
                        "name": "Victoria Ruvinskaya"
                    }
                ],
                "author_detail": {
                    "name": "Victoria Ruvinskaya"
                },
                "author": "Victoria Ruvinskaya",
                "arxiv_doi": "10.15276/ict.02.2025.47",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.15276/ict.02.2025.47",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2511.05040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 5 figures. XI International conference \"Informatics.\n  Culture. Technique.\" (2025)",
                "arxiv_journal_ref": "XI International conference \"Informatics. Culture. Technique.\"\n  (2025) 308-314",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17803v2",
                "updated": "2025-11-07T07:16:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    7,
                    16,
                    46,
                    4,
                    311,
                    0
                ],
                "published": "2025-08-25T08:47:36Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    8,
                    47,
                    36,
                    0,
                    237,
                    0
                ],
                "title": "DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in\n  Reasoning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in\n  Reasoning Large Language Models"
                },
                "summary": "Reasoning large language models (RLLMs), such as OpenAI-O3 and DeepSeek-R1,\nhave recently demonstrated remarkable capabilities by performing structured and\nmulti-step reasoning. However, recent studies reveal that RLLMs often suffer\nfrom overthinking, i.e., producing unnecessarily lengthy reasoning chains even\nfor simple questions, leading to excessive token consumption and computational\ninefficiency. Interestingly, we observe that when processing multiple questions\nin batch mode, RLLMs exhibit more resource-efficient behavior by dynamically\ncompressing reasoning steps for easier problems, due to implicit resource\ncompetition. Inspired by this, we propose Dynamic Reasoning Quota Allocation\n(DRQA), a novel method that transfers the benefits of resource competition from\nbatch processing to single-question inference. Specifically, DRQA leverages\nbatch-generated preference data and reinforcement learning to train the model\nto allocate reasoning resources adaptively. By encouraging the model to\ninternalize a preference for responses that are both accurate and concise, DRQA\nenables it to generate concise answers for simple questions while retaining\nsufficient reasoning depth for more challenging ones. Extensive experiments on\na wide range of mathematical and scientific reasoning benchmarks demonstrate\nthat DRQA significantly reduces token usage while maintaining, and in many\ncases improving, answer accuracy. By effectively mitigating the overthinking\nproblem, DRQA offers a promising direction for more efficient and scalable\ndeployment of RLLMs, and we hope it inspires further exploration into\nfine-grained control of reasoning behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning large language models (RLLMs), such as OpenAI-O3 and DeepSeek-R1,\nhave recently demonstrated remarkable capabilities by performing structured and\nmulti-step reasoning. However, recent studies reveal that RLLMs often suffer\nfrom overthinking, i.e., producing unnecessarily lengthy reasoning chains even\nfor simple questions, leading to excessive token consumption and computational\ninefficiency. Interestingly, we observe that when processing multiple questions\nin batch mode, RLLMs exhibit more resource-efficient behavior by dynamically\ncompressing reasoning steps for easier problems, due to implicit resource\ncompetition. Inspired by this, we propose Dynamic Reasoning Quota Allocation\n(DRQA), a novel method that transfers the benefits of resource competition from\nbatch processing to single-question inference. Specifically, DRQA leverages\nbatch-generated preference data and reinforcement learning to train the model\nto allocate reasoning resources adaptively. By encouraging the model to\ninternalize a preference for responses that are both accurate and concise, DRQA\nenables it to generate concise answers for simple questions while retaining\nsufficient reasoning depth for more challenging ones. Extensive experiments on\na wide range of mathematical and scientific reasoning benchmarks demonstrate\nthat DRQA significantly reduces token usage while maintaining, and in many\ncases improving, answer accuracy. By effectively mitigating the overthinking\nproblem, DRQA offers a promising direction for more efficient and scalable\ndeployment of RLLMs, and we hope it inspires further exploration into\nfine-grained control of reasoning behaviors."
                },
                "authors": [
                    {
                        "name": "Kaiwen Yan"
                    },
                    {
                        "name": "Xuanqing Shi"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Chengwei Qin"
                    }
                ],
                "author_detail": {
                    "name": "Chengwei Qin"
                },
                "author": "Chengwei Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24145v2",
                "updated": "2025-11-07T07:03:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    7,
                    3,
                    20,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-28T07:38:15Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    7,
                    38,
                    15,
                    1,
                    301,
                    0
                ],
                "title": "From Observability Data to Diagnosis: An Evolving Multi-agent System for\n  Incident Management in Cloud Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Observability Data to Diagnosis: An Evolving Multi-agent System for\n  Incident Management in Cloud Systems"
                },
                "summary": "Incident management (IM) is central to the reliability of large-scale cloud\nsystems. Yet manual IM, where on-call engineers examine metrics, logs, and\ntraces is labor-intensive and error-prone in the face of massive and\nheterogeneous observability data. Existing automated IM approaches often\nstruggle to generalize across systems, provide limited interpretability, and\nincur high deployment costs, which hinders adoption in practice. In this paper,\nwe present OpsAgent, a lightweight, self-evolving multi-agent system for IM\nthat employs a training-free data processor to convert heterogeneous\nobservability data into structured textual descriptions, along with a\nmulti-agent collaboration framework that makes diagnostic inference transparent\nand auditable. To support continual capability growth, OpsAgent also introduces\na dual self-evolution mechanism that integrates internal model updates with\nexternal experience accumulation, thereby closing the deployment loop.\nComprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art\nperformance and show that OpsAgent is generalizable, interpretable,\ncost-efficient, and self-evolving, making it a practically deployable and\nsustainable solution for long-term operation in real-world cloud systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incident management (IM) is central to the reliability of large-scale cloud\nsystems. Yet manual IM, where on-call engineers examine metrics, logs, and\ntraces is labor-intensive and error-prone in the face of massive and\nheterogeneous observability data. Existing automated IM approaches often\nstruggle to generalize across systems, provide limited interpretability, and\nincur high deployment costs, which hinders adoption in practice. In this paper,\nwe present OpsAgent, a lightweight, self-evolving multi-agent system for IM\nthat employs a training-free data processor to convert heterogeneous\nobservability data into structured textual descriptions, along with a\nmulti-agent collaboration framework that makes diagnostic inference transparent\nand auditable. To support continual capability growth, OpsAgent also introduces\na dual self-evolution mechanism that integrates internal model updates with\nexternal experience accumulation, thereby closing the deployment loop.\nComprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art\nperformance and show that OpsAgent is generalizable, interpretable,\ncost-efficient, and self-evolving, making it a practically deployable and\nsustainable solution for long-term operation in real-world cloud systems."
                },
                "authors": [
                    {
                        "name": "Yu Luo"
                    },
                    {
                        "name": "Jiamin Jiang"
                    },
                    {
                        "name": "Jingfei Feng"
                    },
                    {
                        "name": "Lei Tao"
                    },
                    {
                        "name": "Qingliang Zhang"
                    },
                    {
                        "name": "Xidao Wen"
                    },
                    {
                        "name": "Yongqian Sun"
                    },
                    {
                        "name": "Shenglin Zhang"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13438v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13438v3",
                "updated": "2025-11-07T07:01:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    7,
                    1,
                    30,
                    4,
                    311,
                    0
                ],
                "published": "2025-05-19T17:58:44Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    58,
                    44,
                    0,
                    139,
                    0
                ],
                "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization"
                },
                "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency."
                },
                "authors": [
                    {
                        "name": "Penghui Qi"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Wee Sun Lee"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13438v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13438v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21150v2",
                "updated": "2025-11-07T06:59:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    59,
                    25,
                    4,
                    311,
                    0
                ],
                "published": "2025-10-24T04:43:50Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    4,
                    43,
                    50,
                    4,
                    297,
                    0
                ],
                "title": "String Seed of Thought: Prompting LLMs for Distribution-Faithful and\n  Diverse Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "String Seed of Thought: Prompting LLMs for Distribution-Faithful and\n  Diverse Generation"
                },
                "summary": "We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs\nthat improves Probabilistic Instruction Following (PIF). We define PIF as a\ntask requiring an LLM to select its answer from a predefined set of options,\neach associated with a specific probability, such that the empirical\ndistribution of the generated answers aligns with the target distribution when\nprompted multiple times. While LLMs excel at tasks with single, deterministic\nanswers, they often fail at PIF, exhibiting biases problematic for applications\nrequiring non-deterministic behaviors, such as human-behavior simulation,\ncontent diversification, and multiplayer games. It also harms the diversity of\ngenerated responses, a crucial factor in test-time scaling, by causing the\noutputs to collapse into a limited set of answers. To address this, we propose\nSSoT, a simple prompting method that instructs an LLM to first output a random\nstring to generate sufficient entropy. SSoT also instructs the LLM to extract\nrandomness by manipulating this string to derive a final answer, thereby\npreserving diversity while adhering to specific constraints. We demonstrate\nthat SSoT significantly improves the PIF performance of LLMs, approaching the\nideal performance of a pseudo-random number generator. Furthermore, our\nexperiments on NoveltyBench show SSoT's benefits extend beyond closed-set tasks\nto open-ended tasks by enhancing response diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs\nthat improves Probabilistic Instruction Following (PIF). We define PIF as a\ntask requiring an LLM to select its answer from a predefined set of options,\neach associated with a specific probability, such that the empirical\ndistribution of the generated answers aligns with the target distribution when\nprompted multiple times. While LLMs excel at tasks with single, deterministic\nanswers, they often fail at PIF, exhibiting biases problematic for applications\nrequiring non-deterministic behaviors, such as human-behavior simulation,\ncontent diversification, and multiplayer games. It also harms the diversity of\ngenerated responses, a crucial factor in test-time scaling, by causing the\noutputs to collapse into a limited set of answers. To address this, we propose\nSSoT, a simple prompting method that instructs an LLM to first output a random\nstring to generate sufficient entropy. SSoT also instructs the LLM to extract\nrandomness by manipulating this string to derive a final answer, thereby\npreserving diversity while adhering to specific constraints. We demonstrate\nthat SSoT significantly improves the PIF performance of LLMs, approaching the\nideal performance of a pseudo-random number generator. Furthermore, our\nexperiments on NoveltyBench show SSoT's benefits extend beyond closed-set tasks\nto open-ended tasks by enhancing response diversity."
                },
                "authors": [
                    {
                        "name": "Kou Misaki"
                    },
                    {
                        "name": "Takuya Akiba"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Akiba"
                },
                "author": "Takuya Akiba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05021v1",
                "updated": "2025-11-07T06:52:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    52,
                    28,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T06:52:28Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    52,
                    28,
                    4,
                    311,
                    0
                ],
                "title": "Continuous-variable Measurement Device Independent MIMO Quantum Key\n  Distribution for THz Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous-variable Measurement Device Independent MIMO Quantum Key\n  Distribution for THz Communications"
                },
                "summary": "Although multiple-input multiple-output (MIMO) terahertz (THz)\ncontinuous-variable quantum key distribution (CVQKD) is theoretically secure,\npractical vulnerabilities may arise due to detector imperfections. This paper\nexplores a CV measurement-device-independent (MDI) QKD system operating at THz\nfrequencies within a MIMO framework. In this system, measurement is delegated\nto an untrusted third party, Charlie, rather than the receiver, eliminating all\ndetector attacks and significantly enhancing the system's practical security.\nUsing transmit-receive beamforming techniques, the system transforms MIMO\nchannels into multiple parallel lossy quantum channels, enabling robust key\ndistribution between Alice and Bob. This study examines entanglement-based and\nprepare-and-measure protocols, deriving secret key rates for both asymptotic\nand finite code scenarios. Simulations reveal the critical role of multiple\nantenna configurations and efficient homodyne detection in mitigating\nfree-space path loss and maximizing key rates. Results indicate that system\nperformance is optimized at lower THz frequencies for long-range transmissions\nand higher frequencies for short-range applications. The proposed protocol\noffers a scalable solution for secure quantum communications in next-generation\nwireless networks, demonstrating potential for deployment in both indoor and\noutdoor environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although multiple-input multiple-output (MIMO) terahertz (THz)\ncontinuous-variable quantum key distribution (CVQKD) is theoretically secure,\npractical vulnerabilities may arise due to detector imperfections. This paper\nexplores a CV measurement-device-independent (MDI) QKD system operating at THz\nfrequencies within a MIMO framework. In this system, measurement is delegated\nto an untrusted third party, Charlie, rather than the receiver, eliminating all\ndetector attacks and significantly enhancing the system's practical security.\nUsing transmit-receive beamforming techniques, the system transforms MIMO\nchannels into multiple parallel lossy quantum channels, enabling robust key\ndistribution between Alice and Bob. This study examines entanglement-based and\nprepare-and-measure protocols, deriving secret key rates for both asymptotic\nand finite code scenarios. Simulations reveal the critical role of multiple\nantenna configurations and efficient homodyne detection in mitigating\nfree-space path loss and maximizing key rates. Results indicate that system\nperformance is optimized at lower THz frequencies for long-range transmissions\nand higher frequencies for short-range applications. The proposed protocol\noffers a scalable solution for secure quantum communications in next-generation\nwireless networks, demonstrating potential for deployment in both indoor and\noutdoor environments."
                },
                "authors": [
                    {
                        "name": "Leixin Wu"
                    },
                    {
                        "name": "Congtian Deng"
                    },
                    {
                        "name": "Jiayu Pan"
                    },
                    {
                        "name": "Lingtao Zhang"
                    },
                    {
                        "name": "Yanyan Feng"
                    },
                    {
                        "name": "Runbo Zhao"
                    },
                    {
                        "name": "Yang Shen"
                    },
                    {
                        "name": "Yuying Zhang"
                    },
                    {
                        "name": "Jian Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhou"
                },
                "author": "Jian Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04412v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04412v5",
                "updated": "2025-11-07T06:44:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    44,
                    51,
                    4,
                    311,
                    0
                ],
                "published": "2025-03-06T13:10:40Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    10,
                    40,
                    3,
                    65,
                    0
                ],
                "title": "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive\n  Branching Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive\n  Branching Tree Search"
                },
                "summary": "Recent advances demonstrate that increasing inference-time computation can\nsignificantly boost the reasoning capabilities of large language models (LLMs).\nAlthough repeated sampling (i.e., generating multiple candidate outputs) is a\nhighly effective strategy, it does not leverage external feedback signals for\nrefinement, which are often available in tasks like coding. In this work, we\npropose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel\ninference-time framework that generalizes repeated sampling with principled\nmulti-turn exploration and exploitation. At each node in the search tree,\nAB-MCTS dynamically decides whether to \"go wider\" by expanding new candidate\nresponses or \"go deeper\" by revisiting existing ones based on external feedback\nsignals. We evaluate our method on complex coding and engineering tasks using\nfrontier models. Empirical results show that AB-MCTS consistently outperforms\nboth repeated sampling and standard MCTS, underscoring the importance of\ncombining the response diversity of LLMs with multi-turn solution refinement\nfor effective inference-time scaling. Code is available at\nhttps://github.com/SakanaAI/treequest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances demonstrate that increasing inference-time computation can\nsignificantly boost the reasoning capabilities of large language models (LLMs).\nAlthough repeated sampling (i.e., generating multiple candidate outputs) is a\nhighly effective strategy, it does not leverage external feedback signals for\nrefinement, which are often available in tasks like coding. In this work, we\npropose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel\ninference-time framework that generalizes repeated sampling with principled\nmulti-turn exploration and exploitation. At each node in the search tree,\nAB-MCTS dynamically decides whether to \"go wider\" by expanding new candidate\nresponses or \"go deeper\" by revisiting existing ones based on external feedback\nsignals. We evaluate our method on complex coding and engineering tasks using\nfrontier models. Empirical results show that AB-MCTS consistently outperforms\nboth repeated sampling and standard MCTS, underscoring the importance of\ncombining the response diversity of LLMs with multi-turn solution refinement\nfor effective inference-time scaling. Code is available at\nhttps://github.com/SakanaAI/treequest ."
                },
                "authors": [
                    {
                        "name": "Yuichi Inoue"
                    },
                    {
                        "name": "Kou Misaki"
                    },
                    {
                        "name": "Yuki Imajuku"
                    },
                    {
                        "name": "So Kuroki"
                    },
                    {
                        "name": "Taishi Nakamura"
                    },
                    {
                        "name": "Takuya Akiba"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Akiba"
                },
                "author": "Takuya Akiba",
                "arxiv_comment": "Accepted as a spotlight at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04412v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04412v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05018v1",
                "updated": "2025-11-07T06:43:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    43,
                    1,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T06:43:01Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    43,
                    1,
                    4,
                    311,
                    0
                ],
                "title": "Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to\n  Custom Behavioral Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to\n  Custom Behavioral Policies"
                },
                "summary": "Large language models (LLMs) are typically aligned to a universal set of\nsafety and usage principles intended for broad public acceptability. Yet,\nreal-world applications of LLMs often take place within organizational\necosystems shaped by distinctive corporate policies, regulatory requirements,\nuse cases, brand guidelines, and ethical commitments. This reality highlights\nthe need for rigorous and comprehensive evaluation of LLMs with pluralistic\nalignment goals, an alignment paradigm that emphasizes adaptability to diverse\nuser values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE\n(PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs'\ncapacity to adhere to pluralistic alignment specifications in multi-turn,\ninteractive conversations. PBSUITE consists of (1) a diverse dataset of 300\nrealistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic\nevaluation framework for stress-testing model compliance with custom behavioral\nspecifications under adversarial conditions. Using PBSUITE, We find that\nleading open- and closed-source LLMs maintain robust adherence to behavioral\npolicies in single-turn settings (less than 4% failure rates), but their\ncompliance weakens substantially in multi-turn adversarial interactions (up to\n84% failure rates). These findings highlight that existing model alignment and\nsafety moderation methods fall short in coherently enforcing pluralistic\nbehavioral policies in real-world LLM interactions. Our work contributes both\nthe dataset and analytical framework to support future research toward robust\nand context-aware pluralistic alignment techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically aligned to a universal set of\nsafety and usage principles intended for broad public acceptability. Yet,\nreal-world applications of LLMs often take place within organizational\necosystems shaped by distinctive corporate policies, regulatory requirements,\nuse cases, brand guidelines, and ethical commitments. This reality highlights\nthe need for rigorous and comprehensive evaluation of LLMs with pluralistic\nalignment goals, an alignment paradigm that emphasizes adaptability to diverse\nuser values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE\n(PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs'\ncapacity to adhere to pluralistic alignment specifications in multi-turn,\ninteractive conversations. PBSUITE consists of (1) a diverse dataset of 300\nrealistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic\nevaluation framework for stress-testing model compliance with custom behavioral\nspecifications under adversarial conditions. Using PBSUITE, We find that\nleading open- and closed-source LLMs maintain robust adherence to behavioral\npolicies in single-turn settings (less than 4% failure rates), but their\ncompliance weakens substantially in multi-turn adversarial interactions (up to\n84% failure rates). These findings highlight that existing model alignment and\nsafety moderation methods fall short in coherently enforcing pluralistic\nbehavioral policies in real-world LLM interactions. Our work contributes both\nthe dataset and analytical framework to support future research toward robust\nand context-aware pluralistic alignment techniques."
                },
                "authors": [
                    {
                        "name": "Prasoon Varshney"
                    },
                    {
                        "name": "Makesh Narsimhan Sreedhar"
                    },
                    {
                        "name": "Liwei Jiang"
                    },
                    {
                        "name": "Traian Rebedea"
                    },
                    {
                        "name": "Christopher Parisien"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Parisien"
                },
                "author": "Christopher Parisien",
                "arxiv_comment": "Accepted at the Multi-Turn Interactions workshop at the 39th\n  Conference on Neural Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04270v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04270v4",
                "updated": "2025-11-07T06:42:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    42,
                    8,
                    4,
                    311,
                    0
                ],
                "published": "2025-07-06T07:03:27Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    7,
                    3,
                    27,
                    6,
                    187,
                    0
                ],
                "title": "ZERO: Industry-ready Vision Foundation Model with Multi-modal Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZERO: Industry-ready Vision Foundation Model with Multi-modal Prompts"
                },
                "summary": "Foundation models have revolutionized AI, yet they struggle with zero-shot\ndeployment in real-world industrial settings due to a lack of high-quality,\ndomain-specific datasets. To bridge this gap, Superb AI introduces ZERO, an\nindustry-ready vision foundation model that leverages multi-modal prompting\n(textual and visual) for generalization without retraining. Trained on a\ncompact yet representative 0.9 million annotated samples from a proprietary\nbillion-scale industrial dataset, ZERO demonstrates competitive performance on\nacademic benchmarks like LVIS-Val and significantly outperforms existing models\nacross 37 diverse industrial datasets. Furthermore, ZERO achieved 2nd place in\nthe CVPR 2025 Object Instance Detection Challenge and 4th place in the\nFoundational Few-shot Object Detection Challenge, highlighting its practical\ndeployability and generalizability with minimal adaptation and limited data. To\nthe best of our knowledge, ZERO is the first vision foundation model explicitly\nbuilt for domain-specific, zero-shot industrial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models have revolutionized AI, yet they struggle with zero-shot\ndeployment in real-world industrial settings due to a lack of high-quality,\ndomain-specific datasets. To bridge this gap, Superb AI introduces ZERO, an\nindustry-ready vision foundation model that leverages multi-modal prompting\n(textual and visual) for generalization without retraining. Trained on a\ncompact yet representative 0.9 million annotated samples from a proprietary\nbillion-scale industrial dataset, ZERO demonstrates competitive performance on\nacademic benchmarks like LVIS-Val and significantly outperforms existing models\nacross 37 diverse industrial datasets. Furthermore, ZERO achieved 2nd place in\nthe CVPR 2025 Object Instance Detection Challenge and 4th place in the\nFoundational Few-shot Object Detection Challenge, highlighting its practical\ndeployability and generalizability with minimal adaptation and limited data. To\nthe best of our knowledge, ZERO is the first vision foundation model explicitly\nbuilt for domain-specific, zero-shot industrial applications."
                },
                "authors": [
                    {
                        "name": "Sangbum Choi"
                    },
                    {
                        "name": "Kyeongryeol Go"
                    },
                    {
                        "name": "Taewoong Jang"
                    }
                ],
                "author_detail": {
                    "name": "Taewoong Jang"
                },
                "author": "Taewoong Jang",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04270v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04270v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06980v2",
                "updated": "2025-11-07T06:21:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    21,
                    5,
                    4,
                    311,
                    0
                ],
                "published": "2025-03-10T06:52:35Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    6,
                    52,
                    35,
                    0,
                    69,
                    0
                ],
                "title": "Exploring Multimodal Perception in Large Language Models Through\n  Perceptual Strength Ratings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Multimodal Perception in Large Language Models Through\n  Perceptual Strength Ratings"
                },
                "summary": "This study investigated whether multimodal large language models can achieve\nhuman-like sensory grounding by examining their ability to capture perceptual\nstrength ratings across sensory modalities. We explored how model\ncharacteristics (size, multimodal capabilities, architectural generation)\ninfluence grounding performance, distributional factor dependencies (word\nfrequency, embeddings, feature distances), and human-model processing\ndifferences. We evaluated 21 models from four families (GPT, Gemini, LLaMA,\nQwen) using 3,611 words from the Lancaster Sensorimotor Norms through\ncorrelation, distance metrics, and qualitative analysis. Results showed that\nlarger (6 out of 8 comparisons), multimodal (5 of 7), and newer models (5 of 8)\ngenerally outperformed their smaller, text-based, and older counterparts. Top\nmodels achieved 85-90% accuracy and 0.58-0.65 correlations with human ratings,\ndemonstrating substantial similarity. Moreover, distributional factors showed\nminimal impact, not exceeding human dependency levels. However, despite strong\nalignment, models were not identical to humans, as even top performers showed\ndifferences in distance and correlation measures, with qualitative analysis\nrevealing processing patterns related to absent sensory grounding.\nAdditionally, it remains questionable whether introducing multimodality\nresolves this grounding deficit. Although multimodality improved performance,\nit seems to provide similar information as massive text rather than\nqualitatively different data, as benefits occurred across unrelated sensory\ndimensions and massive text-only models achieved comparable results. Our\nfindings demonstrate that while advanced LLMs can approximate human\nsensory-linguistic associations through statistical learning, they still differ\nfrom human embodied cognition in processing mechanisms, even with multimodal\nintegration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigated whether multimodal large language models can achieve\nhuman-like sensory grounding by examining their ability to capture perceptual\nstrength ratings across sensory modalities. We explored how model\ncharacteristics (size, multimodal capabilities, architectural generation)\ninfluence grounding performance, distributional factor dependencies (word\nfrequency, embeddings, feature distances), and human-model processing\ndifferences. We evaluated 21 models from four families (GPT, Gemini, LLaMA,\nQwen) using 3,611 words from the Lancaster Sensorimotor Norms through\ncorrelation, distance metrics, and qualitative analysis. Results showed that\nlarger (6 out of 8 comparisons), multimodal (5 of 7), and newer models (5 of 8)\ngenerally outperformed their smaller, text-based, and older counterparts. Top\nmodels achieved 85-90% accuracy and 0.58-0.65 correlations with human ratings,\ndemonstrating substantial similarity. Moreover, distributional factors showed\nminimal impact, not exceeding human dependency levels. However, despite strong\nalignment, models were not identical to humans, as even top performers showed\ndifferences in distance and correlation measures, with qualitative analysis\nrevealing processing patterns related to absent sensory grounding.\nAdditionally, it remains questionable whether introducing multimodality\nresolves this grounding deficit. Although multimodality improved performance,\nit seems to provide similar information as massive text rather than\nqualitatively different data, as benefits occurred across unrelated sensory\ndimensions and massive text-only models achieved comparable results. Our\nfindings demonstrate that while advanced LLMs can approximate human\nsensory-linguistic associations through statistical learning, they still differ\nfrom human embodied cognition in processing mechanisms, even with multimodal\nintegration."
                },
                "authors": [
                    {
                        "name": "Jonghyun Lee"
                    },
                    {
                        "name": "Dojun Park"
                    },
                    {
                        "name": "Jiwoo Lee"
                    },
                    {
                        "name": "Hoekeon Choi"
                    },
                    {
                        "name": "Sung-Eun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sung-Eun Lee"
                },
                "author": "Sung-Eun Lee",
                "arxiv_doi": "10.1109/ACCESS.2025.3618700",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3618700",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.06980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in IEEE Access",
                "arxiv_journal_ref": "IEEE Access, vol. 13, pp. 176751-176769, 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.05000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.05000v1",
                "updated": "2025-11-07T06:06:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    6,
                    9,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T06:06:09Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    6,
                    9,
                    4,
                    311,
                    0
                ],
                "title": "Query Generation Pipeline with Enhanced Answerability Assessment for\n  Financial Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query Generation Pipeline with Enhanced Answerability Assessment for\n  Financial Information Retrieval"
                },
                "summary": "As financial applications of large language models (LLMs) gain attention,\naccurate Information Retrieval (IR) remains crucial for reliable AI services.\nHowever, existing benchmarks fail to capture the complex and domain-specific\ninformation needs of real-world banking scenarios. Building domain-specific IR\nbenchmarks is costly and constrained by legal restrictions on using real\ncustomer data. To address these challenges, we propose a systematic methodology\nfor constructing domain-specific IR benchmarks through LLM-based query\ngeneration. As a concrete implementation of this methodology, our pipeline\ncombines single and multi-document query generation with an enhanced and\nreasoning-augmented answerability assessment method, achieving stronger\nalignment with human judgments than prior approaches. Using this methodology,\nwe construct KoBankIR, comprising 815 queries derived from 204 official banking\ndocuments. Our experiments show that existing retrieval models struggle with\nthe complex multi-document queries in KoBankIR, demonstrating the value of our\nsystematic approach for domain-specific benchmark construction and underscoring\nthe need for improved retrieval techniques in financial domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As financial applications of large language models (LLMs) gain attention,\naccurate Information Retrieval (IR) remains crucial for reliable AI services.\nHowever, existing benchmarks fail to capture the complex and domain-specific\ninformation needs of real-world banking scenarios. Building domain-specific IR\nbenchmarks is costly and constrained by legal restrictions on using real\ncustomer data. To address these challenges, we propose a systematic methodology\nfor constructing domain-specific IR benchmarks through LLM-based query\ngeneration. As a concrete implementation of this methodology, our pipeline\ncombines single and multi-document query generation with an enhanced and\nreasoning-augmented answerability assessment method, achieving stronger\nalignment with human judgments than prior approaches. Using this methodology,\nwe construct KoBankIR, comprising 815 queries derived from 204 official banking\ndocuments. Our experiments show that existing retrieval models struggle with\nthe complex multi-document queries in KoBankIR, demonstrating the value of our\nsystematic approach for domain-specific benchmark construction and underscoring\nthe need for improved retrieval techniques in financial domains."
                },
                "authors": [
                    {
                        "name": "Hyunkyu Kim"
                    },
                    {
                        "name": "Yeeun Yoo"
                    },
                    {
                        "name": "Youngjun Kwak"
                    }
                ],
                "author_detail": {
                    "name": "Youngjun Kwak"
                },
                "author": "Youngjun Kwak",
                "arxiv_comment": "Accepted(Oral) by ICAIF 2025. Hyunkyu Kim and Yeeun Yoo contributed\n  equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.05000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.05000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05636v2",
                "updated": "2025-11-07T05:58:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    5,
                    58,
                    26,
                    4,
                    311,
                    0
                ],
                "published": "2025-07-08T03:29:27Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    3,
                    29,
                    27,
                    1,
                    189,
                    0
                ],
                "title": "Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Learning"
                },
                "summary": "Graph learning has rapidly evolved into a critical subfield of machine\nlearning and artificial intelligence (AI). Its development began with early\ngraph-theoretic methods, gaining significant momentum with the advent of graph\nneural networks (GNNs). Over the past decade, progress in scalable\narchitectures, dynamic graph modeling, multimodal learning, generative AI,\nexplainable AI (XAI), and responsible AI has broadened the applicability of\ngraph learning to various challenging environments. Graph learning is\nsignificant due to its ability to model complex, non-Euclidean relationships\nthat traditional machine learning struggles to capture, thus better supporting\nreal-world applications ranging from drug discovery and fraud detection to\nrecommender systems and scientific reasoning. However, challenges like\nscalability, generalization, heterogeneity, interpretability, and\ntrustworthiness must be addressed to unlock its full potential. This survey\nprovides a comprehensive introduction to graph learning, focusing on key\ndimensions including scalable, temporal, multimodal, generative, explainable,\nand responsible graph learning. We review state-of-the-art techniques for\nefficiently handling large-scale graphs, capturing dynamic temporal\ndependencies, integrating heterogeneous data modalities, generating novel graph\nsamples, and enhancing interpretability to foster trust and transparency. We\nalso explore ethical considerations, such as privacy and fairness, to ensure\nresponsible deployment of graph learning models. Additionally, we identify and\ndiscuss emerging topics, highlighting recent integration of graph learning and\nother AI paradigms and offering insights into future directions. This survey\nserves as a valuable resource for researchers and practitioners seeking to\nnavigate the rapidly evolving landscape of graph learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph learning has rapidly evolved into a critical subfield of machine\nlearning and artificial intelligence (AI). Its development began with early\ngraph-theoretic methods, gaining significant momentum with the advent of graph\nneural networks (GNNs). Over the past decade, progress in scalable\narchitectures, dynamic graph modeling, multimodal learning, generative AI,\nexplainable AI (XAI), and responsible AI has broadened the applicability of\ngraph learning to various challenging environments. Graph learning is\nsignificant due to its ability to model complex, non-Euclidean relationships\nthat traditional machine learning struggles to capture, thus better supporting\nreal-world applications ranging from drug discovery and fraud detection to\nrecommender systems and scientific reasoning. However, challenges like\nscalability, generalization, heterogeneity, interpretability, and\ntrustworthiness must be addressed to unlock its full potential. This survey\nprovides a comprehensive introduction to graph learning, focusing on key\ndimensions including scalable, temporal, multimodal, generative, explainable,\nand responsible graph learning. We review state-of-the-art techniques for\nefficiently handling large-scale graphs, capturing dynamic temporal\ndependencies, integrating heterogeneous data modalities, generating novel graph\nsamples, and enhancing interpretability to foster trust and transparency. We\nalso explore ethical considerations, such as privacy and fairness, to ensure\nresponsible deployment of graph learning models. Additionally, we identify and\ndiscuss emerging topics, highlighting recent integration of graph learning and\nother AI paradigms and offering insights into future directions. This survey\nserves as a valuable resource for researchers and practitioners seeking to\nnavigate the rapidly evolving landscape of graph learning."
                },
                "authors": [
                    {
                        "name": "Feng Xia"
                    },
                    {
                        "name": "Ciyuan Peng"
                    },
                    {
                        "name": "Jing Ren"
                    },
                    {
                        "name": "Falih Gozi Febrinanto"
                    },
                    {
                        "name": "Renqiang Luo"
                    },
                    {
                        "name": "Vidya Saikrishna"
                    },
                    {
                        "name": "Shuo Yu"
                    },
                    {
                        "name": "Xiangjie Kong"
                    }
                ],
                "author_detail": {
                    "name": "Xiangjie Kong"
                },
                "author": "Xiangjie Kong",
                "arxiv_doi": "10.1561/2000000137",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1561/2000000137",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.05636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "185 pages",
                "arxiv_journal_ref": "Foundations and Trends in Signal Processing, Vol. 19, No. 4, pp\n  371-551. 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T09, 68R10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; G.2.2; E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10473v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10473v3",
                "updated": "2025-11-07T05:53:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    5,
                    53,
                    48,
                    4,
                    311,
                    0
                ],
                "published": "2025-05-15T16:23:51Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    23,
                    51,
                    3,
                    135,
                    0
                ],
                "title": "ControlGS: Consistent Structural Compression Control for\n  Deployment-Aware Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ControlGS: Consistent Structural Compression Control for\n  Deployment-Aware Gaussian Splatting"
                },
                "summary": "3D Gaussian Splatting (3DGS) is a highly deployable real-time method for\nnovel view synthesis. In practice, it requires a universal, consistent control\nmechanism that adjusts the trade-off between rendering quality and model\ncompression without scene-specific tuning, enabling automated deployment across\ndifferent device performances and communication bandwidths. In this work, we\npresent ControlGS, a control-oriented optimization framework that maps the\ntrade-off between Gaussian count and rendering quality to a continuous,\nscene-agnostic, and highly responsive control axis. Extensive experiments\nacross a wide range of scene scales and types (from small objects to large\noutdoor scenes) demonstrate that, by adjusting a globally unified control\nhyperparameter, ControlGS can flexibly generate models biased toward either\nstructural compactness or high fidelity, regardless of the specific scene scale\nor complexity, while achieving markedly higher rendering quality with the same\nor fewer Gaussians compared to potential competing methods. Project page:\nhttps://zhang-fengdi.github.io/ControlGS/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) is a highly deployable real-time method for\nnovel view synthesis. In practice, it requires a universal, consistent control\nmechanism that adjusts the trade-off between rendering quality and model\ncompression without scene-specific tuning, enabling automated deployment across\ndifferent device performances and communication bandwidths. In this work, we\npresent ControlGS, a control-oriented optimization framework that maps the\ntrade-off between Gaussian count and rendering quality to a continuous,\nscene-agnostic, and highly responsive control axis. Extensive experiments\nacross a wide range of scene scales and types (from small objects to large\noutdoor scenes) demonstrate that, by adjusting a globally unified control\nhyperparameter, ControlGS can flexibly generate models biased toward either\nstructural compactness or high fidelity, regardless of the specific scene scale\nor complexity, while achieving markedly higher rendering quality with the same\nor fewer Gaussians compared to potential competing methods. Project page:\nhttps://zhang-fengdi.github.io/ControlGS/"
                },
                "authors": [
                    {
                        "name": "Fengdi Zhang"
                    },
                    {
                        "name": "Yibao Sun"
                    },
                    {
                        "name": "Hongkun Cao"
                    },
                    {
                        "name": "Ruqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Huang"
                },
                "author": "Ruqi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10473v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10473v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04995v1",
                "updated": "2025-11-07T05:44:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    5,
                    44,
                    15,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T05:44:15Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    5,
                    44,
                    15,
                    4,
                    311,
                    0
                ],
                "title": "Enhancing Public Speaking Skills in Engineering Students Through AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Public Speaking Skills in Engineering Students Through AI"
                },
                "summary": "This research-to-practice full paper was inspired by the persistent challenge\nin effective communication among engineering students. Public speaking is a\nnecessary skill for future engineers as they have to communicate technical\nknowledge with diverse stakeholders. While universities offer courses or\nworkshops, they are unable to offer sustained and personalized training to\nstudents. Providing comprehensive feedback on both verbal and non-verbal\naspects of public speaking is time-intensive, making consistent and\nindividualized assessment impractical. This study integrates research on verbal\nand non-verbal cues in public speaking to develop an AI-driven assessment model\nfor engineering students. Our approach combines speech analysis, computer\nvision, and sentiment detection into a multi-modal AI system that provides\nassessment and feedback. The model evaluates (1) verbal communication (pitch,\nloudness, pacing, intonation), (2) non-verbal communication (facial\nexpressions, gestures, posture), and (3) expressive coherence, a novel\nintegration ensuring alignment between speech and body language. Unlike\nprevious systems that assess these aspects separately, our model fuses multiple\nmodalities to deliver personalized, scalable feedback. Preliminary testing\ndemonstrated that our AI-generated feedback was moderately aligned with expert\nevaluations. Among the state-of-the-art AI models evaluated, all of which were\nLarge Language Models (LLMs), including Gemini and OpenAI models, Gemini Pro\nemerged as the best-performing, showing the strongest agreement with human\nannotators. By eliminating reliance on human evaluators, this AI-driven public\nspeaking trainer enables repeated practice, helping students naturally align\ntheir speech with body language and emotion, crucial for impactful and\nprofessional communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research-to-practice full paper was inspired by the persistent challenge\nin effective communication among engineering students. Public speaking is a\nnecessary skill for future engineers as they have to communicate technical\nknowledge with diverse stakeholders. While universities offer courses or\nworkshops, they are unable to offer sustained and personalized training to\nstudents. Providing comprehensive feedback on both verbal and non-verbal\naspects of public speaking is time-intensive, making consistent and\nindividualized assessment impractical. This study integrates research on verbal\nand non-verbal cues in public speaking to develop an AI-driven assessment model\nfor engineering students. Our approach combines speech analysis, computer\nvision, and sentiment detection into a multi-modal AI system that provides\nassessment and feedback. The model evaluates (1) verbal communication (pitch,\nloudness, pacing, intonation), (2) non-verbal communication (facial\nexpressions, gestures, posture), and (3) expressive coherence, a novel\nintegration ensuring alignment between speech and body language. Unlike\nprevious systems that assess these aspects separately, our model fuses multiple\nmodalities to deliver personalized, scalable feedback. Preliminary testing\ndemonstrated that our AI-generated feedback was moderately aligned with expert\nevaluations. Among the state-of-the-art AI models evaluated, all of which were\nLarge Language Models (LLMs), including Gemini and OpenAI models, Gemini Pro\nemerged as the best-performing, showing the strongest agreement with human\nannotators. By eliminating reliance on human evaluators, this AI-driven public\nspeaking trainer enables repeated practice, helping students naturally align\ntheir speech with body language and emotion, crucial for impactful and\nprofessional communication."
                },
                "authors": [
                    {
                        "name": "Amol Harsh"
                    },
                    {
                        "name": "Brainerd Prince"
                    },
                    {
                        "name": "Siddharth Siddharth"
                    },
                    {
                        "name": "Deepan Raj Prabakar Muthirayan"
                    },
                    {
                        "name": "Kabir S Bhalla"
                    },
                    {
                        "name": "Esraaj Sarkar Gupta"
                    },
                    {
                        "name": "Siddharth Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Sahu"
                },
                "author": "Siddharth Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03279v2",
                "updated": "2025-11-07T05:30:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    5,
                    30,
                    41,
                    4,
                    311,
                    0
                ],
                "published": "2025-07-04T03:55:39Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    3,
                    55,
                    39,
                    4,
                    185,
                    0
                ],
                "title": "Conformal Information Pursuit for Interactively Guiding Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Information Pursuit for Interactively Guiding Large Language\n  Models"
                },
                "summary": "A significant use case of instruction-finetuned Large Language Models (LLMs)\nis to solve question-answering tasks interactively. In this setting, an LLM\nagent is tasked with making a prediction by sequentially querying relevant\ninformation from the user, as opposed to a single-turn conversation. This paper\nexplores sequential querying strategies that aim to minimize the expected\nnumber of queries. One such strategy is Information Pursuit (IP), a greedy\nalgorithm that at each iteration selects the query that maximizes information\ngain or equivalently minimizes uncertainty. However, obtaining accurate\nestimates of mutual information or conditional entropy for LLMs is very\ndifficult in practice due to over- or under-confident LLM proba- bilities,\nwhich leads to suboptimal query selection and predictive performance. To better\nestimate the uncertainty at each iteration, we propose Conformal Information\nPursuit (C-IP), an alternative approach to sequential information gain based on\nconformal prediction sets. More specifically, C-IP leverages a relationship\nbetween prediction sets and conditional entropy at each iteration to estimate\nuncertainty based on the average size of conformal prediction sets. In contrast\nto conditional entropy, we find that conformal prediction sets are a\ndistribution-free and robust method of measuring uncertainty. Experiments with\n20 Questions show that C-IP obtains better predictive performance and shorter\nquery-answer chains compared to previous approaches to IP and uncertainty-based\nchain-of-thought methods. Furthermore, extending to an interactive medical\nsetting between a doctor and a patient on the MediQ dataset, C-IP achieves\ncompetitive performance with direct single-turn prediction while offering\ngreater interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A significant use case of instruction-finetuned Large Language Models (LLMs)\nis to solve question-answering tasks interactively. In this setting, an LLM\nagent is tasked with making a prediction by sequentially querying relevant\ninformation from the user, as opposed to a single-turn conversation. This paper\nexplores sequential querying strategies that aim to minimize the expected\nnumber of queries. One such strategy is Information Pursuit (IP), a greedy\nalgorithm that at each iteration selects the query that maximizes information\ngain or equivalently minimizes uncertainty. However, obtaining accurate\nestimates of mutual information or conditional entropy for LLMs is very\ndifficult in practice due to over- or under-confident LLM proba- bilities,\nwhich leads to suboptimal query selection and predictive performance. To better\nestimate the uncertainty at each iteration, we propose Conformal Information\nPursuit (C-IP), an alternative approach to sequential information gain based on\nconformal prediction sets. More specifically, C-IP leverages a relationship\nbetween prediction sets and conditional entropy at each iteration to estimate\nuncertainty based on the average size of conformal prediction sets. In contrast\nto conditional entropy, we find that conformal prediction sets are a\ndistribution-free and robust method of measuring uncertainty. Experiments with\n20 Questions show that C-IP obtains better predictive performance and shorter\nquery-answer chains compared to previous approaches to IP and uncertainty-based\nchain-of-thought methods. Furthermore, extending to an interactive medical\nsetting between a doctor and a patient on the MediQ dataset, C-IP achieves\ncompetitive performance with direct single-turn prediction while offering\ngreater interpretability."
                },
                "authors": [
                    {
                        "name": "Kwan Ho Ryan Chan"
                    },
                    {
                        "name": "Yuyan Ge"
                    },
                    {
                        "name": "Edgar Dobriban"
                    },
                    {
                        "name": "Hamed Hassani"
                    },
                    {
                        "name": "René Vidal"
                    }
                ],
                "author_detail": {
                    "name": "René Vidal"
                },
                "author": "René Vidal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04989v1",
                "updated": "2025-11-07T05:27:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    5,
                    27,
                    35,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T05:27:35Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    5,
                    27,
                    35,
                    4,
                    311,
                    0
                ],
                "title": "Acquiring Common Chinese Emotional Events Using Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Acquiring Common Chinese Emotional Events Using Large Language Model"
                },
                "summary": "Knowledge about emotional events is an important kind of knowledge which has\nbeen applied to improve the effectiveness of different applications. However,\nemotional events cannot be easily acquired, especially common or generalized\nemotional events that are context-independent. The goal of this paper is to\nobtain common emotional events in Chinese language such as \"win a prize\" and\n\"be criticized\". Our approach begins by collecting a comprehensive list of\nChinese emotional event indicators. Then, we generate emotional events by\nprompting a Chinese large language model (LLM) using these indicators. To\nensure the quality of these emotional events, we train a filter to discard\ninvalid generated results. We also classify these emotional events as being\npositive events and negative events using different techniques. Finally, we\nharvest a total of 102,218 high-quality common emotional events with sentiment\npolarity labels, which is the only large-scale commonsense knowledge base of\nemotional events in Chinese language. Intrinsic evaluation results show that\nthe proposed method in this paper can be effectively used to acquire common\nChinese emotional events. An extrinsic use case also demonstrates the strong\npotential of common emotional events in the field of emotion cause extraction\n(ECE). Related resources including emotional event indicators and emotional\nevents will be released after the publication of this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge about emotional events is an important kind of knowledge which has\nbeen applied to improve the effectiveness of different applications. However,\nemotional events cannot be easily acquired, especially common or generalized\nemotional events that are context-independent. The goal of this paper is to\nobtain common emotional events in Chinese language such as \"win a prize\" and\n\"be criticized\". Our approach begins by collecting a comprehensive list of\nChinese emotional event indicators. Then, we generate emotional events by\nprompting a Chinese large language model (LLM) using these indicators. To\nensure the quality of these emotional events, we train a filter to discard\ninvalid generated results. We also classify these emotional events as being\npositive events and negative events using different techniques. Finally, we\nharvest a total of 102,218 high-quality common emotional events with sentiment\npolarity labels, which is the only large-scale commonsense knowledge base of\nemotional events in Chinese language. Intrinsic evaluation results show that\nthe proposed method in this paper can be effectively used to acquire common\nChinese emotional events. An extrinsic use case also demonstrates the strong\npotential of common emotional events in the field of emotion cause extraction\n(ECE). Related resources including emotional event indicators and emotional\nevents will be released after the publication of this paper."
                },
                "authors": [
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Guangzheng Zhu"
                    },
                    {
                        "name": "Cungen Cao"
                    },
                    {
                        "name": "Jingjing Li"
                    },
                    {
                        "name": "He Li"
                    },
                    {
                        "name": "Xin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Huang"
                },
                "author": "Xin Huang",
                "arxiv_comment": "I am the second author (Guangzheng Zhu) and I am submitting this\n  paper on behalf of all co-authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16395v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16395v3",
                "updated": "2025-11-07T04:33:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    4,
                    33,
                    52,
                    4,
                    311,
                    0
                ],
                "published": "2025-02-23T01:15:50Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    1,
                    15,
                    50,
                    6,
                    54,
                    0
                ],
                "title": "AIRepr: An Analyst-Inspector Framework for Evaluating Reproducibility of\n  LLMs in Data Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIRepr: An Analyst-Inspector Framework for Evaluating Reproducibility of\n  LLMs in Data Science"
                },
                "summary": "Large language models (LLMs) are increasingly used to automate data analysis\nthrough executable code generation. Yet, data science tasks often admit\nmultiple statistically valid solutions, e.g. different modeling strategies,\nmaking it critical to understand the reasoning behind analyses, not just their\noutcomes. While manual review of LLM-generated code can help ensure statistical\nsoundness, it is labor-intensive and requires expertise. A more scalable\napproach is to evaluate the underlying workflows-the logical plans guiding code\ngeneration. However, it remains unclear how to assess whether an LLM-generated\nworkflow supports reproducible implementations.\n  To address this, we present AIRepr, an Analyst-Inspector framework for\nautomatically evaluating and improving the reproducibility of LLM-generated\ndata analysis workflows. Our framework is grounded in statistical principles\nand supports scalable, automated assessment. We introduce two novel\nreproducibility-enhancing prompting strategies and benchmark them against\nstandard prompting across 15 analyst-inspector LLM pairs and 1,032 tasks from\nthree public benchmarks. Our findings show that workflows with higher\nreproducibility also yield more accurate analyses, and that\nreproducibility-enhancing prompts substantially improve both metrics. This work\nprovides a foundation for transparent, reliable, and efficient human-AI\ncollaboration in data science. Our code is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used to automate data analysis\nthrough executable code generation. Yet, data science tasks often admit\nmultiple statistically valid solutions, e.g. different modeling strategies,\nmaking it critical to understand the reasoning behind analyses, not just their\noutcomes. While manual review of LLM-generated code can help ensure statistical\nsoundness, it is labor-intensive and requires expertise. A more scalable\napproach is to evaluate the underlying workflows-the logical plans guiding code\ngeneration. However, it remains unclear how to assess whether an LLM-generated\nworkflow supports reproducible implementations.\n  To address this, we present AIRepr, an Analyst-Inspector framework for\nautomatically evaluating and improving the reproducibility of LLM-generated\ndata analysis workflows. Our framework is grounded in statistical principles\nand supports scalable, automated assessment. We introduce two novel\nreproducibility-enhancing prompting strategies and benchmark them against\nstandard prompting across 15 analyst-inspector LLM pairs and 1,032 tasks from\nthree public benchmarks. Our findings show that workflows with higher\nreproducibility also yield more accurate analyses, and that\nreproducibility-enhancing prompts substantially improve both metrics. This work\nprovides a foundation for transparent, reliable, and efficient human-AI\ncollaboration in data science. Our code is publicly available."
                },
                "authors": [
                    {
                        "name": "Qiuhai Zeng"
                    },
                    {
                        "name": "Claire Jin"
                    },
                    {
                        "name": "Xinyue Wang"
                    },
                    {
                        "name": "Yuhan Zheng"
                    },
                    {
                        "name": "Qunhua Li"
                    }
                ],
                "author_detail": {
                    "name": "Qunhua Li"
                },
                "author": "Qunhua Li",
                "arxiv_comment": "Accepted to 2025 EMNLP findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16395v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16395v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04969v1",
                "updated": "2025-11-07T04:12:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    4,
                    12,
                    2,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T04:12:02Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    4,
                    12,
                    2,
                    4,
                    311,
                    0
                ],
                "title": "Sharing Intelligent Reflecting Surfaces in Multi-Operator Communication\n  Systems for Sustainable 6G Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharing Intelligent Reflecting Surfaces in Multi-Operator Communication\n  Systems for Sustainable 6G Networks"
                },
                "summary": "In this study, we investigate the use of intelligent reflecting surfaces\n(IRSs) in multi-operator communication systems for 6G networks, focusing on\nsustainable and efficient resource management. This research is motivated by\ntwo critical challenges: limited coverage provided by mmWave frequencies and\nhigh infrastructure costs associated with current technologies. IRSs can help\neliminate these issues because they can reflect electromagnetic waves to\nenhance signal propagation, thereby reducing blockages and extending network\ncoverage. However, deploying a separate IRS for each mobile network operator\n(MNO) can result in inefficiencies, redundant infrastructure, potential\nconflicts over placement, and interoperator interference. To address these\nchallenges, in this study, an IRS sharing system is proposed in which multiple\nMNOs collaborate to use a common IRS infrastructure. This approach not only\nenhances network flexibility and reduces costs but also minimizes the effect of\ninteroperator interference. Through numerical analysis, we demonstrate that IRS\nsharing effectively balances performance and fairness among MNOs, outperforming\nMNO-specific deployment methods in multi-MNO scenarios. This study provides\ninsights into the potential of IRS sharing to support sustainable 6G networks,\nthereby contributing to the efficient deployment and operation of\nnext-generation wireless communication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate the use of intelligent reflecting surfaces\n(IRSs) in multi-operator communication systems for 6G networks, focusing on\nsustainable and efficient resource management. This research is motivated by\ntwo critical challenges: limited coverage provided by mmWave frequencies and\nhigh infrastructure costs associated with current technologies. IRSs can help\neliminate these issues because they can reflect electromagnetic waves to\nenhance signal propagation, thereby reducing blockages and extending network\ncoverage. However, deploying a separate IRS for each mobile network operator\n(MNO) can result in inefficiencies, redundant infrastructure, potential\nconflicts over placement, and interoperator interference. To address these\nchallenges, in this study, an IRS sharing system is proposed in which multiple\nMNOs collaborate to use a common IRS infrastructure. This approach not only\nenhances network flexibility and reduces costs but also minimizes the effect of\ninteroperator interference. Through numerical analysis, we demonstrate that IRS\nsharing effectively balances performance and fairness among MNOs, outperforming\nMNO-specific deployment methods in multi-MNO scenarios. This study provides\ninsights into the potential of IRS sharing to support sustainable 6G networks,\nthereby contributing to the efficient deployment and operation of\nnext-generation wireless communication systems."
                },
                "authors": [
                    {
                        "name": "Hiroaki Hashida"
                    },
                    {
                        "name": "Yuichi Kawamoto"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "arxiv_doi": "10.1109/MWC.2025.3599988",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MWC.2025.3599988",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2511.04969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in IEEE Wireless Communications",
                "arxiv_journal_ref": "H. Hashida, Y. Kawamoto and N. Kato, \"Sharing Intelligent\n  Reflecting Surfaces in Multi-Operator Communication Systems for Sustainable\n  6G Networks,\" in IEEE Wireless Communications, 2025",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04964v1",
                "updated": "2025-11-07T03:57:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    3,
                    57,
                    47,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T03:57:47Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    3,
                    57,
                    47,
                    4,
                    311,
                    0
                ],
                "title": "Scientific judgment drifts over time in AI ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific judgment drifts over time in AI ideation"
                },
                "summary": "Scientific discovery begins with ideas, yet evaluating early-stage research\nconcepts is a subtle and subjective human judgment. As large language models\n(LLMs) are increasingly tasked with generating scientific hypotheses, most\nsystems assume that scientists' evaluations form a fixed gold standard, and\nthat scientists' judgments do not change. Here we challenge this assumption. In\na two-wave study with 7,182 ratings from 57 active researchers across six\nscientific departments, each participant repeatedly evaluated a constant\n\"control\" research idea alongside AI-generated ideas. We show that scientists'\nratings of the very same idea systematically drift over time: overall quality\nscores increased by 0.61 points on a 0-10 scale (P = 0.005), and test-retest\nreliability was only moderate across core dimensions of scientific value,\nrevealing systematic temporal drift in perceived idea quality. Yet the internal\nstructure of judgment remained stable, such as the relative importance placed\non originality, feasibility, clarity. We then aligned an LLM-based ideation\nsystem to first-wave human ratings and used it to select new ideas. Although\nalignment improved agreement with Wave-1 evaluations, its apparent gains\ndisappeared once drift in human standards was accounted for. Thus, tuning to a\nfixed human snapshot produced improvements that were transient rather than\npersistent. These findings reveal that human evaluation of scientific ideas is\nnot static but a dynamic process with stable priorities and requires shifting\ncalibration. Treating one-time human ratings as immutable ground truth risks\noverstating progress in AI-assisted ideation and obscuring the challenge of\nco-evolving with changing expert standards. Drift-aware evaluation protocols\nand longitudinal benchmarks may therefore be essential for building AI systems\nthat reliably augment, rather than overfit to, human scientific judgment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific discovery begins with ideas, yet evaluating early-stage research\nconcepts is a subtle and subjective human judgment. As large language models\n(LLMs) are increasingly tasked with generating scientific hypotheses, most\nsystems assume that scientists' evaluations form a fixed gold standard, and\nthat scientists' judgments do not change. Here we challenge this assumption. In\na two-wave study with 7,182 ratings from 57 active researchers across six\nscientific departments, each participant repeatedly evaluated a constant\n\"control\" research idea alongside AI-generated ideas. We show that scientists'\nratings of the very same idea systematically drift over time: overall quality\nscores increased by 0.61 points on a 0-10 scale (P = 0.005), and test-retest\nreliability was only moderate across core dimensions of scientific value,\nrevealing systematic temporal drift in perceived idea quality. Yet the internal\nstructure of judgment remained stable, such as the relative importance placed\non originality, feasibility, clarity. We then aligned an LLM-based ideation\nsystem to first-wave human ratings and used it to select new ideas. Although\nalignment improved agreement with Wave-1 evaluations, its apparent gains\ndisappeared once drift in human standards was accounted for. Thus, tuning to a\nfixed human snapshot produced improvements that were transient rather than\npersistent. These findings reveal that human evaluation of scientific ideas is\nnot static but a dynamic process with stable priorities and requires shifting\ncalibration. Treating one-time human ratings as immutable ground truth risks\noverstating progress in AI-assisted ideation and obscuring the challenge of\nco-evolving with changing expert standards. Drift-aware evaluation protocols\nand longitudinal benchmarks may therefore be essential for building AI systems\nthat reliably augment, rather than overfit to, human scientific judgment."
                },
                "authors": [
                    {
                        "name": "Lingyu Zhang"
                    },
                    {
                        "name": "Mitchell Wang"
                    },
                    {
                        "name": "Boyuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Boyuan Chen"
                },
                "author": "Boyuan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04962v1",
                "updated": "2025-11-07T03:50:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    3,
                    50,
                    52,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T03:50:52Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    3,
                    50,
                    52,
                    4,
                    311,
                    0
                ],
                "title": "Too Good to be Bad: On the Failure of LLMs to Role-Play Villains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Too Good to be Bad: On the Failure of LLMs to Role-Play Villains"
                },
                "summary": "Large Language Models (LLMs) are increasingly tasked with creative\ngeneration, including the simulation of fictional characters. However, their\nability to portray non-prosocial, antagonistic personas remains largely\nunexamined. We hypothesize that the safety alignment of modern LLMs creates a\nfundamental conflict with the task of authentically role-playing morally\nambiguous or villainous characters. To investigate this, we introduce the Moral\nRolePlay benchmark, a new dataset featuring a four-level moral alignment scale\nand a balanced test set for rigorous evaluation. We task state-of-the-art LLMs\nwith role-playing characters from moral paragons to pure villains. Our\nlarge-scale evaluation reveals a consistent, monotonic decline in role-playing\nfidelity as character morality decreases. We find that models struggle most\nwith traits directly antithetical to safety principles, such as ``Deceitful''\nand ``Manipulative'', often substituting nuanced malevolence with superficial\naggression. Furthermore, we demonstrate that general chatbot proficiency is a\npoor predictor of villain role-playing ability, with highly safety-aligned\nmodels performing particularly poorly. Our work provides the first systematic\nevidence of this critical limitation, highlighting a key tension between model\nsafety and creative fidelity. Our benchmark and findings pave the way for\ndeveloping more nuanced, context-aware alignment methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly tasked with creative\ngeneration, including the simulation of fictional characters. However, their\nability to portray non-prosocial, antagonistic personas remains largely\nunexamined. We hypothesize that the safety alignment of modern LLMs creates a\nfundamental conflict with the task of authentically role-playing morally\nambiguous or villainous characters. To investigate this, we introduce the Moral\nRolePlay benchmark, a new dataset featuring a four-level moral alignment scale\nand a balanced test set for rigorous evaluation. We task state-of-the-art LLMs\nwith role-playing characters from moral paragons to pure villains. Our\nlarge-scale evaluation reveals a consistent, monotonic decline in role-playing\nfidelity as character morality decreases. We find that models struggle most\nwith traits directly antithetical to safety principles, such as ``Deceitful''\nand ``Manipulative'', often substituting nuanced malevolence with superficial\naggression. Furthermore, we demonstrate that general chatbot proficiency is a\npoor predictor of villain role-playing ability, with highly safety-aligned\nmodels performing particularly poorly. Our work provides the first systematic\nevidence of this critical limitation, highlighting a key tension between model\nsafety and creative fidelity. Our benchmark and findings pave the way for\ndeveloping more nuanced, context-aware alignment methods."
                },
                "authors": [
                    {
                        "name": "Zihao Yi"
                    },
                    {
                        "name": "Qingxuan Jiang"
                    },
                    {
                        "name": "Ruotian Ma"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Qu Yang"
                    },
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Ying Shen"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Linus"
                    }
                ],
                "author_detail": {
                    "name": "Linus"
                },
                "author": "Linus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.04956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.04956v1",
                "updated": "2025-11-07T03:48:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    3,
                    48,
                    5,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-07T03:48:05Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    3,
                    48,
                    5,
                    4,
                    311,
                    0
                ],
                "title": "ORCHID: Orchestrated Retrieval-Augmented Classification with\n  Human-in-the-Loop Intelligent Decision-Making for High-Risk Property",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORCHID: Orchestrated Retrieval-Augmented Classification with\n  Human-in-the-Loop Intelligent Decision-Making for High-Risk Property"
                },
                "summary": "High-Risk Property (HRP) classification is critical at U.S. Department of\nEnergy (DOE) sites, where inventories include sensitive and often dual-use\nequipment. Compliance must track evolving rules designated by various export\ncontrol policies to make transparent and auditable decisions. Traditional\nexpert-only workflows are time-consuming, backlog-prone, and struggle to keep\npace with shifting regulatory boundaries. We demo ORCHID, a modular agentic\nsystem for HRP classification that pairs retrieval-augmented generation (RAG)\nwith human oversight to produce policy-based outputs that can be audited. Small\ncooperating agents, retrieval, description refiner, classifier, validator, and\nfeedback logger, coordinate via agent-to-agent messaging and invoke tools\nthrough the Model Context Protocol (MCP) for model-agnostic on-premise\noperation. The interface follows an Item to Evidence to Decision loop with\nstep-by-step reasoning, on-policy citations, and append-only audit bundles\n(run-cards, prompts, evidence). In preliminary tests on real HRP cases, ORCHID\nimproves accuracy and traceability over a non-agentic baseline while deferring\nuncertain items to Subject Matter Experts (SMEs). The demonstration shows\nsingle item submission, grounded citations, SME feedback capture, and\nexportable audit artifacts, illustrating a practical path to trustworthy LLM\nassistance in sensitive DOE compliance workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Risk Property (HRP) classification is critical at U.S. Department of\nEnergy (DOE) sites, where inventories include sensitive and often dual-use\nequipment. Compliance must track evolving rules designated by various export\ncontrol policies to make transparent and auditable decisions. Traditional\nexpert-only workflows are time-consuming, backlog-prone, and struggle to keep\npace with shifting regulatory boundaries. We demo ORCHID, a modular agentic\nsystem for HRP classification that pairs retrieval-augmented generation (RAG)\nwith human oversight to produce policy-based outputs that can be audited. Small\ncooperating agents, retrieval, description refiner, classifier, validator, and\nfeedback logger, coordinate via agent-to-agent messaging and invoke tools\nthrough the Model Context Protocol (MCP) for model-agnostic on-premise\noperation. The interface follows an Item to Evidence to Decision loop with\nstep-by-step reasoning, on-policy citations, and append-only audit bundles\n(run-cards, prompts, evidence). In preliminary tests on real HRP cases, ORCHID\nimproves accuracy and traceability over a non-agentic baseline while deferring\nuncertain items to Subject Matter Experts (SMEs). The demonstration shows\nsingle item submission, grounded citations, SME feedback capture, and\nexportable audit artifacts, illustrating a practical path to trustworthy LLM\nassistance in sensitive DOE compliance workflows."
                },
                "authors": [
                    {
                        "name": "Maria Mahbub"
                    },
                    {
                        "name": "Vanessa Lama"
                    },
                    {
                        "name": "Sanjay Das"
                    },
                    {
                        "name": "Brian Starks"
                    },
                    {
                        "name": "Christopher Polchek"
                    },
                    {
                        "name": "Saffell Silvers"
                    },
                    {
                        "name": "Lauren Deck"
                    },
                    {
                        "name": "Prasanna Balaprakash"
                    },
                    {
                        "name": "Tirthankar Ghosal"
                    }
                ],
                "author_detail": {
                    "name": "Tirthankar Ghosal"
                },
                "author": "Tirthankar Ghosal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.04956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.04956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.03929v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.03929v2",
                "updated": "2025-11-07T03:45:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    3,
                    45,
                    7,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-06T00:10:19Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    0,
                    10,
                    19,
                    3,
                    310,
                    0
                ],
                "title": "NVIDIA Nemotron Nano V2 VL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVIDIA Nemotron Nano V2 VL"
                },
                "summary": "We introduce Nemotron Nano V2 VL, the latest model of the Nemotron\nvision-language series designed for strong real-world document understanding,\nlong video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers\nsignificant improvements over our previous model,\nLlama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major\nenhancements in model architecture, datasets, and training recipes. Nemotron\nNano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and\ninnovative token reduction techniques to achieve higher inference throughput in\nlong document and video scenarios. We are releasing model checkpoints in BF16,\nFP8, and FP4 formats and sharing large parts of our datasets, recipes and\ntraining code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Nemotron Nano V2 VL, the latest model of the Nemotron\nvision-language series designed for strong real-world document understanding,\nlong video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers\nsignificant improvements over our previous model,\nLlama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major\nenhancements in model architecture, datasets, and training recipes. Nemotron\nNano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and\ninnovative token reduction techniques to achieve higher inference throughput in\nlong document and video scenarios. We are releasing model checkpoints in BF16,\nFP8, and FP4 formats and sharing large parts of our datasets, recipes and\ntraining code."
                },
                "authors": [
                    {
                        "name": "NVIDIA"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Amala Sanjay Deshmukh"
                    },
                    {
                        "name": "Kateryna Chumachenko"
                    },
                    {
                        "name": "Tuomas Rintamaki"
                    },
                    {
                        "name": "Matthieu Le"
                    },
                    {
                        "name": "Tyler Poon"
                    },
                    {
                        "name": "Danial Mohseni Taheri"
                    },
                    {
                        "name": "Ilia Karmanov"
                    },
                    {
                        "name": "Guilin Liu"
                    },
                    {
                        "name": "Jarno Seppanen"
                    },
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Karan Sapra"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Adi Renduchintala"
                    },
                    {
                        "name": "Charles Wang"
                    },
                    {
                        "name": "Peter Jin"
                    },
                    {
                        "name": "Arushi Goel"
                    },
                    {
                        "name": "Mike Ranzinger"
                    },
                    {
                        "name": "Lukas Voegtle"
                    },
                    {
                        "name": "Philipp Fischer"
                    },
                    {
                        "name": "Timo Roman"
                    },
                    {
                        "name": "Wei Ping"
                    },
                    {
                        "name": "Boxin Wang"
                    },
                    {
                        "name": "Zhuolin Yang"
                    },
                    {
                        "name": "Nayeon Lee"
                    },
                    {
                        "name": "Shaokun Zhang"
                    },
                    {
                        "name": "Fuxiao Liu"
                    },
                    {
                        "name": "Zhiqi Li"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Parth Mannan"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Jane Polak Scowcroft"
                    },
                    {
                        "name": "Tom Balough"
                    },
                    {
                        "name": "Subhashree Radhakrishnan"
                    },
                    {
                        "name": "Paris Zhang"
                    },
                    {
                        "name": "Sean Cha"
                    },
                    {
                        "name": "Ratnesh Kumar"
                    },
                    {
                        "name": "Zaid Pervaiz Bhat"
                    },
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Darragh Hanley"
                    },
                    {
                        "name": "Pritam Biswas"
                    },
                    {
                        "name": "Jesse Oliver"
                    },
                    {
                        "name": "Kevin Vasques"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Duncan Riach"
                    },
                    {
                        "name": "Oluwatobi Olabiyi"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Bilal Kartal"
                    },
                    {
                        "name": "Pritam Gundecha"
                    },
                    {
                        "name": "Khanh Nguyen"
                    },
                    {
                        "name": "Alexandre Milesi"
                    },
                    {
                        "name": "Eugene Khvedchenia"
                    },
                    {
                        "name": "Ran Zilberstein"
                    },
                    {
                        "name": "Ofri Masad"
                    },
                    {
                        "name": "Natan Bagrov"
                    },
                    {
                        "name": "Nave Assaf"
                    },
                    {
                        "name": "Tomer Asida"
                    },
                    {
                        "name": "Daniel Afrimi"
                    },
                    {
                        "name": "Amit Zuker"
                    },
                    {
                        "name": "Netanel Haber"
                    },
                    {
                        "name": "Zhiyu Cheng"
                    },
                    {
                        "name": "Jingyu Xin"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Nik Spirin"
                    },
                    {
                        "name": "Maryam Moosaei"
                    },
                    {
                        "name": "Roman Ageev"
                    },
                    {
                        "name": "Vanshil Atul Shah"
                    },
                    {
                        "name": "Yuting Wu"
                    },
                    {
                        "name": "Daniel Korzekwa"
                    },
                    {
                        "name": "Unnikrishnan Kizhakkemadam Sreekumar"
                    },
                    {
                        "name": "Wanli Jiang"
                    },
                    {
                        "name": "Padmavathy Subramanian"
                    },
                    {
                        "name": "Alejandra Rico"
                    },
                    {
                        "name": "Sandip Bhaskar"
                    },
                    {
                        "name": "Saeid Motiian"
                    },
                    {
                        "name": "Kedi Wu"
                    },
                    {
                        "name": "Annie Surla"
                    },
                    {
                        "name": "Chia-Chih Chen"
                    },
                    {
                        "name": "Hayden Wolff"
                    },
                    {
                        "name": "Matthew Feinberg"
                    },
                    {
                        "name": "Melissa Corpuz"
                    },
                    {
                        "name": "Marek Wawrzos"
                    },
                    {
                        "name": "Eileen Long"
                    },
                    {
                        "name": "Aastha Jhunjhunwala"
                    },
                    {
                        "name": "Paul Hendricks"
                    },
                    {
                        "name": "Farzan Memarian"
                    },
                    {
                        "name": "Benika Hall"
                    },
                    {
                        "name": "Xin-Yu Wang"
                    },
                    {
                        "name": "David Mosallanezhad"
                    },
                    {
                        "name": "Soumye Singhal"
                    },
                    {
                        "name": "Luis Vega"
                    },
                    {
                        "name": "Katherine Cheung"
                    },
                    {
                        "name": "Krzysztof Pawelec"
                    },
                    {
                        "name": "Michael Evans"
                    },
                    {
                        "name": "Katherine Luna"
                    },
                    {
                        "name": "Jie Lou"
                    },
                    {
                        "name": "Erick Galinkin"
                    },
                    {
                        "name": "Akshay Hazare"
                    },
                    {
                        "name": "Kaustubh Purandare"
                    },
                    {
                        "name": "Ann Guan"
                    },
                    {
                        "name": "Anna Warno"
                    },
                    {
                        "name": "Chen Cui"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Shibani Likhite"
                    },
                    {
                        "name": "Seph Mard"
                    },
                    {
                        "name": "Meredith Price"
                    },
                    {
                        "name": "Laya Sleiman"
                    },
                    {
                        "name": "Saori Kaji"
                    },
                    {
                        "name": "Udi Karpas"
                    },
                    {
                        "name": "Kari Briski"
                    },
                    {
                        "name": "Joey Conway"
                    },
                    {
                        "name": "Michael Lightstone"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Jonathen Cohen"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    },
                    {
                        "name": "Andrew Tao"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Catanzaro"
                },
                "author": "Bryan Catanzaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.03929v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.03929v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03740v3",
                "updated": "2025-11-07T03:19:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    3,
                    19,
                    42,
                    4,
                    311,
                    0
                ],
                "published": "2024-10-01T02:43:54Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    2,
                    43,
                    54,
                    1,
                    275,
                    0
                ],
                "title": "LEME: Open Large Language Models for Ophthalmology with Advanced\n  Reasoning and Clinical Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEME: Open Large Language Models for Ophthalmology with Advanced\n  Reasoning and Clinical Validation"
                },
                "summary": "The rising prevalence of eye diseases poses a growing public health burden.\nLarge language models (LLMs) offer a promising path to reduce documentation\nworkload and support clinical decision-making. However, few have been tailored\nfor ophthalmology, and most evaluations focus mainly on knowledge-based QA\nwithout clinically relevant benchmarks or real-world validation. Here, we\npresent LEME, a suite of open-weight LLMs developed through a two-stage\nprocess: (1) instruction tuning on 200,000 samples from clinical guidelines,\ntextbooks, and case reports to enhance reasoning and task-following, and (2)\nreinforcement learning with ~30,000 preference labels to enhance accuracy and\ninformativeness. LEME was evaluated on five curated zero-shot benchmarks\nspanning tasks such as patient QA, consultation, and treatment planning. It\noutperformed all seven baselines (all p < 0.004), exceeding GPT-4o by 3.32%\n(absolute ROUGE-L gain). It was further evaluated on three downstream tasks\nusing deidentified patient data, reviewed by clinicians. In patient QA, LEME\nreceived the highest ratings from attending clinicians in 3 out of 4 criteria,\nwith scores of 4.67 for factuality, 4.77 for specificity, 4.79 for\ncompleteness, and 4.88 for safety (1-5 scale). Its completeness score surpassed\nthat of expert-written answers (4.79 vs. 4.56; p = 0.015). In visual acuity\nextraction, LEME achieved the highest F1, outperforming LLaMA-3 by 14.1% and\nEye-LLaMA by 59.0%. In a pilot evaluation on assessment and treatment planning\nfor diabetic retinopathy, AMD, and glaucoma, LEME received scores of 4.36 for\nfactuality, 4.55 for specificity, 4.42 for completeness, and 4.36 for safety,\napproaching attending-level performance. All models, data, and code will be\nreleased to support further development and clinical translation, laying the\ngroundwork for improved efficiency and patient care",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rising prevalence of eye diseases poses a growing public health burden.\nLarge language models (LLMs) offer a promising path to reduce documentation\nworkload and support clinical decision-making. However, few have been tailored\nfor ophthalmology, and most evaluations focus mainly on knowledge-based QA\nwithout clinically relevant benchmarks or real-world validation. Here, we\npresent LEME, a suite of open-weight LLMs developed through a two-stage\nprocess: (1) instruction tuning on 200,000 samples from clinical guidelines,\ntextbooks, and case reports to enhance reasoning and task-following, and (2)\nreinforcement learning with ~30,000 preference labels to enhance accuracy and\ninformativeness. LEME was evaluated on five curated zero-shot benchmarks\nspanning tasks such as patient QA, consultation, and treatment planning. It\noutperformed all seven baselines (all p < 0.004), exceeding GPT-4o by 3.32%\n(absolute ROUGE-L gain). It was further evaluated on three downstream tasks\nusing deidentified patient data, reviewed by clinicians. In patient QA, LEME\nreceived the highest ratings from attending clinicians in 3 out of 4 criteria,\nwith scores of 4.67 for factuality, 4.77 for specificity, 4.79 for\ncompleteness, and 4.88 for safety (1-5 scale). Its completeness score surpassed\nthat of expert-written answers (4.79 vs. 4.56; p = 0.015). In visual acuity\nextraction, LEME achieved the highest F1, outperforming LLaMA-3 by 14.1% and\nEye-LLaMA by 59.0%. In a pilot evaluation on assessment and treatment planning\nfor diabetic retinopathy, AMD, and glaucoma, LEME received scores of 4.36 for\nfactuality, 4.55 for specificity, 4.42 for completeness, and 4.36 for safety,\napproaching attending-level performance. All models, data, and code will be\nreleased to support further development and clinical translation, laying the\ngroundwork for improved efficiency and patient care"
                },
                "authors": [
                    {
                        "name": "Hyunjae Kim"
                    },
                    {
                        "name": "Xuguang Ai"
                    },
                    {
                        "name": "Sahana Srinivasan"
                    },
                    {
                        "name": "Aidan Gilson"
                    },
                    {
                        "name": "Maxwell B. Singer"
                    },
                    {
                        "name": "Krithi Pushpanathan"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Jungwoo Park"
                    },
                    {
                        "name": "Serina Applebaum"
                    },
                    {
                        "name": "Gabriel Dawei Yang"
                    },
                    {
                        "name": "Minjie Zou"
                    },
                    {
                        "name": "David Ziyou Chen"
                    },
                    {
                        "name": "Ke Zou"
                    },
                    {
                        "name": "Soshian Sarrafpour"
                    },
                    {
                        "name": "Ji Liu"
                    },
                    {
                        "name": "Yu Yin"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Quang Ngoc Nguyen"
                    },
                    {
                        "name": "Erping Long"
                    },
                    {
                        "name": "Peixing Wan"
                    },
                    {
                        "name": "Dianbo Liu"
                    },
                    {
                        "name": "Richard Hintz"
                    },
                    {
                        "name": "W. Jim Zheng"
                    },
                    {
                        "name": "Sophia Y. Wang"
                    },
                    {
                        "name": "Lucila Ohno-Machado"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Ron A. Adelman"
                    },
                    {
                        "name": "Luciano V. Del Priore"
                    },
                    {
                        "name": "Yih-Chung Tham"
                    },
                    {
                        "name": "Qingyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qingyu Chen"
                },
                "author": "Qingyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00879v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00879v3",
                "updated": "2025-11-07T03:17:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    3,
                    17,
                    4,
                    4,
                    311,
                    0
                ],
                "published": "2025-02-02T19:07:13Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    19,
                    7,
                    13,
                    6,
                    33,
                    0
                ],
                "title": "Generating Computational Cognitive Models using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Computational Cognitive Models using Large Language Models"
                },
                "summary": "Computational cognitive models, which formalize theories of cognition, enable\nresearchers to quantify cognitive processes and arbitrate between competing\ntheories by fitting models to behavioral data. Traditionally, these models are\nhandcrafted, which requires significant domain knowledge, coding expertise, and\ntime investment. However, recent advances in machine learning offer solutions\nto these challenges. In particular, Large Language Models (LLMs) have\ndemonstrated remarkable capabilities for in-context pattern recognition,\nleveraging knowledge from diverse domains to solve complex problems, and\ngenerating executable code that can be used to facilitate the generation of\ncognitive models. Building on this potential, we introduce a pipeline for\nGuided generation of Computational Cognitive Models (GeCCo). Given task\ninstructions, participant data, and a template function, GeCCo prompts an LLM\nto propose candidate models, fits proposals to held-out data, and iteratively\nrefines them based on feedback constructed from their predictive performance.\nWe benchmark this approach across four different cognitive domains -- decision\nmaking, learning, planning, and memory -- using three open-source LLMs,\nspanning different model sizes, capacities, and families. On four human\nbehavioral data sets, the LLM generated models that consistently matched or\noutperformed the best domain-specific models from the cognitive science\nliterature. Taken together, our results suggest that LLMs can generate\ncognitive models with conceptually plausible theories that rival -- or even\nsurpass -- the best models from the literature across diverse task domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational cognitive models, which formalize theories of cognition, enable\nresearchers to quantify cognitive processes and arbitrate between competing\ntheories by fitting models to behavioral data. Traditionally, these models are\nhandcrafted, which requires significant domain knowledge, coding expertise, and\ntime investment. However, recent advances in machine learning offer solutions\nto these challenges. In particular, Large Language Models (LLMs) have\ndemonstrated remarkable capabilities for in-context pattern recognition,\nleveraging knowledge from diverse domains to solve complex problems, and\ngenerating executable code that can be used to facilitate the generation of\ncognitive models. Building on this potential, we introduce a pipeline for\nGuided generation of Computational Cognitive Models (GeCCo). Given task\ninstructions, participant data, and a template function, GeCCo prompts an LLM\nto propose candidate models, fits proposals to held-out data, and iteratively\nrefines them based on feedback constructed from their predictive performance.\nWe benchmark this approach across four different cognitive domains -- decision\nmaking, learning, planning, and memory -- using three open-source LLMs,\nspanning different model sizes, capacities, and families. On four human\nbehavioral data sets, the LLM generated models that consistently matched or\noutperformed the best domain-specific models from the cognitive science\nliterature. Taken together, our results suggest that LLMs can generate\ncognitive models with conceptually plausible theories that rival -- or even\nsurpass -- the best models from the literature across diverse task domains."
                },
                "authors": [
                    {
                        "name": "Milena Rmus"
                    },
                    {
                        "name": "Akshay K. Jagadish"
                    },
                    {
                        "name": "Marvin Mathony"
                    },
                    {
                        "name": "Tobias Ludwig"
                    },
                    {
                        "name": "Eric Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Eric Schulz"
                },
                "author": "Eric Schulz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00879v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00879v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26616v2",
                "updated": "2025-11-07T03:12:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    3,
                    12,
                    30,
                    4,
                    311,
                    0
                ],
                "published": "2025-09-30T17:54:25Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    54,
                    25,
                    1,
                    273,
                    0
                ],
                "title": "Black-box Context-free Grammar Inference for Readable & Natural Grammars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Context-free Grammar Inference for Readable & Natural Grammars"
                },
                "summary": "Black-box context-free grammar inference is crucial for program analysis,\nreverse engineering, and security, yet existing tools such as Arvada, TreeVada,\nand Kedavra struggle with scalability, readability, and accuracy on large,\ncomplex languages. We present NatGI, a novel LLM-guided grammar inference\nframework that extends TreeVada's parse tree recovery with three key\ninnovations: bracket-guided bubble exploration, LLM-driven bubble generation\nand non-terminal labeling, and hierarchical delta debugging (HDD) for\nsystematic tree simplification. Bracket-guided exploration leverages syntactic\ncues such as parentheses to propose well-structured grammar fragments, while\nLLM guidance produces meaningful non-terminal names and selects more promising\nmerges. Finally, HDD incrementally reduces unnecessary rules, which makes the\ngrammars both compact and interpretable. In our experiments, we evaluate NatGI\non a comprehensive benchmark suite ranging from small languages to larger ones\nsuch as lua, c, and mysql. Our results show that NatGI consistently outperforms\nstrong baselines in terms of F1 score. On average, NatGI achieves an F1 score\nof 0.57, which is 25pp (percentage points) higher than the best-performing\nbaseline, TreeVada. In the case of interpretability, our generated grammars\nperform significantly better than those produced by existing approaches.\nLeveraging LLM-based node renaming and bubble exploration, NatGI produces rules\nwith meaningful non-terminal names and compact structures that align more\nclosely with human intuition. As a result, developers and researchers can\nachieve higher accuracy while still being able to easily inspect, verify, and\nreason about the structure and semantics of the induced grammars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box context-free grammar inference is crucial for program analysis,\nreverse engineering, and security, yet existing tools such as Arvada, TreeVada,\nand Kedavra struggle with scalability, readability, and accuracy on large,\ncomplex languages. We present NatGI, a novel LLM-guided grammar inference\nframework that extends TreeVada's parse tree recovery with three key\ninnovations: bracket-guided bubble exploration, LLM-driven bubble generation\nand non-terminal labeling, and hierarchical delta debugging (HDD) for\nsystematic tree simplification. Bracket-guided exploration leverages syntactic\ncues such as parentheses to propose well-structured grammar fragments, while\nLLM guidance produces meaningful non-terminal names and selects more promising\nmerges. Finally, HDD incrementally reduces unnecessary rules, which makes the\ngrammars both compact and interpretable. In our experiments, we evaluate NatGI\non a comprehensive benchmark suite ranging from small languages to larger ones\nsuch as lua, c, and mysql. Our results show that NatGI consistently outperforms\nstrong baselines in terms of F1 score. On average, NatGI achieves an F1 score\nof 0.57, which is 25pp (percentage points) higher than the best-performing\nbaseline, TreeVada. In the case of interpretability, our generated grammars\nperform significantly better than those produced by existing approaches.\nLeveraging LLM-based node renaming and bubble exploration, NatGI produces rules\nwith meaningful non-terminal names and compact structures that align more\nclosely with human intuition. As a result, developers and researchers can\nachieve higher accuracy while still being able to easily inspect, verify, and\nreason about the structure and semantics of the induced grammars."
                },
                "authors": [
                    {
                        "name": "Mohammad Rifat Arefin"
                    },
                    {
                        "name": "Shanto Rahman"
                    },
                    {
                        "name": "Christoph Csallner"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Csallner"
                },
                "author": "Christoph Csallner",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q42, 68Q45 (Primary), 68T50 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5; F.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01602v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01602v3",
                "updated": "2025-11-07T03:06:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    3,
                    6,
                    56,
                    4,
                    311,
                    0
                ],
                "published": "2025-11-03T14:04:22Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    14,
                    4,
                    22,
                    0,
                    307,
                    0
                ],
                "title": "L2T-Tune:LLM-Guided Hybrid Database Tuning with LHS and TD3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L2T-Tune:LLM-Guided Hybrid Database Tuning with LHS and TD3"
                },
                "summary": "Configuration tuning is critical for database performance. Although recent\nadvancements in database tuning have shown promising results in throughput and\nlatency improvement, challenges remain. First, the vast knob space makes direct\noptimization unstable and slow to converge. Second, reinforcement learning\npipelines often lack effective warm-start guidance and require long offline\ntraining. Third, transferability is limited: when hardware or workloads change,\nexisting models typically require substantial retraining to recover\nperformance.\n  To address these limitations, we propose L2T-Tune, a new LLM-guided hybrid\ndatabase tuning framework that features a three-stage pipeline: Stage one\nperforms a warm start that simultaneously generates uniform samples across the\nknob space and logs them into a shared pool; Stage two leverages a large\nlanguage model to mine and prioritize tuning hints from manuals and community\ndocuments for rapid convergence. Stage three uses the warm-start sample pool to\nreduce the dimensionality of knobs and state features, then fine-tunes the\nconfiguration with the Twin Delayed Deep Deterministic Policy Gradient\nalgorithm.\n  We conduct experiments on L2T-Tune and the state-of-the-art models. Compared\nwith the best-performing alternative, our approach improves performance by an\naverage of 37.1% across all workloads, and by up to 73% on TPC-C. Compared with\nmodels trained with reinforcement learning, it achieves rapid convergence in\nthe offline tuning stage on a single server. Moreover, during the online tuning\nstage, it only takes 30 steps to achieve best results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Configuration tuning is critical for database performance. Although recent\nadvancements in database tuning have shown promising results in throughput and\nlatency improvement, challenges remain. First, the vast knob space makes direct\noptimization unstable and slow to converge. Second, reinforcement learning\npipelines often lack effective warm-start guidance and require long offline\ntraining. Third, transferability is limited: when hardware or workloads change,\nexisting models typically require substantial retraining to recover\nperformance.\n  To address these limitations, we propose L2T-Tune, a new LLM-guided hybrid\ndatabase tuning framework that features a three-stage pipeline: Stage one\nperforms a warm start that simultaneously generates uniform samples across the\nknob space and logs them into a shared pool; Stage two leverages a large\nlanguage model to mine and prioritize tuning hints from manuals and community\ndocuments for rapid convergence. Stage three uses the warm-start sample pool to\nreduce the dimensionality of knobs and state features, then fine-tunes the\nconfiguration with the Twin Delayed Deep Deterministic Policy Gradient\nalgorithm.\n  We conduct experiments on L2T-Tune and the state-of-the-art models. Compared\nwith the best-performing alternative, our approach improves performance by an\naverage of 37.1% across all workloads, and by up to 73% on TPC-C. Compared with\nmodels trained with reinforcement learning, it achieves rapid convergence in\nthe offline tuning stage on a single server. Moreover, during the online tuning\nstage, it only takes 30 steps to achieve best results."
                },
                "authors": [
                    {
                        "name": "Xinyue Yang"
                    },
                    {
                        "name": "Chen Zheng"
                    },
                    {
                        "name": "Yaoyang Hou"
                    },
                    {
                        "name": "Renhao Zhang"
                    },
                    {
                        "name": "Yinyan Zhang"
                    },
                    {
                        "name": "Yanjun Wu"
                    },
                    {
                        "name": "Heng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Heng Zhang"
                },
                "author": "Heng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01602v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01602v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]