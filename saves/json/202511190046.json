[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.13717v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13717v1",
                "title": "TZ-LLM: Protecting On-Device Large Language Models with Arm TrustZone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TZ-LLM: Protecting On-Device Large Language Models with Arm TrustZone"
                },
                "updated": "2025-11-17T18:59:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    59,
                    20,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13717v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) deployed on mobile devices offer benefits like user privacy and reduced network latency, but introduce a significant security risk: the leakage of proprietary models to end users.\n  To mitigate this risk, we propose a system design for protecting on-device LLMs using Arm Trusted Execution Environment (TEE), TrustZone. Our system addresses two primary challenges: (1) The dilemma between memory efficiency and fast inference (caching model parameters within TEE memory). (2) The lack of efficient and secure Neural Processing Unit (NPU) time-sharing between Rich Execution Environment (REE) and TEE.\n  Our approach incorporates two key innovations. First, we employ pipelined restoration, leveraging the deterministic memory access patterns of LLM inference to prefetch parameters on demand, hiding memory allocation, I/O and decryption latency under computation time. Second, we introduce a co-driver design, creating a minimal data plane NPU driver in the TEE that collaborates with the full-fledged REE driver. This reduces the TEE TCB size and eliminates control plane reinitialization overhead during NPU world switches.\n  We implemented our system on the emerging OpenHarmony OS and the llama.cpp inference framework, and evaluated it with various LLMs on an Arm Rockchip device. Compared to a strawman TEE baseline lacking our optimizations, our system reduces TTFT by up to 90.9% and increases decoding speed by up to 23.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) deployed on mobile devices offer benefits like user privacy and reduced network latency, but introduce a significant security risk: the leakage of proprietary models to end users.\n  To mitigate this risk, we propose a system design for protecting on-device LLMs using Arm Trusted Execution Environment (TEE), TrustZone. Our system addresses two primary challenges: (1) The dilemma between memory efficiency and fast inference (caching model parameters within TEE memory). (2) The lack of efficient and secure Neural Processing Unit (NPU) time-sharing between Rich Execution Environment (REE) and TEE.\n  Our approach incorporates two key innovations. First, we employ pipelined restoration, leveraging the deterministic memory access patterns of LLM inference to prefetch parameters on demand, hiding memory allocation, I/O and decryption latency under computation time. Second, we introduce a co-driver design, creating a minimal data plane NPU driver in the TEE that collaborates with the full-fledged REE driver. This reduces the TEE TCB size and eliminates control plane reinitialization overhead during NPU world switches.\n  We implemented our system on the emerging OpenHarmony OS and the llama.cpp inference framework, and evaluated it with various LLMs on an Arm Rockchip device. Compared to a strawman TEE baseline lacking our optimizations, our system reduces TTFT by up to 90.9% and increases decoding speed by up to 23.2%."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:59:20Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    59,
                    20,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Xunjie Wang"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Zhichao Hua"
                    },
                    {
                        "name": "Jinyu Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinyu Gu"
                },
                "author": "Jinyu Gu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13679v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13679v1",
                "title": "QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention"
                },
                "updated": "2025-11-17T18:34:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    34,
                    4,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13679v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:34:04Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    34,
                    4,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted to DATE 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Hyunwoo Oh"
                    },
                    {
                        "name": "Hanning Chen"
                    },
                    {
                        "name": "Sanggeon Yun"
                    },
                    {
                        "name": "Yang Ni"
                    },
                    {
                        "name": "Wenjun Huang"
                    },
                    {
                        "name": "Tamoghno Das"
                    },
                    {
                        "name": "Suyeon Jang"
                    },
                    {
                        "name": "Mohsen Imani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Imani"
                },
                "author": "Mohsen Imani"
            },
            {
                "id": "http://arxiv.org/abs/2511.13644v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13644v1",
                "title": "CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding"
                },
                "updated": "2025-11-17T17:56:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    56,
                    14,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13644v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:56:14Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    56,
                    14,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shrenik Patel"
                    },
                    {
                        "name": "Daivik Patel"
                    }
                ],
                "author_detail": {
                    "name": "Daivik Patel"
                },
                "author": "Daivik Patel"
            },
            {
                "id": "http://arxiv.org/abs/2511.13587v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13587v1",
                "title": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping"
                },
                "updated": "2025-11-17T16:50:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    50,
                    58,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13587v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:50:58Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    50,
                    58,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Haotian Dong"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Rongwei Lu"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.13412v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13412v1",
                "title": "Microwave-acoustic-driven power electronics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microwave-acoustic-driven power electronics"
                },
                "updated": "2025-11-17T14:25:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    25,
                    37,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13412v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Electrical isolation is critical to ensure safety and minimize electromagnetic interference (EMI), yet existing methods struggle to simultaneously transmit power and signals through a unified channel. Here we demonstrate a mechanically-isolated gate driver based on microwave-frequency surface acoustic wave (SAW) device on lithium niobate that achieves galvanic isolation of 2.75 kV with ultralow isolation capacitance (0.032 pF) over 1.25 mm mechanical propagation length, delivering 13.4 V open-circuit voltage and 44.4 mA short-circuit current. We demonstrate isolated gate driving for a gallium nitride (GaN) high-electron-mobility transistor, achieving a turn-on time of 108.8 ns comparable to commercial drivers and validate its operation in a buck converter. In addition, our SAW device operates over an ultrawide temperature range from 0.5 K (-272.6 °C) to 544 K (271 °C). The microwave-frequency SAW devices offer inherent EMI immunity and potential for heterogeneous integration on multiple semiconductor platforms, enabling compact, high-performance isolated power and signal transmission in advanced power electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrical isolation is critical to ensure safety and minimize electromagnetic interference (EMI), yet existing methods struggle to simultaneously transmit power and signals through a unified channel. Here we demonstrate a mechanically-isolated gate driver based on microwave-frequency surface acoustic wave (SAW) device on lithium niobate that achieves galvanic isolation of 2.75 kV with ultralow isolation capacitance (0.032 pF) over 1.25 mm mechanical propagation length, delivering 13.4 V open-circuit voltage and 44.4 mA short-circuit current. We demonstrate isolated gate driving for a gallium nitride (GaN) high-electron-mobility transistor, achieving a turn-on time of 108.8 ns comparable to commercial drivers and validate its operation in a buck converter. In addition, our SAW device operates over an ultrawide temperature range from 0.5 K (-272.6 °C) to 544 K (271 °C). The microwave-frequency SAW devices offer inherent EMI immunity and potential for heterogeneous integration on multiple semiconductor platforms, enabling compact, high-performance isolated power and signal transmission in advanced power electronics."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:25:37Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    25,
                    37,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Liyang Jin"
                    },
                    {
                        "name": "Zichen Xi"
                    },
                    {
                        "name": "Joseph G. Thomas"
                    },
                    {
                        "name": "Jun Ji"
                    },
                    {
                        "name": "Yuanzhi Zhang"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Yizheng Zhu"
                    },
                    {
                        "name": "Linbo Shao"
                    },
                    {
                        "name": "Liyan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liyan Zhu"
                },
                "author": "Liyan Zhu"
            },
            {
                "id": "http://arxiv.org/abs/2505.20334v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.20334v2",
                "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query"
                },
                "updated": "2025-11-17T13:29:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    29,
                    25,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.20334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.20334v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-24T10:34:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "arxiv_comment": "Accepted by EMNLP 2025 Main",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che"
            },
            {
                "id": "http://arxiv.org/abs/2511.13319v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13319v1",
                "title": "Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs"
                },
                "updated": "2025-11-17T12:56:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    56,
                    33,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13319v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.\n  In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $ε$-local differential privacy ($ε$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.\n  In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $ε$-local differential privacy ($ε$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T12:56:33Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    56,
                    33,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Chelsea McMurray"
                    },
                    {
                        "name": "Hayder Tirmazi"
                    }
                ],
                "author_detail": {
                    "name": "Hayder Tirmazi"
                },
                "author": "Hayder Tirmazi"
            },
            {
                "id": "http://arxiv.org/abs/2506.01215v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.01215v2",
                "title": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers"
                },
                "updated": "2025-11-17T12:29:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    29,
                    7,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.01215v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.01215v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the model's pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 52% and 34% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on Infinite-Bench, RepoEval, and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the model's pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 52% and 34% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on Infinite-Bench, RepoEval, and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-01T23:49:14Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    23,
                    49,
                    14,
                    6,
                    152,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Woomin Song"
                    },
                    {
                        "name": "Sai Muralidhar Jayanthi"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Kanthashree Mysore Sathyendra"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "Aram Galstyan"
                    },
                    {
                        "name": "Shubham Katiyar"
                    },
                    {
                        "name": "Sravan Babu Bodapati"
                    }
                ],
                "author_detail": {
                    "name": "Sravan Babu Bodapati"
                },
                "author": "Sravan Babu Bodapati"
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.06261v4",
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "updated": "2025-11-17T11:11:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    11,
                    28,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.06261v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.06261v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the LLM instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's memory in the concurrent KV cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's memory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the LLM instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's memory in the concurrent KV cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's memory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "arxiv_comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Erik Schultheis"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh"
            },
            {
                "id": "http://arxiv.org/abs/2511.13078v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13078v1",
                "title": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning"
                },
                "updated": "2025-11-17T07:27:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    7,
                    27,
                    52,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13078v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T07:27:52Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    7,
                    27,
                    52,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Liuyi Jin"
                    },
                    {
                        "name": "Pasan Gunawardena"
                    },
                    {
                        "name": "Amran Haroon"
                    },
                    {
                        "name": "Runzhi Wang"
                    },
                    {
                        "name": "Sangwoo Lee"
                    },
                    {
                        "name": "Radu Stoleru"
                    },
                    {
                        "name": "Michael Middleton"
                    },
                    {
                        "name": "Zepeng Huo"
                    },
                    {
                        "name": "Jeeeun Kim"
                    },
                    {
                        "name": "Jason Moats"
                    }
                ],
                "author_detail": {
                    "name": "Jason Moats"
                },
                "author": "Jason Moats"
            },
            {
                "id": "http://arxiv.org/abs/2507.21761v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.21761v3",
                "title": "IMC-Net: A Lightweight Content-Conditioned Encoder with Multi-Pass Processing for Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IMC-Net: A Lightweight Content-Conditioned Encoder with Multi-Pass Processing for Image Classification"
                },
                "updated": "2025-11-17T06:40:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    6,
                    40,
                    18,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.21761v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.21761v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a compact encoder for image categorization that emphasizes computation economy through content-conditioned multi-pass processing. The model employs a single lightweight core block that can be re-applied a small number of times, while a simple score-based selector decides whether further passes are beneficial for each region unit in the feature map. This design provides input-conditioned depth without introducing heavy auxiliary modules or specialized pretraining. On standard benchmarks, the approach attains competitive accuracy with reduced parameters, lower floating-point operations, and faster inference compared to similarly sized baselines. The method keeps the architecture minimal, implements module reuse to control footprint, and preserves stable training via mild regularization on selection scores. We discuss implementation choices for efficient masking, pass control, and representation caching, and show that the multi-pass strategy transfers well to several datasets without requiring task-specific customization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a compact encoder for image categorization that emphasizes computation economy through content-conditioned multi-pass processing. The model employs a single lightweight core block that can be re-applied a small number of times, while a simple score-based selector decides whether further passes are beneficial for each region unit in the feature map. This design provides input-conditioned depth without introducing heavy auxiliary modules or specialized pretraining. On standard benchmarks, the approach attains competitive accuracy with reduced parameters, lower floating-point operations, and faster inference compared to similarly sized baselines. The method keeps the architecture minimal, implements module reuse to control footprint, and preserves stable training via mild regularization on selection scores. We discuss implementation choices for efficient masking, pass control, and representation caching, and show that the multi-pass strategy transfers well to several datasets without requiring task-specific customization."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-29T12:46:36Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    12,
                    46,
                    36,
                    1,
                    210,
                    0
                ],
                "arxiv_comment": "13 pages,6 figuers",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "YiZhou Li"
                    }
                ],
                "author_detail": {
                    "name": "YiZhou Li"
                },
                "author": "YiZhou Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.12979v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12979v1",
                "title": "RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems"
                },
                "updated": "2025-11-17T05:06:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    5,
                    6,
                    47,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12979v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T05:06:47Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    5,
                    6,
                    47,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zhengchao Wang"
                    },
                    {
                        "name": "Yitao Hu"
                    },
                    {
                        "name": "Jianing Ye"
                    },
                    {
                        "name": "Zhuxuan Chang"
                    },
                    {
                        "name": "Jiazheng Yu"
                    },
                    {
                        "name": "Youpeng Deng"
                    },
                    {
                        "name": "Keqiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Keqiu Li"
                },
                "author": "Keqiu Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.00090v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00090v2",
                "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation"
                },
                "updated": "2025-11-17T02:55:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    2,
                    55,
                    48,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00090v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T04:57:26Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    4,
                    57,
                    26,
                    3,
                    303,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 Spotlight",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Huanlin Gao"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Fuyuan Shi"
                    },
                    {
                        "name": "Chao Tan"
                    },
                    {
                        "name": "Zhaoxiang Liu"
                    },
                    {
                        "name": "Fang Zhao"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Shiguo Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shiguo Lian"
                },
                "author": "Shiguo Lian"
            },
            {
                "id": "http://arxiv.org/abs/2511.12876v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12876v1",
                "title": "Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making"
                },
                "updated": "2025-11-17T02:09:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    2,
                    9,
                    18,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12876v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T02:09:18Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    2,
                    9,
                    18,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Extended version of a submission to AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Heyang Ma"
                    },
                    {
                        "name": "Qirui Mi"
                    },
                    {
                        "name": "Qipeng Yang"
                    },
                    {
                        "name": "Zijun Fan"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Haifeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Zhang"
                },
                "author": "Haifeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.06838v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06838v3",
                "title": "P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats"
                },
                "updated": "2025-11-16T22:19:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    16,
                    22,
                    19,
                    39,
                    6,
                    320,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06838v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06838v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The substantial memory bandwidth and computational demands of large language models (LLMs) present critical challenges for efficient inference. To tackle this, the literature has explored heterogeneous systems that combine neural processing units (NPUs) with DRAM-based processing-in-memory (PIM) for LLM acceleration. However, existing high-precision (e.g., FP16) PIM compute units incur significant area and power overhead in DRAM technology, limiting the effective computation throughput. In this paper, we introduce P3-LLM, a novel NPU-PIM integrated accelerator for LLM inference using hybrid numerical formats. Our approach is threefold: First, we propose a flexible mixed-precision quantization scheme, which leverages hybrid numerical formats to quantize different LLM operands with high compression efficiency and minimal accuracy loss. Second, we architect an efficient PIM accelerator for P3-LLM, featuring enhanced compute units to support hybrid numerical formats. Our careful choice of numerical formats allows to co-design low-precision PIM compute units that significantly boost the computation throughput under iso-area constraints. Third, we optimize the low-precision dataflow of different LLM modules by applying operator fusion to minimize the overhead of runtime dequantization. Evaluation on a diverse set of representative LLMs and tasks demonstrates that P3-LLM achieves state-of-the-art accuracy in terms of both KV-cache quantization and weight-activation quantization. Combining the proposed quantization scheme with PIM architecture co-design, P3-LLM yields an average of $4.9\\times$, $2.0\\times$, and $3.4\\times$ speedups over the state-of-the-art LLM accelerators HBM-PIM, Ecco, and Pimba, respectively. Our quantization code is available at https://github.com/yc2367/P3-LLM.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The substantial memory bandwidth and computational demands of large language models (LLMs) present critical challenges for efficient inference. To tackle this, the literature has explored heterogeneous systems that combine neural processing units (NPUs) with DRAM-based processing-in-memory (PIM) for LLM acceleration. However, existing high-precision (e.g., FP16) PIM compute units incur significant area and power overhead in DRAM technology, limiting the effective computation throughput. In this paper, we introduce P3-LLM, a novel NPU-PIM integrated accelerator for LLM inference using hybrid numerical formats. Our approach is threefold: First, we propose a flexible mixed-precision quantization scheme, which leverages hybrid numerical formats to quantize different LLM operands with high compression efficiency and minimal accuracy loss. Second, we architect an efficient PIM accelerator for P3-LLM, featuring enhanced compute units to support hybrid numerical formats. Our careful choice of numerical formats allows to co-design low-precision PIM compute units that significantly boost the computation throughput under iso-area constraints. Third, we optimize the low-precision dataflow of different LLM modules by applying operator fusion to minimize the overhead of runtime dequantization. Evaluation on a diverse set of representative LLMs and tasks demonstrates that P3-LLM achieves state-of-the-art accuracy in terms of both KV-cache quantization and weight-activation quantization. Combining the proposed quantization scheme with PIM architecture co-design, P3-LLM yields an average of $4.9\\times$, $2.0\\times$, and $3.4\\times$ speedups over the state-of-the-art LLM accelerators HBM-PIM, Ecco, and Pimba, respectively. Our quantization code is available at https://github.com/yc2367/P3-LLM.git"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T08:29:34Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    8,
                    29,
                    34,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "Preprint. Under review",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Chao Fang"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Yuheng Wu"
                    },
                    {
                        "name": "Thierry Tambe"
                    },
                    {
                        "name": "Marian Verhelst"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah"
            },
            {
                "id": "http://arxiv.org/abs/2511.12752v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12752v1",
                "title": "Whose Narrative is it Anyway? A KV Cache Manipulation Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whose Narrative is it Anyway? A KV Cache Manipulation Attack"
                },
                "updated": "2025-11-16T19:38:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    16,
                    19,
                    38,
                    28,
                    6,
                    320,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12752v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Key Value(KV) cache is an important component for efficient inference in autoregressive Large Language Models (LLMs), but its role as a representation of the model's internal state makes it a potential target for integrity attacks. This paper introduces \"History Swapping,\" a novel block-level attack that manipulates the KV cache to steer model generation without altering the user-facing prompt. The attack involves overwriting a contiguous segment of the active generation's cache with a precomputed cache from a different topic. We empirically evaluate this method across 324 configurations on the Qwen 3 family of models, analyzing the impact of timing, magnitude, and layer depth of the cache overwrite. Our findings reveal that only full-layer overwrites can successfully hijack the conversation's topic, leading to three distinct behaviors: immediate and persistent topic shift, partial recovery, or a delayed hijack. Furthermore, we observe that high-level structural plans are encoded early in the generation process and local discourse structure is maintained by the final layers of the model. This work demonstrates that the KV cache is a significant vector for security analysis, as it encodes not just context but also topic trajectory and structural planning, making it a powerful interface for manipulating model behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key Value(KV) cache is an important component for efficient inference in autoregressive Large Language Models (LLMs), but its role as a representation of the model's internal state makes it a potential target for integrity attacks. This paper introduces \"History Swapping,\" a novel block-level attack that manipulates the KV cache to steer model generation without altering the user-facing prompt. The attack involves overwriting a contiguous segment of the active generation's cache with a precomputed cache from a different topic. We empirically evaluate this method across 324 configurations on the Qwen 3 family of models, analyzing the impact of timing, magnitude, and layer depth of the cache overwrite. Our findings reveal that only full-layer overwrites can successfully hijack the conversation's topic, leading to three distinct behaviors: immediate and persistent topic shift, partial recovery, or a delayed hijack. Furthermore, we observe that high-level structural plans are encoded early in the generation process and local discourse structure is maintained by the final layers of the model. This work demonstrates that the KV cache is a significant vector for security analysis, as it encodes not just context but also topic trajectory and structural planning, making it a powerful interface for manipulating model behavior."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-16T19:38:28Z",
                "published_parsed": [
                    2025,
                    11,
                    16,
                    19,
                    38,
                    28,
                    6,
                    320,
                    0
                ],
                "arxiv_comment": "7 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Mukkesh Ganesh"
                    },
                    {
                        "name": "Kaushik Iyer"
                    },
                    {
                        "name": "Arun Baalaaji Sankar Ananthan"
                    }
                ],
                "author_detail": {
                    "name": "Arun Baalaaji Sankar Ananthan"
                },
                "author": "Arun Baalaaji Sankar Ananthan"
            },
            {
                "id": "http://arxiv.org/abs/2511.12631v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12631v1",
                "title": "Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation"
                },
                "updated": "2025-11-16T14:52:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    16,
                    14,
                    52,
                    54,
                    6,
                    320,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12631v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-16T14:52:54Z",
                "published_parsed": [
                    2025,
                    11,
                    16,
                    14,
                    52,
                    54,
                    6,
                    320,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yushe Cao"
                    },
                    {
                        "name": "Dianxi Shi"
                    },
                    {
                        "name": "Xing Fu"
                    },
                    {
                        "name": "Xuechao Zou"
                    },
                    {
                        "name": "Haikuo Peng"
                    },
                    {
                        "name": "Xueqi Li"
                    },
                    {
                        "name": "Chun Yu"
                    },
                    {
                        "name": "Junliang Xing"
                    }
                ],
                "author_detail": {
                    "name": "Junliang Xing"
                },
                "author": "Junliang Xing"
            },
            {
                "id": "http://arxiv.org/abs/2511.12286v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12286v1",
                "title": "Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing"
                },
                "updated": "2025-11-15T16:39:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    15,
                    16,
                    39,
                    51,
                    5,
                    319,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12286v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-15T16:39:51Z",
                "published_parsed": [
                    2025,
                    11,
                    15,
                    16,
                    39,
                    51,
                    5,
                    319,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Khyati Kiyawat"
                    },
                    {
                        "name": "Zhenxing Fan"
                    },
                    {
                        "name": "Yasas Seneviratne"
                    },
                    {
                        "name": "Morteza Baradaran"
                    },
                    {
                        "name": "Akhil Shekar"
                    },
                    {
                        "name": "Zihan Xia"
                    },
                    {
                        "name": "Mingu Kang"
                    },
                    {
                        "name": "Kevin Skadron"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Skadron"
                },
                "author": "Kevin Skadron"
            },
            {
                "id": "http://arxiv.org/abs/2511.12201v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12201v1",
                "title": "OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs"
                },
                "updated": "2025-11-15T13:14:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    15,
                    13,
                    14,
                    17,
                    5,
                    319,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12201v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-15T13:14:17Z",
                "published_parsed": [
                    2025,
                    11,
                    15,
                    13,
                    14,
                    17,
                    5,
                    319,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Shaoxuan He"
                    },
                    {
                        "name": "Yuanyu He"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Lequan Lin"
                    },
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Zhaoyang Li"
                    },
                    {
                        "name": "Jiyuan Zhang"
                    },
                    {
                        "name": "Zhenbang Sun"
                    },
                    {
                        "name": "Bohan Zhuang"
                    },
                    {
                        "name": "Qi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Wu"
                },
                "author": "Qi Wu"
            },
            {
                "id": "http://arxiv.org/abs/2511.12136v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12136v1",
                "title": "Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware"
                },
                "updated": "2025-11-15T10:02:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    15,
                    10,
                    2,
                    23,
                    5,
                    319,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12136v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-15T10:02:23Z",
                "published_parsed": [
                    2025,
                    11,
                    15,
                    10,
                    2,
                    23,
                    5,
                    319,
                    0
                ],
                "arxiv_comment": "6 pages, 6 figures, 1 table; code available at https://github.com/karol-jurzec/snn-generator/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Karol C. Jurzec"
                    },
                    {
                        "name": "Tomasz Szydlo"
                    },
                    {
                        "name": "Maciej Wielgosz"
                    }
                ],
                "author_detail": {
                    "name": "Maciej Wielgosz"
                },
                "author": "Maciej Wielgosz"
            },
            {
                "id": "http://arxiv.org/abs/2511.12031v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12031v1",
                "title": "Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding"
                },
                "updated": "2025-11-15T04:49:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    15,
                    4,
                    49,
                    23,
                    5,
                    319,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12031v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-15T04:49:23Z",
                "published_parsed": [
                    2025,
                    11,
                    15,
                    4,
                    49,
                    23,
                    5,
                    319,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Arun Ramachandran"
                    },
                    {
                        "name": "Ramaswamy Govindarajan"
                    },
                    {
                        "name": "Murali Annavaram"
                    },
                    {
                        "name": "Prakash Raghavendra"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Chaoyi Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Chaoyi Jiang"
                },
                "author": "Chaoyi Jiang"
            },
            {
                "id": "http://arxiv.org/abs/2511.11907v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11907v1",
                "title": "KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference"
                },
                "updated": "2025-11-14T22:37:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    22,
                    37,
                    57,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11907v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \\emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size.\n  We present KVSwap, a software framework to break this memory wall by offloading the KV cache to non-volatile secondary storage (disk). KVSwap leverages the observation that only a small, dynamically changing subset of KV entries is critical for generation. It stores the full cache on disk, uses a compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining the generation quality when compared with existing KV cache offloading schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \\emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size.\n  We present KVSwap, a software framework to break this memory wall by offloading the KV cache to non-volatile secondary storage (disk). KVSwap leverages the observation that only a small, dynamically changing subset of KV entries is critical for generation. It stores the full cache on disk, uses a compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining the generation quality when compared with existing KV cache offloading schemes."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T22:37:57Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    22,
                    37,
                    57,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Huawei Zhang"
                    },
                    {
                        "name": "Chunwei Xia"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.03092v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.03092v4",
                "title": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators"
                },
                "updated": "2025-11-14T19:14:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    19,
                    14,
                    59,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.03092v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.03092v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-05T00:38:31Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    0,
                    38,
                    31,
                    2,
                    309,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Nasim Farahini"
                    },
                    {
                        "name": "Evgenii Iuliugin"
                    },
                    {
                        "name": "Magnus Vesterlund"
                    },
                    {
                        "name": "Christian Häggström"
                    },
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Ayush Sachdeva"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Faline Fu"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Ayesha Siddiqua"
                    },
                    {
                        "name": "John Long"
                    },
                    {
                        "name": "Tuowen Zhao"
                    },
                    {
                        "name": "Matheen Musaddiq"
                    },
                    {
                        "name": "Håkan Zeffer"
                    },
                    {
                        "name": "Yun Du"
                    },
                    {
                        "name": "Mingran Wang"
                    },
                    {
                        "name": "Qinghua Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    },
                    {
                        "name": "Raghu Prabhakar"
                    }
                ],
                "author_detail": {
                    "name": "Raghu Prabhakar"
                },
                "author": "Raghu Prabhakar"
            },
            {
                "id": "http://arxiv.org/abs/2511.11519v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11519v1",
                "title": "Experience-Guided Adaptation of Inference-Time Reasoning Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experience-Guided Adaptation of Inference-Time Reasoning Strategies"
                },
                "updated": "2025-11-14T17:45:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    17,
                    45,
                    28,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11519v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T17:45:28Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    17,
                    45,
                    28,
                    4,
                    318,
                    0
                ],
                "arxiv_comment": "29 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Adam Stein"
                    },
                    {
                        "name": "Matthew Trager"
                    },
                    {
                        "name": "Benjamin Bowman"
                    },
                    {
                        "name": "Michael Kleinman"
                    },
                    {
                        "name": "Aditya Chattopadhyay"
                    },
                    {
                        "name": "Wei Xia"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto"
            },
            {
                "id": "http://arxiv.org/abs/2508.07570v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.07570v2",
                "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models"
                },
                "updated": "2025-11-14T15:34:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    15,
                    34,
                    24,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.07570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.07570v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but suffer performance degradation under distribution shifts in downstream tasks, particularly in the absence of labeled data. Test-Time Adaptation (TTA) addresses this challenge by enabling online optimization of VLMs during inference, eliminating the need for annotated data. Cache-based TTA methods exploit historical knowledge by maintaining a dynamic memory cache of low-entropy or high-confidence samples, promoting efficient adaptation to out-of-distribution data. Nevertheless, these methods face two critical challenges: (1) unreliable confidence metrics under significant distribution shifts, resulting in error accumulation within the cache and degraded adaptation performance; and (2) rigid decision boundaries that fail to accommodate substantial distributional variations, leading to suboptimal predictions. To overcome these limitations, we introduce the Adaptive Cache Enhancement (ACE) framework, which constructs a robust cache by selectively storing high-confidence or low-entropy image embeddings per class, guided by dynamic, class-specific thresholds initialized from zero-shot statistics and iteratively refined using an exponential moving average and exploration-augmented updates. This approach enables adaptive, class-wise decision boundaries, ensuring robust and accurate predictions across diverse visual distributions. Extensive experiments on 15 diverse benchmark datasets demonstrate that ACE achieves state-of-the-art performance, delivering superior robustness and generalization compared to existing TTA methods in challenging out-of-distribution scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but suffer performance degradation under distribution shifts in downstream tasks, particularly in the absence of labeled data. Test-Time Adaptation (TTA) addresses this challenge by enabling online optimization of VLMs during inference, eliminating the need for annotated data. Cache-based TTA methods exploit historical knowledge by maintaining a dynamic memory cache of low-entropy or high-confidence samples, promoting efficient adaptation to out-of-distribution data. Nevertheless, these methods face two critical challenges: (1) unreliable confidence metrics under significant distribution shifts, resulting in error accumulation within the cache and degraded adaptation performance; and (2) rigid decision boundaries that fail to accommodate substantial distributional variations, leading to suboptimal predictions. To overcome these limitations, we introduce the Adaptive Cache Enhancement (ACE) framework, which constructs a robust cache by selectively storing high-confidence or low-entropy image embeddings per class, guided by dynamic, class-specific thresholds initialized from zero-shot statistics and iteratively refined using an exponential moving average and exploration-augmented updates. This approach enables adaptive, class-wise decision boundaries, ensuring robust and accurate predictions across diverse visual distributions. Extensive experiments on 15 diverse benchmark datasets demonstrate that ACE achieves state-of-the-art performance, delivering superior robustness and generalization compared to existing TTA methods in challenging out-of-distribution scenarios."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-11T03:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "arxiv_comment": "12 pages, Under review",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Khanh-Binh Nguyen"
                    },
                    {
                        "name": "Phuoc-Nguyen Bui"
                    },
                    {
                        "name": "Hyunseung Choo"
                    },
                    {
                        "name": "Duc Thanh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Duc Thanh Nguyen"
                },
                "author": "Duc Thanh Nguyen"
            },
            {
                "id": "http://arxiv.org/abs/2508.19257v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.19257v3",
                "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models"
                },
                "updated": "2025-11-14T12:35:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    12,
                    35,
                    36,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.19257v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.19257v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\\% vs 68.4\\% baseline), cross-environment validation on SimplerEnv (4.8\\% relative improvement), and 8.7\\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\\% vs 68.4\\% baseline), cross-environment validation on SimplerEnv (4.8\\% relative improvement), and 8.7\\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-15T12:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    3,
                    34,
                    4,
                    227,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026. Camera-ready version",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Chengxuan Li"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Shixin Wu"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan"
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2407.15743v3",
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter Optimization"
                },
                "updated": "2025-11-14T11:01:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    11,
                    1,
                    15,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2407.15743v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2407.15743v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded caching~(CC) and spatial multiplexing gains to enhance communication efficiency. In this paper, we analyze both the achievable degrees of freedom~(DoF) under linear processing constraint and the finite-SNR performance of a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\) transmit antennas communicates with \\(K\\) users, each equipped with \\(G\\) receive antennas. We first demonstrate that the enhanced DoF of \\(\\max_{β, Ω} Ω\\times β\\) is achievable with linear processing, where the number of users \\(Ω\\) served in each transmission is fine-tuned to maximize DoF, and \\(β\\le \\min\\big(G, \\nicefrac{L \\binom{Ω-1}{t}}{\\big(1 + (Ω- t - 1)\\binom{Ω-1}{t}}\\big)\\big)\\) represents the number of parallel streams decoded by each user. Then, we propose a new class of MIMO-CC schemes using a novel scheduling mechanism leveraging maximal multicasting opportunities to maximize delivery rates at given SNR levels while still adhering to linear processing constraints. This new class of schemes is paired with an efficient linear multicast beamformer design, resulting in a more practical, high-performance solution for integrating CC in future MIMO systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded caching~(CC) and spatial multiplexing gains to enhance communication efficiency. In this paper, we analyze both the achievable degrees of freedom~(DoF) under linear processing constraint and the finite-SNR performance of a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\) transmit antennas communicates with \\(K\\) users, each equipped with \\(G\\) receive antennas. We first demonstrate that the enhanced DoF of \\(\\max_{β, Ω} Ω\\times β\\) is achievable with linear processing, where the number of users \\(Ω\\) served in each transmission is fine-tuned to maximize DoF, and \\(β\\le \\min\\big(G, \\nicefrac{L \\binom{Ω-1}{t}}{\\big(1 + (Ω- t - 1)\\binom{Ω-1}{t}}\\big)\\big)\\) represents the number of parallel streams decoded by each user. Then, we propose a new class of MIMO-CC schemes using a novel scheduling mechanism leveraging maximal multicasting opportunities to maximize delivery rates at given SNR levels while still adhering to linear processing constraints. This new class of schemes is paired with an efficient linear multicast beamformer design, resulting in a more practical, high-performance solution for integrating CC in future MIMO systems."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli"
            },
            {
                "id": "http://arxiv.org/abs/2511.11106v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11106v1",
                "title": "AccKV: Towards Efficient Audio-Video LLMs Inference via Adaptive-Focusing and Cross-Calibration KV Cache Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccKV: Towards Efficient Audio-Video LLMs Inference via Adaptive-Focusing and Cross-Calibration KV Cache Optimization"
                },
                "updated": "2025-11-14T09:31:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    9,
                    31,
                    11,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11106v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advancements in Audio-Video Large Language Models (AV-LLMs) have enhanced their capabilities in tasks like audio-visual question answering and multimodal dialog systems. Video and audio introduce an extended temporal dimension, resulting in a larger key-value (KV) cache compared to static image embedding. A naive optimization strategy is to selectively focus on and retain KV caches of audio or video based on task. However, in the experiment, we observed that the attention of AV-LLMs to various modalities in the high layers is not strictly dependent on the task. In higher layers, the attention of AV-LLMs shifts more towards the video modality. In addition, we also found that directly integrating temporal KV of audio and spatial-temporal KV of video may lead to information confusion and significant performance degradation of AV-LLMs. If audio and video are processed indiscriminately, it may also lead to excessive compression or reservation of a certain modality, thereby disrupting the alignment between modalities. To address these challenges, we propose AccKV, an Adaptive-Focusing and Cross-Calibration KV cache optimization framework designed specifically for efficient AV-LLMs inference. Our method is based on layer adaptive focusing technology, selectively focusing on key modalities according to the characteristics of different layers, and enhances the recognition of heavy hitter tokens through attention redistribution. In addition, we propose a Cross-Calibration technique that first integrates inefficient KV caches within the audio and video modalities, and then aligns low-priority modalities with high-priority modalities to selectively evict KV cache of low-priority modalities. The experimental results show that AccKV can significantly improve the computational efficiency of AV-LLMs while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Audio-Video Large Language Models (AV-LLMs) have enhanced their capabilities in tasks like audio-visual question answering and multimodal dialog systems. Video and audio introduce an extended temporal dimension, resulting in a larger key-value (KV) cache compared to static image embedding. A naive optimization strategy is to selectively focus on and retain KV caches of audio or video based on task. However, in the experiment, we observed that the attention of AV-LLMs to various modalities in the high layers is not strictly dependent on the task. In higher layers, the attention of AV-LLMs shifts more towards the video modality. In addition, we also found that directly integrating temporal KV of audio and spatial-temporal KV of video may lead to information confusion and significant performance degradation of AV-LLMs. If audio and video are processed indiscriminately, it may also lead to excessive compression or reservation of a certain modality, thereby disrupting the alignment between modalities. To address these challenges, we propose AccKV, an Adaptive-Focusing and Cross-Calibration KV cache optimization framework designed specifically for efficient AV-LLMs inference. Our method is based on layer adaptive focusing technology, selectively focusing on key modalities according to the characteristics of different layers, and enhances the recognition of heavy hitter tokens through attention redistribution. In addition, we propose a Cross-Calibration technique that first integrates inefficient KV caches within the audio and video modalities, and then aligns low-priority modalities with high-priority modalities to selectively evict KV cache of low-priority modalities. The experimental results show that AccKV can significantly improve the computational efficiency of AV-LLMs while maintaining accuracy."
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T09:31:11Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    9,
                    31,
                    11,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM"
                },
                "authors": [
                    {
                        "name": "Zhonghua Jiang"
                    },
                    {
                        "name": "Kui Chen"
                    },
                    {
                        "name": "Kunxi Li"
                    },
                    {
                        "name": "Keting Yin"
                    },
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Zhaode Wang"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.11031v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11031v1",
                "title": "Accelerating Controllable Generation via Hybrid-grained Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Controllable Generation via Hybrid-grained Cache"
                },
                "updated": "2025-11-14T07:35:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    7,
                    35,
                    50,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11031v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Controllable generative models have been widely used to improve the realism of synthetic visual content. However, such models must handle control conditions and content generation computational requirements, resulting in generally low generation efficiency. To address this issue, we propose a Hybrid-Grained Cache (HGC) approach that reduces computational overhead by adopting cache strategies with different granularities at different computational stages. Specifically, (1) we use a coarse-grained cache (block-level) based on feature reuse to dynamically bypass redundant computations in encoder-decoder blocks between each step of model reasoning. (2) We design a fine-grained cache (prompt-level) that acts within a module, where the fine-grained cache reuses cross-attention maps within consecutive reasoning steps and extends them to the corresponding module computations of adjacent steps. These caches of different granularities can be seamlessly integrated into each computational link of the controllable generation process. We verify the effectiveness of HGC on four benchmark datasets, especially its advantages in balancing generation efficiency and visual quality. For example, on the COCO-Stuff segmentation benchmark, our HGC significantly reduces the computational cost (MACs) by 63% (from 18.22T to 6.70T), while keeping the loss of semantic fidelity (quantized performance degradation) within 1.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable generative models have been widely used to improve the realism of synthetic visual content. However, such models must handle control conditions and content generation computational requirements, resulting in generally low generation efficiency. To address this issue, we propose a Hybrid-Grained Cache (HGC) approach that reduces computational overhead by adopting cache strategies with different granularities at different computational stages. Specifically, (1) we use a coarse-grained cache (block-level) based on feature reuse to dynamically bypass redundant computations in encoder-decoder blocks between each step of model reasoning. (2) We design a fine-grained cache (prompt-level) that acts within a module, where the fine-grained cache reuses cross-attention maps within consecutive reasoning steps and extends them to the corresponding module computations of adjacent steps. These caches of different granularities can be seamlessly integrated into each computational link of the controllable generation process. We verify the effectiveness of HGC on four benchmark datasets, especially its advantages in balancing generation efficiency and visual quality. For example, on the COCO-Stuff segmentation benchmark, our HGC significantly reduces the computational cost (MACs) by 63% (from 18.22T to 6.70T), while keeping the loss of semantic fidelity (quantized performance degradation) within 1.5%."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T07:35:50Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    7,
                    35,
                    50,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Huixia Ben"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shengeng Tang"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao"
            },
            {
                "id": "http://arxiv.org/abs/2511.10991v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10991v1",
                "title": "Rethinking Autoregressive Models for Lossless Image Compression via Hierarchical Parallelism and Progressive Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Autoregressive Models for Lossless Image Compression via Hierarchical Parallelism and Progressive Adaptation"
                },
                "updated": "2025-11-14T06:27:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    6,
                    27,
                    58,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10991v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive (AR) models, the theoretical performance benchmark for learned lossless image compression, are often dismissed as impractical due to prohibitive computational cost. This work re-thinks this paradigm, introducing a framework built on hierarchical parallelism and progressive adaptation that re-establishes pure autoregression as a top-performing and practical solution. Our approach is embodied in the Hierarchical Parallel Autoregressive ConvNet (HPAC), an ultra-lightweight pre-trained model using a hierarchical factorized structure and content-aware convolutional gating to efficiently capture spatial dependencies. We introduce two key optimizations for practicality: Cache-then-Select Inference (CSI), which accelerates coding by eliminating redundant computations, and Adaptive Focus Coding (AFC), which efficiently extends the framework to high bit-depth images. Building on this efficient foundation, our progressive adaptation strategy is realized by Spatially-Aware Rate-Guided Progressive Fine-tuning (SARP-FT). This instance-level strategy fine-tunes the model for each test image by optimizing low-rank adapters on progressively larger, spatially-continuous regions selected via estimated information density. Experiments on diverse datasets (natural, satellite, medical) validate that our method achieves new state-of-the-art compression. Notably, our approach sets a new benchmark in learned lossless compression, showing a carefully designed AR framework can offer significant gains over existing methods with a small parameter count and competitive coding speeds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models, the theoretical performance benchmark for learned lossless image compression, are often dismissed as impractical due to prohibitive computational cost. This work re-thinks this paradigm, introducing a framework built on hierarchical parallelism and progressive adaptation that re-establishes pure autoregression as a top-performing and practical solution. Our approach is embodied in the Hierarchical Parallel Autoregressive ConvNet (HPAC), an ultra-lightweight pre-trained model using a hierarchical factorized structure and content-aware convolutional gating to efficiently capture spatial dependencies. We introduce two key optimizations for practicality: Cache-then-Select Inference (CSI), which accelerates coding by eliminating redundant computations, and Adaptive Focus Coding (AFC), which efficiently extends the framework to high bit-depth images. Building on this efficient foundation, our progressive adaptation strategy is realized by Spatially-Aware Rate-Guided Progressive Fine-tuning (SARP-FT). This instance-level strategy fine-tunes the model for each test image by optimizing low-rank adapters on progressively larger, spatially-continuous regions selected via estimated information density. Experiments on diverse datasets (natural, satellite, medical) validate that our method achieves new state-of-the-art compression. Notably, our approach sets a new benchmark in learned lossless compression, showing a carefully designed AR framework can offer significant gains over existing methods with a small parameter count and competitive coding speeds."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T06:27:58Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    6,
                    27,
                    58,
                    4,
                    318,
                    0
                ],
                "arxiv_comment": "15 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Wenbo Zhao"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianming Liu"
                },
                "author": "Xianming Liu"
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v8",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.16242v8",
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising 1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising 1-Consistency"
                },
                "updated": "2025-11-14T03:18:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    3,
                    18,
                    36,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.16242v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.16242v8",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce excessive computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only O(1) additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce excessive computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only O(1) additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "arxiv_comment": "Accepted to NeurIPS 2025. https://neurips.cc/virtual/2025/poster/116615",
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng"
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.00625v2",
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "updated": "2025-11-14T01:23:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    14,
                    1,
                    23,
                    14,
                    4,
                    318,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.00625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.00625v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present NetGent, an AI-agent framework for automating complex application workflows to generate realistic network traffic datasets. Developing generalizable ML models for networking requires data collection from network environments with traffic that results from a diverse set of real-world web applications. However, using existing browser automation tools that are diverse, repeatable, realistic, and efficient remains fragile and costly. NetGent addresses this challenge by allowing users to specify workflows as natural-language rules that define state-dependent actions. These abstract specifications are compiled into nondeterministic finite automata (NFAs), which a state synthesis component translates into reusable, executable code. This design enables deterministic replay, reduces redundant LLM calls through state caching, and adapts quickly when application interfaces change. In experiments, NetGent automated more than 50+ workflows spanning video-on-demand streaming, live video streaming, video conferencing, social media, and web scraping, producing realistic traffic traces while remaining robust to UI variability. By combining the flexibility of language-based agents with the reliability of compiled execution, NetGent provides a scalable foundation for generating the diverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application workflows to generate realistic network traffic datasets. Developing generalizable ML models for networking requires data collection from network environments with traffic that results from a diverse set of real-world web applications. However, using existing browser automation tools that are diverse, repeatable, realistic, and efficient remains fragile and costly. NetGent addresses this challenge by allowing users to specify workflows as natural-language rules that define state-dependent actions. These abstract specifications are compiled into nondeterministic finite automata (NFAs), which a state synthesis component translates into reusable, executable code. This design enables deterministic replay, reduces redundant LLM calls through state caching, and adapts quickly when application interfaces change. In experiments, NetGent automated more than 50+ workflows spanning video-on-demand streaming, live video streaming, video conferencing, social media, and web scraping, producing realistic traffic traces while remaining robust to UI variability. By combining the flexibility of language-based agents with the reliability of compiled execution, NetGent provides a scalable foundation for generating the diverse, repeatable datasets needed to advance ML in networking."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta"
            },
            {
                "id": "http://arxiv.org/abs/2508.16166v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.16166v2",
                "title": "Terahertz third-harmonic generation of lightwave driven Weyl fermions far from equilibrium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz third-harmonic generation of lightwave driven Weyl fermions far from equilibrium"
                },
                "updated": "2025-11-13T15:44:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    44,
                    30,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.16166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.16166v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We report on time-resolved ultrafast terahertz third-harmonic generation spectroscopy of nonequilibrium dynamics of Weyl fermions in a nanometer thin film of the Weyl semimetal TaP. Terahertz third-harmonic generation is observed at room temperature under the drive of a multicycle narrowband terahertz pulse with a peak field strength of down to tens of kV/cm. The observed terahertz third-harmonic generation exhibits a perturbative cubic power-law dependence on the terahertz drive. By varying the polarization of the drive pulse from linear to elliptical, we realize a sensitive tuning of the third harmonic yield. By carrying out theoretical analysis based on the Boltzmann transport theory, we can properly describe the experimental results and ascribe the observed THz nonlinearity to field-driven kinetics of the Weyl fermions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on time-resolved ultrafast terahertz third-harmonic generation spectroscopy of nonequilibrium dynamics of Weyl fermions in a nanometer thin film of the Weyl semimetal TaP. Terahertz third-harmonic generation is observed at room temperature under the drive of a multicycle narrowband terahertz pulse with a peak field strength of down to tens of kV/cm. The observed terahertz third-harmonic generation exhibits a perturbative cubic power-law dependence on the terahertz drive. By varying the polarization of the drive pulse from linear to elliptical, we realize a sensitive tuning of the third harmonic yield. By carrying out theoretical analysis based on the Boltzmann transport theory, we can properly describe the experimental results and ascribe the observed THz nonlinearity to field-driven kinetics of the Weyl fermions."
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-22T07:42:10Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    42,
                    10,
                    4,
                    234,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el"
                },
                "authors": [
                    {
                        "name": "Patrick Pilch"
                    },
                    {
                        "name": "Changqing Zhu"
                    },
                    {
                        "name": "Sergey Kovalev"
                    },
                    {
                        "name": "Renato M. A. Dantas"
                    },
                    {
                        "name": "Amilcar Bedoya-Pinto"
                    },
                    {
                        "name": "Stuart S. P. Parkin"
                    },
                    {
                        "name": "Zhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Wang"
                },
                "author": "Zhe Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.10394v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10394v1",
                "title": "LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components"
                },
                "updated": "2025-11-13T15:14:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    14,
                    34,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10394v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The health condition of wind turbine (WT) components is crucial for ensuring stable and reliable operation. However, existing fault detection methods are largely limited to visual recognition, producing structured outputs that lack semantic interpretability and fail to support maintenance decision-making. To address these limitations, this study proposes an integrated framework that combines YOLOMS with a large language model (LLM) for intelligent fault analysis and diagnosis. Specifically, YOLOMS employs multi-scale detection and sliding-window cropping to enhance fault feature extraction, while a lightweight key-value (KV) mapping module bridges the gap between visual outputs and textual inputs. This module converts YOLOMS detection results into structured textual representations enriched with both qualitative and quantitative attributes. A domain-tuned LLM then performs semantic reasoning to generate interpretable fault analyses and maintenance recommendations. Experiments on real-world datasets demonstrate that the proposed framework achieves a fault detection accuracy of 90.6\\% and generates maintenance reports with an average accuracy of 89\\%, thereby improving the interpretability of diagnostic results and providing practical decision support for the operation and maintenance of wind turbines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The health condition of wind turbine (WT) components is crucial for ensuring stable and reliable operation. However, existing fault detection methods are largely limited to visual recognition, producing structured outputs that lack semantic interpretability and fail to support maintenance decision-making. To address these limitations, this study proposes an integrated framework that combines YOLOMS with a large language model (LLM) for intelligent fault analysis and diagnosis. Specifically, YOLOMS employs multi-scale detection and sliding-window cropping to enhance fault feature extraction, while a lightweight key-value (KV) mapping module bridges the gap between visual outputs and textual inputs. This module converts YOLOMS detection results into structured textual representations enriched with both qualitative and quantitative attributes. A domain-tuned LLM then performs semantic reasoning to generate interpretable fault analyses and maintenance recommendations. Experiments on real-world datasets demonstrate that the proposed framework achieves a fault detection accuracy of 90.6\\% and generates maintenance reports with an average accuracy of 89\\%, thereby improving the interpretability of diagnostic results and providing practical decision support for the operation and maintenance of wind turbines."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:14:34Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    14,
                    34,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "Journal resubmission",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yaru Li"
                    },
                    {
                        "name": "Yanxue Wang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Xinming Li"
                    },
                    {
                        "name": "Jianbo Feng"
                    }
                ],
                "author_detail": {
                    "name": "Jianbo Feng"
                },
                "author": "Jianbo Feng"
            },
            {
                "id": "http://arxiv.org/abs/2511.05534v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05534v2",
                "title": "FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference"
                },
                "updated": "2025-11-13T14:25:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    14,
                    25,
                    8,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05534v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05534v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-29T13:20:16Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    20,
                    16,
                    2,
                    302,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kunxi Li"
                    },
                    {
                        "name": "Yufan Xiong"
                    },
                    {
                        "name": "Zhonghua Jiang"
                    },
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Zhaode Wang"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.19248v2",
                "title": "Reconfigurable Intelligent Surface-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surface-Assisted Multiple-Antenna Coded Caching"
                },
                "updated": "2025-11-13T11:36:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    11,
                    36,
                    29,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.19248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.19248v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reconfigurable Intelligent Surface (RIS) has emerged as a promising technology to enhance the wireless propagation environment for next-generation wireless communication systems. This paper introduces a new RIS-assisted multiple-antenna coded caching problem. Unlike the existing multi-antenna coded caching models, our considered model incorporates a passive RIS with a limited number of elements aimed at enhancing the multicast gain (i.e., Degrees of Freedom (DoF)). The system consists of a server equipped with multiple antennas and several single-antenna users. The RIS, which functions as a passive and configurable relay, improves communication by selectively erasing certain transmission paths between transmit and receive antennas, thereby reducing interference. We first propose a new RIS-assisted interference nulling algorithm to determine the phase-shift coefficients of the RIS. This algorithm achieves faster convergence compared to the existing approach. By strategically nulling certain interference paths in each time slot, the transmission process is divided into multiple interference-free groups. Each group consists of a set of transmit antennas that serve a corresponding set of users without any interference from other groups. The optimal grouping strategy to maximize the DoF is formulated as a combinatorial optimization problem. To efficiently solve this, we design a low-complexity algorithm that identifies the optimal solution and develops a corresponding coded caching scheme to achieve the maximum DoF. Building on the optimal grouping strategy, we introduce a new framework, referred to as RIS-assisted Multiple-Antenna Placement Delivery Array (RMAPDA), to construct the cache placement and delivery phases. Then we propose a general RMAPDA design to achieve the maximum DoF under the optimal grouping strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surface (RIS) has emerged as a promising technology to enhance the wireless propagation environment for next-generation wireless communication systems. This paper introduces a new RIS-assisted multiple-antenna coded caching problem. Unlike the existing multi-antenna coded caching models, our considered model incorporates a passive RIS with a limited number of elements aimed at enhancing the multicast gain (i.e., Degrees of Freedom (DoF)). The system consists of a server equipped with multiple antennas and several single-antenna users. The RIS, which functions as a passive and configurable relay, improves communication by selectively erasing certain transmission paths between transmit and receive antennas, thereby reducing interference. We first propose a new RIS-assisted interference nulling algorithm to determine the phase-shift coefficients of the RIS. This algorithm achieves faster convergence compared to the existing approach. By strategically nulling certain interference paths in each time slot, the transmission process is divided into multiple interference-free groups. Each group consists of a set of transmit antennas that serve a corresponding set of users without any interference from other groups. The optimal grouping strategy to maximize the DoF is formulated as a combinatorial optimization problem. To efficiently solve this, we design a low-complexity algorithm that identifies the optimal solution and develops a corresponding coded caching scheme to achieve the maximum DoF. Building on the optimal grouping strategy, we introduce a new framework, referred to as RIS-assisted Multiple-Antenna Placement Delivery Array (RMAPDA), to construct the cache placement and delivery phases. Then we propose a general RMAPDA design to achieve the maximum DoF under the optimal grouping strategy."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "arxiv_comment": "Submitted to IEEE Trans. Information Theory, 40 pages",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire"
            },
            {
                "id": "http://arxiv.org/abs/2511.10116v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10116v1",
                "title": "Lanthanides-Based Nanoparticles Conjugated with Rose Bengal for FRET-Mediated X-Ray-Induced PDT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lanthanides-Based Nanoparticles Conjugated with Rose Bengal for FRET-Mediated X-Ray-Induced PDT"
                },
                "updated": "2025-11-13T09:20:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    9,
                    20,
                    38,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10116v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In order to find a good candidate for F{ö}rster Resonance Energy Transfer (FRET)-mediated X-ray-induced photodynamic therapy (X-PDT) for the treatment of cancer, lanthanide (Ln)-based AGuIX nanoparticles (NPs) conjugated with Rose Bengal (RB) as a photosensitizer (PS) were synthesized. X-PDT overcomes the problem of the poor penetration of visible light into tissues, which limits the efficacy of PDT in the treatment of deep-seated tumors. It is essential to optimize FRET efficiency by maximizing the overlap integral between donor emission and acceptor absorption and lengthening the duration of the donor emission. In this study, we optimized energy transfer between a scintillator (Sc) as a donor and a PS as an acceptor. Terbium (Tb) and Gadolinium (Gd) as Scs and Rose RB as a PS were chosen. The study of energy transfer between Tb, Gd and RB in solution and chelated on AGuIX NPs proved to be FRET-like. RB was conjugated directly onto AGuIX NPs (i.e., AGuIX Ln@RB), and the use of a spacer arm (i.e., AGuIX Ln@spacer arm-RB) increased FRET efficiency. Singlet oxygen production by these NPs was observed under UV--visible illumination and X-ray irradiation. The in vitro bioassay demonstrated 52% cell death of U-251MG derived from human malignant glioblastoma multiforme at a concentration of 1 $μ$M RB after illumination and irradiation (2 Gy, 320 kV, 10 mA, 3 Gy/min at 47 cm). In addition, the RB-coupled NRP-1-targeting peptide (i.e., K(RB)DKPPR) was conjugated onto AGuIX NPs by a thiol-maleimide click chemistry reaction, and an affinity in the nM range was observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to find a good candidate for F{ö}rster Resonance Energy Transfer (FRET)-mediated X-ray-induced photodynamic therapy (X-PDT) for the treatment of cancer, lanthanide (Ln)-based AGuIX nanoparticles (NPs) conjugated with Rose Bengal (RB) as a photosensitizer (PS) were synthesized. X-PDT overcomes the problem of the poor penetration of visible light into tissues, which limits the efficacy of PDT in the treatment of deep-seated tumors. It is essential to optimize FRET efficiency by maximizing the overlap integral between donor emission and acceptor absorption and lengthening the duration of the donor emission. In this study, we optimized energy transfer between a scintillator (Sc) as a donor and a PS as an acceptor. Terbium (Tb) and Gadolinium (Gd) as Scs and Rose RB as a PS were chosen. The study of energy transfer between Tb, Gd and RB in solution and chelated on AGuIX NPs proved to be FRET-like. RB was conjugated directly onto AGuIX NPs (i.e., AGuIX Ln@RB), and the use of a spacer arm (i.e., AGuIX Ln@spacer arm-RB) increased FRET efficiency. Singlet oxygen production by these NPs was observed under UV--visible illumination and X-ray irradiation. The in vitro bioassay demonstrated 52% cell death of U-251MG derived from human malignant glioblastoma multiforme at a concentration of 1 $μ$M RB after illumination and irradiation (2 Gy, 320 kV, 10 mA, 3 Gy/min at 47 cm). In addition, the RB-coupled NRP-1-targeting peptide (i.e., K(RB)DKPPR) was conjugated onto AGuIX NPs by a thiol-maleimide click chemistry reaction, and an affinity in the nM range was observed."
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T09:20:38Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    9,
                    20,
                    38,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph"
                },
                "arxiv_journal_ref": "Pharmaceuticals, 2025, 18 (5), pp.672",
                "authors": [
                    {
                        "name": "Batoul Dhaini"
                    },
                    {
                        "name": "Joël Daouk"
                    },
                    {
                        "name": "Hervé Schohn"
                    },
                    {
                        "name": "Philippe Arnoux"
                    },
                    {
                        "name": "Valérie Jouan-Hureaux"
                    },
                    {
                        "name": "Albert Moussaron"
                    },
                    {
                        "name": "Agnès Hagege"
                    },
                    {
                        "name": "Mathilde Achard"
                    },
                    {
                        "name": "Samir Acherar"
                    },
                    {
                        "name": "Tayssir Hamieh"
                    },
                    {
                        "name": "Céline Frochot"
                    }
                ],
                "author_detail": {
                    "name": "Céline Frochot"
                },
                "arxiv_affiliation": "LRGP",
                "author": "Céline Frochot"
            },
            {
                "id": "http://arxiv.org/abs/2511.09956v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09956v1",
                "title": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction"
                },
                "updated": "2025-11-13T04:37:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    13,
                    4,
                    37,
                    52,
                    3,
                    317,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09956v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T04:37:52Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    4,
                    37,
                    52,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Mani Tofigh"
                    },
                    {
                        "name": "Edward Guo"
                    },
                    {
                        "name": "Weiwei Jia"
                    },
                    {
                        "name": "Xiaoning Ding"
                    },
                    {
                        "name": "Jianchen Shan"
                    }
                ],
                "author_detail": {
                    "name": "Jianchen Shan"
                },
                "author": "Jianchen Shan"
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.17741v2",
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments"
                },
                "updated": "2025-11-12T22:17:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    22,
                    17,
                    44,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.17741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.17741v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3725843.3756083",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their deployment, imposing substantial demands on inference clusters. These clusters must handle numerous concurrent queries for different LLM downstream tasks. To handle multi-task settings with vast LLM parameter counts, methods like Low-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most of the base LLM model across tasks. Hence, they allow concurrent task serving with minimal memory requirements. However, existing LLM serving systems face inefficiencies: they overlook workload heterogeneity, impose high link bandwidth from frequent adapter loading, and suffer from head-of-line blocking in their schedulers. To address these challenges, we present Chameleon, a novel LLM serving system optimized for many adapter environments, that relies on two core ideas: adapter caching and adapter-aware scheduling. First, Chameleon caches popular adapters in GPU memory, minimizing the adapter loading times. Importantly, it uses the otherwise idle GPU memory, avoiding extra memory costs. Second, Chameleon uses a non-preemptive multi-queue scheduling to efficiently account for workload heterogeneity. In this way, Chameleon simultaneously prevents head of line blocking and starvation. We implement Chameleon on top of a state-of-the-art LLM serving platform and evaluate it with real-world production traces and open-source LLMs. Under high loads, Chameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively, while improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their deployment, imposing substantial demands on inference clusters. These clusters must handle numerous concurrent queries for different LLM downstream tasks. To handle multi-task settings with vast LLM parameter counts, methods like Low-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most of the base LLM model across tasks. Hence, they allow concurrent task serving with minimal memory requirements. However, existing LLM serving systems face inefficiencies: they overlook workload heterogeneity, impose high link bandwidth from frequent adapter loading, and suffer from head-of-line blocking in their schedulers. To address these challenges, we present Chameleon, a novel LLM serving system optimized for many adapter environments, that relies on two core ideas: adapter caching and adapter-aware scheduling. First, Chameleon caches popular adapters in GPU memory, minimizing the adapter loading times. Importantly, it uses the otherwise idle GPU memory, avoiding extra memory costs. Second, Chameleon uses a non-preemptive multi-queue scheduling to efficiently account for workload heterogeneity. In this way, Chameleon simultaneously prevents head of line blocking and starvation. We implement Chameleon on top of a state-of-the-art LLM serving platform and evaluate it with real-world production traces and open-source LLMs. Under high loads, Chameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively, while improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "arxiv_comment": "Accepted at MICRO '25",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "arxiv_journal_ref": "MICRO 58, 2025, 15",
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "arxiv_doi": "10.1145/3725843.3756083"
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.15212v3",
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning"
                },
                "updated": "2025-11-12T16:02:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    16,
                    2,
                    3,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.15212v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.15212v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "arxiv_comment": "accepted to AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.09052v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09052v1",
                "title": "Efficient Distributed Exact Subgraph Matching via GNN-PE: Load Balancing, Cache Optimization, and Query Plan Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Distributed Exact Subgraph Matching via GNN-PE: Load Balancing, Cache Optimization, and Query Plan Ranking"
                },
                "updated": "2025-11-12T07:06:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    7,
                    6,
                    33,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09052v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Exact subgraph matching on large-scale graphs remains a challenging problem due to high computational complexity and distributed system constraints. Existing GNN-based path embedding (GNN-PE) frameworks achieve efficient exact matching on single machines but lack scalability and optimization for distributed environments. To address this gap, we propose three core innovations to extend GNN-PE to distributed systems: (1) a lightweight dynamic correlation-aware load balancing and hot migration mechanism that fuses multi-dimensional metrics (CPU, communication, memory) and guarantees index consistency; (2) an online incremental learning-based multi-GPU collaborative dynamic caching strategy with heterogeneous GPU adaptation and graph-structure-aware replacement; (3) a query plan ranking method driven by dominance embedding pruning potential (PE-score) that optimizes execution order. Through METIS partitioning, parallel offline preprocessing, and lightweight metadata management, our approach achieves \"minimum edge cut + load balancing + non-interruptible queries\" in distributed scenarios (tens of machines), significantly improving the efficiency and stability of distributed subgraph matching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact subgraph matching on large-scale graphs remains a challenging problem due to high computational complexity and distributed system constraints. Existing GNN-based path embedding (GNN-PE) frameworks achieve efficient exact matching on single machines but lack scalability and optimization for distributed environments. To address this gap, we propose three core innovations to extend GNN-PE to distributed systems: (1) a lightweight dynamic correlation-aware load balancing and hot migration mechanism that fuses multi-dimensional metrics (CPU, communication, memory) and guarantees index consistency; (2) an online incremental learning-based multi-GPU collaborative dynamic caching strategy with heterogeneous GPU adaptation and graph-structure-aware replacement; (3) a query plan ranking method driven by dominance embedding pruning potential (PE-score) that optimizes execution order. Through METIS partitioning, parallel offline preprocessing, and lightweight metadata management, our approach achieves \"minimum edge cut + load balancing + non-interruptible queries\" in distributed scenarios (tens of machines), significantly improving the efficiency and stability of distributed subgraph matching."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T07:06:33Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    7,
                    6,
                    33,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "10 pages",
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiake Ge"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.06029v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06029v2",
                "title": "Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving"
                },
                "updated": "2025-11-12T03:53:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    3,
                    53,
                    30,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06029v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T14:52:43Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    14,
                    52,
                    43,
                    5,
                    312,
                    0
                ],
                "arxiv_comment": "aaai26 camera-ready version, 12 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Hui Zeng"
                    },
                    {
                        "name": "Daming Zhao"
                    },
                    {
                        "name": "Pengfei Yang"
                    },
                    {
                        "name": "WenXuan Hou"
                    },
                    {
                        "name": "Tianyang Zheng"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Weiye Ji"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai"
            },
            {
                "id": "http://arxiv.org/abs/2511.08923v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08923v1",
                "title": "TiDAR: Think in Diffusion, Talk in Autoregression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TiDAR: Think in Diffusion, Talk in Autoregression"
                },
                "updated": "2025-11-12T02:59:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    12,
                    2,
                    59,
                    33,
                    2,
                    316,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08923v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T02:59:33Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    2,
                    59,
                    33,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "NVIDIA-Tech Report",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Zhifan Ye"
                    },
                    {
                        "name": "Rishabh Mehta"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Vartika Singh"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov"
            },
            {
                "id": "http://arxiv.org/abs/2510.25977v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.25977v3",
                "title": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium"
                },
                "updated": "2025-11-11T23:18:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    23,
                    18,
                    58,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.25977v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.25977v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "AI accelerators, customized to AI workloads, provide cost-effective and high-performance solutions for training and inference. Trainium, an AI accelerator recently developed by Amazon Web Services (AWS), provides an attractive option for LLM training and inference through its heterogeneous architecture. However, leveraging Trainium architecture for high performance can be challenging because of its systolic array architecture and special requirement on data layout. In this paper, we design high-performance matrix multiplication (matmul), a critical compute kernel, for LLM inference on Trainium. We introduce a series of techniques customized to Trainium based on kernel fusion and novel caching strategies to reduce data movement across the software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive matrix transpose. Evaluating with nine datasets and four recent LLMs, we show that our system largely outperforms the state-of-the-art matmul implemented by AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x speedup (up to 2.22x), which translates to an average 1.66x speedup (up to 2.49x) for end-to-end LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI accelerators, customized to AI workloads, provide cost-effective and high-performance solutions for training and inference. Trainium, an AI accelerator recently developed by Amazon Web Services (AWS), provides an attractive option for LLM training and inference through its heterogeneous architecture. However, leveraging Trainium architecture for high performance can be challenging because of its systolic array architecture and special requirement on data layout. In this paper, we design high-performance matrix multiplication (matmul), a critical compute kernel, for LLM inference on Trainium. We introduce a series of techniques customized to Trainium based on kernel fusion and novel caching strategies to reduce data movement across the software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive matrix transpose. Evaluating with nine datasets and four recent LLMs, we show that our system largely outperforms the state-of-the-art matmul implemented by AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x speedup (up to 2.22x), which translates to an average 1.66x speedup (up to 2.49x) for end-to-end LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-29T21:22:08Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    22,
                    8,
                    2,
                    302,
                    0
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Jierui Xu"
                    },
                    {
                        "name": "Weichu Yang"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li"
            },
            {
                "id": "http://arxiv.org/abs/2510.25979v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.25979v3",
                "title": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache"
                },
                "updated": "2025-11-11T23:07:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    23,
                    7,
                    38,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.25979v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.25979v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model encodes input sequences without performing autoregressive decoding. In these prefill only scenarios, the self-attention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length. In this paper, we observe that semantically different sentences often produce similar attention maps across layers and heads. Building on this insight, we propose AttnCache, a framework that accelerates the prefill stage of LLM inference by retrieving and reusing similar attention maps. Based on an attention map memorization database, AttnCache employs efficient caching and similarity search techniques to identify and reuse pre-cached attention maps during inference, thereby reducing the computational overhead of self-attention. Experimental results show that AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model encodes input sequences without performing autoregressive decoding. In these prefill only scenarios, the self-attention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length. In this paper, we observe that semantically different sentences often produce similar attention maps across layers and heads. Building on this insight, we propose AttnCache, a framework that accelerates the prefill stage of LLM inference by retrieving and reusing similar attention maps. Based on an attention map memorization database, AttnCache employs efficient caching and similarity search techniques to identify and reuse pre-cached attention maps during inference, thereby reducing the computational overhead of self-attention. Experimental results show that AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-29T21:26:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    26,
                    17,
                    2,
                    302,
                    0
                ],
                "arxiv_comment": "10 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Shangye Chen"
                    },
                    {
                        "name": "Cyril Guyot"
                    },
                    {
                        "name": "Filip Blagojevic"
                    },
                    {
                        "name": "Hyeran Jeon"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.08826v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08826v1",
                "title": "FlashMap: A Flash Optimized Key-Value Store",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashMap: A Flash Optimized Key-Value Store"
                },
                "updated": "2025-11-11T22:48:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    22,
                    48,
                    29,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08826v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Key-value stores are a fundamental class of NoSQL databases that offer a simple yet powerful model for data storage and retrieval, representing information as pairs of unique keys and associated values. Their minimal structure enables exceptionally fast access times, scalability, and flexibility in storing diverse data types, making them ideal for high-performance applications such as caching, session management, and distributed systems. As modern computing increasingly demands responsiveness and scalability, key-value stores have become a critical component of the data infrastructure in both industry and research contexts. In this work, we present FlashMap, a high-performance key-value store optimized for Flash-based solid-state drives (SSDs). Experiments show that FlashMap achieves outstanding throughput, averaging 19.8 million inserts and 23.8 million random lookups per second with a 100-byte payload, all on a single data center-grade server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value stores are a fundamental class of NoSQL databases that offer a simple yet powerful model for data storage and retrieval, representing information as pairs of unique keys and associated values. Their minimal structure enables exceptionally fast access times, scalability, and flexibility in storing diverse data types, making them ideal for high-performance applications such as caching, session management, and distributed systems. As modern computing increasingly demands responsiveness and scalability, key-value stores have become a critical component of the data infrastructure in both industry and research contexts. In this work, we present FlashMap, a high-performance key-value store optimized for Flash-based solid-state drives (SSDs). Experiments show that FlashMap achieves outstanding throughput, averaging 19.8 million inserts and 23.8 million random lookups per second with a 100-byte payload, all on a single data center-grade server."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T22:48:29Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    22,
                    48,
                    29,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "6 pages, 2 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Zonglin Guo"
                    },
                    {
                        "name": "Tony Givargis"
                    }
                ],
                "author_detail": {
                    "name": "Tony Givargis"
                },
                "author": "Tony Givargis"
            },
            {
                "id": "http://arxiv.org/abs/2511.08568v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08568v1",
                "title": "Machine Learning-Guided Memory Optimization for DLRM Inference on Tiered Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning-Guided Memory Optimization for DLRM Inference on Tiered Memory"
                },
                "updated": "2025-11-11T18:49:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    18,
                    49,
                    53,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08568v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep learning recommendation models (DLRMs) are widely used in industry, and their memory capacity requirements reach the terabyte scale. Tiered memory architectures provide a cost-effective solution but introduce challenges in embedding-vector placement due to complex embedding-access patterns. We propose RecMG, a machine learning (ML)-guided system for vector caching and prefetching on tiered memory. RecMG accurately predicts accesses to embedding vectors with long reuse distances or few reuses. The design of RecMG focuses on making ML feasible in the context of DLRM inference by addressing unique challenges in data labeling and navigating the search space for embedding-vector placement. By employing separate ML models for caching and prefetching, plus a novel differentiable loss function, RecMG narrows the prefetching search space and minimizes on-demand fetches. Compared to state-of-the-art temporal, spatial, and ML-based prefetchers, RecMG reduces on-demand fetches by 2.2x, 2.8x, and 1.5x, respectively. In industrial-scale DLRM inference scenarios, RecMG effectively reduces end-to-end DLRM inference time by up to 43%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning recommendation models (DLRMs) are widely used in industry, and their memory capacity requirements reach the terabyte scale. Tiered memory architectures provide a cost-effective solution but introduce challenges in embedding-vector placement due to complex embedding-access patterns. We propose RecMG, a machine learning (ML)-guided system for vector caching and prefetching on tiered memory. RecMG accurately predicts accesses to embedding vectors with long reuse distances or few reuses. The design of RecMG focuses on making ML feasible in the context of DLRM inference by addressing unique challenges in data labeling and navigating the search space for embedding-vector placement. By employing separate ML models for caching and prefetching, plus a novel differentiable loss function, RecMG narrows the prefetching search space and minimizes on-demand fetches. Compared to state-of-the-art temporal, spatial, and ML-based prefetchers, RecMG reduces on-demand fetches by 2.2x, 2.8x, and 1.5x, respectively. In industrial-scale DLRM inference scenarios, RecMG effectively reduces end-to-end DLRM inference time by up to 43%."
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T18:49:53Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    18,
                    49,
                    53,
                    1,
                    315,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF"
                },
                "authors": [
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Shuangyan Yang"
                    },
                    {
                        "name": "Benjamin Francis"
                    },
                    {
                        "name": "Ehsan K. Ardestani"
                    },
                    {
                        "name": "Min Si"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li"
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.15683v3",
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models"
                },
                "updated": "2025-11-11T13:52:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    13,
                    52,
                    57,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.15683v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.15683v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.10069v3",
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism"
                },
                "updated": "2025-11-11T12:44:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    12,
                    44,
                    32,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.10069v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.10069v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs)."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "arxiv_comment": "Accepted at NeurIPS 2025 Oral (Thirty-Ninth Conference on Neural Information Processing Systems)",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao"
            },
            {
                "id": "http://arxiv.org/abs/2502.20969v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.20969v3",
                "title": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval"
                },
                "updated": "2025-11-11T09:41:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    9,
                    41,
                    30,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.20969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.20969v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, creating a significant system challenge: achieving high throughput and low latency is difficult, especially when GPU memory is limited. To address these challenges, we propose TeleRAG, an efficient inference system that reduces latency and improves throughput with minimal GPU memory requirements. The core innovation of TeleRAG is lookahead retrieval, a prefetching mechanism that predicts required data and transfers them from CPU to GPU in parallel with LLM generation. In addition, TeleRAG adopts a prefetching scheduler and a cache-aware scheduler to support efficient multi-GPU inference with minimal overhead. Evaluations show TeleRAG achieves up to a 1.53x average end-to-end latency reduction (single-query) and 1.83x higher average throughput (batched), as well as good scalability in throughput. This confirms the practical utility of TeleRAG for faster and more memory-efficient deployments of RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, creating a significant system challenge: achieving high throughput and low latency is difficult, especially when GPU memory is limited. To address these challenges, we propose TeleRAG, an efficient inference system that reduces latency and improves throughput with minimal GPU memory requirements. The core innovation of TeleRAG is lookahead retrieval, a prefetching mechanism that predicts required data and transfers them from CPU to GPU in parallel with LLM generation. In addition, TeleRAG adopts a prefetching scheduler and a cache-aware scheduler to support efficient multi-GPU inference with minimal overhead. Evaluations show TeleRAG achieves up to a 1.53x average end-to-end latency reduction (single-query) and 1.83x higher average throughput (batched), as well as good scalability in throughput. This confirms the practical utility of TeleRAG for faster and more memory-efficient deployments of RAG applications."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-28T11:32:22Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    32,
                    22,
                    4,
                    59,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Keisuke Kamahori"
                    },
                    {
                        "name": "Yiyu Liu"
                    },
                    {
                        "name": "Xiaoxiang Shi"
                    },
                    {
                        "name": "Madhav Kashyap"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Rulin Shao"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Rohan Kadekodi"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci"
            },
            {
                "id": "http://arxiv.org/abs/2511.08003v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08003v1",
                "title": "Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning"
                },
                "updated": "2025-11-11T09:07:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    9,
                    7,
                    40,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08003v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T09:07:40Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    9,
                    7,
                    40,
                    1,
                    315,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jialong Qin"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Di Lu"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu"
            },
            {
                "id": "http://arxiv.org/abs/2511.07969v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07969v1",
                "title": "Unified Work Embeddings: Contrastive Learning of a Bidirectional Multi-task Ranker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Work Embeddings: Contrastive Learning of a Bidirectional Multi-task Ranker"
                },
                "updated": "2025-11-11T08:28:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    8,
                    28,
                    26,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07969v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks. To this end, we introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, establishing a common ground for multi-task progress. Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction. UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks. To this end, we introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, establishing a common ground for multi-task progress. Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction. UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T08:28:26Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    8,
                    28,
                    26,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "Preprint",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Matthias De Lange"
                    },
                    {
                        "name": "Jens-Joris Decorte"
                    },
                    {
                        "name": "Jeroen Van Hautte"
                    }
                ],
                "author_detail": {
                    "name": "Jeroen Van Hautte"
                },
                "author": "Jeroen Van Hautte"
            },
            {
                "id": "http://arxiv.org/abs/2511.07762v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07762v1",
                "title": "Kerr Polarization Transport: Accuracy and Performance in General Relativistic Light Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kerr Polarization Transport: Accuracy and Performance in General Relativistic Light Propagation"
                },
                "updated": "2025-11-11T02:26:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    11,
                    2,
                    26,
                    27,
                    1,
                    315,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07762v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a compact and reproducible method for general relativistic polarization transport in the Kerr metric that achieves median electric vector position angle (EVPA) residuals of $\\langle Δ\\mathrm{PA} \\rangle \\approx 0.09^\\circ$, a 95th percentile of $0.31^\\circ$, and a worst case $Δ\\mathrm{PA} \\lesssim 0.32^\\circ$ for spins up to $|a/M|=0.9$, while maintaining a fivefold or greater speedup relative to a strict reference integrator. Across the benchmark grid, typical residuals remain at the sub-tenth-degree level, with only modest degradation ($Δ\\mathrm{PA} \\lesssim 2^\\circ$) near the Thorne spin limit. Photon four-momenta $k^μ$ and polarization four-vectors $f^μ$ are advanced using a fourth order Runge-Kutta scheme with cached Christoffel symbols, maintaining the constraints $u\\cdot f=0$ and $n\\cdot f=0$, where $u^μ$ is the ZAMO four-velocity and $n^μ$ is the disk normal, while keeping $k\\cdot f \\simeq 0$. A physically motivated gauge is enforced by projecting the polarization into the local zero-angular-momentum observer (ZAMO) screen at every substep, ensuring numerical stability of the orthogonality constraints. Accuracy and performance are benchmarked over a representative grid in spin, inclination, image-plane azimuth, and radius. The method comfortably meets IXPE and NICER polarization tolerances and approaches EHT requirements. The approach provides a practical foundation for future general relativistic polarimetry and simulation pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a compact and reproducible method for general relativistic polarization transport in the Kerr metric that achieves median electric vector position angle (EVPA) residuals of $\\langle Δ\\mathrm{PA} \\rangle \\approx 0.09^\\circ$, a 95th percentile of $0.31^\\circ$, and a worst case $Δ\\mathrm{PA} \\lesssim 0.32^\\circ$ for spins up to $|a/M|=0.9$, while maintaining a fivefold or greater speedup relative to a strict reference integrator. Across the benchmark grid, typical residuals remain at the sub-tenth-degree level, with only modest degradation ($Δ\\mathrm{PA} \\lesssim 2^\\circ$) near the Thorne spin limit. Photon four-momenta $k^μ$ and polarization four-vectors $f^μ$ are advanced using a fourth order Runge-Kutta scheme with cached Christoffel symbols, maintaining the constraints $u\\cdot f=0$ and $n\\cdot f=0$, where $u^μ$ is the ZAMO four-velocity and $n^μ$ is the disk normal, while keeping $k\\cdot f \\simeq 0$. A physically motivated gauge is enforced by projecting the polarization into the local zero-angular-momentum observer (ZAMO) screen at every substep, ensuring numerical stability of the orthogonality constraints. Accuracy and performance are benchmarked over a representative grid in spin, inclination, image-plane azimuth, and radius. The method comfortably meets IXPE and NICER polarization tolerances and approaches EHT requirements. The approach provides a practical foundation for future general relativistic polarimetry and simulation pipelines."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T02:26:27Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    2,
                    26,
                    27,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "11 pages, 2 figures. Submitted to arXiv; to be submitted to The Astrophysical Journal",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "Shakibul Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Shakibul Chowdhury"
                },
                "author": "Shakibul Chowdhury"
            },
            {
                "id": "http://arxiv.org/abs/2511.07399v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07399v1",
                "title": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation"
                },
                "updated": "2025-11-10T18:51:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    18,
                    51,
                    28,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07399v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T18:51:28Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    18,
                    51,
                    28,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "Project Page: http://streamdiffusionv2.github.io",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Lvmin Zhang"
                    },
                    {
                        "name": "Keting Yang"
                    },
                    {
                        "name": "Kelly Peng"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Maneesh Agrawala"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Akio Kodaira"
                    },
                    {
                        "name": "Chenfeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenfeng Xu"
                },
                "author": "Chenfeng Xu"
            },
            {
                "id": "http://arxiv.org/abs/2511.07278v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07278v1",
                "title": "StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression"
                },
                "updated": "2025-11-10T16:25:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    16,
                    25,
                    3,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07278v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Video Large Language Models (Video-LLMs) have demonstrated significant potential in the areas of video captioning, search, and summarization. However, current Video-LLMs still face challenges with long real-world videos. Recent methods have introduced a retrieval mechanism that retrieves query-relevant KV caches for question answering, enhancing the efficiency and accuracy of long real-world videos. However, the compression and retrieval of KV caches are still not fully explored. In this paper, we propose \\textbf{StreamKV}, a training-free framework that seamlessly equips Video-LLMs with advanced KV cache retrieval and compression. Compared to previous methods that used uniform partitioning, StreamKV dynamically partitions video streams into semantic segments, which better preserves semantic information. For KV cache retrieval, StreamKV calculates a summary vector for each segment to retain segment-level information essential for retrieval. For KV cache compression, StreamKV introduces a guidance prompt designed to capture the key semantic elements within each segment, ensuring only the most informative KV caches are retained for answering questions. Moreover, StreamKV unifies KV cache retrieval and compression within a single module, performing both in a layer-adaptive manner, thereby further improving the effectiveness of streaming video question answering. Extensive experiments on public StreamingVQA benchmarks demonstrate that StreamKV significantly outperforms existing Online Video-LLMs, achieving superior accuracy while substantially improving both memory efficiency and computational latency. The code has been released at https://github.com/sou1p0wer/StreamKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (Video-LLMs) have demonstrated significant potential in the areas of video captioning, search, and summarization. However, current Video-LLMs still face challenges with long real-world videos. Recent methods have introduced a retrieval mechanism that retrieves query-relevant KV caches for question answering, enhancing the efficiency and accuracy of long real-world videos. However, the compression and retrieval of KV caches are still not fully explored. In this paper, we propose \\textbf{StreamKV}, a training-free framework that seamlessly equips Video-LLMs with advanced KV cache retrieval and compression. Compared to previous methods that used uniform partitioning, StreamKV dynamically partitions video streams into semantic segments, which better preserves semantic information. For KV cache retrieval, StreamKV calculates a summary vector for each segment to retain segment-level information essential for retrieval. For KV cache compression, StreamKV introduces a guidance prompt designed to capture the key semantic elements within each segment, ensuring only the most informative KV caches are retained for answering questions. Moreover, StreamKV unifies KV cache retrieval and compression within a single module, performing both in a layer-adaptive manner, thereby further improving the effectiveness of streaming video question answering. Extensive experiments on public StreamingVQA benchmarks demonstrate that StreamKV significantly outperforms existing Online Video-LLMs, achieving superior accuracy while substantially improving both memory efficiency and computational latency. The code has been released at https://github.com/sou1p0wer/StreamKV."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T16:25:03Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    16,
                    25,
                    3,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Xiang Bai"
                    },
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Chengyu Bai"
                    },
                    {
                        "name": "Yuhan Dai"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.07229v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07229v1",
                "title": "LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and Serving Techniques in LLM Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and Serving Techniques in LLM Infrastructure"
                },
                "updated": "2025-11-10T15:47:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    15,
                    47,
                    53,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07229v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/LCA.2025.3628325",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This paper introduces LLMServingSim2.0, a system simulator designed for exploring heterogeneous hardware in large-scale LLM serving systems. LLMServingSim2.0 addresses two key limitations of its predecessor: (1) integrating hardware models into system-level simulators is non-trivial due to the lack of a clear abstraction, and (2) existing simulators support only a narrow subset of serving techniques, leaving no infrastructure that captures the breadth of approaches in modern LLM serving. To overcome these issues, LLMServingSim2.0 adopts trace-driven performance modeling, accompanied by an operator-level latency profiler, enabling the integration of new accelerators with a single command. It further embeds up-to-date serving techniques while exposing flexible interfaces for request routing, cache management, and scheduling policies. In a TPU case study, our profiler requires 18.5x fewer LoC and outperforms the predecessor's hardware-simulator integration, demonstrating LLMServingSim2.0's low-effort hardware extensibility. Our experiments further show that LLMServingSim2.0 reproduces GPU-based LLM serving with 1.9% error, while maintaining practical simulation time, making it a comprehensive platform for both hardware developers and LLM service providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LLMServingSim2.0, a system simulator designed for exploring heterogeneous hardware in large-scale LLM serving systems. LLMServingSim2.0 addresses two key limitations of its predecessor: (1) integrating hardware models into system-level simulators is non-trivial due to the lack of a clear abstraction, and (2) existing simulators support only a narrow subset of serving techniques, leaving no infrastructure that captures the breadth of approaches in modern LLM serving. To overcome these issues, LLMServingSim2.0 adopts trace-driven performance modeling, accompanied by an operator-level latency profiler, enabling the integration of new accelerators with a single command. It further embeds up-to-date serving techniques while exposing flexible interfaces for request routing, cache management, and scheduling policies. In a TPU case study, our profiler requires 18.5x fewer LoC and outperforms the predecessor's hardware-simulator integration, demonstrating LLMServingSim2.0's low-effort hardware extensibility. Our experiments further show that LLMServingSim2.0 reproduces GPU-based LLM serving with 1.9% error, while maintaining practical simulation time, making it a comprehensive platform for both hardware developers and LLM service providers."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T15:47:53Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    15,
                    47,
                    53,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "4 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "arxiv_journal_ref": "IEEE Computer Architecture Letters (CAL) 2025",
                "authors": [
                    {
                        "name": "Jaehong Cho"
                    },
                    {
                        "name": "Hyunmin Choi"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_doi": "10.1109/LCA.2025.3628325"
            },
            {
                "id": "http://arxiv.org/abs/2510.19670v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.19670v2",
                "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation"
                },
                "updated": "2025-11-10T14:37:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    14,
                    37,
                    47,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.19670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.19670v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-22T15:16:56Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "arxiv_comment": "19 pages,8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Aina Binti Abdullah"
                    },
                    {
                        "name": "Pieter van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Pieter van der Merwe"
                },
                "author": "Pieter van der Merwe"
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.18480v3",
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices"
                },
                "updated": "2025-11-10T13:10:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    13,
                    10,
                    25,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.18480v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.18480v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2511.10676v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10676v1",
                "title": "Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models"
                },
                "updated": "2025-11-10T13:05:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    13,
                    5,
                    7,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10676v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T13:05:07Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    13,
                    5,
                    7,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shien Zhu"
                    },
                    {
                        "name": "Samuel Bohl"
                    },
                    {
                        "name": "Robin Oester"
                    },
                    {
                        "name": "Gustavo Alonso"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Alonso"
                },
                "author": "Gustavo Alonso"
            },
            {
                "id": "http://arxiv.org/abs/2506.08009v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.08009v2",
                "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion"
                },
                "updated": "2025-11-10T04:36:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    4,
                    36,
                    27,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.08009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.08009v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-09T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 spotlight. Project website: http://self-forcing.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xun Huang"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Guande He"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Eli Shechtman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shechtman"
                },
                "author": "Eli Shechtman"
            },
            {
                "id": "http://arxiv.org/abs/2507.02397v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.02397v2",
                "title": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission Dynamics"
                },
                "updated": "2025-11-10T03:55:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    3,
                    55,
                    22,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.02397v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While field-driven electron emission is theoretically understood down to the subcycle regime, its direct experimental temporal characterization using long-wavelength terahertz (THz) fields remains elusive. Here, by driving a graphite tip with phase-stable quasi-single-cycle THz pulses, we reveal distinct subcycle electron emission dynamics including: (1) At a carrier-envelope phase (CEP) zero, spectral peaks scale linearly with THz field strength, characteristic of subcycle emission; (2) At nearly opposite CEP, dominant deceleration fields generate stationary low-energy peaks. Crucially, we develop a pump-probe-free, direct reconstruction method extracting electron pulse profiles solely from measured energy spectra, obtaining durations from 73.0 to 81.0 fs as the field increases (191-290 kV/cm). Phase-resolved simulations further reveal a 72.8% modulation in the cutoff energy and a near-total (99.7%) suppression of the emission current. This work not only validates the field-emisssion theory under THz excitation but also establishes a general framework for the direct temporal characterization of subcycle electron emission, opening pathways for precise electron control in ultrafast electron sources and lightwave nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While field-driven electron emission is theoretically understood down to the subcycle regime, its direct experimental temporal characterization using long-wavelength terahertz (THz) fields remains elusive. Here, by driving a graphite tip with phase-stable quasi-single-cycle THz pulses, we reveal distinct subcycle electron emission dynamics including: (1) At a carrier-envelope phase (CEP) zero, spectral peaks scale linearly with THz field strength, characteristic of subcycle emission; (2) At nearly opposite CEP, dominant deceleration fields generate stationary low-energy peaks. Crucially, we develop a pump-probe-free, direct reconstruction method extracting electron pulse profiles solely from measured energy spectra, obtaining durations from 73.0 to 81.0 fs as the field increases (191-290 kV/cm). Phase-resolved simulations further reveal a 72.8% modulation in the cutoff energy and a near-total (99.7%) suppression of the emission current. This work not only validates the field-emisssion theory under THz excitation but also establishes a general framework for the direct temporal characterization of subcycle electron emission, opening pathways for precise electron control in ultrafast electron sources and lightwave nanoelectronics."
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-03T07:49:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "arxiv_comment": "18 pages, 5 figures, references added",
                "arxiv_primary_category": {
                    "term": "physics.optics"
                },
                "authors": [
                    {
                        "name": "Jiakang Mao"
                    },
                    {
                        "name": "Yushan Zeng"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Liwei Song"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ruxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruxin Li"
                },
                "author": "Ruxin Li"
            },
            {
                "id": "http://arxiv.org/abs/2510.27070v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.27070v2",
                "title": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review"
                },
                "updated": "2025-11-10T02:03:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    2,
                    3,
                    6,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.27070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.27070v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The security and efficiency of modern computing systems are fundamentally undermined by the absence of a native architectural mechanism to propagate high-level program semantics, such as object identity, bounds, and lifetime, across the hardware/software interface. This paper presents a comprehensive survey of the architectural paradigm designed to bridge this semantic gap: descriptor-based, object-aware memory systems. By elevating the descriptor to a first-class architectural abstraction, this paradigm enables hardware to dynamically acquire and enforce the rich semantics of software-defined objects. This survey systematically charts the evolution and current landscape of this approach. We establish the foundational concepts of memory objects and descriptors and introduce a novel taxonomy of descriptor addressing modes, providing a structured framework for analyzing and comparing diverse implementations. Our unified analysis reveals how this paradigm holistically addresses the intertwined challenges of memory protection, management, and processing. As a culminating case study, we re-examine the CentroID model, demonstrating how its hybrid tagged-pointer encoding and descriptor processing mechanisms embody the path toward practical and efficient object-aware designs. Finally, we outline how the explicit cross-layer communication of object semantics provides a foundational research direction for next-generation cache hierarchies, unified virtual memory, and even 128-bit architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security and efficiency of modern computing systems are fundamentally undermined by the absence of a native architectural mechanism to propagate high-level program semantics, such as object identity, bounds, and lifetime, across the hardware/software interface. This paper presents a comprehensive survey of the architectural paradigm designed to bridge this semantic gap: descriptor-based, object-aware memory systems. By elevating the descriptor to a first-class architectural abstraction, this paradigm enables hardware to dynamically acquire and enforce the rich semantics of software-defined objects. This survey systematically charts the evolution and current landscape of this approach. We establish the foundational concepts of memory objects and descriptors and introduce a novel taxonomy of descriptor addressing modes, providing a structured framework for analyzing and comparing diverse implementations. Our unified analysis reveals how this paradigm holistically addresses the intertwined challenges of memory protection, management, and processing. As a culminating case study, we re-examine the CentroID model, demonstrating how its hybrid tagged-pointer encoding and descriptor processing mechanisms embody the path toward practical and efficient object-aware designs. Finally, we outline how the explicit cross-layer communication of object semantics provides a foundational research direction for next-generation cache hierarchies, unified virtual memory, and even 128-bit architectures."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-31T00:39:27Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Dong Tong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Tong"
                },
                "author": "Dong Tong"
            },
            {
                "id": "http://arxiv.org/abs/2511.06605v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06605v1",
                "title": "DMA Collectives for Efficient ML Communication Offloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DMA Collectives for Efficient ML Communication Offloads"
                },
                "updated": "2025-11-10T01:28:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    1,
                    28,
                    58,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06605v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Offloading machine learning (ML) communication collectives to direct memory access (DMA) engines has emerged as an interesting and low-cost solution to efficiently overlap computation and communication in inference and training. Doing so delivers superior concurrent performance by freeing up all GPU cores for computation and also lowers interference in the memory sub-system (caches). While DMA collectives show strong promise, prior works have only studied them in limited context (bandwidth-bound transfer sizes only, performance-only).\n  To address this, we provide a comprehensive performance, power/energy and synchronization costs analysis of offloading ML communication collectives (all-gather, all-to-all) to DMA engines on state-of-the-art AMD Instinct MI300X GPUs. Our analysis reveals that, compared to the state-of-the-art RCCL communication collectives library, DMA collectives are at-par or better for large sizes (10s of MB to GB) in terms of both performance (16% better) and power (32% better). However, they significantly lag for latency-bound small sizes; 4.5X and 2.5X slower for all-gather and all-to-all, respectively. We provide a detailed latency breakdown of a DMA transfer and identify that DMA command scheduling and synchronization costs can limit DMA collective performance. To tackle this, we harness existing DMA architecture innovations, hitherto untapped, to build optimized DMA collectives and demonstrate their efficacy on real hardware. Our optimized implementations considerably close the performance gap for DMA collectives at smaller sizes (30% slower and 20% faster all-gather and all-to-all, respectively) and further improves performance (by 7%) and power savings at larger sizes (3-10%). Overall, this work represents a significant step toward making DMA collectives suitable for adoption in mainstream collective libraries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offloading machine learning (ML) communication collectives to direct memory access (DMA) engines has emerged as an interesting and low-cost solution to efficiently overlap computation and communication in inference and training. Doing so delivers superior concurrent performance by freeing up all GPU cores for computation and also lowers interference in the memory sub-system (caches). While DMA collectives show strong promise, prior works have only studied them in limited context (bandwidth-bound transfer sizes only, performance-only).\n  To address this, we provide a comprehensive performance, power/energy and synchronization costs analysis of offloading ML communication collectives (all-gather, all-to-all) to DMA engines on state-of-the-art AMD Instinct MI300X GPUs. Our analysis reveals that, compared to the state-of-the-art RCCL communication collectives library, DMA collectives are at-par or better for large sizes (10s of MB to GB) in terms of both performance (16% better) and power (32% better). However, they significantly lag for latency-bound small sizes; 4.5X and 2.5X slower for all-gather and all-to-all, respectively. We provide a detailed latency breakdown of a DMA transfer and identify that DMA command scheduling and synchronization costs can limit DMA collective performance. To tackle this, we harness existing DMA architecture innovations, hitherto untapped, to build optimized DMA collectives and demonstrate their efficacy on real hardware. Our optimized implementations considerably close the performance gap for DMA collectives at smaller sizes (30% slower and 20% faster all-gather and all-to-all, respectively) and further improves performance (by 7%) and power savings at larger sizes (3-10%). Overall, this work represents a significant step toward making DMA collectives suitable for adoption in mainstream collective libraries."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T01:28:58Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    1,
                    28,
                    58,
                    0,
                    314,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Mohamed Assem Ibrahim"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Assem Ibrahim"
                },
                "author": "Mohamed Assem Ibrahim"
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.16002v4",
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "updated": "2025-11-10T00:11:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    0,
                    11,
                    57,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.16002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.16002v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we investigate a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation when using KV caches computed independently for each document, KVLink introduces two key techniques: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, and using trainable special tokens to restore self-attention across independently encoded documents. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 96% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Additionally, KVLink can be combined with KV cache compression to further save cache loading and storage overhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we investigate a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation when using KV caches computed independently for each document, KVLink introduces two key techniques: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, and using trainable special tokens to restore self-attention across independently encoded documents. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 96% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Additionally, KVLink can be combined with KV cache compression to further save cache loading and storage overhead while outperforming the baselines."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang"
            },
            {
                "id": "http://arxiv.org/abs/2510.13797v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.13797v2",
                "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons"
                },
                "updated": "2025-11-10T00:06:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    10,
                    0,
                    6,
                    46,
                    0,
                    314,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.13797v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.13797v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-15T17:57:21Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Yair Feldman"
                    },
                    {
                        "name": "Shankar Padmanabhan"
                    },
                    {
                        "name": "Kianté Brantley"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi"
            },
            {
                "id": "http://arxiv.org/abs/2511.06460v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06460v1",
                "title": "Guidelines for Building Indexes on Partially Cache-Coherent CXL Shared Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guidelines for Building Indexes on Partially Cache-Coherent CXL Shared Memory"
                },
                "updated": "2025-11-09T16:55:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    9,
                    16,
                    55,
                    0,
                    6,
                    313,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06460v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The \\emph{Partial Cache-Coherence (PCC)} model maintains hardware cache coherence only within subsets of cores, enabling large-scale memory sharing with emerging memory interconnect technologies like Compute Express Link (CXL). However, PCC's relaxation of global cache coherence compromises the correctness of existing single-machine software.\n  This paper focuses on building consistent and efficient indexes on PCC platforms. We present that existing indexes designed for cache-coherent platforms can be made consistent on PCC platforms following SP guidelines, i.e., we identify \\emph{sync-data} and \\emph{protected-data} according to the index's concurrency control mechanisms, and synchronize them accordingly. However, conversion with SP guidelines introduces performance overhead. To mitigate the overhead, we identify several unique performance bottlenecks on PCC platforms, and propose P$^3$ guidelines (i.e., using Out-of-\\underline{P}lace update, Re\\underline{P}licated shared variable, S\\underline{P}eculative Reading) to improve the efficiency of converted indexes on PCC platforms.\n  With SP and P$^3$ guidelines, we convert and optimize two indexes (CLevelHash and BwTree) for PCC platforms. Evaluation shows that converted indexes' throughput improves up to 16$\\times$ following P$^3$ guidelines, and the optimized indexes outperform their message-passing-based and disaggregated-memory-based counterparts by up to 16$\\times$ and 19$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \\emph{Partial Cache-Coherence (PCC)} model maintains hardware cache coherence only within subsets of cores, enabling large-scale memory sharing with emerging memory interconnect technologies like Compute Express Link (CXL). However, PCC's relaxation of global cache coherence compromises the correctness of existing single-machine software.\n  This paper focuses on building consistent and efficient indexes on PCC platforms. We present that existing indexes designed for cache-coherent platforms can be made consistent on PCC platforms following SP guidelines, i.e., we identify \\emph{sync-data} and \\emph{protected-data} according to the index's concurrency control mechanisms, and synchronize them accordingly. However, conversion with SP guidelines introduces performance overhead. To mitigate the overhead, we identify several unique performance bottlenecks on PCC platforms, and propose P$^3$ guidelines (i.e., using Out-of-\\underline{P}lace update, Re\\underline{P}licated shared variable, S\\underline{P}eculative Reading) to improve the efficiency of converted indexes on PCC platforms.\n  With SP and P$^3$ guidelines, we convert and optimize two indexes (CLevelHash and BwTree) for PCC platforms. Evaluation shows that converted indexes' throughput improves up to 16$\\times$ following P$^3$ guidelines, and the optimized indexes outperform their message-passing-based and disaggregated-memory-based counterparts by up to 16$\\times$ and 19$\\times$."
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-09T16:55:00Z",
                "published_parsed": [
                    2025,
                    11,
                    9,
                    16,
                    55,
                    0,
                    6,
                    313,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Fangnuo Wu"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Wenjun Cai"
                    },
                    {
                        "name": "Jingsheng Yan"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.06446v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06446v1",
                "title": "SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention"
                },
                "updated": "2025-11-09T16:27:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    9,
                    16,
                    27,
                    55,
                    6,
                    313,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06446v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries. Unlike traditional retrieval-augmented generation methods that rely heavily on the performance of external retrievers and multi-stage pipelines, SR-KI supports end-to-end inference by performing retrieval entirely within the models latent space. This design enables efficient compression of injected knowledge and facilitates dynamic knowledge updates. Comprehensive experiments demonstrate that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries. Unlike traditional retrieval-augmented generation methods that rely heavily on the performance of external retrievers and multi-stage pipelines, SR-KI supports end-to-end inference by performing retrieval entirely within the models latent space. This design enables efficient compression of injected knowledge and facilitates dynamic knowledge updates. Comprehensive experiments demonstrate that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-09T16:27:55Z",
                "published_parsed": [
                    2025,
                    11,
                    9,
                    16,
                    27,
                    55,
                    6,
                    313,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.06174v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06174v1",
                "title": "LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs"
                },
                "updated": "2025-11-09T01:17:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    9,
                    1,
                    17,
                    8,
                    6,
                    313,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06174v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid progress of large language models (LLMs) has advanced numerous applications, yet efficient single-batch inference remains vital for on-device intelligence. While FPGAs offer fine-grained data control and high energy efficiency, recent GPU optimizations have narrowed their advantage, especially under arithmetic-based computation. To overcome this, we leverage FPGAs' abundant on-chip memory to shift LLM inference from arithmetic- to memory-based computation through table lookups. We present LUT-LLM, the first FPGA accelerator enabling 1B+ LLM inference via vector-quantized memory operations. Our analysis identifies activation-weight co-quantization as the most effective scheme, supported by (1) bandwidth-aware parallel centroid search, (2) efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency gain over A100.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of large language models (LLMs) has advanced numerous applications, yet efficient single-batch inference remains vital for on-device intelligence. While FPGAs offer fine-grained data control and high energy efficiency, recent GPU optimizations have narrowed their advantage, especially under arithmetic-based computation. To overcome this, we leverage FPGAs' abundant on-chip memory to shift LLM inference from arithmetic- to memory-based computation through table lookups. We present LUT-LLM, the first FPGA accelerator enabling 1B+ LLM inference via vector-quantized memory operations. Our analysis identifies activation-weight co-quantization as the most effective scheme, supported by (1) bandwidth-aware parallel centroid search, (2) efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency gain over A100."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-09T01:17:08Z",
                "published_parsed": [
                    2025,
                    11,
                    9,
                    1,
                    17,
                    8,
                    6,
                    313,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Zifan He"
                    },
                    {
                        "name": "Shengyu Ye"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong"
            },
            {
                "id": "http://arxiv.org/abs/2511.06010v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06010v1",
                "title": "MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference"
                },
                "updated": "2025-11-08T13:40:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    8,
                    13,
                    40,
                    16,
                    5,
                    312,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06010v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/LCA.2025.3627539",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The escalating context length in Large Language Models (LLMs) creates a severe performance bottleneck around the Key-Value (KV) cache, whose memory-bound nature leads to significant GPU under-utilization. This paper introduces Mixture of Shared KV Attention (MoSKA), an architecture that addresses this challenge by exploiting the heterogeneity of context data. It differentiates between per-request unique and massively reused shared sequences. The core of MoSKA is a novel Shared KV Attention mechanism that transforms the attention on shared data from a series of memory-bound GEMV operations into a single, compute-bound GEMM by batching concurrent requests. This is supported by an MoE-inspired sparse attention strategy that prunes the search space and a tailored Disaggregated Infrastructure that specializes hardware for unique and shared data. This comprehensive approach demonstrates a throughput increase of up to 538.7x over baselines in workloads with high context sharing, offering a clear architectural path toward scalable LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The escalating context length in Large Language Models (LLMs) creates a severe performance bottleneck around the Key-Value (KV) cache, whose memory-bound nature leads to significant GPU under-utilization. This paper introduces Mixture of Shared KV Attention (MoSKA), an architecture that addresses this challenge by exploiting the heterogeneity of context data. It differentiates between per-request unique and massively reused shared sequences. The core of MoSKA is a novel Shared KV Attention mechanism that transforms the attention on shared data from a series of memory-bound GEMV operations into a single, compute-bound GEMM by batching concurrent requests. This is supported by an MoE-inspired sparse attention strategy that prunes the search space and a tailored Disaggregated Infrastructure that specializes hardware for unique and shared data. This comprehensive approach demonstrates a throughput increase of up to 538.7x over baselines in workloads with high context sharing, offering a clear architectural path toward scalable LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T13:40:16Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    13,
                    40,
                    16,
                    5,
                    312,
                    0
                ],
                "arxiv_comment": "4 pages, 5 figures, accepted for publication at IEEE Computer Architecture Letters (IEEE CAL), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Myunghyun Rhee"
                    },
                    {
                        "name": "Sookyung Choi"
                    },
                    {
                        "name": "Euiseok Kim"
                    },
                    {
                        "name": "Joonseop Sim"
                    },
                    {
                        "name": "Youngpyo Joo"
                    },
                    {
                        "name": "Hoshik Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hoshik Kim"
                },
                "author": "Hoshik Kim",
                "arxiv_doi": "10.1109/LCA.2025.3627539"
            },
            {
                "id": "http://arxiv.org/abs/2508.02240v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.02240v3",
                "title": "Forecasting When to Forecast: Accelerating Diffusion Models with Confidence-Gated Taylor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting When to Forecast: Accelerating Diffusion Models with Confidence-Gated Taylor"
                },
                "updated": "2025-11-08T11:53:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    8,
                    11,
                    53,
                    33,
                    5,
                    312,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.02240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.02240v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in visual generation tasks. However, their low inference speed limits their deployment in low-resource applications. Recent training-free approaches exploit the redundancy of features across timesteps by caching and reusing past representations to accelerate inference. Building on this idea, TaylorSeer instead uses cached features to predict future ones via Taylor expansion. However, its module-level prediction across all transformer blocks (e.g., attention or feedforward modules) requires storing fine-grained intermediate features, leading to notable memory and computation overhead. Moreover, it adopts a fixed caching schedule without considering the varying accuracy of predictions across timesteps, which can lead to degraded outputs when prediction fails. To address these limitations, we propose a novel approach to better leverage Taylor-based acceleration. First, we shift the Taylor prediction target from the module level to the last block level, significantly reducing the number of cached features. Furthermore, observing strong sequential dependencies among Transformer blocks, we propose to use the error between the Taylor-estimated and actual outputs of the first block as an indicator of prediction reliability. If the error is small, we trust the Taylor prediction for the last block; otherwise, we fall back to full computation, thereby enabling a dynamic caching mechanism. Empirical results show that our method achieves a better balance between speed and quality, achieving a 3.17x acceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible quality drop. The Project Page is \\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in visual generation tasks. However, their low inference speed limits their deployment in low-resource applications. Recent training-free approaches exploit the redundancy of features across timesteps by caching and reusing past representations to accelerate inference. Building on this idea, TaylorSeer instead uses cached features to predict future ones via Taylor expansion. However, its module-level prediction across all transformer blocks (e.g., attention or feedforward modules) requires storing fine-grained intermediate features, leading to notable memory and computation overhead. Moreover, it adopts a fixed caching schedule without considering the varying accuracy of predictions across timesteps, which can lead to degraded outputs when prediction fails. To address these limitations, we propose a novel approach to better leverage Taylor-based acceleration. First, we shift the Taylor prediction target from the module level to the last block level, significantly reducing the number of cached features. Furthermore, observing strong sequential dependencies among Transformer blocks, we propose to use the error between the Taylor-estimated and actual outputs of the first block as an indicator of prediction reliability. If the error is small, we trust the Taylor prediction for the last block; otherwise, we fall back to full computation, thereby enabling a dynamic caching mechanism. Empirical results show that our method achieves a better balance between speed and quality, achieving a 3.17x acceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible quality drop. The Project Page is \\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-04T09:39:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    31,
                    0,
                    216,
                    0
                ],
                "arxiv_comment": "15 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Jiaxing Yan"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Zetao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu"
            },
            {
                "id": "http://arxiv.org/abs/2511.05958v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05958v1",
                "title": "MT4G: A Tool for Reliable Auto-Discovery of NVIDIA and AMD GPU Compute and Memory Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MT4G: A Tool for Reliable Auto-Discovery of NVIDIA and AMD GPU Compute and Memory Topologies"
                },
                "updated": "2025-11-08T10:27:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    8,
                    10,
                    27,
                    32,
                    5,
                    312,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05958v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3731599.3767518",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Understanding GPU topology is essential for performance-related tasks in HPC or AI. Yet, unlike for CPUs with tools like hwloc, GPU information is hard to come by, incomplete, and vendor-specific.\n  In this work, we address this gap and present MT4G, an open-source and vendor-agnostic tool that automatically discovers GPU compute and memory topologies and configurations, including cache sizes, bandwidths, and physical layouts. MT4G combines existing APIs with a suite of over 50 microbenchmarks, applying statistical methods, such as the Kolmogorov-Smirnov test, to automatically and reliably identify otherwise programmatically unavailable topological attributes.\n  We showcase MT4G's universality on ten different GPUs and demonstrate its impact through integration into three workflows: GPU performance modeling, GPUscout bottleneck analysis, and dynamic resource partitioning. These scenarios highlight MT4G's role in understanding system performance and characteristics across NVIDIA and AMD GPUs, providing an automated, portable solution for modern HPC and AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding GPU topology is essential for performance-related tasks in HPC or AI. Yet, unlike for CPUs with tools like hwloc, GPU information is hard to come by, incomplete, and vendor-specific.\n  In this work, we address this gap and present MT4G, an open-source and vendor-agnostic tool that automatically discovers GPU compute and memory topologies and configurations, including cache sizes, bandwidths, and physical layouts. MT4G combines existing APIs with a suite of over 50 microbenchmarks, applying statistical methods, such as the Kolmogorov-Smirnov test, to automatically and reliably identify otherwise programmatically unavailable topological attributes.\n  We showcase MT4G's universality on ten different GPUs and demonstrate its impact through integration into three workflows: GPU performance modeling, GPUscout bottleneck analysis, and dynamic resource partitioning. These scenarios highlight MT4G's role in understanding system performance and characteristics across NVIDIA and AMD GPUs, providing an automated, portable solution for modern HPC and AI systems."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T10:27:32Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    10,
                    27,
                    32,
                    5,
                    312,
                    0
                ],
                "arxiv_comment": "14 pages, including Appendix and References, 5 figures, to be published in Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC Workshops '25)",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Stepan Vanecek"
                    },
                    {
                        "name": "Manuel Walter Mussbacher"
                    },
                    {
                        "name": "Dominik Groessler"
                    },
                    {
                        "name": "Urvij Saroliya"
                    },
                    {
                        "name": "Martin Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Martin Schulz"
                },
                "author": "Martin Schulz",
                "arxiv_doi": "10.1145/3731599.3767518"
            },
            {
                "id": "http://arxiv.org/abs/2511.05814v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05814v1",
                "title": "In-depth Analysis on Caching and Pre-fetching in Mixture of Experts Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-depth Analysis on Caching and Pre-fetching in Mixture of Experts Offloading"
                },
                "updated": "2025-11-08T03:04:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    8,
                    3,
                    4,
                    11,
                    5,
                    312,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05814v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In today's landscape, Mixture of Experts (MoE) is a crucial architecture that has been used by many of the most advanced models. One of the major challenges of MoE models is that they usually require much more memory than their dense counterparts due to their unique architecture, and hence are harder to deploy in environments with limited GPU memory, such as edge devices. MoE offloading is a promising technique proposed to overcome this challenge, especially if it is enhanced with caching and pre-fetching, but prior work stopped at suboptimal caching algorithm and offered limited insights. In this work, we study MoE offloading in depth and make the following contributions: 1. We analyze the expert activation and LRU caching behavior in detail and provide traces. 2. We propose LFU caching optimization based on our analysis and obtain strong improvements from LRU. 3. We implement and experiment speculative expert pre-fetching, providing detailed trace showing its huge potential . 4. In addition, our study extensively covers the behavior of the MoE architecture itself, offering information on the characteristic of the gating network and experts. This can inspire future work on the interpretation of MoE models and the development of pruning techniques for MoE architecture with minimal performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In today's landscape, Mixture of Experts (MoE) is a crucial architecture that has been used by many of the most advanced models. One of the major challenges of MoE models is that they usually require much more memory than their dense counterparts due to their unique architecture, and hence are harder to deploy in environments with limited GPU memory, such as edge devices. MoE offloading is a promising technique proposed to overcome this challenge, especially if it is enhanced with caching and pre-fetching, but prior work stopped at suboptimal caching algorithm and offered limited insights. In this work, we study MoE offloading in depth and make the following contributions: 1. We analyze the expert activation and LRU caching behavior in detail and provide traces. 2. We propose LFU caching optimization based on our analysis and obtain strong improvements from LRU. 3. We implement and experiment speculative expert pre-fetching, providing detailed trace showing its huge potential . 4. In addition, our study extensively covers the behavior of the MoE architecture itself, offering information on the characteristic of the gating network and experts. This can inspire future work on the interpretation of MoE models and the development of pruning techniques for MoE architecture with minimal performance loss."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T03:04:11Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    3,
                    4,
                    11,
                    5,
                    312,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Shuning Lin"
                    },
                    {
                        "name": "Yifan He"
                    },
                    {
                        "name": "Yitong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yitong Chen"
                },
                "author": "Yitong Chen"
            },
            {
                "id": "http://arxiv.org/abs/2504.06319v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.06319v2",
                "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching"
                },
                "updated": "2025-11-08T02:40:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    8,
                    2,
                    40,
                    48,
                    5,
                    312,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.06319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.06319v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-08T09:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "arxiv_comment": "8 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yanhao Dong"
                    },
                    {
                        "name": "Yubo Miao"
                    },
                    {
                        "name": "Weinan Li"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Jiesheng Wu"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu"
            },
            {
                "id": "http://arxiv.org/abs/2508.17032v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.17032v2",
                "title": "Learned Structure in Cartridges: Keys as Shareable Routers in Self-Studied Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Structure in Cartridges: Keys as Shareable Routers in Self-Studied Representations"
                },
                "updated": "2025-11-07T21:01:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    21,
                    1,
                    10,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.17032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.17032v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A bottleneck for long-context LLM inference is the linearly growing KV cache. Recent work has proposed Cartridges, an approach which leverages offline compute to train a much smaller KV cache than is typically required for a full document (up to 40x less memory usage at inference time). In this paper, we present the first mechanistic exploration of the learned Cartridge key-value cache structure. In particular, we propose that (1) Cartridge keys act as stable, shareable retrieval routers for the compressed corpora and (2) most of the learned compression occurs within the Cartridge value vectors. We present empirical evidence of our routing theory across tasks, model families, and model sizes; for example, we can ablate the learned Cartridge key vectors between tasks with little performance loss. Finally, we propose a slight improvement in initialization called Sampled Chunk Initialization (SCI). We suggest that SCI can lead to faster Cartridge convergence than previously demonstrated in the literature. Our findings lay the groundwork for broader empirical study of Cartridge training optimization which may be crucial for further scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A bottleneck for long-context LLM inference is the linearly growing KV cache. Recent work has proposed Cartridges, an approach which leverages offline compute to train a much smaller KV cache than is typically required for a full document (up to 40x less memory usage at inference time). In this paper, we present the first mechanistic exploration of the learned Cartridge key-value cache structure. In particular, we propose that (1) Cartridge keys act as stable, shareable retrieval routers for the compressed corpora and (2) most of the learned compression occurs within the Cartridge value vectors. We present empirical evidence of our routing theory across tasks, model families, and model sizes; for example, we can ablate the learned Cartridge key vectors between tasks with little performance loss. Finally, we propose a slight improvement in initialization called Sampled Chunk Initialization (SCI). We suggest that SCI can lead to faster Cartridge convergence than previously demonstrated in the literature. Our findings lay the groundwork for broader empirical study of Cartridge training optimization which may be crucial for further scaling."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-23T14:20:06Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Maurizio Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Diaz"
                },
                "author": "Maurizio Diaz"
            },
            {
                "id": "http://arxiv.org/abs/2406.06483v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2406.06483v4",
                "title": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection Correctness, Security, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection Correctness, Security, and Performance"
                },
                "updated": "2025-11-07T18:03:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    18,
                    3,
                    32,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2406.06483v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2406.06483v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The battle for a more secure Internet is waged on many fronts, including the most basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an IPv4 header field as old as the Internet with an equally long history as an exploited side channel for scanning network properties, inferring off-path connections, and poisoning DNS caches. This article taxonomizes the 25-year history of IPID-based exploits and the corresponding changes to IPID selection methods. By mathematically analyzing these methods' correctness and security and empirically evaluating their performance, we reveal recommendations for best practice as well as shortcomings of current operating system implementations, emphasizing the value of systematic evaluations in network security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The battle for a more secure Internet is waged on many fronts, including the most basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an IPv4 header field as old as the Internet with an equally long history as an exploited side channel for scanning network properties, inferring off-path connections, and poisoning DNS caches. This article taxonomizes the 25-year history of IPID-based exploits and the corresponding changes to IPID selection methods. By mathematically analyzing these methods' correctness and security and empirically evaluating their performance, we reveal recommendations for best practice as well as shortcomings of current operating system implementations, emphasizing the value of systematic evaluations in network security."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-06-10T17:22:17Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    22,
                    17,
                    0,
                    162,
                    0
                ],
                "arxiv_comment": "37 pages, 11 figures, 2 tables, 1 algorithm",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Joshua J. Daymude"
                    },
                    {
                        "name": "Antonio M. Espinoza"
                    },
                    {
                        "name": "Holly Bergen"
                    },
                    {
                        "name": "Benjamin Mixon-Baca"
                    },
                    {
                        "name": "Jeffrey Knockel"
                    },
                    {
                        "name": "Jedidiah R. Crandall"
                    }
                ],
                "author_detail": {
                    "name": "Jedidiah R. Crandall"
                },
                "author": "Jedidiah R. Crandall"
            },
            {
                "id": "http://arxiv.org/abs/2506.05345v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.05345v2",
                "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Hyper-Scaling with KV Cache Compression"
                },
                "updated": "2025-11-07T16:42:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    16,
                    42,
                    30,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.05345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.05345v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8$\\times$ compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference latency and memory load. For instance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and 9.7 on LiveCodeBench on average for an equivalent number of memory reads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8$\\times$ compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference latency and memory load. For instance, we enhance Qwen-R1 32B by 12.0 points on AIME 24, 8.6 on GPQA, and 9.7 on LiveCodeBench on average for an equivalent number of memory reads."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti"
            },
            {
                "id": "http://arxiv.org/abs/2511.05299v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05299v1",
                "title": "LiveStar: Live Streaming Assistant for Real-World Online Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveStar: Live Streaming Assistant for Real-World Online Video Understanding"
                },
                "updated": "2025-11-07T15:00:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    0,
                    37,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05299v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-07T15:00:37Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    15,
                    0,
                    37,
                    4,
                    311,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 Accepted",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Kairui Zhang"
                    },
                    {
                        "name": "Yuhang Hu"
                    },
                    {
                        "name": "Bing Wang"
                    },
                    {
                        "name": "Shengsheng Qian"
                    },
                    {
                        "name": "Bin Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Weiming Dong"
                    },
                    {
                        "name": "Changsheng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Changsheng Xu"
                },
                "author": "Changsheng Xu"
            },
            {
                "id": "http://arxiv.org/abs/2511.05060v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05060v1",
                "title": "kV-Class Lateral NiOx/GaN Super-Heterojunction Diode via Ammonia Molecular Beam Epitaxy (NH3-MBE)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "kV-Class Lateral NiOx/GaN Super-Heterojunction Diode via Ammonia Molecular Beam Epitaxy (NH3-MBE)"
                },
                "updated": "2025-11-07T08:07:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    8,
                    7,
                    19,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05060v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work reports the demonstration of lateral p-NiOx/p-GaN/n-GaN-based super-heterojunction (SHJ) diodes using p-GaN with additional sputtered p-type nickel oxide (NiOx) layers to realize charge-balanced structures. The heterojunction diode capacitance-voltage (C-V) model is applied to extract effective the acceptor concentration from the p-NiOx. Net donor and acceptor concentration in n-GaN and p-GaN are extracted by using metal-oxide-semiconductor (MOS) test structures. The fabricated p-NiOx/p-GaN/n-GaN SHJ diodes with charge-balanced region between anode and cathode exhibit a forward on-state current density of 10-30 mA/mm across an anode-to-cathode distance (LAC) from 16 μm to 80 μm. The SHJ diodes show rectifying behavior with a maximum on/off ratio of 10^9 and a low reverse leakage density. The highest breakdown voltage achieved for the SHJ diodes is ~2.8 kV with reverse leakage density of 10^-4 mA/mm at ~80% of devices catastrophic breakdown voltage. The SHJ diodes across all types of dimensions exhibit significant breakdown voltage improvements (~6X on average) with ultra-low reverse leakage current compared to corresponding reference structures without a charge-balanced extension, clearly demonstrating the superjunction effect for devices fabricated on GaN epitaxial layer with ~10^17 cm^-3 electron density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the demonstration of lateral p-NiOx/p-GaN/n-GaN-based super-heterojunction (SHJ) diodes using p-GaN with additional sputtered p-type nickel oxide (NiOx) layers to realize charge-balanced structures. The heterojunction diode capacitance-voltage (C-V) model is applied to extract effective the acceptor concentration from the p-NiOx. Net donor and acceptor concentration in n-GaN and p-GaN are extracted by using metal-oxide-semiconductor (MOS) test structures. The fabricated p-NiOx/p-GaN/n-GaN SHJ diodes with charge-balanced region between anode and cathode exhibit a forward on-state current density of 10-30 mA/mm across an anode-to-cathode distance (LAC) from 16 μm to 80 μm. The SHJ diodes show rectifying behavior with a maximum on/off ratio of 10^9 and a low reverse leakage density. The highest breakdown voltage achieved for the SHJ diodes is ~2.8 kV with reverse leakage density of 10^-4 mA/mm at ~80% of devices catastrophic breakdown voltage. The SHJ diodes across all types of dimensions exhibit significant breakdown voltage improvements (~6X on average) with ultra-low reverse leakage current compared to corresponding reference structures without a charge-balanced extension, clearly demonstrating the superjunction effect for devices fabricated on GaN epitaxial layer with ~10^17 cm^-3 electron density."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-07T08:07:19Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    8,
                    7,
                    19,
                    4,
                    311,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Zachary J. Biegler"
                    },
                    {
                        "name": "Ashley E. Wissel-Garcia"
                    },
                    {
                        "name": "James S. Speck"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy"
            },
            {
                "id": "http://arxiv.org/abs/2511.05022v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05022v1",
                "title": "AWARE: Evaluating PriorityFresh Caching for Offline Emergency Warning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AWARE: Evaluating PriorityFresh Caching for Offline Emergency Warning Systems"
                },
                "updated": "2025-11-07T06:53:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    53,
                    48,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05022v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "PriorityFresh is a semantic, actionability-first caching policy designed for offline emergency warning systems. Within the AWARE system's simulation environment, PriorityFresh optimizes which alerts to retain and surface under constrained connectivity. Experiments indicate improved actionability-first performance without harming efficiency. A separate Priority Forecasting model is used only to synthesize realistic alert sequences for controlled experiments and does not influence caching or push decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PriorityFresh is a semantic, actionability-first caching policy designed for offline emergency warning systems. Within the AWARE system's simulation environment, PriorityFresh optimizes which alerts to retain and surface under constrained connectivity. Experiments indicate improved actionability-first performance without harming efficiency. A separate Priority Forecasting model is used only to synthesize realistic alert sequences for controlled experiments and does not influence caching or push decisions."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-07T06:53:48Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    53,
                    48,
                    4,
                    311,
                    0
                ],
                "arxiv_comment": "Preprint version",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Charles Melvin"
                    },
                    {
                        "name": "N. Rich Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "N. Rich Nguyen"
                },
                "author": "N. Rich Nguyen"
            },
            {
                "id": "http://arxiv.org/abs/2511.05017v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.05017v1",
                "title": "Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings"
                },
                "updated": "2025-11-07T06:39:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    39,
                    54,
                    4,
                    311,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.05017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.05017v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations -- and to show that refining textual embeddings with visual information mitigates this issue -- we leave exploration of advanced fusion strategies for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations -- and to show that refining textual embeddings with visual information mitigates this issue -- we leave exploration of advanced fusion strategies for future work."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-07T06:39:54Z",
                "published_parsed": [
                    2025,
                    11,
                    7,
                    6,
                    39,
                    54,
                    4,
                    311,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Aakriti Agrawal"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Rohith Aralikatti"
                    },
                    {
                        "name": "Gauri Jagatap"
                    },
                    {
                        "name": "Jiaxin Yuan"
                    },
                    {
                        "name": "Vijay Kamarshi"
                    },
                    {
                        "name": "Andrea Fanelli"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang"
            },
            {
                "id": "http://arxiv.org/abs/2505.24095v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.24095v2",
                "title": "SkyWalker: A Locality-Aware Cross-Region Load Balancer for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyWalker: A Locality-Aware Cross-Region Load Balancer for LLM Inference"
                },
                "updated": "2025-11-06T21:05:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    21,
                    5,
                    23,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.24095v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.24095v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Serving Large Language Models (LLMs) efficiently in multi-region setups remains a challenge. Due to cost and GPU availability concerns, providers typically deploy LLMs in multiple regions using instance with long-term commitments, like reserved instances or on-premise clusters, which are often underutilized due to their region-local traffic handling and diurnal traffic variance. In this paper, we introduce SkyWalker, a multi-region load balancer for LLM inference that aggregates regional diurnal patterns through cross-region traffic handling. By doing so, SkyWalker enables providers to reserve instances based on expected global demand, rather than peak demand in each individual region. Meanwhile, SkyWalker preserves KV-Cache locality and load balancing, ensuring cost efficiency without sacrificing performance. SkyWalker achieves this with a cache-aware cross-region traffic handler and a selective pushing based load balancing mechanism. Our evaluation on real-world workloads shows that it achieves 1.12-2.06x higher throughput and 1.74-6.30x lower latency compared to existing load balancers, while reducing total serving cost by 25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) efficiently in multi-region setups remains a challenge. Due to cost and GPU availability concerns, providers typically deploy LLMs in multiple regions using instance with long-term commitments, like reserved instances or on-premise clusters, which are often underutilized due to their region-local traffic handling and diurnal traffic variance. In this paper, we introduce SkyWalker, a multi-region load balancer for LLM inference that aggregates regional diurnal patterns through cross-region traffic handling. By doing so, SkyWalker enables providers to reserve instances based on expected global demand, rather than peak demand in each individual region. Meanwhile, SkyWalker preserves KV-Cache locality and load balancing, ensuring cost efficiency without sacrificing performance. SkyWalker achieves this with a cache-aware cross-region traffic handler and a selective pushing based load balancing mechanism. Our evaluation on real-world workloads shows that it achieves 1.12-2.06x higher throughput and 1.74-6.30x lower latency compared to existing load balancers, while reducing total serving cost by 25%."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-30T00:46:18Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    0,
                    46,
                    18,
                    4,
                    150,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Jamison Kerney"
                    },
                    {
                        "name": "Ethan J. Jackson"
                    },
                    {
                        "name": "Zhifei Li"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica"
            },
            {
                "id": "http://arxiv.org/abs/2511.04804v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04804v1",
                "title": "Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator"
                },
                "updated": "2025-11-06T20:49:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    20,
                    49,
                    13,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04804v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial predictor that represents f: R^d -> R^k as a globally C^r finite-element field on a learned simplicial mesh in an optionally warped input space. Each query activates exactly one simplex and at most d+1 basis functions via barycentric coordinates, yielding explicit locality, controllable smoothness, and cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with a light invertible warp and trains end-to-end with shape regularization, semi-discrete OT coverage, and differentiable edge flips. Under standard shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic approximation tasks, tabular regression/classification, and as a drop-in head on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter budgets, improves calibration (lower ECE/Brier), and reduces inference latency due to geometric locality. These properties make SiFEN a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial predictor that represents f: R^d -> R^k as a globally C^r finite-element field on a learned simplicial mesh in an optionally warped input space. Each query activates exactly one simplex and at most d+1 basis functions via barycentric coordinates, yielding explicit locality, controllable smoothness, and cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with a light invertible warp and trains end-to-end with shape regularization, semi-discrete OT coverage, and differentiable edge flips. Under standard shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic approximation tasks, tabular regression/classification, and as a drop-in head on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter budgets, improves calibration (lower ECE/Brier), and reduces inference latency due to geometric locality. These properties make SiFEN a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T20:49:13Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    20,
                    49,
                    13,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Chaymae Yahyati"
                    },
                    {
                        "name": "Ismail Lamaakal"
                    },
                    {
                        "name": "Khalid El Makkaoui"
                    },
                    {
                        "name": "Ibrahim Ouahbi"
                    },
                    {
                        "name": "Yassine Maleh"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Maleh"
                },
                "author": "Yassine Maleh"
            },
            {
                "id": "http://arxiv.org/abs/2511.04791v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04791v1",
                "title": "DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing"
                },
                "updated": "2025-11-06T20:18:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    20,
                    18,
                    34,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04791v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern LLM serving systems must sustain high throughput while meeting strict latency SLOs across two distinct inference phases: compute-intensive prefill and memory-bound decode phases. Existing approaches either (1) aggregate both phases on shared GPUs, leading to interference between prefill and decode phases, which degrades time-between-tokens (TBT); or (2) disaggregate the two phases across GPUs, improving latency but wasting resources through duplicated models and KV cache transfers. We present DuetServe, a unified LLM serving framework that achieves disaggregation-level isolation within a single GPU. DuetServe operates in aggregated mode by default and dynamically activates SM-level GPU spatial multiplexing when TBT degradation is predicted. Its key idea is to decouple prefill and decode execution only when needed through fine-grained, adaptive SM partitioning that provides phase isolation only when contention threatens latency service level objectives (SLOs). DuetServe integrates (1) an attention-aware roofline model to forecast iteration latency, (2) a partitioning optimizer that selects the optimal SM split to maximize throughput under TBT constraints, and (3) an interruption-free execution engine that eliminates CPU-GPU synchronization overhead. Evaluations show that DuetServe improves total throughput by up to 1.3x while maintaining low generation latency compared to state-of-the-art frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM serving systems must sustain high throughput while meeting strict latency SLOs across two distinct inference phases: compute-intensive prefill and memory-bound decode phases. Existing approaches either (1) aggregate both phases on shared GPUs, leading to interference between prefill and decode phases, which degrades time-between-tokens (TBT); or (2) disaggregate the two phases across GPUs, improving latency but wasting resources through duplicated models and KV cache transfers. We present DuetServe, a unified LLM serving framework that achieves disaggregation-level isolation within a single GPU. DuetServe operates in aggregated mode by default and dynamically activates SM-level GPU spatial multiplexing when TBT degradation is predicted. Its key idea is to decouple prefill and decode execution only when needed through fine-grained, adaptive SM partitioning that provides phase isolation only when contention threatens latency service level objectives (SLOs). DuetServe integrates (1) an attention-aware roofline model to forecast iteration latency, (2) a partitioning optimizer that selects the optimal SM split to maximize throughput under TBT constraints, and (3) an interruption-free execution engine that eliminates CPU-GPU synchronization overhead. Evaluations show that DuetServe improves total throughput by up to 1.3x while maintaining low generation latency compared to state-of-the-art frameworks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T20:18:34Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    20,
                    18,
                    34,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram"
            },
            {
                "id": "http://arxiv.org/abs/2506.05410v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.05410v2",
                "title": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs"
                },
                "updated": "2025-11-06T17:09:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    17,
                    9,
                    52,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.05410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.05410v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in Large Language Models (LLMs) have highlighted the critical importance of extending context length, yet the quadratic complexity of attention mechanisms poses significant challenges for efficient long-context modeling. KV cache compression has emerged as a key approach to address this challenge. Through extensive empirical analysis, we reveal a fundamental yet previously overlooked asymmetry in KV caches: while adjacent keys receive similar attention weights ({\\it local homogeneity}), adjacent values demonstrate distinct {\\it heterogeneous} distributions. This key-value asymmetry reveals a critical limitation in existing compression methods that treat keys and values uniformly. To address the limitation, we propose a training-free compression framework (AsymKV) that combines homogeneity-based key merging with a mathematically proven lossless value compression. Extensive experiments demonstrate that AsymKV consistently outperforms existing long-context methods across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing SOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in this link:https://github.com/the-scale-lab/Asymkv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have highlighted the critical importance of extending context length, yet the quadratic complexity of attention mechanisms poses significant challenges for efficient long-context modeling. KV cache compression has emerged as a key approach to address this challenge. Through extensive empirical analysis, we reveal a fundamental yet previously overlooked asymmetry in KV caches: while adjacent keys receive similar attention weights ({\\it local homogeneity}), adjacent values demonstrate distinct {\\it heterogeneous} distributions. This key-value asymmetry reveals a critical limitation in existing compression methods that treat keys and values uniformly. To address the limitation, we propose a training-free compression framework (AsymKV) that combines homogeneity-based key merging with a mathematically proven lossless value compression. Extensive experiments demonstrate that AsymKV consistently outperforms existing long-context methods across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing SOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in this link:https://github.com/the-scale-lab/Asymkv."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-04T16:10:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    10,
                    44,
                    2,
                    155,
                    0
                ],
                "arxiv_comment": "14 pages,7 figures;Accepted by NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Wanyun Cui"
                    },
                    {
                        "name": "Mingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mingwei Xu"
                },
                "author": "Mingwei Xu"
            },
            {
                "id": "http://arxiv.org/abs/2511.04489v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04489v1",
                "title": "Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear Fusion"
                },
                "updated": "2025-11-06T16:08:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    8,
                    24,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04489v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "EIRENE [1] is a Monte Carlo neutral transport solver heavily used in the fusion community. EIRENE does not implement domain decomposition, making it impossible to use for simulations where the grid data does not fit on one compute node (see e.g. [2]). This paper presents a domain-decomposed Monte Carlo (DDMC) algorithm implemented in a new open source Monte Carlo code, Eiron. Two parallel algorithms currently used in EIRENE are also implemented in Eiron, and the three algorithms are compared by running strong scaling tests, with DDMC performing better than the other two algorithms in nearly all cases. On the supercomputer Mahti [3], DDMC strong scaling is superlinear for grids that do not fit into an L3 cache slice (4 MiB). The DDMC algorithm is also scaled up to 16384 cores in weak scaling tests, with a weak scaling efficiency of 45% in a high-collisional (heavier compute load) case, and 26% in a low-collisional (lighter compute load) case. We conclude that implementing this domain decomposition algorithm in EIRENE would improve performance and enable simulations that are currently impossible due to memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EIRENE [1] is a Monte Carlo neutral transport solver heavily used in the fusion community. EIRENE does not implement domain decomposition, making it impossible to use for simulations where the grid data does not fit on one compute node (see e.g. [2]). This paper presents a domain-decomposed Monte Carlo (DDMC) algorithm implemented in a new open source Monte Carlo code, Eiron. Two parallel algorithms currently used in EIRENE are also implemented in Eiron, and the three algorithms are compared by running strong scaling tests, with DDMC performing better than the other two algorithms in nearly all cases. On the supercomputer Mahti [3], DDMC strong scaling is superlinear for grids that do not fit into an L3 cache slice (4 MiB). The DDMC algorithm is also scaled up to 16384 cores in weak scaling tests, with a weak scaling efficiency of 45% in a high-collisional (heavier compute load) case, and 26% in a low-collisional (lighter compute load) case. We conclude that implementing this domain decomposition algorithm in EIRENE would improve performance and enable simulations that are currently impossible due to memory constraints."
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T16:08:24Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    8,
                    24,
                    3,
                    310,
                    0
                ],
                "arxiv_comment": "19 pages, 3 figures, submitted to Journal of Computational Physics",
                "arxiv_primary_category": {
                    "term": "physics.comp-ph"
                },
                "authors": [
                    {
                        "name": "Oskar Lappi"
                    },
                    {
                        "name": "Huw Leggate"
                    },
                    {
                        "name": "Yannick Marandet"
                    },
                    {
                        "name": "Jan Åström"
                    },
                    {
                        "name": "Keijo Heljanko"
                    },
                    {
                        "name": "Dmitriy V. Borodin"
                    }
                ],
                "author_detail": {
                    "name": "Dmitriy V. Borodin"
                },
                "author": "Dmitriy V. Borodin"
            },
            {
                "id": "http://arxiv.org/abs/2511.04464v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04464v1",
                "title": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context"
                },
                "updated": "2025-11-06T15:37:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    37,
                    11,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04464v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditional vehicle routing systems efficiently optimize singular metrics like time or distance, and when considering multiple metrics, they need more processes to optimize . However, they lack the capability to interpret and integrate the complex, semantic, and dynamic contexts of human drivers, such as multi-step tasks, situational constraints, or urgent needs. This paper introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a hybrid agentic assistant designed to augment classical pathfinding algorithms with contextual reasoning. Our approach employs a Large Language Model (LLM) agent that operates on a candidate set of routes generated by a multi-objective (time, CO2) Dijkstra algorithm. The agent evaluates these options against user-provided tasks, preferences, and avoidance rules by leveraging a pre-processed geospatial cache of urban Points of Interest (POIs). In a benchmark of realistic urban scenarios, PAVe successfully used complex user intent into appropriate route modifications, achieving over 88% accuracy in its initial route selections with a local model. We conclude that combining classical routing algorithms with an LLM-based semantic reasoning layer is a robust and effective approach for creating personalized, adaptive, and scalable solutions for urban mobility optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional vehicle routing systems efficiently optimize singular metrics like time or distance, and when considering multiple metrics, they need more processes to optimize . However, they lack the capability to interpret and integrate the complex, semantic, and dynamic contexts of human drivers, such as multi-step tasks, situational constraints, or urgent needs. This paper introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a hybrid agentic assistant designed to augment classical pathfinding algorithms with contextual reasoning. Our approach employs a Large Language Model (LLM) agent that operates on a candidate set of routes generated by a multi-objective (time, CO2) Dijkstra algorithm. The agent evaluates these options against user-provided tasks, preferences, and avoidance rules by leveraging a pre-processed geospatial cache of urban Points of Interest (POIs). In a benchmark of realistic urban scenarios, PAVe successfully used complex user intent into appropriate route modifications, achieving over 88% accuracy in its initial route selections with a local model. We conclude that combining classical routing algorithms with an LLM-based semantic reasoning layer is a robust and effective approach for creating personalized, adaptive, and scalable solutions for urban mobility optimization."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T15:37:11Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    15,
                    37,
                    11,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Carnot Braun"
                    },
                    {
                        "name": "Rafael O. Jarczewski"
                    },
                    {
                        "name": "Gabriel U. Talasso"
                    },
                    {
                        "name": "Leandro A. Villas"
                    },
                    {
                        "name": "Allan M. de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Allan M. de Souza"
                },
                "author": "Allan M. de Souza"
            },
            {
                "id": "http://arxiv.org/abs/2511.04421v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04421v1",
                "title": "Temporal Action Selection for Action Chunking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Action Selection for Action Chunking"
                },
                "updated": "2025-11-06T14:52:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    52,
                    54,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04421v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Action chunking is a widely adopted approach in Learning from Demonstration (LfD). By modeling multi-step action chunks rather than single-step actions, action chunking significantly enhances modeling capabilities for human expert policies. However, the reduced decision frequency restricts the utilization of recent observations, degrading reactivity - particularly evident in the inadequate adaptation to sensor noise and dynamic environmental changes. Existing efforts to address this issue have primarily resorted to trading off reactivity against decision consistency, without achieving both. To address this limitation, we propose a novel algorithm, Temporal Action Selector (TAS), which caches predicted action chunks from multiple timesteps and dynamically selects the optimal action through a lightweight selector network. TAS achieves balanced optimization across three critical dimensions: reactivity, decision consistency, and motion coherence. Experiments across multiple tasks with diverse base policies show that TAS significantly improves success rates - yielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a base policy with residual reinforcement learning (RL) substantially enhances training efficiency and elevates the performance plateau. Experiments in both simulation and physical robots confirm the method's efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action chunking is a widely adopted approach in Learning from Demonstration (LfD). By modeling multi-step action chunks rather than single-step actions, action chunking significantly enhances modeling capabilities for human expert policies. However, the reduced decision frequency restricts the utilization of recent observations, degrading reactivity - particularly evident in the inadequate adaptation to sensor noise and dynamic environmental changes. Existing efforts to address this issue have primarily resorted to trading off reactivity against decision consistency, without achieving both. To address this limitation, we propose a novel algorithm, Temporal Action Selector (TAS), which caches predicted action chunks from multiple timesteps and dynamically selects the optimal action through a lightweight selector network. TAS achieves balanced optimization across three critical dimensions: reactivity, decision consistency, and motion coherence. Experiments across multiple tasks with diverse base policies show that TAS significantly improves success rates - yielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a base policy with residual reinforcement learning (RL) substantially enhances training efficiency and elevates the performance plateau. Experiments in both simulation and physical robots confirm the method's efficacy."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T14:52:54Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    52,
                    54,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Yueyang Weng"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    },
                    {
                        "name": "Yongjin Mu"
                    },
                    {
                        "name": "Yingcong Zhu"
                    },
                    {
                        "name": "Yanjie Li"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.04406v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04406v1",
                "title": "Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning"
                },
                "updated": "2025-11-06T14:33:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    33,
                    29,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04406v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Data quality and its effective selection are fundamental to improving the performance of machine translation models, serving as cornerstones for achieving robust and reliable translation systems. This paper presents a data selection methodology specifically designed for fine-tuning machine translation systems, which leverages the synergy between a learner model and a pre-trained reference model to enhance overall training effectiveness. By defining a learnability score, our approach systematically evaluates the utility of data points for training, ensuring that only the most relevant and impactful examples contribute to the fine-tuning process. Furthermore, our method employs a batch selection strategy which considers interdependencies among data points, optimizing the efficiency of the training process while maintaining a focus on data relevance. Experiments on English to Persian and several other language pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that our method can achieve up to a fivefold improvement in data efficiency compared to an iid baseline. Experimental results indicate that our approach improves computational efficiency by 24 when utilizing cached embeddings, as it requires fewer training data points. Additionally, it enhances generalization, resulting in superior translation performance compared to random selection method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data quality and its effective selection are fundamental to improving the performance of machine translation models, serving as cornerstones for achieving robust and reliable translation systems. This paper presents a data selection methodology specifically designed for fine-tuning machine translation systems, which leverages the synergy between a learner model and a pre-trained reference model to enhance overall training effectiveness. By defining a learnability score, our approach systematically evaluates the utility of data points for training, ensuring that only the most relevant and impactful examples contribute to the fine-tuning process. Furthermore, our method employs a batch selection strategy which considers interdependencies among data points, optimizing the efficiency of the training process while maintaining a focus on data relevance. Experiments on English to Persian and several other language pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that our method can achieve up to a fivefold improvement in data efficiency compared to an iid baseline. Experimental results indicate that our approach improves computational efficiency by 24 when utilizing cached embeddings, as it requires fewer training data points. Additionally, it enhances generalization, resulting in superior translation performance compared to random selection method."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T14:33:29Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    14,
                    33,
                    29,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mohammad Amin Ghanizadeh"
                    },
                    {
                        "name": "Mohammad Javad Dousti"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Javad Dousti"
                },
                "author": "Mohammad Javad Dousti"
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.01110v3",
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory"
                },
                "updated": "2025-11-06T13:44:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    13,
                    44,
                    12,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.01110v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.01110v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Thomas Köhler"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger"
            },
            {
                "id": "http://arxiv.org/abs/2511.04002v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04002v1",
                "title": "Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing"
                },
                "updated": "2025-11-06T02:55:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    2,
                    55,
                    7,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04002v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have achieved near-human performance across diverse reasoning tasks, yet their deployment on resource-constrained Internet-of-Things (IoT) devices remains impractical due to massive parameter footprints and memory-intensive autoregressive decoding. While split computing offers a promising solution by partitioning model execution between edge devices and cloud servers, existing approaches fail to address the unique challenges of autoregressive inference, particularly the iterative token generation process and expanding key-value (KV) cache requirements. This work introduces the first autoregressive-aware split computing framework designed explicitly for LLM deployment on edge devices. Our approach makes three key contributions. First, we develop one-point split compression (OPSC), a mixed-precision quantization scheme that prevents out-of-memory failures by strategically partitioning models into front-end and back-end segments with different precision levels. Second, we propose a two-stage intermediate compression pipeline that combines threshold splitting (TS) and token-wise adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations while dramatically reducing communication overhead. Third, we formulate a unified optimization framework that jointly selects optimal split points, quantization settings, and sequence lengths to satisfy strict memory and latency constraints. Extensive evaluations across diverse LLMs and hardware platforms demonstrate superior performance compared to state-of-the-art quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework achieves a 1.49 inference speedup and significant communication overhead reduction while maintaining or improving model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved near-human performance across diverse reasoning tasks, yet their deployment on resource-constrained Internet-of-Things (IoT) devices remains impractical due to massive parameter footprints and memory-intensive autoregressive decoding. While split computing offers a promising solution by partitioning model execution between edge devices and cloud servers, existing approaches fail to address the unique challenges of autoregressive inference, particularly the iterative token generation process and expanding key-value (KV) cache requirements. This work introduces the first autoregressive-aware split computing framework designed explicitly for LLM deployment on edge devices. Our approach makes three key contributions. First, we develop one-point split compression (OPSC), a mixed-precision quantization scheme that prevents out-of-memory failures by strategically partitioning models into front-end and back-end segments with different precision levels. Second, we propose a two-stage intermediate compression pipeline that combines threshold splitting (TS) and token-wise adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations while dramatically reducing communication overhead. Third, we formulate a unified optimization framework that jointly selects optimal split points, quantization settings, and sequence lengths to satisfy strict memory and latency constraints. Extensive evaluations across diverse LLMs and hardware platforms demonstrate superior performance compared to state-of-the-art quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework achieves a 1.49 inference speedup and significant communication overhead reduction while maintaining or improving model accuracy."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T02:55:07Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    2,
                    55,
                    7,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Mingyu Sung"
                    },
                    {
                        "name": "Vikas Palakonda"
                    },
                    {
                        "name": "Suhwan Im"
                    },
                    {
                        "name": "Sunghwan Moon"
                    },
                    {
                        "name": "Il-Min Kim"
                    },
                    {
                        "name": "Sangseok Yun"
                    },
                    {
                        "name": "Jae-Mo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Mo Kang"
                },
                "author": "Jae-Mo Kang"
            },
            {
                "id": "http://arxiv.org/abs/2505.24722v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.24722v2",
                "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts"
                },
                "updated": "2025-11-06T01:18:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    1,
                    18,
                    3,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.24722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.24722v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-30T15:42:42Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    42,
                    42,
                    4,
                    150,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Neil He"
                    },
                    {
                        "name": "Rishabh Anand"
                    },
                    {
                        "name": "Hiren Madhu"
                    },
                    {
                        "name": "Ali Maatouk"
                    },
                    {
                        "name": "Smita Krishnaswamy"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying"
            },
            {
                "id": "http://arxiv.org/abs/2511.03944v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.03944v1",
                "title": "From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies"
                },
                "updated": "2025-11-06T00:42:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    0,
                    42,
                    29,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.03944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.03944v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a simple, storage-memory-economics-based heuristic for deciding when data should live in DRAM rather than on storage. Subsequent revisits to the rule largely retained that economics-only view, leaving host costs, feasibility limits, and workload behavior out of scope. This paper revisits the rule from first principles, integrating host costs, DRAM bandwidth/capacity, and physics-grounded models of SSD performance and cost, and then embedding these elements in a constraint- and workload-aware framework that yields actionable provisioning guidance. We show that, for modern AI platforms, especially GPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained random access, the DRAM-to-flash caching threshold collapses from minutes to a few seconds. This shift reframes NAND flash memory as an active data tier and exposes a broad research space across the hardware-software stack. We further introduce MQSim-Next, a calibrated SSD simulator that supports validation and sensitivity analysis and facilitates future architectural and system research. Finally, we present two concrete case studies that showcase the software system design space opened by such memory hierarchy paradigm shift. Overall, we turn a classical heuristic into an actionable, feasibility-aware analysis and provisioning framework and set the stage for further research on AI-era memory hierarchy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a simple, storage-memory-economics-based heuristic for deciding when data should live in DRAM rather than on storage. Subsequent revisits to the rule largely retained that economics-only view, leaving host costs, feasibility limits, and workload behavior out of scope. This paper revisits the rule from first principles, integrating host costs, DRAM bandwidth/capacity, and physics-grounded models of SSD performance and cost, and then embedding these elements in a constraint- and workload-aware framework that yields actionable provisioning guidance. We show that, for modern AI platforms, especially GPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained random access, the DRAM-to-flash caching threshold collapses from minutes to a few seconds. This shift reframes NAND flash memory as an active data tier and exposes a broad research space across the hardware-software stack. We further introduce MQSim-Next, a calibrated SSD simulator that supports validation and sensitivity analysis and facilitates future architectural and system research. Finally, we present two concrete case studies that showcase the software system design space opened by such memory hierarchy paradigm shift. Overall, we turn a classical heuristic into an actionable, feasibility-aware analysis and provisioning framework and set the stage for further research on AI-era memory hierarchy."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T00:42:29Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    0,
                    42,
                    29,
                    3,
                    310,
                    0
                ],
                "arxiv_comment": "13 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Chris J. Newburn"
                    },
                    {
                        "name": "Teresa Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jiangpeng Li"
                    },
                    {
                        "name": "Hao Zhong"
                    },
                    {
                        "name": "Wen-Mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Mei Hwu"
                },
                "author": "Wen-Mei Hwu"
            },
            {
                "id": "http://arxiv.org/abs/2505.22913v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.22913v2",
                "title": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference"
                },
                "updated": "2025-11-06T00:11:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    6,
                    0,
                    11,
                    18,
                    3,
                    310,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.22913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.22913v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We demonstrate that unstructured sparsity significantly improves KV cache compression for LLMs, enabling sparsity levels up to 70% without compromising accuracy or requiring fine-tuning. We conduct a systematic exploration of pruning strategies and find per-token magnitude-based pruning as highly effective for both Key and Value caches under unstructured sparsity, surpassing prior structured pruning schemes. The Key cache benefits from prominent outlier elements, while the Value cache surprisingly benefits from a simple magnitude-based pruning despite its uniform distribution. KV cache size is the major bottleneck in decode performance due to high memory overhead for large context lengths. To address this, we use a bitmap-based sparse format and a custom attention kernel capable of compressing and directly computing over compressed caches pruned to arbitrary sparsity patterns, significantly accelerating memory-bound operations in decode computations and thereby compensating for the overhead of runtime pruning and compression. Our custom attention kernel coupled with the bitmap-based format delivers substantial compression of KV cache upto 45% of dense inference and thereby enables longer context length and increased tokens/sec throughput of upto 2.23x compared to dense inference. Our pruning mechanism and sparse attention kernel is available at https://github.com/dhjoo98/mustafar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that unstructured sparsity significantly improves KV cache compression for LLMs, enabling sparsity levels up to 70% without compromising accuracy or requiring fine-tuning. We conduct a systematic exploration of pruning strategies and find per-token magnitude-based pruning as highly effective for both Key and Value caches under unstructured sparsity, surpassing prior structured pruning schemes. The Key cache benefits from prominent outlier elements, while the Value cache surprisingly benefits from a simple magnitude-based pruning despite its uniform distribution. KV cache size is the major bottleneck in decode performance due to high memory overhead for large context lengths. To address this, we use a bitmap-based sparse format and a custom attention kernel capable of compressing and directly computing over compressed caches pruned to arbitrary sparsity patterns, significantly accelerating memory-bound operations in decode computations and thereby compensating for the overhead of runtime pruning and compression. Our custom attention kernel coupled with the bitmap-based format delivers substantial compression of KV cache upto 45% of dense inference and thereby enables longer context length and increased tokens/sec throughput of upto 2.23x compared to dense inference. Our pruning mechanism and sparse attention kernel is available at https://github.com/dhjoo98/mustafar."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-28T22:32:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    22,
                    32,
                    15,
                    2,
                    148,
                    0
                ],
                "arxiv_comment": "20 pages, 9 figures, NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Donghyeon Joo"
                    },
                    {
                        "name": "Helya Hosseini"
                    },
                    {
                        "name": "Ramyad Hadidi"
                    },
                    {
                        "name": "Bahar Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Bahar Asgari"
                },
                "author": "Bahar Asgari"
            },
            {
                "id": "http://arxiv.org/abs/2511.03830v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.03830v1",
                "title": "Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification"
                },
                "updated": "2025-11-05T19:53:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    19,
                    53,
                    51,
                    2,
                    309,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.03830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.03830v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce a method for efficient multi-label text classification with large language models (LLMs), built on reformulating classification tasks as sequences of dichotomic (yes/no) decisions. Instead of generating all labels in a single structured response, each target dimension is queried independently, which, combined with a prefix caching mechanism, yields substantial efficiency gains for short-text inference without loss of accuracy. To demonstrate the approach, we focus on affective text analysis, covering 24 dimensions including emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator model (DeepSeek-V3) provides multiple annotations per text, which are aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B, Gemma3-1B). The fine-tuned models show significant improvements over zero-shot baselines, particularly on the dimensions seen during training. Our findings suggest that decomposing multi-label classification into dichotomic queries, combined with distillation and cache-aware inference, offers a scalable and effective framework for LLM-based classification. While we validate the method on affective states, the approach is general and applicable across domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for efficient multi-label text classification with large language models (LLMs), built on reformulating classification tasks as sequences of dichotomic (yes/no) decisions. Instead of generating all labels in a single structured response, each target dimension is queried independently, which, combined with a prefix caching mechanism, yields substantial efficiency gains for short-text inference without loss of accuracy. To demonstrate the approach, we focus on affective text analysis, covering 24 dimensions including emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator model (DeepSeek-V3) provides multiple annotations per text, which are aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B, Gemma3-1B). The fine-tuned models show significant improvements over zero-shot baselines, particularly on the dimensions seen during training. Our findings suggest that decomposing multi-label classification into dichotomic queries, combined with distillation and cache-aware inference, offers a scalable and effective framework for LLM-based classification. While we validate the method on affective states, the approach is general and applicable across domains."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-05T19:53:51Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    19,
                    53,
                    51,
                    2,
                    309,
                    0
                ],
                "arxiv_comment": "9 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mikołaj Langner"
                    },
                    {
                        "name": "Jan Eliasz"
                    },
                    {
                        "name": "Ewa Rudnicka"
                    },
                    {
                        "name": "Jan Kocoń"
                    }
                ],
                "author_detail": {
                    "name": "Jan Kocoń"
                },
                "author": "Jan Kocoń"
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.02558v2",
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "updated": "2025-11-05T14:29:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    14,
                    29,
                    12,
                    2,
                    309,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.02558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.02558v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness. The code is available at https://github.com/OpenMOSS/Sparse-dLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness. The code is available at https://github.com/OpenMOSS/Sparse-dLLM."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "arxiv_comment": "12 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu"
            },
            {
                "id": "http://arxiv.org/abs/2511.03475v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.03475v1",
                "title": "RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse"
                },
                "updated": "2025-11-05T13:59:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.03475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.03475v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with retrieved context but often suffers from downgraded prefill performance as modern applications demand longer and more complex inputs. Existing caching techniques either preserve accuracy with low cache reuse or improve reuse at the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG system that achieves high cache reuse without sacrificing accuracy through accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items across concurrent sessions and multi-turn interactions, using efficient context indexing, ordering, and de-duplication to maximize reuse, while lightweight contextual hints maintain reasoning fidelity. It integrates seamlessly with existing LLM inference engines and improves their prefill performance by 1.5-3X over state-of-the-art methods, while preserving or even enhancing reasoning accuracy across diverse RAG and agentic AI workloads. Our code is released at: https://github.com/Edinburgh-AgenticAI/RAGBoost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with retrieved context but often suffers from downgraded prefill performance as modern applications demand longer and more complex inputs. Existing caching techniques either preserve accuracy with low cache reuse or improve reuse at the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG system that achieves high cache reuse without sacrificing accuracy through accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items across concurrent sessions and multi-turn interactions, using efficient context indexing, ordering, and de-duplication to maximize reuse, while lightweight contextual hints maintain reasoning fidelity. It integrates seamlessly with existing LLM inference engines and improves their prefill performance by 1.5-3X over state-of-the-art methods, while preserving or even enhancing reasoning accuracy across diverse RAG and agentic AI workloads. Our code is released at: https://github.com/Edinburgh-AgenticAI/RAGBoost."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-05T13:59:01Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    59,
                    1,
                    2,
                    309,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yinsicheng Jiang"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Liang Cheng"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xuan Sun"
                    },
                    {
                        "name": "Luo Mai"
                    }
                ],
                "author_detail": {
                    "name": "Luo Mai"
                },
                "author": "Luo Mai"
            },
            {
                "id": "http://arxiv.org/abs/2511.11617v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11617v1",
                "title": "AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism"
                },
                "updated": "2025-11-05T13:21:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    21,
                    34,
                    2,
                    309,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11617v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Model (LLM) inference services demand exceptionally high availability and low latency, yet multi-GPU Tensor Parallelism (TP) makes them vulnerable to single-GPU failures. We present AnchorTP, a state-preserving elastic TP framework for fast recovery. It (i) enables Elastic Tensor Parallelism (ETP) with unequal-width partitioning over any number of GPUs and compatibility with Mixture-of-Experts (MoE), and (ii) preserves model parameters and KV caches in GPU memory via a daemon decoupled from the inference process. To minimize downtime, we propose a bandwidth-aware planner based on a Continuous Minimal Migration (CMM) algorithm that minimizes reload bytes under a byte-cost dominance assumption, and an execution scheduler that pipelines P2P transfers with reloads. These components jointly restore service quickly with minimal data movement and without changing service interfaces. In typical failure scenarios, AnchorTP reduces Time to First Success (TFS) by up to 11x and Time to Peak (TTP) by up to 59% versus restart-and-reload.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference services demand exceptionally high availability and low latency, yet multi-GPU Tensor Parallelism (TP) makes them vulnerable to single-GPU failures. We present AnchorTP, a state-preserving elastic TP framework for fast recovery. It (i) enables Elastic Tensor Parallelism (ETP) with unequal-width partitioning over any number of GPUs and compatibility with Mixture-of-Experts (MoE), and (ii) preserves model parameters and KV caches in GPU memory via a daemon decoupled from the inference process. To minimize downtime, we propose a bandwidth-aware planner based on a Continuous Minimal Migration (CMM) algorithm that minimizes reload bytes under a byte-cost dominance assumption, and an execution scheduler that pipelines P2P transfers with reloads. These components jointly restore service quickly with minimal data movement and without changing service interfaces. In typical failure scenarios, AnchorTP reduces Time to First Success (TFS) by up to 11x and Time to Peak (TTP) by up to 59% versus restart-and-reload."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-05T13:21:34Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    13,
                    21,
                    34,
                    2,
                    309,
                    0
                ],
                "arxiv_comment": "accpeted paper by Design, Automation and Test in Europe Conference (DATE'26). 8 pages in total with 6 figures and 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Wendong Xu"
                    },
                    {
                        "name": "Chujie Chen"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Wenyong Zhou"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong"
            },
            {
                "id": "http://arxiv.org/abs/2506.09045v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.09045v2",
                "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagCache: Fast Video Generation with Magnitude-Aware Cache"
                },
                "updated": "2025-11-05T09:18:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    9,
                    18,
                    48,
                    2,
                    309,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.09045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.09045v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features. These approaches typically require extensive calibration with curated prompts and risk inconsistent outputs due to prompt-specific overfitting. In this paper, we introduce a novel and robust discovery: a unified magnitude law observed across different models and prompts. Specifically, the magnitude ratio of successive residual outputs decreases monotonically, steadily in most timesteps while rapidly in the last several steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache) that adaptively skips unimportant timesteps using an error modeling mechanism and adaptive caching strategy. Unlike existing methods requiring dozens of curated samples for calibration, MagCache only requires a single sample for calibration. Experimental results show that MagCache achieves 2.10x-2.68x speedups on Open-Sora, CogVideoX, Wan 2.1, and HunyuanVideo, while preserving superior visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM, and PSNR, under similar computational budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features. These approaches typically require extensive calibration with curated prompts and risk inconsistent outputs due to prompt-specific overfitting. In this paper, we introduce a novel and robust discovery: a unified magnitude law observed across different models and prompts. Specifically, the magnitude ratio of successive residual outputs decreases monotonically, steadily in most timesteps while rapidly in the last several steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache) that adaptively skips unimportant timesteps using an error modeling mechanism and adaptive caching strategy. Unlike existing methods requiring dozens of curated samples for calibration, MagCache only requires a single sample for calibration. Experimental results show that MagCache achieves 2.10x-2.68x speedups on Open-Sora, CogVideoX, Wan 2.1, and HunyuanVideo, while preserving superior visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM, and PSNR, under similar computational budgets."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "arxiv_comment": "Project Page: https://zehong-ma.github.io/MagCache Accepted by NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "arxiv_journal_ref": "In Proceedings of NeurIPS 2025",
                "authors": [
                    {
                        "name": "Zehong Ma"
                    },
                    {
                        "name": "Longhui Wei"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian"
            },
            {
                "id": "http://arxiv.org/abs/2511.03159v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.03159v1",
                "title": "Joint Optimization of DNN Model Caching and Request Routing in Mobile Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Optimization of DNN Model Caching and Request Routing in Mobile Edge Computing"
                },
                "updated": "2025-11-05T03:40:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    5,
                    3,
                    40,
                    44,
                    2,
                    309,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.03159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.03159v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near end-users, providing low-latency services and improving users' quality of experience (QoE). However, caching all DNN models at edge servers with limited capacity is difficult, and the impact of model loading time on QoE remains underexplored. Hence, we introduce dynamic DNNs in edge scenarios, disassembling a complete DNN model into interrelated submodels for more fine-grained and flexible model caching and request routing solutions. This raises the pressing issue of jointly deciding request routing and submodel caching for dynamic DNNs to balance model inference precision and loading latency for QoE optimization. In this paper, we study the joint dynamic model caching and request routing problem in MEC networks, aiming to maximize user request inference precision under constraints of server resources, latency, and model loading time. To tackle this problem, we propose CoCaR, an offline algorithm based on linear programming and random rounding that leverages dynamic DNNs to optimize caching and routing schemes, achieving near-optimal performance. Furthermore, we develop an online variant of CoCaR, named CoCaR-OL, enabling effective adaptation to dynamic and unpredictable online request patterns. The simulation results demonstrate that the proposed CoCaR improves the average inference precision of user requests by 46\\% compared to state-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves an improvement of no less than 32.3\\% in user QoE over competitive baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near end-users, providing low-latency services and improving users' quality of experience (QoE). However, caching all DNN models at edge servers with limited capacity is difficult, and the impact of model loading time on QoE remains underexplored. Hence, we introduce dynamic DNNs in edge scenarios, disassembling a complete DNN model into interrelated submodels for more fine-grained and flexible model caching and request routing solutions. This raises the pressing issue of jointly deciding request routing and submodel caching for dynamic DNNs to balance model inference precision and loading latency for QoE optimization. In this paper, we study the joint dynamic model caching and request routing problem in MEC networks, aiming to maximize user request inference precision under constraints of server resources, latency, and model loading time. To tackle this problem, we propose CoCaR, an offline algorithm based on linear programming and random rounding that leverages dynamic DNNs to optimize caching and routing schemes, achieving near-optimal performance. Furthermore, we develop an online variant of CoCaR, named CoCaR-OL, enabling effective adaptation to dynamic and unpredictable online request patterns. The simulation results demonstrate that the proposed CoCaR improves the average inference precision of user requests by 46\\% compared to state-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves an improvement of no less than 32.3\\% in user QoE over competitive baselines."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-05T03:40:44Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    3,
                    40,
                    44,
                    2,
                    309,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Shuting Qiu"
                    },
                    {
                        "name": "Fang Dong"
                    },
                    {
                        "name": "Siyu Tan"
                    },
                    {
                        "name": "Ruiting Zhou"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Patrick P. C. Lee"
                    },
                    {
                        "name": "Qilin Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qilin Fan"
                },
                "author": "Qilin Fan"
            },
            {
                "id": "http://arxiv.org/abs/2511.02919v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02919v1",
                "title": "Cache Mechanism for Agent RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Mechanism for Agent RAG Systems"
                },
                "updated": "2025-11-04T19:02:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    19,
                    2,
                    29,
                    1,
                    308,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02919v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in Large Language Model (LLM)-based agents have been propelled by Retrieval-Augmented Generation (RAG), which grants the models access to vast external knowledge bases. Despite RAG's success in improving agent performance, agent-level cache management, particularly constructing, maintaining, and updating a compact, relevant corpus dynamically tailored to each agent's need, remains underexplored. Therefore, we introduce ARC (Agent RAG Cache Mechanism), a novel, annotation-free caching framework that dynamically manages small, high-value corpora for each agent. By synthesizing historical query distribution patterns with the intrinsic geometry of cached items in the embedding space, ARC automatically maintains a high-relevance cache. With comprehensive experiments on three retrieval datasets, our experimental results demonstrate that ARC reduces storage requirements to 0.015% of the original corpus while offering up to 79.8% has-answer rate and reducing average retrieval latency by 80%. Our results demonstrate that ARC can drastically enhance efficiency and effectiveness in RAG-powered LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Model (LLM)-based agents have been propelled by Retrieval-Augmented Generation (RAG), which grants the models access to vast external knowledge bases. Despite RAG's success in improving agent performance, agent-level cache management, particularly constructing, maintaining, and updating a compact, relevant corpus dynamically tailored to each agent's need, remains underexplored. Therefore, we introduce ARC (Agent RAG Cache Mechanism), a novel, annotation-free caching framework that dynamically manages small, high-value corpora for each agent. By synthesizing historical query distribution patterns with the intrinsic geometry of cached items in the embedding space, ARC automatically maintains a high-relevance cache. With comprehensive experiments on three retrieval datasets, our experimental results demonstrate that ARC reduces storage requirements to 0.015% of the original corpus while offering up to 79.8% has-answer rate and reducing average retrieval latency by 80%. Our results demonstrate that ARC can drastically enhance efficiency and effectiveness in RAG-powered LLM agents."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-04T19:02:29Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    19,
                    2,
                    29,
                    1,
                    308,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Zhencan Peng"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Xiao Lin"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang"
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.13717v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13717v1",
                "title": "TZ-LLM: Protecting On-Device Large Language Models with Arm TrustZone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TZ-LLM: Protecting On-Device Large Language Models with Arm TrustZone"
                },
                "updated": "2025-11-17T18:59:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    59,
                    20,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13717v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) deployed on mobile devices offer benefits like user privacy and reduced network latency, but introduce a significant security risk: the leakage of proprietary models to end users.\n  To mitigate this risk, we propose a system design for protecting on-device LLMs using Arm Trusted Execution Environment (TEE), TrustZone. Our system addresses two primary challenges: (1) The dilemma between memory efficiency and fast inference (caching model parameters within TEE memory). (2) The lack of efficient and secure Neural Processing Unit (NPU) time-sharing between Rich Execution Environment (REE) and TEE.\n  Our approach incorporates two key innovations. First, we employ pipelined restoration, leveraging the deterministic memory access patterns of LLM inference to prefetch parameters on demand, hiding memory allocation, I/O and decryption latency under computation time. Second, we introduce a co-driver design, creating a minimal data plane NPU driver in the TEE that collaborates with the full-fledged REE driver. This reduces the TEE TCB size and eliminates control plane reinitialization overhead during NPU world switches.\n  We implemented our system on the emerging OpenHarmony OS and the llama.cpp inference framework, and evaluated it with various LLMs on an Arm Rockchip device. Compared to a strawman TEE baseline lacking our optimizations, our system reduces TTFT by up to 90.9% and increases decoding speed by up to 23.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) deployed on mobile devices offer benefits like user privacy and reduced network latency, but introduce a significant security risk: the leakage of proprietary models to end users.\n  To mitigate this risk, we propose a system design for protecting on-device LLMs using Arm Trusted Execution Environment (TEE), TrustZone. Our system addresses two primary challenges: (1) The dilemma between memory efficiency and fast inference (caching model parameters within TEE memory). (2) The lack of efficient and secure Neural Processing Unit (NPU) time-sharing between Rich Execution Environment (REE) and TEE.\n  Our approach incorporates two key innovations. First, we employ pipelined restoration, leveraging the deterministic memory access patterns of LLM inference to prefetch parameters on demand, hiding memory allocation, I/O and decryption latency under computation time. Second, we introduce a co-driver design, creating a minimal data plane NPU driver in the TEE that collaborates with the full-fledged REE driver. This reduces the TEE TCB size and eliminates control plane reinitialization overhead during NPU world switches.\n  We implemented our system on the emerging OpenHarmony OS and the llama.cpp inference framework, and evaluated it with various LLMs on an Arm Rockchip device. Compared to a strawman TEE baseline lacking our optimizations, our system reduces TTFT by up to 90.9% and increases decoding speed by up to 23.2%."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:59:20Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    59,
                    20,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Xunjie Wang"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Zhichao Hua"
                    },
                    {
                        "name": "Jinyu Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinyu Gu"
                },
                "author": "Jinyu Gu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13708v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13708v1",
                "title": "The Scatter of the Many Outweighs the Scatter of the Few: Systematic Error Asymmetry in Steeply-Falling Mass Functions for High-Redshift JWST Galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Scatter of the Many Outweighs the Scatter of the Few: Systematic Error Asymmetry in Steeply-Falling Mass Functions for High-Redshift JWST Galaxies"
                },
                "updated": "2025-11-17T18:55:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    55,
                    24,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13708v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The discovery of massive, high redshift galaxies with JWST has been argued to challenge $Λ$CDM: such systems would require extremely rare halos and baryon-to-stellar-mass conversion efficiencies unphysically approaching--or exceeding--100%. If confirmed at galaxy formation forbidden efficiencies, these galaxies could signal new physics beyond standard cosmological structure formation. We develop a galaxy model framework that ties the linear power spectrum to the inferred efficiencies of galaxy growth in order to test the structure formation models. In addition, we incorporate multiple sources of error, including (i) observational sample variance, (ii) asymmetric scatter induced by the steepness of the high-mass halo tail, and (iii) systematic uncertainties in stellar mass estimates. We find that the inferred efficiency of star formation is dominated by systematic uncertainties on the spectral energy distribution inferred stellar mass of the JWST detected galaxies. The systematic uncertainty augments the asymmetry in scatter that largely brings the inferred efficiencies to be in line with that expected from early galaxy formation models. Our framework can be used to test $Λ$CDM as errors are reduced and further detections are made.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of massive, high redshift galaxies with JWST has been argued to challenge $Λ$CDM: such systems would require extremely rare halos and baryon-to-stellar-mass conversion efficiencies unphysically approaching--or exceeding--100%. If confirmed at galaxy formation forbidden efficiencies, these galaxies could signal new physics beyond standard cosmological structure formation. We develop a galaxy model framework that ties the linear power spectrum to the inferred efficiencies of galaxy growth in order to test the structure formation models. In addition, we incorporate multiple sources of error, including (i) observational sample variance, (ii) asymmetric scatter induced by the steepness of the high-mass halo tail, and (iii) systematic uncertainties in stellar mass estimates. We find that the inferred efficiency of star formation is dominated by systematic uncertainties on the spectral energy distribution inferred stellar mass of the JWST detected galaxies. The systematic uncertainty augments the asymmetry in scatter that largely brings the inferred efficiencies to be in line with that expected from early galaxy formation models. Our framework can be used to test $Λ$CDM as errors are reduced and further detections are made."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:55:24Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    55,
                    24,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "8 pages, 5 figures, 1 table. Code available at https://github.com/jaykrishnan9121/JWST_Early_Galaxies",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "Jay R. Krishnan"
                    },
                    {
                        "name": "Kevork N. Abazajian"
                    }
                ],
                "author_detail": {
                    "name": "Kevork N. Abazajian"
                },
                "author": "Kevork N. Abazajian"
            },
            {
                "id": "http://arxiv.org/abs/2511.13704v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13704v1",
                "title": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models"
                },
                "updated": "2025-11-17T18:52:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    52,
                    44,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13704v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:52:44Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    52,
                    44,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Project: https://haroldchen19.github.io/TiViBench-Page/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Harold Haodong Chen"
                    },
                    {
                        "name": "Disen Lan"
                    },
                    {
                        "name": "Wen-Jie Shu"
                    },
                    {
                        "name": "Qingyang Liu"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Sirui Chen"
                    },
                    {
                        "name": "Wenkai Cheng"
                    },
                    {
                        "name": "Kanghao Chen"
                    },
                    {
                        "name": "Hongfei Zhang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Rongjin Guo"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Cong Chen"
                },
                "author": "Ying-Cong Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.13703v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13703v1",
                "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations"
                },
                "updated": "2025-11-17T18:52:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    52,
                    22,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13703v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:52:22Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    52,
                    22,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Lavender Y. Jiang"
                    },
                    {
                        "name": "Angelica Chen"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Xujin Chris Liu"
                    },
                    {
                        "name": "Radhika Dua"
                    },
                    {
                        "name": "Kevin Eaton"
                    },
                    {
                        "name": "Frederick Wolff"
                    },
                    {
                        "name": "Robert Steele"
                    },
                    {
                        "name": "Jeff Zhang"
                    },
                    {
                        "name": "Anton Alyakin"
                    },
                    {
                        "name": "Qingkai Pan"
                    },
                    {
                        "name": "Yanbing Chen"
                    },
                    {
                        "name": "Karl L. Sangwon"
                    },
                    {
                        "name": "Daniel A. Alber"
                    },
                    {
                        "name": "Jaden Stryker"
                    },
                    {
                        "name": "Jin Vivian Lee"
                    },
                    {
                        "name": "Yindalon Aphinyanaphongs"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    },
                    {
                        "name": "Eric Karl Oermann"
                    }
                ],
                "author_detail": {
                    "name": "Eric Karl Oermann"
                },
                "author": "Eric Karl Oermann"
            },
            {
                "id": "http://arxiv.org/abs/2511.13701v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13701v1",
                "title": "Learning stochasticity: a nonparametric framework for intrinsic noise estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning stochasticity: a nonparametric framework for intrinsic noise estimation"
                },
                "updated": "2025-11-17T18:52:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    52,
                    5,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13701v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:52:05Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    52,
                    5,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Gianluigi Pillonetto"
                    },
                    {
                        "name": "Alberto Giaretta"
                    },
                    {
                        "name": "Mauro Bisiacco"
                    }
                ],
                "author_detail": {
                    "name": "Mauro Bisiacco"
                },
                "author": "Mauro Bisiacco"
            },
            {
                "id": "http://arxiv.org/abs/2511.13692v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13692v1",
                "title": "The physical properties of post-mass-transfer binaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The physical properties of post-mass-transfer binaries"
                },
                "updated": "2025-11-17T18:46:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    46,
                    39,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13692v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Aims. We present and analyse the detailed physical properties of six binary stellar systems, originally proposed as possible star-black hole binaries on the basis of radial velocities from Gaia's third data release, but soon recognised as likely post-mass-transfer binary systems with stripped companions. Methods. We used multi-epoch high-resolution FEROS spectra and spectral disentangling paired with stellar templates to derive effective temperatures, $T_\\mathrm{eff}$; stellar radii, R*; and projected rotational velocities, v$\\sin{i}$ for both components in all systems along with the mass ratio, q = $M_\\mathrm{accretor}/M_\\mathrm{donor}$ and the components' flux ratio as a function of wavelength. Results. Our analysis directly confirms that all systems are post-mass-transfer binaries with two luminous stars, i.e. no black hole companions. Each system contains an A-type accretor component that is rapidly rotating and a cooler very low-mass donor (~ 0.25M$\\odot$) that is overluminous. Five of the systems show no trace of any emission lines, implying that there is no current mass transfer, consistent with our inferred radii, in all cases within the Roche volume. The data are generally consistent with stable case AB mass transfer with $β$ (the fraction of mass lost from the accretor) less than 0.7. While the accretor components rotate rapidly, they rotate well below the critical rotation rate, $v_\\mathrm{crit}$, even though there must have been enough mass transfer to spin them up to a significant fraction of $v_\\mathrm{crit}$, according to theoretical models of angular momentum transfer. As neither magnetic braking nor tidal synchronisation should have been effective in spinning down the stars, our results suggest that either mass accretion does not increase the angular momentum of the accretors to their critical values or the systems never reached these values in the first place.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aims. We present and analyse the detailed physical properties of six binary stellar systems, originally proposed as possible star-black hole binaries on the basis of radial velocities from Gaia's third data release, but soon recognised as likely post-mass-transfer binary systems with stripped companions. Methods. We used multi-epoch high-resolution FEROS spectra and spectral disentangling paired with stellar templates to derive effective temperatures, $T_\\mathrm{eff}$; stellar radii, R*; and projected rotational velocities, v$\\sin{i}$ for both components in all systems along with the mass ratio, q = $M_\\mathrm{accretor}/M_\\mathrm{donor}$ and the components' flux ratio as a function of wavelength. Results. Our analysis directly confirms that all systems are post-mass-transfer binaries with two luminous stars, i.e. no black hole companions. Each system contains an A-type accretor component that is rapidly rotating and a cooler very low-mass donor (~ 0.25M$\\odot$) that is overluminous. Five of the systems show no trace of any emission lines, implying that there is no current mass transfer, consistent with our inferred radii, in all cases within the Roche volume. The data are generally consistent with stable case AB mass transfer with $β$ (the fraction of mass lost from the accretor) less than 0.7. While the accretor components rotate rapidly, they rotate well below the critical rotation rate, $v_\\mathrm{crit}$, even though there must have been enough mass transfer to spin them up to a significant fraction of $v_\\mathrm{crit}$, according to theoretical models of angular momentum transfer. As neither magnetic braking nor tidal synchronisation should have been effective in spinning down the stars, our results suggest that either mass accretion does not increase the angular momentum of the accretors to their critical values or the systems never reached these values in the first place."
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:46:39Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    46,
                    39,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted for publication in Astronomy and Astrophysics",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR"
                },
                "authors": [
                    {
                        "name": "Rhys Seeburger"
                    },
                    {
                        "name": "Hans-Walter Rix"
                    },
                    {
                        "name": "Kareem El-Badry"
                    },
                    {
                        "name": "Johanna Müller-Horn"
                    },
                    {
                        "name": "Alex J. Dimoff"
                    },
                    {
                        "name": "Jan Henneco"
                    },
                    {
                        "name": "Jaime I. Villaseñor"
                    }
                ],
                "author_detail": {
                    "name": "Jaime I. Villaseñor"
                },
                "author": "Jaime I. Villaseñor"
            },
            {
                "id": "http://arxiv.org/abs/2511.13689v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13689v1",
                "title": "Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation"
                },
                "updated": "2025-11-17T18:41:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    41,
                    16,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13689v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:41:16Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    41,
                    16,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Sofia Jamil"
                    },
                    {
                        "name": "Kotla Sai Charan"
                    },
                    {
                        "name": "Sriparna Saha"
                    },
                    {
                        "name": "Koustava Goswami"
                    },
                    {
                        "name": "Joseph K J"
                    }
                ],
                "author_detail": {
                    "name": "Joseph K J"
                },
                "author": "Joseph K J"
            },
            {
                "id": "http://arxiv.org/abs/2510.03898v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.03898v2",
                "title": "Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles"
                },
                "updated": "2025-11-17T18:39:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    39,
                    10,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.03898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.03898v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Detecting media bias is crucial, specifically in the South Asian region. Despite this, annotated datasets and computational studies for Bangla political bias research remain scarce. Crucially because, political stance detection in Bangla news requires understanding of linguistic cues, cultural context, subtle biases, rhetorical strategies, code-switching, implicit sentiment, and socio-political background. To address this, we introduce the first benchmark dataset of 200 politically significant and highly debated Bangla news articles, labeled for government-leaning, government-critique, and neutral stances, alongside diagnostic analyses for evaluating large language models (LLMs). Our comprehensive evaluation of 28 proprietary and open-source LLMs shows strong performance in detecting government-critique content (F1 up to 0.83) but substantial difficulty with neutral articles (F1 as low as 0.00). Models also tend to over-predict government-leaning stances, often misinterpreting ambiguous narratives. This dataset and its associated diagnostics provide a foundation for advancing stance detection in Bangla media research and offer insights for improving LLM performance in low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting media bias is crucial, specifically in the South Asian region. Despite this, annotated datasets and computational studies for Bangla political bias research remain scarce. Crucially because, political stance detection in Bangla news requires understanding of linguistic cues, cultural context, subtle biases, rhetorical strategies, code-switching, implicit sentiment, and socio-political background. To address this, we introduce the first benchmark dataset of 200 politically significant and highly debated Bangla news articles, labeled for government-leaning, government-critique, and neutral stances, alongside diagnostic analyses for evaluating large language models (LLMs). Our comprehensive evaluation of 28 proprietary and open-source LLMs shows strong performance in detecting government-critique content (F1 up to 0.83) but substantial difficulty with neutral articles (F1 as low as 0.00). Models also tend to over-predict government-leaning stances, often misinterpreting ambiguous narratives. This dataset and its associated diagnostics provide a foundation for advancing stance detection in Bangla media research and offer insights for improving LLM performance in low-resource languages."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-04T18:34:34Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    18,
                    34,
                    34,
                    5,
                    277,
                    0
                ],
                "arxiv_comment": "Accepted to BLP at AACL-IJCNLP 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Nusrat Jahan Lia"
                    },
                    {
                        "name": "Shubhashis Roy Dipta"
                    },
                    {
                        "name": "Abdullah Khan Zehady"
                    },
                    {
                        "name": "Naymul Islam"
                    },
                    {
                        "name": "Madhusodan Chakraborty"
                    },
                    {
                        "name": "Abdullah Al Wasif"
                    }
                ],
                "author_detail": {
                    "name": "Abdullah Al Wasif"
                },
                "author": "Abdullah Al Wasif"
            },
            {
                "id": "http://arxiv.org/abs/2506.13613v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.13613v2",
                "title": "Variational Inference with Mixtures of Isotropic Gaussians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Inference with Mixtures of Isotropic Gaussians"
                },
                "updated": "2025-11-17T18:36:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    36,
                    4,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.13613v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.13613v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Variational inference (VI) is a popular approach in Bayesian inference, that looks for the best approximation of the posterior distribution within a parametric family, minimizing a loss that is typically the (reverse) Kullback-Leibler (KL) divergence. In this paper, we focus on the following parametric family: mixtures of isotropic Gaussians (i.e., with diagonal covariance matrices proportional to the identity) and uniform weights. We develop a variational framework and provide efficient algorithms suited for this family. In contrast with mixtures of Gaussian with generic covariance matrices, this choice presents a balance between accurate approximations of multimodal Bayesian posteriors, while being memory and computationally efficient. Our algorithms implement gradient descent on the location of the mixture components (the modes of the Gaussians), and either (an entropic) Mirror or Bures descent on their variance parameters. We illustrate the performance of our algorithms on numerical experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational inference (VI) is a popular approach in Bayesian inference, that looks for the best approximation of the posterior distribution within a parametric family, minimizing a loss that is typically the (reverse) Kullback-Leibler (KL) divergence. In this paper, we focus on the following parametric family: mixtures of isotropic Gaussians (i.e., with diagonal covariance matrices proportional to the identity) and uniform weights. We develop a variational framework and provide efficient algorithms suited for this family. In contrast with mixtures of Gaussian with generic covariance matrices, this choice presents a balance between accurate approximations of multimodal Bayesian posteriors, while being memory and computationally efficient. Our algorithms implement gradient descent on the location of the mixture components (the modes of the Gaussians), and either (an entropic) Mirror or Bures descent on their variance parameters. We illustrate the performance of our algorithms on numerical experiments."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-16T15:42:15Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    42,
                    15,
                    0,
                    167,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Marguerite Petit-Talamon"
                    },
                    {
                        "name": "Marc Lambert"
                    },
                    {
                        "name": "Anna Korba"
                    }
                ],
                "author_detail": {
                    "name": "Anna Korba"
                },
                "author": "Anna Korba"
            },
            {
                "id": "http://arxiv.org/abs/2511.13680v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13680v1",
                "title": "Cross-Learning from Scarce Data via Multi-Task Constrained Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Learning from Scarce Data via Multi-Task Constrained Optimization"
                },
                "updated": "2025-11-17T18:35:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    35,
                    59,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13680v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \\emph{cross-learning} framework to overcome data scarcity by jointly estimating \\emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \\emph{cross-learning} framework to overcome data scarcity by jointly estimating \\emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:35:59Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    35,
                    59,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "13 pages, 11 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Leopoldo Agorio"
                    },
                    {
                        "name": "Juan Cerviño"
                    },
                    {
                        "name": "Miguel Calvo-Fullana"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Juan Andrés Bazerque"
                    }
                ],
                "author_detail": {
                    "name": "Juan Andrés Bazerque"
                },
                "author": "Juan Andrés Bazerque"
            },
            {
                "id": "http://arxiv.org/abs/2511.13676v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13676v1",
                "title": "T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization"
                },
                "updated": "2025-11-17T18:32:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    32,
                    3,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13676v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:32:03Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    32,
                    3,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted to DATE 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Hyunwoo Oh"
                    },
                    {
                        "name": "KyungIn Nam"
                    },
                    {
                        "name": "Rajat Bhattacharjya"
                    },
                    {
                        "name": "Hanning Chen"
                    },
                    {
                        "name": "Tamoghno Das"
                    },
                    {
                        "name": "Sanggeon Yun"
                    },
                    {
                        "name": "Suyeon Jang"
                    },
                    {
                        "name": "Andrew Ding"
                    },
                    {
                        "name": "Nikil Dutt"
                    },
                    {
                        "name": "Mohsen Imani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Imani"
                },
                "author": "Mohsen Imani"
            },
            {
                "id": "http://arxiv.org/abs/2506.08334v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.08334v3",
                "title": "iTACO: Interactable Digital Twins of Articulated Objects from Casually Captured RGBD Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iTACO: Interactable Digital Twins of Articulated Objects from Casually Captured RGBD Videos"
                },
                "updated": "2025-11-17T18:31:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    31,
                    53,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.08334v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Articulated objects are prevalent in daily life. Interactable digital twins of such objects have numerous applications in embodied AI and robotics. Unfortunately, current methods to digitize articulated real-world objects require carefully captured data, preventing practical, scalable, and generalizable acquisition. We focus on motion analysis and part-level segmentation of an articulated object from a casually captured RGBD video shot with a hand-held camera. A casually captured video of an interaction with an articulated object is easy to obtain at scale using smartphones. However, this setting is challenging due to simultaneous object and camera motion and significant occlusions as the person interacts with the object. To tackle these challenges, we introduce iTACO: a coarse-to-fine framework that infers joint parameters and segments movable parts of the object from a dynamic RGBD video. To evaluate our method under this new setting, we build a dataset of 784 videos containing 284 objects across 11 categories that is 20$\\times$ larger than available in prior work. We then compare our approach with existing methods that also take video as input. Our experiments show that iTACO outperforms existing articulated object digital twin methods on both synthetic and real casually captured RGBD videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Articulated objects are prevalent in daily life. Interactable digital twins of such objects have numerous applications in embodied AI and robotics. Unfortunately, current methods to digitize articulated real-world objects require carefully captured data, preventing practical, scalable, and generalizable acquisition. We focus on motion analysis and part-level segmentation of an articulated object from a casually captured RGBD video shot with a hand-held camera. A casually captured video of an interaction with an articulated object is easy to obtain at scale using smartphones. However, this setting is challenging due to simultaneous object and camera motion and significant occlusions as the person interacts with the object. To tackle these challenges, we introduce iTACO: a coarse-to-fine framework that infers joint parameters and segments movable parts of the object from a dynamic RGBD video. To evaluate our method under this new setting, we build a dataset of 784 videos containing 284 objects across 11 categories that is 20$\\times$ larger than available in prior work. We then compare our approach with existing methods that also take video as input. Our experiments show that iTACO outperforms existing articulated object digital twin methods on both synthetic and real casually captured RGBD videos."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-10T01:41:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    1,
                    41,
                    46,
                    1,
                    161,
                    0
                ],
                "arxiv_comment": "3DV 2026 camera-ready version. Project website can be found at https://3dlg-hcvc.github.io/video2articulation/",
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Weikun Peng"
                    },
                    {
                        "name": "Jun Lv"
                    },
                    {
                        "name": "Cewu Lu"
                    },
                    {
                        "name": "Manolis Savva"
                    }
                ],
                "author_detail": {
                    "name": "Manolis Savva"
                },
                "author": "Manolis Savva"
            },
            {
                "id": "http://arxiv.org/abs/2406.18966v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2406.18966v5",
                "title": "DataGen: Unified Synthetic Dataset Generation via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataGen: Unified Synthetic Dataset Generation via Large Language Models"
                },
                "updated": "2025-11-17T18:22:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    22,
                    49,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2406.18966v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2406.18966v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents DataGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. DataGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, DataGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by DataGen, and each module within DataGen plays a critical role in this enhancement. Additionally, DataGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that DataGen effectively supports dynamic and evolving benchmarking and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents DataGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. DataGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, DataGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by DataGen, and each module within DataGen plays a critical role in this enhancement. Additionally, DataGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that DataGen effectively supports dynamic and evolving benchmarking and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-06-27T07:56:44Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    7,
                    56,
                    44,
                    3,
                    179,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.13669v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13669v1",
                "title": "Investigating the Dark Energy Constraint from Strongly Lensed AGN at LSST-Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Dark Energy Constraint from Strongly Lensed AGN at LSST-Scale"
                },
                "updated": "2025-11-17T18:21:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    21,
                    54,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13669v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Strongly lensed Active Galactic Nuclei (AGN) with an observable time delay can be used to constrain the expansion history of the Universe through time-delay cosmography (TDC). As the sample of time-delay lenses grows to statistical size, with $\\mathcal{O}$(1000) lensed AGN forecast to be observed by the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST), there is an emerging opportunity to use TDC as an independent probe of dark energy. To take advantage of this statistical sample, we implement a scalable hierarchical inference tool which computes the cosmological likelihood for hundreds of strong lenses simultaneously. With this new technique, we investigate the cosmological constraining power from a simulation of the full LSST sample. We start from individual lenses, and emulate the full joint hierarchical TDC analysis, including image-based modeling, time-delay measurement, velocity dispersion measurement, and external convergence prediction. We fully account for the mass-sheet and mass-anisotropy degeneracies. We assume a sample of 800 lenses, with varying levels of follow-up fidelity based on existing campaigns. With our baseline assumptions, within a flexible $w_0w_a$CDM cosmology, we simultaneously forecast a $\\sim$2.5% constraint on H0 and a dark energy figure of merit (DE FOM) of 6.7. We show that by expanding the sample from 50 lenses to include an additional 750 lenses with plausible LSST time-delay measurements, we improve the forecasted DE FOM by nearly a factor of 3, demonstrating the value of incorporating this portion of the sample. We also investigate different follow-up campaign strategies, and find significant improvements in the DE FOM with additional stellar kinematics measurements and higher-precision time-delay measurements. We also demonstrate how the redshift configuration of time-delay lenses impacts constraining power in $w_0w_a$CDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strongly lensed Active Galactic Nuclei (AGN) with an observable time delay can be used to constrain the expansion history of the Universe through time-delay cosmography (TDC). As the sample of time-delay lenses grows to statistical size, with $\\mathcal{O}$(1000) lensed AGN forecast to be observed by the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST), there is an emerging opportunity to use TDC as an independent probe of dark energy. To take advantage of this statistical sample, we implement a scalable hierarchical inference tool which computes the cosmological likelihood for hundreds of strong lenses simultaneously. With this new technique, we investigate the cosmological constraining power from a simulation of the full LSST sample. We start from individual lenses, and emulate the full joint hierarchical TDC analysis, including image-based modeling, time-delay measurement, velocity dispersion measurement, and external convergence prediction. We fully account for the mass-sheet and mass-anisotropy degeneracies. We assume a sample of 800 lenses, with varying levels of follow-up fidelity based on existing campaigns. With our baseline assumptions, within a flexible $w_0w_a$CDM cosmology, we simultaneously forecast a $\\sim$2.5% constraint on H0 and a dark energy figure of merit (DE FOM) of 6.7. We show that by expanding the sample from 50 lenses to include an additional 750 lenses with plausible LSST time-delay measurements, we improve the forecasted DE FOM by nearly a factor of 3, demonstrating the value of incorporating this portion of the sample. We also investigate different follow-up campaign strategies, and find significant improvements in the DE FOM with additional stellar kinematics measurements and higher-precision time-delay measurements. We also demonstrate how the redshift configuration of time-delay lenses impacts constraining power in $w_0w_a$CDM."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:21:54Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    21,
                    54,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "submitting to the Open Journal of Astrophysics, comments welcome",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Sydney Erickson"
                    },
                    {
                        "name": "Martin Millon"
                    },
                    {
                        "name": "Padmavathi Venkatraman"
                    },
                    {
                        "name": "Tian Li"
                    },
                    {
                        "name": "Philip Holloway"
                    },
                    {
                        "name": "Phil Marshall"
                    },
                    {
                        "name": "Anowar Shajib"
                    },
                    {
                        "name": "Simon Birrer"
                    },
                    {
                        "name": "Xiang-Yu Huang"
                    },
                    {
                        "name": "Timo Anguita"
                    },
                    {
                        "name": "Steven Dillmann"
                    },
                    {
                        "name": "Narayan Khadka"
                    },
                    {
                        "name": "Kate Napier"
                    },
                    {
                        "name": "Aaron Roodman"
                    },
                    {
                        "name": "The LSST Dark Energy Science Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "The LSST Dark Energy Science Collaboration"
                },
                "author": "The LSST Dark Energy Science Collaboration"
            },
            {
                "id": "http://arxiv.org/abs/2511.13668v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13668v1",
                "title": "Integrative Model for Interoception and Exteroception: predictive coding, points of modulation, and testable predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrative Model for Interoception and Exteroception: predictive coding, points of modulation, and testable predictions"
                },
                "updated": "2025-11-17T18:21:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    21,
                    36,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13668v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Interoception and exteroception provide continuous feedback about the body and the environment, yet how they are dynamically integrated within a unified predictive coding framework has remained under-specified. This paper develops and empirically validates an integrative predictive coding model that treats interoceptive and exteroceptive inference as parallel hierarchical systems exchanging precision-weighted prediction errors. Within this framework, arbitration between the two streams is governed by relative precision weights (w) and integrated within the anterior insula (AIC) and anterior cingulate cortex (ACC). Computational simulations of the model reproduced biologically plausible dynamics: prediction errors decayed exponentially while arbitration weights self-normalized toward equilibrium (w = 0.5), demonstrating stable convergence and coherent integration. Simulated anxiety and PTSD profiles, characterized respectively by interoceptive and exteroceptive overweighting, yielded rigid, self-sustaining imbalances (w to 1 or w to 0) and slowed recalibration. Empirical application of the arbitration equation to published EEG-fMRI datasets further validated the model. The framework contributes a unifying account of how dysregulated precision weighting may underlie anxiety (overweighted interoception) and PTSD (underweighted interoception). Building on this validation, a proposed experimental paradigm is outlined to test the model's predictions in humans. It examines recalibration across anxiety, neutral, and PTSD groups following targeted interoceptive or exteroceptive therapies. Key predictions include identifiable neural markers of coherence, modulation of heartbeat-evoked potentials by vagal stimulation, and precision-sensitive behavioral signatures in interoceptive-exteroceptive congruency tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interoception and exteroception provide continuous feedback about the body and the environment, yet how they are dynamically integrated within a unified predictive coding framework has remained under-specified. This paper develops and empirically validates an integrative predictive coding model that treats interoceptive and exteroceptive inference as parallel hierarchical systems exchanging precision-weighted prediction errors. Within this framework, arbitration between the two streams is governed by relative precision weights (w) and integrated within the anterior insula (AIC) and anterior cingulate cortex (ACC). Computational simulations of the model reproduced biologically plausible dynamics: prediction errors decayed exponentially while arbitration weights self-normalized toward equilibrium (w = 0.5), demonstrating stable convergence and coherent integration. Simulated anxiety and PTSD profiles, characterized respectively by interoceptive and exteroceptive overweighting, yielded rigid, self-sustaining imbalances (w to 1 or w to 0) and slowed recalibration. Empirical application of the arbitration equation to published EEG-fMRI datasets further validated the model. The framework contributes a unifying account of how dysregulated precision weighting may underlie anxiety (overweighted interoception) and PTSD (underweighted interoception). Building on this validation, a proposed experimental paradigm is outlined to test the model's predictions in humans. It examines recalibration across anxiety, neutral, and PTSD groups following targeted interoceptive or exteroceptive therapies. Key predictions include identifiable neural markers of coherence, modulation of heartbeat-evoked potentials by vagal stimulation, and precision-sensitive behavioral signatures in interoceptive-exteroceptive congruency tasks."
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:21:36Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    21,
                    36,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC"
                },
                "authors": [
                    {
                        "name": "Pranjal Balar"
                    },
                    {
                        "name": "Sundeep Kapila"
                    }
                ],
                "author_detail": {
                    "name": "Sundeep Kapila"
                },
                "author": "Sundeep Kapila"
            },
            {
                "id": "http://arxiv.org/abs/2511.13666v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13666v1",
                "title": "A model-independent assessment of the late-time dark energy density evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A model-independent assessment of the late-time dark energy density evolution"
                },
                "updated": "2025-11-17T18:21:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    21,
                    5,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13666v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Combined measurements of Baryon Acoustic Oscillations (BAO) from the Dark Energy Spectroscopic Survey (DESI), the Cosmic Microwave Background (CMB) and Type Ia Supernovae (SN Ia), have recently challenged the $Λ$-Cold Dark Matter ($Λ$CDM) paradigm, indicating potential evidence for a dynamical dark energy component. These results are usually obtained in the context of the dark energy equation-of-state (EoS) parameterizations, generally implying in phantom-crossing at intermediate redshifts. However, a general mapping between these parameterizations that yields approximately the same background observables clouds the inference of the true nature of dark energy in the context of these parametric methods. In this work, we propose a model-independent reconstruction of the dark energy density, which is more directly constrained than its EoS, based on the Gaussian Process (GP) regression method with the use of DESI DR2 BAO data and the Pantheon+, Union3 and DESY5 SN Ia samples. In addition, we perform a statistical comparison between the energy densities of $Λ$, a non-phantom thawing quintessence-type dark energy, and the Chevallier-Polarski-Linder parameterization with the reconstructed function. We find that all models agree with the GP reconstruction at 95\\% C.L., with the largest discrepancy coming from $Λ$CDM with DESY5 at low redshifts. Even in this case, our findings suggest that it may be premature to claim statistically significant evidence for evolving or phantom dark energy with current DESI and SN Ia measurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combined measurements of Baryon Acoustic Oscillations (BAO) from the Dark Energy Spectroscopic Survey (DESI), the Cosmic Microwave Background (CMB) and Type Ia Supernovae (SN Ia), have recently challenged the $Λ$-Cold Dark Matter ($Λ$CDM) paradigm, indicating potential evidence for a dynamical dark energy component. These results are usually obtained in the context of the dark energy equation-of-state (EoS) parameterizations, generally implying in phantom-crossing at intermediate redshifts. However, a general mapping between these parameterizations that yields approximately the same background observables clouds the inference of the true nature of dark energy in the context of these parametric methods. In this work, we propose a model-independent reconstruction of the dark energy density, which is more directly constrained than its EoS, based on the Gaussian Process (GP) regression method with the use of DESI DR2 BAO data and the Pantheon+, Union3 and DESY5 SN Ia samples. In addition, we perform a statistical comparison between the energy densities of $Λ$, a non-phantom thawing quintessence-type dark energy, and the Chevallier-Polarski-Linder parameterization with the reconstructed function. We find that all models agree with the GP reconstruction at 95\\% C.L., with the largest discrepancy coming from $Λ$CDM with DESY5 at low redshifts. Even in this case, our findings suggest that it may be premature to claim statistically significant evidence for evolving or phantom dark energy with current DESI and SN Ia measurements."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:21:05Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    21,
                    5,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "13 pages, 5 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Rayff de Souza"
                    },
                    {
                        "name": "Agripino Sousa-Neto"
                    },
                    {
                        "name": "Javier E. González"
                    },
                    {
                        "name": "Jailson Alcaniz"
                    }
                ],
                "author_detail": {
                    "name": "Jailson Alcaniz"
                },
                "author": "Jailson Alcaniz"
            },
            {
                "id": "http://arxiv.org/abs/2511.13663v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13663v1",
                "title": "Cost-Driven Synthesis of Sound Abstract Interpreters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Driven Synthesis of Sound Abstract Interpreters"
                },
                "updated": "2025-11-17T18:16:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    16,
                    36,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13663v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:16:36Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    16,
                    36,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "37 pages, 20 figures",
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Qiuhan Gu"
                    },
                    {
                        "name": "Avaljot Singh"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh"
            },
            {
                "id": "http://arxiv.org/abs/2511.13658v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13658v1",
                "title": "Why is \"Chicago\" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why is \"Chicago\" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues"
                },
                "updated": "2025-11-17T18:15:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    15,
                    13,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13658v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:15:13Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    15,
                    13,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jiaming Qu"
                    },
                    {
                        "name": "Mengtian Guo"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.13655v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13655v1",
                "title": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation"
                },
                "updated": "2025-11-17T18:06:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    6,
                    26,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13655v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\\href{https://github.com/allenai/olmoearth_pretrain}{\\text{https://github.com/allenai/olmoearth_pretrain}}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\\href{https://github.com/allenai/olmoearth_pretrain}{\\text{https://github.com/allenai/olmoearth_pretrain}}$."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:06:26Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    6,
                    26,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Henry Herzog"
                    },
                    {
                        "name": "Favyen Bastani"
                    },
                    {
                        "name": "Yawen Zhang"
                    },
                    {
                        "name": "Gabriel Tseng"
                    },
                    {
                        "name": "Joseph Redmon"
                    },
                    {
                        "name": "Hadrien Sablon"
                    },
                    {
                        "name": "Ryan Park"
                    },
                    {
                        "name": "Jacob Morrison"
                    },
                    {
                        "name": "Alexandra Buraczynski"
                    },
                    {
                        "name": "Karen Farley"
                    },
                    {
                        "name": "Joshua Hansen"
                    },
                    {
                        "name": "Andrew Howe"
                    },
                    {
                        "name": "Patrick Alan Johnson"
                    },
                    {
                        "name": "Mark Otterlee"
                    },
                    {
                        "name": "Ted Schmitt"
                    },
                    {
                        "name": "Hunter Pitelka"
                    },
                    {
                        "name": "Stephen Daspit"
                    },
                    {
                        "name": "Rachel Ratner"
                    },
                    {
                        "name": "Christopher Wilhelm"
                    },
                    {
                        "name": "Sebastian Wood"
                    },
                    {
                        "name": "Mike Jacobi"
                    },
                    {
                        "name": "Hannah Kerner"
                    },
                    {
                        "name": "Evan Shelhamer"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Patrick Beukema"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Beukema"
                },
                "author": "Patrick Beukema"
            },
            {
                "id": "http://arxiv.org/abs/2505.17708v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.17708v3",
                "title": "The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations"
                },
                "updated": "2025-11-17T18:00:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    0,
                    52,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.17708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.17708v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Causal reasoning and discovery, two fundamental tasks of causal analysis, often face challenges in applications due to the complexity, noisiness, and high-dimensionality of real-world data. Despite recent progress in identifying latent causal structures using causal representation learning (CRL), what makes learned representations useful for causal downstream tasks and how to evaluate them are still not well understood. In this paper, we reinterpret CRL using a measurement model framework, where the learned representations are viewed as proxy measurements of the latent causal variables. Our approach clarifies the conditions under which learned representations support downstream causal reasoning and provides a principled basis for quantitatively assessing the quality of representations using a new Test-based Measurement EXclusivity (T-MEX) score. We validate T-MEX across diverse causal inference scenarios, including numerical simulations and real-world ecological video analysis, demonstrating that the proposed framework and corresponding score effectively assess the identification of learned representations and their usefulness for causal downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal reasoning and discovery, two fundamental tasks of causal analysis, often face challenges in applications due to the complexity, noisiness, and high-dimensionality of real-world data. Despite recent progress in identifying latent causal structures using causal representation learning (CRL), what makes learned representations useful for causal downstream tasks and how to evaluate them are still not well understood. In this paper, we reinterpret CRL using a measurement model framework, where the learned representations are viewed as proxy measurements of the latent causal variables. Our approach clarifies the conditions under which learned representations support downstream causal reasoning and provides a principled basis for quantitatively assessing the quality of representations using a new Test-based Measurement EXclusivity (T-MEX) score. We validate T-MEX across diverse causal inference scenarios, including numerical simulations and real-world ecological video analysis, demonstrating that the proposed framework and corresponding score effectively assess the identification of learned representations and their usefulness for causal downstream tasks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-23T10:25:17Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    10,
                    25,
                    17,
                    4,
                    143,
                    0
                ],
                "arxiv_comment": "Camera-ready version for NeurIPS2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Dingling Yao"
                    },
                    {
                        "name": "Shimeng Huang"
                    },
                    {
                        "name": "Riccardo Cadei"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Francesco Locatello"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Locatello"
                },
                "author": "Francesco Locatello"
            },
            {
                "id": "http://arxiv.org/abs/2511.13649v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13649v1",
                "title": "Distribution Matching Distillation Meets Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distribution Matching Distillation Meets Reinforcement Learning"
                },
                "updated": "2025-11-17T17:59:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    59,
                    54,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13649v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:59:54Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    59,
                    54,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "The synergy of reinforcement learning and distribution matching distillation. See more: https://github.com/vvvvvjdy/dmdr",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Dengyang Jiang"
                    },
                    {
                        "name": "Dongyang Liu"
                    },
                    {
                        "name": "Zanyi Wang"
                    },
                    {
                        "name": "Qilong Wu"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "David Liu"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Mengmeng Wang"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.13646v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13646v1",
                "title": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?"
                },
                "updated": "2025-11-17T17:58:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    58,
                    18,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13646v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:58:18Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    58,
                    18,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Chunqiu Steven Xia"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Yan Yang"
                    },
                    {
                        "name": "Yuxiang Wei"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.13644v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13644v1",
                "title": "CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding"
                },
                "updated": "2025-11-17T17:56:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    56,
                    14,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13644v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:56:14Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    56,
                    14,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shrenik Patel"
                    },
                    {
                        "name": "Daivik Patel"
                    }
                ],
                "author_detail": {
                    "name": "Daivik Patel"
                },
                "author": "Daivik Patel"
            },
            {
                "id": "http://arxiv.org/abs/2511.13640v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13640v1",
                "title": "Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures"
                },
                "updated": "2025-11-17T17:53:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    53,
                    12,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13640v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:53:12Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    53,
                    12,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Haohui Wang"
                    },
                    {
                        "name": "Jingyuan Qi"
                    },
                    {
                        "name": "Jianpeng Chen"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Lifu Huang"
                    },
                    {
                        "name": "Lecheng Zheng"
                    },
                    {
                        "name": "Kevin Choi"
                    },
                    {
                        "name": "Balaji Veeramani"
                    },
                    {
                        "name": "Edward Bowen"
                    },
                    {
                        "name": "Alison Hu"
                    },
                    {
                        "name": "Tyler Cody"
                    },
                    {
                        "name": "Dawei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Zhou"
                },
                "author": "Dawei Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2511.13633v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13633v1",
                "title": "Multi Messenger Study of GRB 221009A with VHE Gamma-ray and Neutrino Afterglow from a Gaussian Structured Jet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi Messenger Study of GRB 221009A with VHE Gamma-ray and Neutrino Afterglow from a Gaussian Structured Jet"
                },
                "updated": "2025-11-17T17:47:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    47,
                    36,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13633v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent detections of very-high-energy (VHE; $\\gtrsim 100$~GeV) emission from GRB afterglows, notably the unprecedented brightness of GRB~221009A observed by LHAASO, reveal emission components beyond the standard electron synchrotron model. Multi-TeV photons motivate synchrotron self-Compton and possibly hadronic contributions, while the non-detection of coincident neutrinos by IceCube/KM3NeT/GRAND200k constrains the microphysical parameters, jet kinetic energy and ambient medium density. We model the VHE afterglow of GRB 221009A with an external forward shock from a Gaussian structured jet in a uniform density medium. This angular structure reproduces the extreme TeV output at an off-axis angle but without demanding large energies as in a top-hat jet. We also compute the corresponding $pγ$ neutrino flux in the PeV-EeV energies and derive a time-integrated upper limit based on the effective areas of IceCube Gen2 and GRAND200k, providing the contributions of individual GRBs to the neutrino events. The predicted neutrino flux for GRB 221009A with model parameters inferred from multi-wavelength spectral energy distribution lies below the sensitivities of these detectors. Even under highly optimistic microphysical conditions, our correlation analysis infers that the events from this GRB are of order $\\sim 0.1$ for upcoming GRAND200k. We also compare neutrino fluxes for on-axis and off-axis viewing geometries and find that jet orientation alone can introduce nearly an order of magnitude variation in the signal. Thus, our studies imply that a GRB both brighter and closer than GRB 221009A would be crucial for any neutrino detections by upcoming neutrino telescopes. Future GRB detections by the CTA will provide important constraints on their geometry, radiation mechanisms, and any potential associated neutrino signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent detections of very-high-energy (VHE; $\\gtrsim 100$~GeV) emission from GRB afterglows, notably the unprecedented brightness of GRB~221009A observed by LHAASO, reveal emission components beyond the standard electron synchrotron model. Multi-TeV photons motivate synchrotron self-Compton and possibly hadronic contributions, while the non-detection of coincident neutrinos by IceCube/KM3NeT/GRAND200k constrains the microphysical parameters, jet kinetic energy and ambient medium density. We model the VHE afterglow of GRB 221009A with an external forward shock from a Gaussian structured jet in a uniform density medium. This angular structure reproduces the extreme TeV output at an off-axis angle but without demanding large energies as in a top-hat jet. We also compute the corresponding $pγ$ neutrino flux in the PeV-EeV energies and derive a time-integrated upper limit based on the effective areas of IceCube Gen2 and GRAND200k, providing the contributions of individual GRBs to the neutrino events. The predicted neutrino flux for GRB 221009A with model parameters inferred from multi-wavelength spectral energy distribution lies below the sensitivities of these detectors. Even under highly optimistic microphysical conditions, our correlation analysis infers that the events from this GRB are of order $\\sim 0.1$ for upcoming GRAND200k. We also compare neutrino fluxes for on-axis and off-axis viewing geometries and find that jet orientation alone can introduce nearly an order of magnitude variation in the signal. Thus, our studies imply that a GRB both brighter and closer than GRB 221009A would be crucial for any neutrino detections by upcoming neutrino telescopes. Future GRB detections by the CTA will provide important constraints on their geometry, radiation mechanisms, and any potential associated neutrino signals."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:47:36Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    47,
                    36,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "16 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "T. Mondal"
                    },
                    {
                        "name": "S. Razzaque"
                    },
                    {
                        "name": "JC. Joshi"
                    },
                    {
                        "name": "S. Majumder"
                    },
                    {
                        "name": "D. Bose"
                    }
                ],
                "author_detail": {
                    "name": "D. Bose"
                },
                "author": "D. Bose"
            },
            {
                "id": "http://arxiv.org/abs/2511.13630v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13630v1",
                "title": "Beyond Mimicry: Preference Coherence in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Mimicry: Preference Coherence in LLMs"
                },
                "updated": "2025-11-17T17:41:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    41,
                    48,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13630v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:41:48Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    41,
                    48,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Luhan Mikaelson"
                    },
                    {
                        "name": "Derek Shiller"
                    },
                    {
                        "name": "Hayley Clatterbuck"
                    }
                ],
                "author_detail": {
                    "name": "Hayley Clatterbuck"
                },
                "author": "Hayley Clatterbuck"
            },
            {
                "id": "http://arxiv.org/abs/2506.13417v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.13417v2",
                "title": "Chaos, coherence and turbulence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chaos, coherence and turbulence"
                },
                "updated": "2025-11-17T17:41:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    41,
                    36,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.13417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.13417v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1103/lkvl-dlcn",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This paper is a personal overview of the efforts over the last half century to understand fluid turbulence in terms of simpler coherent units. The consequences of chaos and the concept of coherence are first reviewed, using examples from free-shear and wall-bounded shear flows, and including how the simplifications due to coherent structures have been useful in the conceptualization and control of turbulence. It is remarked that, even if this approach has revolutionized our understanding of the flow, most of turbulence cannot yet be described by structures. This includes cascades, both direct and inverse, and possibly junk turbulence, whose role, if any, is currently unknown. This part of the paper is mostly a catalog of questions, some of them answered and others still open. A second part of the paper examines which new techniques can be expected to help in attacking the open questions, and which, in the opinion of the author, are the strengths and limitations of current approaches, such as data-driven science and causal inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper is a personal overview of the efforts over the last half century to understand fluid turbulence in terms of simpler coherent units. The consequences of chaos and the concept of coherence are first reviewed, using examples from free-shear and wall-bounded shear flows, and including how the simplifications due to coherent structures have been useful in the conceptualization and control of turbulence. It is remarked that, even if this approach has revolutionized our understanding of the flow, most of turbulence cannot yet be described by structures. This includes cascades, both direct and inverse, and possibly junk turbulence, whose role, if any, is currently unknown. This part of the paper is mostly a catalog of questions, some of them answered and others still open. A second part of the paper examines which new techniques can be expected to help in attacking the open questions, and which, in the opinion of the author, are the strengths and limitations of current approaches, such as data-driven science and causal inference."
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-16T12:34:16Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    34,
                    16,
                    0,
                    167,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn"
                },
                "arxiv_journal_ref": "Physical Review FLUIDS 10, 100504 (2025)",
                "authors": [
                    {
                        "name": "Javier Jimenez"
                    }
                ],
                "author_detail": {
                    "name": "Javier Jimenez"
                },
                "author": "Javier Jimenez",
                "arxiv_doi": "10.1103/lkvl-dlcn"
            },
            {
                "id": "http://arxiv.org/abs/2508.21323v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.21323v2",
                "title": "LLM-driven Provenance Forensics for Threat Investigation and Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-driven Provenance Forensics for Threat Investigation and Detection"
                },
                "updated": "2025-11-17T17:40:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    40,
                    34,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.21323v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.21323v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce PROVSEEK, an LLM-powered agentic framework for automated provenance-driven forensic analysis and threat intelligence extraction. PROVSEEK employs specialized toolchains to dynamically retrieve relevant context by generating precise, context-aware queries that fuse knowledge from threat reports with evidence from system provenance data. The framework resolves provenance queries, orchestrates multiple role-specific agents, and synthesizes structured, ground-truth verifiable forensic summaries. By combining agent orchestration with Retrieval-Augmented Generation (RAG) and chain-of-thought (CoT) reasoning, data-guided filtration using a behavioral model, PROVSEEK enables adaptive multi-step analysis that iteratively refines hypotheses, verifies supporting evidence, and produces scalable, interpretable forensic explanations of attack behaviors. PROVSEEK is designed for automated threat investigation without task-specific training data, enabling forensic-style investigation even when no prior knowledge of the environment. We conduct a comprehensive evaluation on publicly available DARPA datasets, demonstrating that PROVSEEK outperforms retrieval-based methods for the intelligence extraction task, achieving a 34% improvement in contextual precision/recall; and for threat detection task, PROVSEEK achieves 22%/29% higher precision/recall compared to both a baseline agent approach and State-Of-The-Art (SOTA) Provenance-based Intrusion Detection System (PIDS). In our scalability study, we show that PROVSEEK increases token usage by 1.42x and latency by 1.63x as the database size increases 50x, making it optimal for large-scale deployment. We also conducted an ablation and error analysis study to show how different components of PROVSEEK affect the detection performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PROVSEEK, an LLM-powered agentic framework for automated provenance-driven forensic analysis and threat intelligence extraction. PROVSEEK employs specialized toolchains to dynamically retrieve relevant context by generating precise, context-aware queries that fuse knowledge from threat reports with evidence from system provenance data. The framework resolves provenance queries, orchestrates multiple role-specific agents, and synthesizes structured, ground-truth verifiable forensic summaries. By combining agent orchestration with Retrieval-Augmented Generation (RAG) and chain-of-thought (CoT) reasoning, data-guided filtration using a behavioral model, PROVSEEK enables adaptive multi-step analysis that iteratively refines hypotheses, verifies supporting evidence, and produces scalable, interpretable forensic explanations of attack behaviors. PROVSEEK is designed for automated threat investigation without task-specific training data, enabling forensic-style investigation even when no prior knowledge of the environment. We conduct a comprehensive evaluation on publicly available DARPA datasets, demonstrating that PROVSEEK outperforms retrieval-based methods for the intelligence extraction task, achieving a 34% improvement in contextual precision/recall; and for threat detection task, PROVSEEK achieves 22%/29% higher precision/recall compared to both a baseline agent approach and State-Of-The-Art (SOTA) Provenance-based Intrusion Detection System (PIDS). In our scalability study, we show that PROVSEEK increases token usage by 1.42x and latency by 1.63x as the database size increases 50x, making it optimal for large-scale deployment. We also conducted an ablation and error analysis study to show how different components of PROVSEEK affect the detection performance."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-29T04:39:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    4,
                    39,
                    52,
                    4,
                    241,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Kunal Mukherjee"
                    },
                    {
                        "name": "Murat Kantarcioglu"
                    }
                ],
                "author_detail": {
                    "name": "Murat Kantarcioglu"
                },
                "author": "Murat Kantarcioglu"
            },
            {
                "id": "http://arxiv.org/abs/2504.03784v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.03784v5",
                "title": "Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning"
                },
                "updated": "2025-11-17T17:33:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    33,
                    15,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.03784v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.03784v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset. The code is available at https:// github.com/ VRPO/ VRPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset. The code is available at https:// github.com/ VRPO/ VRPO."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-03T16:16:35Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    16,
                    35,
                    3,
                    93,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Kai Ye"
                    },
                    {
                        "name": "Hongyi Zhou"
                    },
                    {
                        "name": "Jin Zhu"
                    },
                    {
                        "name": "Francesco Quinzan"
                    },
                    {
                        "name": "Chengchun Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chengchun Shi"
                },
                "author": "Chengchun Shi"
            },
            {
                "id": "http://arxiv.org/abs/2510.27176v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.27176v3",
                "title": "Glia: A Human-Inspired AI for Automated Systems Design and Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glia: A Human-Inspired AI for Automated Systems Design and Optimization"
                },
                "updated": "2025-11-17T17:29:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    29,
                    30,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.27176v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.27176v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that grounds abstract reasoning in empirical feedback. Unlike prior ML-for-systems methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for LLM inference, it produces new algorithms for request routing, scheduling, and auto-scaling that perform at human-expert levels in significantly less time, while yielding novel insights into workload behavior. Our results suggest that by combining reasoning LLMs with structured experimentation, an AI can produce creative and understandable designs for complex systems problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that grounds abstract reasoning in empirical feedback. Unlike prior ML-for-systems methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for LLM inference, it produces new algorithms for request routing, scheduling, and auto-scaling that perform at human-expert levels in significantly less time, while yielding novel insights into workload behavior. Our results suggest that by combining reasoning LLMs with structured experimentation, an AI can produce creative and understandable designs for complex systems problems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-31T04:58:00Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    58,
                    0,
                    4,
                    304,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Pantea Karimi"
                    },
                    {
                        "name": "Arash Nasr-Esfahany"
                    },
                    {
                        "name": "Kimia Noorbakhsh"
                    },
                    {
                        "name": "Joseph Chandler"
                    },
                    {
                        "name": "Ali ParandehGheibi"
                    },
                    {
                        "name": "Mohammad Alizadeh"
                    },
                    {
                        "name": "Hari Balakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Hari Balakrishnan"
                },
                "author": "Hari Balakrishnan"
            },
            {
                "id": "http://arxiv.org/abs/2511.10387v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10387v3",
                "title": "Physics informed Transformer-VAE for biophysical parameter estimation: PROSAIL model inversion in Sentinel-2 imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics informed Transformer-VAE for biophysical parameter estimation: PROSAIL model inversion in Sentinel-2 imagery"
                },
                "updated": "2025-11-17T17:20:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    20,
                    9,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10387v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate retrieval of vegetation biophysical variables from satellite imagery is crucial for ecosystem monitoring and agricultural management. In this work, we propose a physics-informed Transformer-VAE architecture to invert the PROSAIL radiative transfer model for simultaneous estimation of key canopy parameters from Sentinel-2 data. Unlike previous hybrid approaches that require real satellite images for self-supevised training. Our model is trained exclusively on simulated data, yet achieves performance on par with state-of-the-art methods that utilize real imagery. The Transformer-VAE incorporates the PROSAIL model as a differentiable physical decoder, ensuring that inferred latent variables correspond to physically plausible leaf and canopy properties. We demonstrate retrieval of leaf area index (LAI) and canopy chlorophyll content (CCC) on real-world field datasets (FRM4Veg and BelSAR) with accuracy comparable to models trained with real Sentinel-2 data. Our method requires no in-situ labels or calibration on real images, offering a cost-effective and self-supervised solution for global vegetation monitoring. The proposed approach illustrates how integrating physical models with advanced deep networks can improve the inversion of RTMs, opening new prospects for large-scale, physically-constrained remote sensing of vegetation traits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate retrieval of vegetation biophysical variables from satellite imagery is crucial for ecosystem monitoring and agricultural management. In this work, we propose a physics-informed Transformer-VAE architecture to invert the PROSAIL radiative transfer model for simultaneous estimation of key canopy parameters from Sentinel-2 data. Unlike previous hybrid approaches that require real satellite images for self-supevised training. Our model is trained exclusively on simulated data, yet achieves performance on par with state-of-the-art methods that utilize real imagery. The Transformer-VAE incorporates the PROSAIL model as a differentiable physical decoder, ensuring that inferred latent variables correspond to physically plausible leaf and canopy properties. We demonstrate retrieval of leaf area index (LAI) and canopy chlorophyll content (CCC) on real-world field datasets (FRM4Veg and BelSAR) with accuracy comparable to models trained with real Sentinel-2 data. Our method requires no in-situ labels or calibration on real images, offering a cost-effective and self-supervised solution for global vegetation monitoring. The proposed approach illustrates how integrating physical models with advanced deep networks can improve the inversion of RTMs, opening new prospects for large-scale, physically-constrained remote sensing of vegetation traits."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:08:33Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    8,
                    33,
                    3,
                    317,
                    0
                ],
                "arxiv_comment": "10 pages, 6 figures, uses fancyhdr.sty",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Prince Mensah"
                    },
                    {
                        "name": "Pelumi Victor Aderinto"
                    },
                    {
                        "name": "Ibrahim Salihu Yusuf"
                    },
                    {
                        "name": "Arnu Pretorius"
                    }
                ],
                "author_detail": {
                    "name": "Arnu Pretorius"
                },
                "author": "Arnu Pretorius"
            },
            {
                "id": "http://arxiv.org/abs/2511.13614v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13614v1",
                "title": "Market-Dependent Communication in Multi-Agent Alpha Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Market-Dependent Communication in Multi-Agent Alpha Generation"
                },
                "updated": "2025-11-17T17:19:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    19,
                    56,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13614v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-strategy hedge funds face a fundamental organizational choice: should analysts generating trading strategies communicate, and if so, how? We investigate this using 5-agent LLM-based trading systems across 450 experiments spanning 21 months, comparing five organizational structures from isolated baseline to collaborative and competitive conversation. We show that communication improves performance, but optimal communication design depends on market characteristics. Competitive conversation excels in volatile technology stocks, while collaborative conversation dominates stable general stocks. Finance stocks resist all communication interventions. Surprisingly, all structures, including isolated agents, converge to similar strategy alignments, challenging assumptions that transparency causes harmful diversity loss. Performance differences stem from behavioral mechanisms: competitive agents focus on stock-level allocation while collaborative agents develop technical frameworks. Conversation quality scores show zero correlation with returns. These findings demonstrate that optimal communication design must match market volatility characteristics, and sophisticated discussions don't guarantee better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-strategy hedge funds face a fundamental organizational choice: should analysts generating trading strategies communicate, and if so, how? We investigate this using 5-agent LLM-based trading systems across 450 experiments spanning 21 months, comparing five organizational structures from isolated baseline to collaborative and competitive conversation. We show that communication improves performance, but optimal communication design depends on market characteristics. Competitive conversation excels in volatile technology stocks, while collaborative conversation dominates stable general stocks. Finance stocks resist all communication interventions. Surprisingly, all structures, including isolated agents, converge to similar strategy alignments, challenging assumptions that transparency causes harmful diversity loss. Performance differences stem from behavioral mechanisms: competitive agents focus on stock-level allocation while collaborative agents develop technical frameworks. Conversation quality scores show zero correlation with returns. These findings demonstrate that optimal communication design must match market volatility characteristics, and sophisticated discussions don't guarantee better performance."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:19:56Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    19,
                    56,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Jerick Shi"
                    },
                    {
                        "name": "Burton Hollifield"
                    }
                ],
                "author_detail": {
                    "name": "Burton Hollifield"
                },
                "author": "Burton Hollifield"
            },
            {
                "id": "http://arxiv.org/abs/2511.13612v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13612v1",
                "title": "P1: Mastering Physics Olympiads with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P1: Mastering Physics Olympiads with Reinforcement Learning"
                },
                "updated": "2025-11-17T17:18:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    18,
                    13,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13612v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:18:13Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    18,
                    13,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jiacheng Chen"
                    },
                    {
                        "name": "Qianjia Cheng"
                    },
                    {
                        "name": "Fangchen Yu"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Shenghe Zheng"
                    },
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Qingyang Zhang"
                    },
                    {
                        "name": "Haonan He"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Yufeng Zhao"
                    },
                    {
                        "name": "Futing Wang"
                    },
                    {
                        "name": "Li Sheng"
                    },
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Yizhuo Li"
                    },
                    {
                        "name": "Wenxauan Zeng"
                    },
                    {
                        "name": "Yulun Wu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Ganqu Cui"
                    }
                ],
                "author_detail": {
                    "name": "Ganqu Cui"
                },
                "author": "Ganqu Cui"
            },
            {
                "id": "http://arxiv.org/abs/2503.11636v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.11636v3",
                "title": "Towards Markov-State Holography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Markov-State Holography"
                },
                "updated": "2025-11-17T17:17:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    17,
                    45,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.11636v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.11636v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Experiments, in particular on biological systems, typically probe lower-dimensional observables which are projections of high-dimensional dynamics. In order to infer consistent models capturing the relevant dynamics of the system, it is important to detect and account for the memory in the dynamics. We develop a method to infer the presence of hidden states and transition pathways based on observable transition probabilities conditioned on history sequences of visited states for projected (i.e. observed) dynamics of Markov processes. Histograms conditioned on histories reveal information on the transition probabilities of hidden paths locally between any specific pair of observed states. The convergence rate of these histograms towards a stationary distribution provides a local quantification of the duration of memory, which reflects how distinct microscopic paths projecting onto the same observed transition decorrelate in path space. This motivates the notion of \"weak Markov order\" and provides insight about the hidden topology of microscopic paths in a holography-like fashion. The method can be used to test for the local Markov property of observables. The information extracted is also helpful in inferring relevant hidden transitions which are not captured by a Markov-state model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experiments, in particular on biological systems, typically probe lower-dimensional observables which are projections of high-dimensional dynamics. In order to infer consistent models capturing the relevant dynamics of the system, it is important to detect and account for the memory in the dynamics. We develop a method to infer the presence of hidden states and transition pathways based on observable transition probabilities conditioned on history sequences of visited states for projected (i.e. observed) dynamics of Markov processes. Histograms conditioned on histories reveal information on the transition probabilities of hidden paths locally between any specific pair of observed states. The convergence rate of these histograms towards a stationary distribution provides a local quantification of the duration of memory, which reflects how distinct microscopic paths projecting onto the same observed transition decorrelate in path space. This motivates the notion of \"weak Markov order\" and provides insight about the hidden topology of microscopic paths in a holography-like fashion. The method can be used to test for the local Markov property of observables. The information extracted is also helpful in inferring relevant hidden transitions which are not captured by a Markov-state model."
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-14T17:54:39Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    54,
                    39,
                    4,
                    73,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech"
                },
                "authors": [
                    {
                        "name": "Xizhu Zhao"
                    },
                    {
                        "name": "Dmitrii E. Makarov"
                    },
                    {
                        "name": "Aljaž Godec"
                    }
                ],
                "author_detail": {
                    "name": "Aljaž Godec"
                },
                "author": "Aljaž Godec"
            },
            {
                "id": "http://arxiv.org/abs/2511.11177v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11177v2",
                "title": "Viper-F1: Fast and Fine-Grained Multimodal Understanding with Cross-Modal State-Space Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viper-F1: Fast and Fine-Grained Multimodal Understanding with Cross-Modal State-Space Modulation"
                },
                "updated": "2025-11-17T17:08:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    8,
                    31,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11177v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11177v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in multimodal large language models (MLLMs) have enabled impressive progress in vision-language understanding, yet their high computational cost limits deployment in resource-constrained scenarios such as robotic manipulation, personal assistants, and smart cameras. Most existing methods rely on Transformer-based cross-attention, whose quadratic complexity hinders efficiency. Moreover, small vision-language models often struggle to precisely capture fine-grained, task-relevant visual regions, leading to degraded performance on fine-grained reasoning tasks that limit their effectiveness in the real world. To address these issues, we introduce Viper-F1, a Hybrid State-Space Vision-Language Model that replaces attention with efficient Liquid State-Space Dynamics. To further enhance visual grounding, we propose a Token-Grid Correlation Module, which computes lightweight correlations between text tokens and image patches and modulates the state-space dynamics via FiLM conditioning. This enables the model to selectively emphasize visual regions relevant to the textual prompt while maintaining linear-time inference. Experimental results across multiple benchmarks demonstrate that Viper-F1 achieves accurate, fine-grained understanding with significantly improved efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal large language models (MLLMs) have enabled impressive progress in vision-language understanding, yet their high computational cost limits deployment in resource-constrained scenarios such as robotic manipulation, personal assistants, and smart cameras. Most existing methods rely on Transformer-based cross-attention, whose quadratic complexity hinders efficiency. Moreover, small vision-language models often struggle to precisely capture fine-grained, task-relevant visual regions, leading to degraded performance on fine-grained reasoning tasks that limit their effectiveness in the real world. To address these issues, we introduce Viper-F1, a Hybrid State-Space Vision-Language Model that replaces attention with efficient Liquid State-Space Dynamics. To further enhance visual grounding, we propose a Token-Grid Correlation Module, which computes lightweight correlations between text tokens and image patches and modulates the state-space dynamics via FiLM conditioning. This enables the model to selectively emphasize visual regions relevant to the textual prompt while maintaining linear-time inference. Experimental results across multiple benchmarks demonstrate that Viper-F1 achieves accurate, fine-grained understanding with significantly improved efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T11:21:48Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    11,
                    21,
                    48,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Quoc-Huy Trinh"
                    },
                    {
                        "name": "Mustapha Abdullahi"
                    },
                    {
                        "name": "Do Duy Hung Trinh"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Debesh Jha"
                    }
                ],
                "author_detail": {
                    "name": "Debesh Jha"
                },
                "author": "Debesh Jha"
            },
            {
                "id": "http://arxiv.org/abs/2502.19662v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.19662v3",
                "title": "HALO: Hardware-aware quantization with low critical-path-delay weights for LLM acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HALO: Hardware-aware quantization with low critical-path-delay weights for LLM acceleration"
                },
                "updated": "2025-11-17T16:57:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    57,
                    58,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.19662v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.19662v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Quantization is critical for efficiently deploying large language models (LLMs). Yet conventional methods remain hardware-agnostic, limited to bit-width constraints, and do not account for intrinsic circuit characteristics such as the timing behaviors and energy profiles of Multiply-Accumulate (MAC) units. This disconnect from circuit-level behavior limits the ability to exploit available timing margins and energy-saving opportunities, reducing the overall efficiency of deployment on modern accelerators.\n  To address these limitations, we propose HALO, a versatile framework for Hardware-Aware Post-Training Quantization (PTQ). Unlike traditional methods, HALO explicitly incorporates detailed hardware characteristics, including critical-path timing and power consumption, into its quantization approach. HALO strategically selects weights with low critical-path-delays enabling higher operational frequencies and dynamic frequency scaling without disrupting the architecture's dataflow. Remarkably, HALO achieves these improvements with only a few dynamic voltage and frequency scaling (DVFS) adjustments, ensuring simplicity and practicality in deployment. Additionally, by reducing switching activity within the MAC units, HALO effectively lowers energy consumption. Evaluations on accelerators such as Tensor Processing Units (TPUs) and Graphics Processing Units (GPUs) demonstrate that HALO significantly enhances inference efficiency, achieving average performance improvements of 270% and energy savings of 51% over baseline quantization methods, all with minimal impact on accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is critical for efficiently deploying large language models (LLMs). Yet conventional methods remain hardware-agnostic, limited to bit-width constraints, and do not account for intrinsic circuit characteristics such as the timing behaviors and energy profiles of Multiply-Accumulate (MAC) units. This disconnect from circuit-level behavior limits the ability to exploit available timing margins and energy-saving opportunities, reducing the overall efficiency of deployment on modern accelerators.\n  To address these limitations, we propose HALO, a versatile framework for Hardware-Aware Post-Training Quantization (PTQ). Unlike traditional methods, HALO explicitly incorporates detailed hardware characteristics, including critical-path timing and power consumption, into its quantization approach. HALO strategically selects weights with low critical-path-delays enabling higher operational frequencies and dynamic frequency scaling without disrupting the architecture's dataflow. Remarkably, HALO achieves these improvements with only a few dynamic voltage and frequency scaling (DVFS) adjustments, ensuring simplicity and practicality in deployment. Additionally, by reducing switching activity within the MAC units, HALO effectively lowers energy consumption. Evaluations on accelerators such as Tensor Processing Units (TPUs) and Graphics Processing Units (GPUs) demonstrate that HALO significantly enhances inference efficiency, achieving average performance improvements of 270% and energy savings of 51% over baseline quantization methods, all with minimal impact on accuracy."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-27T01:08:33Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    1,
                    8,
                    33,
                    3,
                    58,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Rohan Juneja"
                    },
                    {
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Tulika Mitra"
                    },
                    {
                        "name": "Li-Shiuan Peh"
                    }
                ],
                "author_detail": {
                    "name": "Li-Shiuan Peh"
                },
                "author": "Li-Shiuan Peh"
            },
            {
                "id": "http://arxiv.org/abs/2511.13595v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13595v1",
                "title": "Physics-Informed Neural Networks for Nonlinear Output Regulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Neural Networks for Nonlinear Output Regulation"
                },
                "updated": "2025-11-17T16:55:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    55,
                    42,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13595v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work addresses the full-information output regulation problem for nonlinear systems, assuming the states of both the plant and the exosystem are known. In this setting, perfect tracking or rejection is achieved by constructing a zero-regulation-error manifold π(w) and a feedforward input c(w) that render such manifold invariant. The pair (π(w), c(w)) is characterized by the regulator equations, i.e., a system of PDEs with an algebraic constraint. We focus on accurately solving the regulator equations introducing a physics-informed neural network (PINN) approach that directly approximates π(w) and c(w) by minimizing the residuals under boundary and feasibility conditions, without requiring precomputed trajectories or labeled data. The learned operator maps exosystem states to steady state plant states and inputs, enables real-time inference and, critically, generalizes across families of the exosystem with varying initial conditions and parameters. The framework is validated on a regulation task that synchronizes a helicopter's vertical dynamics with a harmonically oscillating platform. The resulting PINN-based solver reconstructs the zero-error manifold with high fidelity and sustains regulation performance under exosystem variations, highlighting the potential of learning-enabled solvers for nonlinear output regulation. The proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work addresses the full-information output regulation problem for nonlinear systems, assuming the states of both the plant and the exosystem are known. In this setting, perfect tracking or rejection is achieved by constructing a zero-regulation-error manifold π(w) and a feedforward input c(w) that render such manifold invariant. The pair (π(w), c(w)) is characterized by the regulator equations, i.e., a system of PDEs with an algebraic constraint. We focus on accurately solving the regulator equations introducing a physics-informed neural network (PINN) approach that directly approximates π(w) and c(w) by minimizing the residuals under boundary and feasibility conditions, without requiring precomputed trajectories or labeled data. The learned operator maps exosystem states to steady state plant states and inputs, enables real-time inference and, critically, generalizes across families of the exosystem with varying initial conditions and parameters. The framework is validated on a regulation task that synchronizes a helicopter's vertical dynamics with a harmonically oscillating platform. The resulting PINN-based solver reconstructs the zero-error manifold with high fidelity and sustains regulation performance under exosystem variations, highlighting the potential of learning-enabled solvers for nonlinear output regulation. The proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:55:42Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    55,
                    42,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Sebastiano Mengozzi"
                    },
                    {
                        "name": "Giovanni B. Esposito"
                    },
                    {
                        "name": "Michelangelo Bin"
                    },
                    {
                        "name": "Andrea Acquaviva"
                    },
                    {
                        "name": "Andrea Bartolini"
                    },
                    {
                        "name": "Lorenzo Marconi"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Marconi"
                },
                "author": "Lorenzo Marconi"
            },
            {
                "id": "http://arxiv.org/abs/2511.13593v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13593v1",
                "title": "Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents"
                },
                "updated": "2025-11-17T16:55:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    55,
                    19,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13593v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:55:19Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    55,
                    19,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Piaohong Wang"
                    },
                    {
                        "name": "Motong Tian"
                    },
                    {
                        "name": "Jiaxian Li"
                    },
                    {
                        "name": "Yuan Liang"
                    },
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Qianben Chen"
                    },
                    {
                        "name": "Tiannan Wang"
                    },
                    {
                        "name": "Zhicong Lu"
                    },
                    {
                        "name": "Jiawei Ma"
                    },
                    {
                        "name": "Yuchen Eleanor Jiang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wangchunshu Zhou"
                },
                "author": "Wangchunshu Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2511.13590v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13590v1",
                "title": "Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation"
                },
                "updated": "2025-11-17T16:52:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    52,
                    19,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13590v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:52:19Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    52,
                    19,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Xiaoming Yin"
                    },
                    {
                        "name": "Xing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xing Chen"
                },
                "author": "Xing Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.13587v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13587v1",
                "title": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping"
                },
                "updated": "2025-11-17T16:50:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    50,
                    58,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13587v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:50:58Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    50,
                    58,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Haotian Dong"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Rongwei Lu"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang"
            },
            {
                "id": "http://arxiv.org/abs/2509.06924v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.06924v2",
                "title": "Neutron Reflectometry by Gradient Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron Reflectometry by Gradient Descent"
                },
                "updated": "2025-11-17T16:48:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    48,
                    43,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.06924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.06924v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neutron reflectometry (NR) is a powerful technique to probe surfaces and interfaces. NR is inherently an indirect measurement technique, access to the physical quantities of interest (layer thickness, scattering length density, roughness), necessitate the solution of an inverse modelling problem, that is inefficient for large amounts of data or complex multiplayer structures (e.g. lithium batteries / electrodes). Recently, surrogate machine learning models have been proposed as an alternative to existing optimisation routines. Although such approaches have been successful, physical intuition is lost when replacing governing equations with fast neural networks. Instead, we propose a novel and efficient approach; to optimise reflectivity data analysis by performing gradient descent on the forward reflection model itself. Herein, automatic differentiation techniques are used to evaluate exact gradients of the error function with respect to the parameters of interest. Access to these quantities enables users of neutron reflectometry to harness a host of powerful modern optimisation and inference techniques that remain thus far unexploited in the context of neutron reflectometry. This paper presents two benchmark case studies; demonstrating state-of-the-art performance on a thick oxide quartz film, and robust co-fitting performance in the high complexity regime of organic LED multilayer devices. Additionally, we provide an open-source library of differentiable reflectometry kernels in the python programming language so that gradient based approaches can readily be applied to other NR datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron reflectometry (NR) is a powerful technique to probe surfaces and interfaces. NR is inherently an indirect measurement technique, access to the physical quantities of interest (layer thickness, scattering length density, roughness), necessitate the solution of an inverse modelling problem, that is inefficient for large amounts of data or complex multiplayer structures (e.g. lithium batteries / electrodes). Recently, surrogate machine learning models have been proposed as an alternative to existing optimisation routines. Although such approaches have been successful, physical intuition is lost when replacing governing equations with fast neural networks. Instead, we propose a novel and efficient approach; to optimise reflectivity data analysis by performing gradient descent on the forward reflection model itself. Herein, automatic differentiation techniques are used to evaluate exact gradients of the error function with respect to the parameters of interest. Access to these quantities enables users of neutron reflectometry to harness a host of powerful modern optimisation and inference techniques that remain thus far unexploited in the context of neutron reflectometry. This paper presents two benchmark case studies; demonstrating state-of-the-art performance on a thick oxide quartz film, and robust co-fitting performance in the high complexity regime of organic LED multilayer devices. Additionally, we provide an open-source library of differentiable reflectometry kernels in the python programming language so that gradient based approaches can readily be applied to other NR datasets."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-08T17:38:01Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    38,
                    1,
                    0,
                    251,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Max D. Champneys"
                    },
                    {
                        "name": "Andrew J. Parnell"
                    },
                    {
                        "name": "Philipp Gutfreund"
                    },
                    {
                        "name": "Maximilian W. A. Skoda"
                    },
                    {
                        "name": ". Patrick A. Fairclough"
                    },
                    {
                        "name": "Timothy J. Rogers"
                    },
                    {
                        "name": "Stephanie L. Burg"
                    }
                ],
                "author_detail": {
                    "name": "Stephanie L. Burg"
                },
                "author": "Stephanie L. Burg"
            },
            {
                "id": "http://arxiv.org/abs/2508.14031v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.14031v2",
                "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation"
                },
                "updated": "2025-11-17T16:48:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    48,
                    6,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.14031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.14031v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-19T17:53:35Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    53,
                    35,
                    1,
                    231,
                    0
                ],
                "arxiv_comment": "Accepted at AAAI 2026 AI Alignment Track, Source code: https://github.com/HahmDY/agentic-ft-safety",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dongyoon Hahm"
                    },
                    {
                        "name": "Taywon Min"
                    },
                    {
                        "name": "Woogyeol Jin"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee"
            },
            {
                "id": "http://arxiv.org/abs/2506.15545v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.15545v2",
                "title": "RATTENTION: Towards the Minimal Sliding Window Size in Local-Global Attention Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RATTENTION: Towards the Minimal Sliding Window Size in Local-Global Attention Models"
                },
                "updated": "2025-11-17T16:40:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    40,
                    2,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.15545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.15545v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Local-global attention models have recently emerged as compelling alternatives to standard Transformers, promising improvements in both training and inference efficiency. However, the crucial choice of window size presents a Pareto tradeoff: larger windows maintain performance akin to full attention but offer minimal efficiency gains in short-context scenarios, while smaller windows can lead to performance degradation. Current models, such as Gemma2 and Mistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining length) to preserve performance. This work investigates strategies to shift this Pareto frontier, enabling local-global models to achieve efficiency gains even in short-context regimes. Our core motivation is to address the intrinsic limitation of local attention -- its complete disregard for tokens outside the defined window. We explore RATTENTION, a variant of local attention integrated with a specialized linear attention mechanism designed to capture information from these out-of-window tokens. Pretraining experiments at the 3B and 12B scales demonstrate that RATTENTION achieves a superior Pareto tradeoff between performance and efficiency. As a sweet spot, RATTENTION with a window size of just 512 consistently matches the performance of full-attention models across diverse settings. Furthermore, the recurrent nature inherent in the linear attention component of RATTENTION contributes to enhanced long-context performance, as validated on the RULER benchmark. Crucially, these improvements do not compromise training efficiency; thanks to a specialized kernel implementation and the reduced window size, RATTENTION maintains training speeds comparable to existing state-of-the-art approaches. We open-sourced our Pallas kernels along with model codes to facilitate further research effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local-global attention models have recently emerged as compelling alternatives to standard Transformers, promising improvements in both training and inference efficiency. However, the crucial choice of window size presents a Pareto tradeoff: larger windows maintain performance akin to full attention but offer minimal efficiency gains in short-context scenarios, while smaller windows can lead to performance degradation. Current models, such as Gemma2 and Mistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining length) to preserve performance. This work investigates strategies to shift this Pareto frontier, enabling local-global models to achieve efficiency gains even in short-context regimes. Our core motivation is to address the intrinsic limitation of local attention -- its complete disregard for tokens outside the defined window. We explore RATTENTION, a variant of local attention integrated with a specialized linear attention mechanism designed to capture information from these out-of-window tokens. Pretraining experiments at the 3B and 12B scales demonstrate that RATTENTION achieves a superior Pareto tradeoff between performance and efficiency. As a sweet spot, RATTENTION with a window size of just 512 consistently matches the performance of full-attention models across diverse settings. Furthermore, the recurrent nature inherent in the linear attention component of RATTENTION contributes to enhanced long-context performance, as validated on the RULER benchmark. Crucially, these improvements do not compromise training efficiency; thanks to a specialized kernel implementation and the reduced window size, RATTENTION maintains training speeds comparable to existing state-of-the-art approaches. We open-sourced our Pallas kernels along with model codes to facilitate further research effort."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-18T15:18:07Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    15,
                    18,
                    7,
                    2,
                    169,
                    0
                ],
                "arxiv_comment": "9 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Ruoming Pang"
                    }
                ],
                "author_detail": {
                    "name": "Ruoming Pang"
                },
                "author": "Ruoming Pang"
            },
            {
                "id": "http://arxiv.org/abs/2508.10501v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.10501v3",
                "title": "PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning"
                },
                "updated": "2025-11-17T16:36:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    36,
                    12,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.10501v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.10501v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-14T10:03:47Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    3,
                    47,
                    3,
                    226,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yushi Feng"
                    },
                    {
                        "name": "Junye Du"
                    },
                    {
                        "name": "Yingying Hong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Lequan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lequan Yu"
                },
                "author": "Lequan Yu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13566v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13566v1",
                "title": "Identification of triadic phase coupling in wall-bounded turbulence using the bispectrum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification of triadic phase coupling in wall-bounded turbulence using the bispectrum"
                },
                "updated": "2025-11-17T16:35:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    35,
                    30,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13566v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The direction and magnitude of energy transfer between turbulence scale brought about by external forcing on a turbulent boundary layer are uncovered through the bispectrum, bicoherence, and biphase. The bispectrum is a third-order, complex-valued spectrum of the streamwise velocity that preserves the phase information between triadically consistent scales. Normalized bispectrum is the bicoherence, a measure of the relative amount of energy at a higher frequency that results from quadratic phase coupling of two lower frequencies. The phase of the bispectrum, the biphase, measures the phase lag between the high frequency and two lower frequencies that add to it, unveiling whether a triadic interaction produces a forward or reverse cascade of energy. Summing the bispectrum over triadically consistent frequencies allows a spectral decomposition of the velocity skewness and asymmetry, unveiling the triadically active scales in the energy transfer processes. An average sense of energy transfer is inferred from the phase of this skewness spectrum, which shows that scales smaller than the boundary layer thickness contribute to a forward cascade on average, while those larger than the boundary layer thickness have a mix of forward and reverse events. These measures show that the forced scales in the perturbed boundary layer have a mixture of forward and reverse energy transfer processes for different sets of triadic scales and wall-normal locations, providing a method of quantifying the effects of external perturbations on turbulent flows without any need for artificial filtering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The direction and magnitude of energy transfer between turbulence scale brought about by external forcing on a turbulent boundary layer are uncovered through the bispectrum, bicoherence, and biphase. The bispectrum is a third-order, complex-valued spectrum of the streamwise velocity that preserves the phase information between triadically consistent scales. Normalized bispectrum is the bicoherence, a measure of the relative amount of energy at a higher frequency that results from quadratic phase coupling of two lower frequencies. The phase of the bispectrum, the biphase, measures the phase lag between the high frequency and two lower frequencies that add to it, unveiling whether a triadic interaction produces a forward or reverse cascade of energy. Summing the bispectrum over triadically consistent frequencies allows a spectral decomposition of the velocity skewness and asymmetry, unveiling the triadically active scales in the energy transfer processes. An average sense of energy transfer is inferred from the phase of this skewness spectrum, which shows that scales smaller than the boundary layer thickness contribute to a forward cascade on average, while those larger than the boundary layer thickness have a mix of forward and reverse events. These measures show that the forced scales in the perturbed boundary layer have a mixture of forward and reverse energy transfer processes for different sets of triadic scales and wall-normal locations, providing a method of quantifying the effects of external perturbations on turbulent flows without any need for artificial filtering."
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:35:30Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    35,
                    30,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "21 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn"
                },
                "authors": [
                    {
                        "name": "Clayton P. Byers"
                    },
                    {
                        "name": "Subrahmanyam Duvvuri"
                    }
                ],
                "author_detail": {
                    "name": "Subrahmanyam Duvvuri"
                },
                "arxiv_affiliation": "Indian Institute of Science",
                "author": "Subrahmanyam Duvvuri"
            },
            {
                "id": "http://arxiv.org/abs/2502.08282v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.08282v3",
                "title": "Individualised Treatment Effects Estimation with Composite Treatments and Composite Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Individualised Treatment Effects Estimation with Composite Treatments and Composite Outcomes"
                },
                "updated": "2025-11-17T16:25:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    25,
                    6,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.08282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.08282v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Estimating individualised treatment effect (ITE) -- that is the causal effect of a set of variables (also called exposures, treatments, actions, policies, or interventions), referred to as \\textit{composite treatments}, on a set of outcome variables of interest, referred to as \\textit{composite outcomes}, for a unit from observational data -- remains a fundamental problem in causal inference with applications across disciplines, such as healthcare, economics, education, social science, marketing, and computer science. Previous work in causal machine learning for ITE estimation is limited to simple settings, like single treatments and single outcomes. This hinders their use in complex real-world scenarios; for example, consider studying the effect of different ICU interventions, such as beta-blockers and statins for a patient admitted for heart surgery, on different outcomes of interest such as atrial fibrillation and in-hospital mortality. The limited research into composite treatments and outcomes is primarily due to data scarcity for all treatments and outcomes. To address the above challenges, we propose a novel and innovative hypernetwork-based approach, called \\emph{H-Learner}, to solve ITE estimation under composite treatments and composite outcomes, which tackles the data scarcity issue by dynamically sharing information across treatments and outcomes. Our empirical analysis with binary and arbitrary composite treatments and outcomes demonstrates the effectiveness of the proposed approach compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating individualised treatment effect (ITE) -- that is the causal effect of a set of variables (also called exposures, treatments, actions, policies, or interventions), referred to as \\textit{composite treatments}, on a set of outcome variables of interest, referred to as \\textit{composite outcomes}, for a unit from observational data -- remains a fundamental problem in causal inference with applications across disciplines, such as healthcare, economics, education, social science, marketing, and computer science. Previous work in causal machine learning for ITE estimation is limited to simple settings, like single treatments and single outcomes. This hinders their use in complex real-world scenarios; for example, consider studying the effect of different ICU interventions, such as beta-blockers and statins for a patient admitted for heart surgery, on different outcomes of interest such as atrial fibrillation and in-hospital mortality. The limited research into composite treatments and outcomes is primarily due to data scarcity for all treatments and outcomes. To address the above challenges, we propose a novel and innovative hypernetwork-based approach, called \\emph{H-Learner}, to solve ITE estimation under composite treatments and composite outcomes, which tackles the data scarcity issue by dynamically sharing information across treatments and outcomes. Our empirical analysis with binary and arbitrary composite treatments and outcomes demonstrates the effectiveness of the proposed approach compared to existing methods."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-12T10:41:21Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    41,
                    21,
                    2,
                    43,
                    0
                ],
                "arxiv_comment": "Accepted to The 47th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (7 pages (double column), 4 figures)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Vinod Kumar Chauhan"
                    },
                    {
                        "name": "Lei Clifton"
                    },
                    {
                        "name": "Gaurav Nigam"
                    },
                    {
                        "name": "David A. Clifton"
                    }
                ],
                "author_detail": {
                    "name": "David A. Clifton"
                },
                "author": "David A. Clifton"
            },
            {
                "id": "http://arxiv.org/abs/2511.06390v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06390v2",
                "title": "Ghost in the Transformer: Tracing LLM Lineage with SVD-Fingerprint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ghost in the Transformer: Tracing LLM Lineage with SVD-Fingerprint"
                },
                "updated": "2025-11-17T16:20:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    20,
                    58,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06390v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06390v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have rapidly advanced and are widely adopted across diverse fields. Due to the substantial computational cost and data requirements of training from scratch, many developers choose to fine-tune or modify existing open-source models. While most adhere to open-source licenses, some falsely claim original training despite clear derivation from public models. This raises pressing concerns about intellectual property protection and highlights the need for reliable methods to verify model provenance. In this paper, we propose GhostSpec, a lightweight yet effective method for verifying LLM lineage without access to training data or modification of model behavior. Our approach constructs compact and robust fingerprints by applying singular value decomposition (SVD) to invariant products of internal attention weight matrices, effectively capturing the structural identity of a model. Unlike watermarking or output-based methods, GhostSpec is fully data-free, non-invasive, and computationally efficient. It demonstrates strong robustness to sequential fine-tuning, pruning, block expansion, and even adversarial transformations. Extensive experiments show that GhostSpec can reliably trace the lineage of transformed models with minimal overhead. By offering a practical solution for model verification and reuse tracking, our method contributes to the protection of intellectual property and fosters a transparent, trustworthy ecosystem for large-scale language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have rapidly advanced and are widely adopted across diverse fields. Due to the substantial computational cost and data requirements of training from scratch, many developers choose to fine-tune or modify existing open-source models. While most adhere to open-source licenses, some falsely claim original training despite clear derivation from public models. This raises pressing concerns about intellectual property protection and highlights the need for reliable methods to verify model provenance. In this paper, we propose GhostSpec, a lightweight yet effective method for verifying LLM lineage without access to training data or modification of model behavior. Our approach constructs compact and robust fingerprints by applying singular value decomposition (SVD) to invariant products of internal attention weight matrices, effectively capturing the structural identity of a model. Unlike watermarking or output-based methods, GhostSpec is fully data-free, non-invasive, and computationally efficient. It demonstrates strong robustness to sequential fine-tuning, pruning, block expansion, and even adversarial transformations. Extensive experiments show that GhostSpec can reliably trace the lineage of transformed models with minimal overhead. By offering a practical solution for model verification and reuse tracking, our method contributes to the protection of intellectual property and fosters a transparent, trustworthy ecosystem for large-scale language models."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-09T13:57:59Z",
                "published_parsed": [
                    2025,
                    11,
                    9,
                    13,
                    57,
                    59,
                    6,
                    313,
                    0
                ],
                "arxiv_comment": "Accepted at AAAI 2026 (Oral)",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Suqing Wang"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Zuchao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zuchao Li"
                },
                "author": "Zuchao Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.13548v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13548v1",
                "title": "ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models"
                },
                "updated": "2025-11-17T16:19:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    19,
                    21,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13548v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \\textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \\textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:19:21Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    19,
                    21,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Siyang Cheng"
                    },
                    {
                        "name": "Gaotian Liu"
                    },
                    {
                        "name": "Rui Mei"
                    },
                    {
                        "name": "Yilin Wang"
                    },
                    {
                        "name": "Kejia Zhang"
                    },
                    {
                        "name": "Kaishuo Wei"
                    },
                    {
                        "name": "Yuqi Yu"
                    },
                    {
                        "name": "Weiping Wen"
                    },
                    {
                        "name": "Xiaojie Wu"
                    },
                    {
                        "name": "Junhua Liu"
                    }
                ],
                "author_detail": {
                    "name": "Junhua Liu"
                },
                "author": "Junhua Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.01668v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.01668v2",
                "title": "Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics"
                },
                "updated": "2025-11-17T16:17:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    17,
                    55,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.01668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.01668v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As artificial intelligence permeates judicial forensics, ensuring the veracity and traceability of legal question answering (QA) has become critical. Conventional large language models (LLMs) are prone to hallucination, risking misleading guidance in legal consultation, while static knowledge bases struggle to keep pace with frequently updated statutes and case law. We present a hybrid legal QA agent tailored for judicial settings that integrates retrieval-augmented generation (RAG) with multi-model ensembling to deliver reliable, auditable, and continuously updatable counsel. The system prioritizes retrieval over generation: when a trusted legal repository yields relevant evidence, answers are produced via RAG; otherwise, multiple LLMs generate candidates that are scored by a specialized selector, with the top-ranked answer returned. High-quality outputs then undergo human review before being written back to the repository, enabling dynamic knowledge evolution and provenance tracking. Experiments on the Law\\_QA dataset show that our hybrid approach significantly outperforms both a single-model baseline and a vanilla RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm the complementary contributions of retrieval prioritization, model ensembling, and the human-in-the-loop update mechanism. The proposed system demonstrably reduces hallucination while improving answer quality and legal compliance, advancing the practical landing of media forensics technologies in judicial scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As artificial intelligence permeates judicial forensics, ensuring the veracity and traceability of legal question answering (QA) has become critical. Conventional large language models (LLMs) are prone to hallucination, risking misleading guidance in legal consultation, while static knowledge bases struggle to keep pace with frequently updated statutes and case law. We present a hybrid legal QA agent tailored for judicial settings that integrates retrieval-augmented generation (RAG) with multi-model ensembling to deliver reliable, auditable, and continuously updatable counsel. The system prioritizes retrieval over generation: when a trusted legal repository yields relevant evidence, answers are produced via RAG; otherwise, multiple LLMs generate candidates that are scored by a specialized selector, with the top-ranked answer returned. High-quality outputs then undergo human review before being written back to the repository, enabling dynamic knowledge evolution and provenance tracking. Experiments on the Law\\_QA dataset show that our hybrid approach significantly outperforms both a single-model baseline and a vanilla RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm the complementary contributions of retrieval prioritization, model ensembling, and the human-in-the-loop update mechanism. The proposed system demonstrably reduces hallucination while improving answer quality and legal compliance, advancing the practical landing of media forensics technologies in judicial scenarios."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T15:30:58Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    30,
                    58,
                    0,
                    307,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yueqing Xi"
                    },
                    {
                        "name": "Yifan Bai"
                    },
                    {
                        "name": "Huasen Luo"
                    },
                    {
                        "name": "Weiliang Wen"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Haoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Haoliang Li"
                },
                "author": "Haoliang Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.13543v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13543v1",
                "title": "In-memory phononic learning toward cognitive mechanical intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory phononic learning toward cognitive mechanical intelligence"
                },
                "updated": "2025-11-17T16:16:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    16,
                    33,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13543v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern autonomous systems are driving the critical need for next-generation adaptive materials and structures with embodied intelligence, i.e., the embodiment of memory, perception, learning, and decision-making within the mechanical domain. A fundamental challenge is the seamless and efficient integration of memory with information processing in a physically interpretable way that enables cognitive learning and decision-making under uncertainty. Prevailing paradigms, from intricate logic cascades to black-box morphological computing or physical neural networks, are seriously limited by trade-offs among efficiency, scalability, interpretability, transparency, and reliance on additional electronics. Here, we introduce in-memory phononic learning, a paradigm-shifting framework that unifies nonvolatile mechanical memory with wave-based perception within a phononic metastructure. Our system encodes spatial information into stable structural states as mechanical memory that directly programs its elastic wave-propagation landscape. This memory/wave-dynamics coupling enables effective sensory perception, decomposing complex patterns into informative geometric features through frequency-selective wave localization. Learning is created by optimizing input waveforms to selectively probe these features for memory-pattern classification, with decisions inferred directly from the output wave energy, thereby completing the entire information loop mechanically through an efficient and physically transparent mechanism without hidden architectures or electronics. This work transcends the paradigm of 'materials that compute' to cognitive matter capable of interpreting dynamic environments, paving the way for future intelligent structural-material systems with low power consumption, more direct interaction with surroundings, and enhanced cybersecurity and resilience in harsh conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern autonomous systems are driving the critical need for next-generation adaptive materials and structures with embodied intelligence, i.e., the embodiment of memory, perception, learning, and decision-making within the mechanical domain. A fundamental challenge is the seamless and efficient integration of memory with information processing in a physically interpretable way that enables cognitive learning and decision-making under uncertainty. Prevailing paradigms, from intricate logic cascades to black-box morphological computing or physical neural networks, are seriously limited by trade-offs among efficiency, scalability, interpretability, transparency, and reliance on additional electronics. Here, we introduce in-memory phononic learning, a paradigm-shifting framework that unifies nonvolatile mechanical memory with wave-based perception within a phononic metastructure. Our system encodes spatial information into stable structural states as mechanical memory that directly programs its elastic wave-propagation landscape. This memory/wave-dynamics coupling enables effective sensory perception, decomposing complex patterns into informative geometric features through frequency-selective wave localization. Learning is created by optimizing input waveforms to selectively probe these features for memory-pattern classification, with decisions inferred directly from the output wave energy, thereby completing the entire information loop mechanically through an efficient and physically transparent mechanism without hidden architectures or electronics. This work transcends the paradigm of 'materials that compute' to cognitive matter capable of interpreting dynamic environments, paving the way for future intelligent structural-material systems with low power consumption, more direct interaction with surroundings, and enhanced cybersecurity and resilience in harsh conditions."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:16:33Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    16,
                    33,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "19 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "K. W. Wang"
                    }
                ],
                "author_detail": {
                    "name": "K. W. Wang"
                },
                "author": "K. W. Wang"
            },
            {
                "id": "http://arxiv.org/abs/2409.14507v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2409.14507v6",
                "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders"
                },
                "updated": "2025-11-17T16:10:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    10,
                    6,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2409.14507v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2409.14507v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features (\"math\" may split into \"algebra\", \"geometry\", etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get \"absorbed\" into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features (\"math\" may split into \"algebra\", \"geometry\", etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get \"absorbed\" into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-09-22T16:11:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    11,
                    2,
                    6,
                    266,
                    0
                ],
                "arxiv_comment": "Accepted at NeurIPS 2025 (Oral)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "James Wilken-Smith"
                    },
                    {
                        "name": "Tomáš Dulka"
                    },
                    {
                        "name": "Hardik Bhatnagar"
                    },
                    {
                        "name": "Satvik Golechha"
                    },
                    {
                        "name": "Joseph Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Bloom"
                },
                "author": "Joseph Bloom"
            },
            {
                "id": "http://arxiv.org/abs/2409.19100v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2409.19100v2",
                "title": "Personalizing Prostate Cancer Education for Patients Using an EHR-Integrated LLM Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalizing Prostate Cancer Education for Patients Using an EHR-Integrated LLM Agent"
                },
                "updated": "2025-11-17T16:06:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    6,
                    12,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2409.19100v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2409.19100v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cancer patients often lack timely education and personalized support due to clinician workload. This quality improvement study develops and evaluates a Large Language Model (LLM) agent, MedEduChat, which is integrated with the clinic's electronic health records (EHR) and designed to enhance prostate cancer patient education. Fifteen non-metastatic prostate cancer patients and three clinicians recruited from the Mayo Clinic interacted with the agent between May 2024 and April 2025. Findings showed that MedEduChat has a high usability score (UMUX 83.7 out of 100) and improves patients' health confidence (Health Confidence Score rose from 9.9 to 13.9). Clinicians evaluated the patient-chat interaction history and rated MedEduChat as highly correct (2.9 out of 3), complete (2.7 out of 3), and safe (2.7 out of 3), with moderate personalization (2.3 out of 3). This study highlights the potential of LLM agents to improve patient engagement and health education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer patients often lack timely education and personalized support due to clinician workload. This quality improvement study develops and evaluates a Large Language Model (LLM) agent, MedEduChat, which is integrated with the clinic's electronic health records (EHR) and designed to enhance prostate cancer patient education. Fifteen non-metastatic prostate cancer patients and three clinicians recruited from the Mayo Clinic interacted with the agent between May 2024 and April 2025. Findings showed that MedEduChat has a high usability score (UMUX 83.7 out of 100) and improves patients' health confidence (Health Confidence Score rose from 9.9 to 13.9). Clinicians evaluated the patient-chat interaction history and rated MedEduChat as highly correct (2.9 out of 3), complete (2.7 out of 3), and safe (2.7 out of 3), with moderate personalization (2.3 out of 3). This study highlights the potential of LLM agents to improve patient engagement and health education."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-09-27T19:04:11Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    19,
                    4,
                    11,
                    4,
                    271,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "arxiv_journal_ref": "npj Digital Medicine 2025",
                "authors": [
                    {
                        "name": "Yuexing Hao"
                    },
                    {
                        "name": "Jason Holmes"
                    },
                    {
                        "name": "Mark R. Waddle"
                    },
                    {
                        "name": "Brian J. Davis"
                    },
                    {
                        "name": "Nathan Y. Yu"
                    },
                    {
                        "name": "Kristin Vickers"
                    },
                    {
                        "name": "Heather Preston"
                    },
                    {
                        "name": "Drew Margolin"
                    },
                    {
                        "name": "Corinna E. Lockenhoff"
                    },
                    {
                        "name": "Aditya Vashistha"
                    },
                    {
                        "name": "Saleh Kalantari"
                    },
                    {
                        "name": "Marzyeh Ghassemi"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13526v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13526v1",
                "title": "Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models"
                },
                "updated": "2025-11-17T16:00:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    0,
                    42,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13526v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:00:42Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    0,
                    42,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "5 pages, 1 figure, 1 table. Accepted at AI4RWC@WI-IAT 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zhengda Wang"
                    },
                    {
                        "name": "Daqian Shi"
                    },
                    {
                        "name": "Jingyi Zhao"
                    },
                    {
                        "name": "Xiaolei Diao"
                    },
                    {
                        "name": "Xiongfeng Tang"
                    },
                    {
                        "name": "Yanguo Qin"
                    }
                ],
                "author_detail": {
                    "name": "Yanguo Qin"
                },
                "author": "Yanguo Qin"
            },
            {
                "id": "http://arxiv.org/abs/2511.13524v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13524v1",
                "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI"
                },
                "updated": "2025-11-17T15:58:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    58,
                    46,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13524v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T15:58:46Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    58,
                    46,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "9 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "arxiv_journal_ref": "AAAI 2026 Oral",
                "authors": [
                    {
                        "name": "Yuhang Peng"
                    },
                    {
                        "name": "Yizhou Pan"
                    },
                    {
                        "name": "Xinning He"
                    },
                    {
                        "name": "Jihaoyu Yang"
                    },
                    {
                        "name": "Xinyu Yin"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Xiaoji Zheng"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Jiangtao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiangtao Gong"
                },
                "author": "Jiangtao Gong"
            },
            {
                "id": "http://arxiv.org/abs/2502.09258v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.09258v3",
                "title": "A Bayesian estimator for peculiar velocity correction in cosmological inference from supernovae data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian estimator for peculiar velocity correction in cosmological inference from supernovae data"
                },
                "updated": "2025-11-17T15:53:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    53,
                    54,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.09258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.09258v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The peculiar motion of the host galaxies introduces bias in estimating cosmological parameters from supernova data. The coherent component of the peculiar motion is usually corrected for using velocity field reconstruction based on the observed galaxy distribution, while the random component is treated statistically by inflating the magnitude uncertainty in the quadrature derived using the standard error propagation. The method of velocity field reconstruction requires assuming an underlying cosmology, which can introduce its own bias in the final inference. On the other hand, the statistical treatment of the random component assumes a locally linear approximation for the magnitude-redshift relation and a Gaussian distribution for the peculiar velocities, which can have extended tails in the non-linear regime. In this work, we present a Bayesian estimator for simultaneously correcting for peculiar motion while fitting a cosmological model to the supernova data, relaxing the assumption of linearity of the model and Gaussianity of the random peculiar motion. Our approach is based on considering the problem of fitting the magnitude-redshift relation as a non-linear model with errors in both dependent and independent variables. To this end, we develop a general method for fitting such non-linear errors-in-variables models. We then specialize it to the case of fitting the magnitude-redshift relation, validating it with simulated datasets at the precision of current and upcoming surveys, and testing it on the Pantheon sample. Our method provides an alternative approach for accounting for the peculiar velocity effects, which is a complementary method for the coherent component, as it does not require independent velocity measurements, and generalizes the treatment of the random component. Moreover, our general method is applicable to various other problems in cosmology and astronomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The peculiar motion of the host galaxies introduces bias in estimating cosmological parameters from supernova data. The coherent component of the peculiar motion is usually corrected for using velocity field reconstruction based on the observed galaxy distribution, while the random component is treated statistically by inflating the magnitude uncertainty in the quadrature derived using the standard error propagation. The method of velocity field reconstruction requires assuming an underlying cosmology, which can introduce its own bias in the final inference. On the other hand, the statistical treatment of the random component assumes a locally linear approximation for the magnitude-redshift relation and a Gaussian distribution for the peculiar velocities, which can have extended tails in the non-linear regime. In this work, we present a Bayesian estimator for simultaneously correcting for peculiar motion while fitting a cosmological model to the supernova data, relaxing the assumption of linearity of the model and Gaussianity of the random peculiar motion. Our approach is based on considering the problem of fitting the magnitude-redshift relation as a non-linear model with errors in both dependent and independent variables. To this end, we develop a general method for fitting such non-linear errors-in-variables models. We then specialize it to the case of fitting the magnitude-redshift relation, validating it with simulated datasets at the precision of current and upcoming surveys, and testing it on the Pantheon sample. Our method provides an alternative approach for accounting for the peculiar velocity effects, which is a complementary method for the coherent component, as it does not require independent velocity measurements, and generalizes the treatment of the random component. Moreover, our general method is applicable to various other problems in cosmology and astronomy."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-13T12:15:32Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    15,
                    32,
                    3,
                    44,
                    0
                ],
                "arxiv_comment": "11 pages, 6 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Ujjwal Upadhyay"
                    },
                    {
                        "name": "Tarun Deep Saini"
                    },
                    {
                        "name": "Shiv K. Sethi"
                    }
                ],
                "author_detail": {
                    "name": "Shiv K. Sethi"
                },
                "author": "Shiv K. Sethi"
            },
            {
                "id": "http://arxiv.org/abs/2504.19838v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.19838v3",
                "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects"
                },
                "updated": "2025-11-17T15:51:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    51,
                    58,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.19838v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.19838v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents. The collection of papers reviewed in this survey will be hosted and regularly updated on the GitHub repository: https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents. The collection of papers reviewed in this survey will be hosted and regularly updated on the GitHub repository: https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-28T14:39:25Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    39,
                    25,
                    0,
                    118,
                    0
                ],
                "arxiv_comment": "Paper accepted to TMLR 2025, Project Homepage: https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Guangyi Liu"
                    },
                    {
                        "name": "Pengxiang Zhao"
                    },
                    {
                        "name": "Yaozhen Liang"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Yaxuan Guo"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Weifeng Lin"
                    },
                    {
                        "name": "Yuxiang Chai"
                    },
                    {
                        "name": "Yue Han"
                    },
                    {
                        "name": "Shuai Ren"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaoyu Liang"
                    },
                    {
                        "name": "WenHao Wang"
                    },
                    {
                        "name": "Tianze Wu"
                    },
                    {
                        "name": "Zhengxi Lu"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "LiLinghao"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Guanjing Xiong"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.13517v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13517v1",
                "title": "Interpretable Ransomware Detection Using Hybrid Large Language Models: A Comparative Analysis of BERT, RoBERTa, and DeBERTa Through LIME and SHAP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Ransomware Detection Using Hybrid Large Language Models: A Comparative Analysis of BERT, RoBERTa, and DeBERTa Through LIME and SHAP"
                },
                "updated": "2025-11-17T15:51:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    51,
                    36,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13517v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ransomware continues to evolve in complexity, making early and explainable detection a critical requirement for modern cybersecurity systems. This study presents a comparative analysis of three Transformer-based Large Language Models (LLMs) (BERT, RoBERTa, and DeBERTa) for ransomware detection using two structured datasets: UGRansome and Process Memory (PM). Since LLMs are primarily designed for natural language processing (NLP), numerical and categorical ransomware features were transformed into textual sequences using KBinsDiscretizer and token-based encoding. This enabled the models to learn behavioural patterns from system activity and network traffic through contextual embeddings. The models were fine-tuned on approximately 2,500 labelled samples and evaluated using accuracy, F1 score, and ROC-AUC. To ensure transparent decision-making in this high-stakes domain, two explainable AI techniques (LIME and SHAP) were applied to interpret feature contributions. The results show that the models learn distinct ransomware-related cues: BERT relies heavily on dominant file-operation features, RoBERTa demonstrates balanced reliance on network and financial signals, while DeBERTa exhibits strong sensitivity to financial and network-traffic indicators. Visualisation of embeddings further reveals structural differences in token representation, with RoBERTa producing more isotropic embeddings and DeBERTa capturing highly directional, disentangled patterns. In general, RoBERTa achieved the strongest F1-score, while BERT yielded the highest ROC-AUC performance. The integration of LLMs with XAI provides a transparent framework capable of identifying feature-level evidence behind ransomware predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ransomware continues to evolve in complexity, making early and explainable detection a critical requirement for modern cybersecurity systems. This study presents a comparative analysis of three Transformer-based Large Language Models (LLMs) (BERT, RoBERTa, and DeBERTa) for ransomware detection using two structured datasets: UGRansome and Process Memory (PM). Since LLMs are primarily designed for natural language processing (NLP), numerical and categorical ransomware features were transformed into textual sequences using KBinsDiscretizer and token-based encoding. This enabled the models to learn behavioural patterns from system activity and network traffic through contextual embeddings. The models were fine-tuned on approximately 2,500 labelled samples and evaluated using accuracy, F1 score, and ROC-AUC. To ensure transparent decision-making in this high-stakes domain, two explainable AI techniques (LIME and SHAP) were applied to interpret feature contributions. The results show that the models learn distinct ransomware-related cues: BERT relies heavily on dominant file-operation features, RoBERTa demonstrates balanced reliance on network and financial signals, while DeBERTa exhibits strong sensitivity to financial and network-traffic indicators. Visualisation of embeddings further reveals structural differences in token representation, with RoBERTa producing more isotropic embeddings and DeBERTa capturing highly directional, disentangled patterns. In general, RoBERTa achieved the strongest F1-score, while BERT yielded the highest ROC-AUC performance. The integration of LLMs with XAI provides a transparent framework capable of identifying feature-level evidence behind ransomware predictions."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T15:51:36Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    51,
                    36,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Elodie Mutombo Ngoie"
                    },
                    {
                        "name": "Mike Nkongolo Wa Nkongolo"
                    },
                    {
                        "name": "Peace Azugo"
                    },
                    {
                        "name": "Mahmut Tokmak"
                    }
                ],
                "author_detail": {
                    "name": "Mahmut Tokmak"
                },
                "author": "Mahmut Tokmak"
            },
            {
                "id": "http://arxiv.org/abs/2507.10800v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.10800v2",
                "title": "ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference"
                },
                "updated": "2025-11-17T15:43:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    43,
                    33,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.10800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.10800v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "ViTs deliver SOTA performance, yet their fixed computational budget prevents scalable deployment across heterogeneous hardware. Recent Matryoshka-style Transformer architectures mitigate this by embedding nested subnetworks within a single model to enable scalable inference. However, these models allocate the same amount of compute to all inputs, regardless of their complexity, which leads to inefficiencies. To address this, we introduce ThinkingViT, a nested ViT architecture that employs progressive thinking stages to dynamically adjust inference computation based on input difficulty. ThinkingViT first activates a small subset of the most important attention heads to produce an initial prediction. If the prediction confidence exceeds a predefined threshold, inference terminates early. Otherwise, within the same backbone, it activates a larger subset of attention heads and conducts a new forward pass. This process continues iteratively until the model reaches the predefined confidence level or exhausts its maximum capacity. To boost the performance of subsequent rounds, we introduce a Token Recycling approach that fuses the input embeddings with the embeddings from the previous stage. Experiments show that ThinkingViT surpasses nested baselines by up to 2.0 percentage points (p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs on ImageNet-1K. We show that the backbone-preserving design of ThinkingViT allows it to serve as a plug-in upgrade for ViTs in downstream tasks such as semantic segmentation. We also demonstrate that ThinkingViT transfers effectively to other architectures such as Swin. The source code is available at https://github.com/ds-kiel/ThinkingViT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViTs deliver SOTA performance, yet their fixed computational budget prevents scalable deployment across heterogeneous hardware. Recent Matryoshka-style Transformer architectures mitigate this by embedding nested subnetworks within a single model to enable scalable inference. However, these models allocate the same amount of compute to all inputs, regardless of their complexity, which leads to inefficiencies. To address this, we introduce ThinkingViT, a nested ViT architecture that employs progressive thinking stages to dynamically adjust inference computation based on input difficulty. ThinkingViT first activates a small subset of the most important attention heads to produce an initial prediction. If the prediction confidence exceeds a predefined threshold, inference terminates early. Otherwise, within the same backbone, it activates a larger subset of attention heads and conducts a new forward pass. This process continues iteratively until the model reaches the predefined confidence level or exhausts its maximum capacity. To boost the performance of subsequent rounds, we introduce a Token Recycling approach that fuses the input embeddings with the embeddings from the previous stage. Experiments show that ThinkingViT surpasses nested baselines by up to 2.0 percentage points (p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs on ImageNet-1K. We show that the backbone-preserving design of ThinkingViT allows it to serve as a plug-in upgrade for ViTs in downstream tasks such as semantic segmentation. We also demonstrate that ThinkingViT transfers effectively to other architectures such as Swin. The source code is available at https://github.com/ds-kiel/ThinkingViT."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-14T20:54:41Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    54,
                    41,
                    0,
                    195,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ali Hojjat"
                    },
                    {
                        "name": "Janek Haberer"
                    },
                    {
                        "name": "Soren Pirk"
                    },
                    {
                        "name": "Olaf Landsiedel"
                    }
                ],
                "author_detail": {
                    "name": "Olaf Landsiedel"
                },
                "author": "Olaf Landsiedel"
            },
            {
                "id": "http://arxiv.org/abs/2511.13505v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13505v1",
                "title": "Applying Large Language Models to Characterize Public Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying Large Language Models to Characterize Public Narratives"
                },
                "updated": "2025-11-17T15:41:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    41,
                    55,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13505v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T15:41:55Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    41,
                    55,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Elinor Poole-Dayan"
                    },
                    {
                        "name": "Daniel T Kessler"
                    },
                    {
                        "name": "Hannah Chiou"
                    },
                    {
                        "name": "Margaret Hughes"
                    },
                    {
                        "name": "Emily S Lin"
                    },
                    {
                        "name": "Marshall Ganz"
                    },
                    {
                        "name": "Deb Roy"
                    }
                ],
                "author_detail": {
                    "name": "Deb Roy"
                },
                "author": "Deb Roy"
            },
            {
                "id": "http://arxiv.org/abs/2511.13502v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13502v1",
                "title": "Tight and Practical Privacy Auditing for Differentially Private In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight and Practical Privacy Auditing for Differentially Private In-Context Learning"
                },
                "updated": "2025-11-17T15:39:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    39,
                    54,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13502v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) perform in-context learning (ICL) by adapting to tasks from prompt demonstrations, which in practice often contain private or proprietary data. Although differential privacy (DP) with private voting is a pragmatic mitigation, DP-ICL implementations are error-prone, and worst-case DP bounds may substantially overestimate actual leakage, calling for practical auditing tools. We present a tight and efficient privacy auditing framework for DP-ICL systems that runs membership inference attacks and translates their success rates into empirical privacy guarantees using Gaussian DP. Our analysis of the private voting mechanism identifies vote configurations that maximize the auditing signal, guiding the design of audit queries that reliably reveal whether a canary demonstration is present in the context. The framework supports both black-box (API-only) and white-box (internal vote) threat models, and unifies auditing for classification and generation by reducing both to a binary decision problem. Experiments on standard text classification and generation benchmarks show that our empirical leakage estimates closely match theoretical DP budgets on classification tasks and are consistently lower on generation tasks due to conservative embedding-sensitivity bounds, making our framework a practical privacy auditor and verifier for real-world DP-ICL deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) perform in-context learning (ICL) by adapting to tasks from prompt demonstrations, which in practice often contain private or proprietary data. Although differential privacy (DP) with private voting is a pragmatic mitigation, DP-ICL implementations are error-prone, and worst-case DP bounds may substantially overestimate actual leakage, calling for practical auditing tools. We present a tight and efficient privacy auditing framework for DP-ICL systems that runs membership inference attacks and translates their success rates into empirical privacy guarantees using Gaussian DP. Our analysis of the private voting mechanism identifies vote configurations that maximize the auditing signal, guiding the design of audit queries that reliably reveal whether a canary demonstration is present in the context. The framework supports both black-box (API-only) and white-box (internal vote) threat models, and unifies auditing for classification and generation by reducing both to a binary decision problem. Experiments on standard text classification and generation benchmarks show that our empirical leakage estimates closely match theoretical DP budgets on classification tasks and are consistently lower on generation tasks due to conservative embedding-sensitivity bounds, making our framework a practical privacy auditor and verifier for real-world DP-ICL deployments."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T15:39:54Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    39,
                    54,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Yuyang Xia"
                    },
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Li Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiong"
                },
                "author": "Li Xiong"
            },
            {
                "id": "http://arxiv.org/abs/2502.17772v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.17772v3",
                "title": "An Improved Privacy and Utility Analysis of Differentially Private SGD with Bounded Domain and Smooth Losses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Improved Privacy and Utility Analysis of Differentially Private SGD with Bounded Domain and Smooth Losses"
                },
                "updated": "2025-11-17T15:28:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    28,
                    55,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.17772v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.17772v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to protect sensitive data during the training of machine learning models, but its privacy guarantee often comes at a large cost of model performance due to the lack of tight theoretical bounds quantifying privacy loss. While recent efforts have achieved more accurate privacy guarantees, they still impose some assumptions prohibited from practical applications, such as convexity and complex parameter requirements, and rarely investigate in-depth the impact of privacy mechanisms on the model's utility. In this paper, we provide a rigorous privacy characterization for DPSGD with general L-smooth and non-convex loss functions, revealing converged privacy loss with iteration in bounded-domain cases. Specifically, we track the privacy loss over multiple iterations, leveraging the noisy smooth-reduction property, and further establish comprehensive convergence analysis in different scenarios. In particular, we show that for DPSGD with a bounded domain, (i) the privacy loss can still converge without the convexity assumption, (ii) a smaller bounded diameter can improve both privacy and utility simultaneously under certain conditions, and (iii) the attainable big-O order of the privacy utility trade-off for DPSGD with gradient clipping (DPSGD-GC) and for DPSGD-GC with bounded domain (DPSGD-DC) and mu-strongly convex population risk function, respectively. Experiments via membership inference attack (MIA) in a practical setting validate insights gained from the theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to protect sensitive data during the training of machine learning models, but its privacy guarantee often comes at a large cost of model performance due to the lack of tight theoretical bounds quantifying privacy loss. While recent efforts have achieved more accurate privacy guarantees, they still impose some assumptions prohibited from practical applications, such as convexity and complex parameter requirements, and rarely investigate in-depth the impact of privacy mechanisms on the model's utility. In this paper, we provide a rigorous privacy characterization for DPSGD with general L-smooth and non-convex loss functions, revealing converged privacy loss with iteration in bounded-domain cases. Specifically, we track the privacy loss over multiple iterations, leveraging the noisy smooth-reduction property, and further establish comprehensive convergence analysis in different scenarios. In particular, we show that for DPSGD with a bounded domain, (i) the privacy loss can still converge without the convexity assumption, (ii) a smaller bounded diameter can improve both privacy and utility simultaneously under certain conditions, and (iii) the attainable big-O order of the privacy utility trade-off for DPSGD with gradient clipping (DPSGD-GC) and for DPSGD-GC with bounded domain (DPSGD-DC) and mu-strongly convex population risk function, respectively. Experiments via membership inference attack (MIA) in a practical setting validate insights gained from the theoretical results."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-25T02:05:41Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    2,
                    5,
                    41,
                    1,
                    56,
                    0
                ],
                "arxiv_comment": "19 pages, 5 figures, accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Wanrong Zhang"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Kaishun Wu"
                    },
                    {
                        "name": "Hong Xing"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xing"
                },
                "author": "Hong Xing"
            },
            {
                "id": "http://arxiv.org/abs/2411.11647v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.11647v3",
                "title": "Near-Optimal Reinforcement Learning with Shuffle Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Optimal Reinforcement Learning with Shuffle Differential Privacy"
                },
                "updated": "2025-11-17T15:22:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    22,
                    52,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.11647v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.11647v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement learning (RL) is a powerful tool for sequential decision-making, but its application is often hindered by privacy concerns arising from its interaction data. This challenge is particularly acute in advanced networked systems, where learning from operational and user data can expose systems to privacy inference attacks. Existing differential privacy (DP) models for RL are often inadequate: the centralized model requires a fully trusted server, creating a single point of failure risk, while the local model incurs significant performance degradation that is unsuitable for many networked applications. This paper addresses this gap by leveraging the emerging shuffle model of privacy, an intermediate trust model that provides strong privacy guarantees without a centralized trust assumption. We present Shuffle Differentially Private Policy Elimination (SDP-PE), the first generic policy elimination-based algorithm for episodic RL under the shuffle model. Our method introduces a novel exponential batching schedule and a ``forgetting'' mechanism to balance the competing demands of privacy and learning performance. Our analysis shows that SDP-PE achieves a near-optimal regret bound, demonstrating a superior privacy-regret trade-off with utility comparable to the centralized model while significantly outperforming the local model. The numerical experiments also corroborate our theoretical results and demonstrate the effectiveness of SDP-PE. This work establishes the viability of the shuffle model for secure data-driven decision-making in networked systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is a powerful tool for sequential decision-making, but its application is often hindered by privacy concerns arising from its interaction data. This challenge is particularly acute in advanced networked systems, where learning from operational and user data can expose systems to privacy inference attacks. Existing differential privacy (DP) models for RL are often inadequate: the centralized model requires a fully trusted server, creating a single point of failure risk, while the local model incurs significant performance degradation that is unsuitable for many networked applications. This paper addresses this gap by leveraging the emerging shuffle model of privacy, an intermediate trust model that provides strong privacy guarantees without a centralized trust assumption. We present Shuffle Differentially Private Policy Elimination (SDP-PE), the first generic policy elimination-based algorithm for episodic RL under the shuffle model. Our method introduces a novel exponential batching schedule and a ``forgetting'' mechanism to balance the competing demands of privacy and learning performance. Our analysis shows that SDP-PE achieves a near-optimal regret bound, demonstrating a superior privacy-regret trade-off with utility comparable to the centralized model while significantly outperforming the local model. The numerical experiments also corroborate our theoretical results and demonstrate the effectiveness of SDP-PE. This work establishes the viability of the shuffle model for secure data-driven decision-making in networked systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-18T15:24:11Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    15,
                    24,
                    11,
                    0,
                    323,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Shaojie Bai"
                    },
                    {
                        "name": "Mohammad Sadegh Talebi"
                    },
                    {
                        "name": "Chengcheng Zhao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Jiming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiming Chen"
                },
                "author": "Jiming Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.13478v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13478v1",
                "title": "Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling"
                },
                "updated": "2025-11-17T15:16:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    16,
                    13,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13478v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T15:16:13Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    16,
                    13,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Adam Hazimeh"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Mark Collier"
                    },
                    {
                        "name": "Gilles Baechler"
                    },
                    {
                        "name": "Efi Kokiopoulou"
                    },
                    {
                        "name": "Pascal Frossard"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Frossard"
                },
                "author": "Pascal Frossard"
            },
            {
                "id": "http://arxiv.org/abs/2511.13476v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13476v1",
                "title": "Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation"
                },
                "updated": "2025-11-17T15:14:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    14,
                    17,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13476v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.3390/app152111619",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T15:14:17Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    14,
                    17,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "arxiv_journal_ref": "Applied Sciences, 2025, 15(21), 11619",
                "authors": [
                    {
                        "name": "Zhipeng Ma"
                    },
                    {
                        "name": "Ali Rida Bahja"
                    },
                    {
                        "name": "Andreas Burgdorf"
                    },
                    {
                        "name": "André Pomp"
                    },
                    {
                        "name": "Tobias Meisen"
                    },
                    {
                        "name": "Bo Nørregaard Jørgensen"
                    },
                    {
                        "name": "Zheng Grace Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Grace Ma"
                },
                "author": "Zheng Grace Ma",
                "arxiv_doi": "10.3390/app152111619"
            },
            {
                "id": "http://arxiv.org/abs/2507.00032v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.00032v2",
                "title": "Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Exercise Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Exercise Recommendation"
                },
                "updated": "2025-11-17T15:10:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    10,
                    41,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.00032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.00032v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Adaptive exercise recommendation (ER) aims to choose the next activity that matches a learner's evolving Zone of Proximal Development (ZPD). We present KUL-Rec, a biologically inspired ER system that couples a fast Hebbian memory with slow replay-based consolidation to enable continual, few-shot personalization from sparse interactions. The model operates in an embedding space, allowing a single architecture to handle both tabular knowledge-tracing logs and open-ended short-answer text. We align evaluation with tutoring needs using bidirectional ranking and rank-sensitive metrics (nDCG, Recall@K). Across ten public datasets, KUL-Rec improves macro nDCG (0.316 vs. 0.265 for the strongest baseline) and Recall@10 (0.305 vs. 0.211), while achieving low inference latency and an $\\approx99$\\% reduction in peak GPU memory relative to a competitive graph-based model. In a 13-week graduate course, KUL-Rec personalized weekly short-answer quizzes generated by a retrieval-augmented pipeline and the personalized quizzes were associated with lower perceived difficulty and higher helpfulness (p < .05). An embedding robustness audit highlights that encoder choice affects semantic alignment, motivating routine audits when deploying open-response assessment. Together, these results indicate that Hebbian replay with bounded consolidation offers a practical path to real-time, interpretable ER that scales across data modalities and classroom settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive exercise recommendation (ER) aims to choose the next activity that matches a learner's evolving Zone of Proximal Development (ZPD). We present KUL-Rec, a biologically inspired ER system that couples a fast Hebbian memory with slow replay-based consolidation to enable continual, few-shot personalization from sparse interactions. The model operates in an embedding space, allowing a single architecture to handle both tabular knowledge-tracing logs and open-ended short-answer text. We align evaluation with tutoring needs using bidirectional ranking and rank-sensitive metrics (nDCG, Recall@K). Across ten public datasets, KUL-Rec improves macro nDCG (0.316 vs. 0.265 for the strongest baseline) and Recall@10 (0.305 vs. 0.211), while achieving low inference latency and an $\\approx99$\\% reduction in peak GPU memory relative to a competitive graph-based model. In a 13-week graduate course, KUL-Rec personalized weekly short-answer quizzes generated by a retrieval-augmented pipeline and the personalized quizzes were associated with lower perceived difficulty and higher helpfulness (p < .05). An embedding robustness audit highlights that encoder choice affects semantic alignment, motivating routine audits when deploying open-response assessment. Together, these results indicate that Hebbian replay with bounded consolidation offers a practical path to real-time, interpretable ER that scales across data modalities and classroom settings."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-18T00:06:28Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    0,
                    6,
                    28,
                    2,
                    169,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Grey Kuling"
                    },
                    {
                        "name": "Marinka Zitnik"
                    }
                ],
                "author_detail": {
                    "name": "Marinka Zitnik"
                },
                "author": "Marinka Zitnik"
            },
            {
                "id": "http://arxiv.org/abs/2112.06861v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2112.06861v3",
                "title": "Tests of General Relativity with GWTC-3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tests of General Relativity with GWTC-3"
                },
                "updated": "2025-11-17T15:05:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    5,
                    31,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2112.06861v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2112.06861v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1103/PhysRevD.112.084080",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The ever-increasing number of detections of gravitational waves (GWs) from compact binaries by the Advanced LIGO and Advanced Virgo detectors allows us to perform ever-more sensitive tests of general relativity (GR) in the dynamical and strong-field regime of gravity. We perform a suite of tests of GR using the compact binary signals observed during the second half of the third observing run of those detectors. We restrict our analysis to the 15 confident signals that have false alarm rates $\\leq 10^{-3}\\, {\\rm yr}^{-1}$. In addition to signals consistent with binary black hole (BH) mergers, the new events include GW200115_042309, a signal consistent with a neutron star--BH merger. We find the residual power, after subtracting the best fit waveform from the data for each event, to be consistent with the detector noise. Additionally, we find all the post-Newtonian deformation coefficients to be consistent with the predictions from GR, with an improvement by a factor of ~2 in the -1PN parameter. We also find that the spin-induced quadrupole moments of the binary BH constituents are consistent with those of Kerr BHs in GR. We find no evidence for dispersion of GWs, non-GR modes of polarization, or post-merger echoes in the events that were analyzed. We update the bound on the mass of the graviton, at 90% credibility, to $m_g \\leq 2.42 \\times 10^{-23} \\mathrm{eV}/c^2$. The final mass and final spin as inferred from the pre-merger and post-merger parts of the waveform are consistent with each other. The studies of the properties of the remnant BHs, including deviations of the quasi-normal mode frequencies and damping times, show consistency with the predictions of GR. In addition to considering signals individually, we also combine results from the catalog of GW signals to calculate more precise population constraints. We find no evidence in support of physics beyond GR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing number of detections of gravitational waves (GWs) from compact binaries by the Advanced LIGO and Advanced Virgo detectors allows us to perform ever-more sensitive tests of general relativity (GR) in the dynamical and strong-field regime of gravity. We perform a suite of tests of GR using the compact binary signals observed during the second half of the third observing run of those detectors. We restrict our analysis to the 15 confident signals that have false alarm rates $\\leq 10^{-3}\\, {\\rm yr}^{-1}$. In addition to signals consistent with binary black hole (BH) mergers, the new events include GW200115_042309, a signal consistent with a neutron star--BH merger. We find the residual power, after subtracting the best fit waveform from the data for each event, to be consistent with the detector noise. Additionally, we find all the post-Newtonian deformation coefficients to be consistent with the predictions from GR, with an improvement by a factor of ~2 in the -1PN parameter. We also find that the spin-induced quadrupole moments of the binary BH constituents are consistent with those of Kerr BHs in GR. We find no evidence for dispersion of GWs, non-GR modes of polarization, or post-merger echoes in the events that were analyzed. We update the bound on the mass of the graviton, at 90% credibility, to $m_g \\leq 2.42 \\times 10^{-23} \\mathrm{eV}/c^2$. The final mass and final spin as inferred from the pre-merger and post-merger parts of the waveform are consistent with each other. The studies of the properties of the remnant BHs, including deviations of the quasi-normal mode frequencies and damping times, show consistency with the predictions of GR. In addition to considering signals individually, we also combine results from the catalog of GW signals to calculate more precise population constraints. We find no evidence in support of physics beyond GR."
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2021-12-13T18:19:04Z",
                "published_parsed": [
                    2021,
                    12,
                    13,
                    18,
                    19,
                    4,
                    0,
                    347,
                    0
                ],
                "arxiv_comment": "accepted in PRD; v3: fixed graviton mass bound in abstract",
                "arxiv_primary_category": {
                    "term": "gr-qc"
                },
                "arxiv_journal_ref": "Phys. Rev. D 112, 084080 (2025)",
                "authors": [
                    {
                        "name": "The LIGO Scientific Collaboration"
                    },
                    {
                        "name": "the Virgo Collaboration"
                    },
                    {
                        "name": "the KAGRA Collaboration"
                    },
                    {
                        "name": "R. Abbott"
                    },
                    {
                        "name": "H. Abe"
                    },
                    {
                        "name": "F. Acernese"
                    },
                    {
                        "name": "K. Ackley"
                    },
                    {
                        "name": "N. Adhikari"
                    },
                    {
                        "name": "R. X. Adhikari"
                    },
                    {
                        "name": "V. K. Adkins"
                    },
                    {
                        "name": "V. B. Adya"
                    },
                    {
                        "name": "C. Affeldt"
                    },
                    {
                        "name": "D. Agarwal"
                    },
                    {
                        "name": "M. Agathos"
                    },
                    {
                        "name": "K. Agatsuma"
                    },
                    {
                        "name": "N. Aggarwal"
                    },
                    {
                        "name": "O. D. Aguiar"
                    },
                    {
                        "name": "L. Aiello"
                    },
                    {
                        "name": "A. Ain"
                    },
                    {
                        "name": "P. Ajith"
                    },
                    {
                        "name": "T. Akutsu"
                    },
                    {
                        "name": "P. F. de Alarcón"
                    },
                    {
                        "name": "S. Albanesi"
                    },
                    {
                        "name": "R. A. Alfaidi"
                    },
                    {
                        "name": "A. Allocca"
                    },
                    {
                        "name": "P. A. Altin"
                    },
                    {
                        "name": "A. Amato"
                    },
                    {
                        "name": "C. Anand"
                    },
                    {
                        "name": "S. Anand"
                    },
                    {
                        "name": "A. Ananyeva"
                    },
                    {
                        "name": "S. B. Anderson"
                    },
                    {
                        "name": "W. G. Anderson"
                    },
                    {
                        "name": "M. Ando"
                    },
                    {
                        "name": "T. Andrade"
                    },
                    {
                        "name": "N. Andres"
                    },
                    {
                        "name": "M. Andrés-Carcasona"
                    },
                    {
                        "name": "T. Andrić"
                    },
                    {
                        "name": "S. V. Angelova"
                    },
                    {
                        "name": "S. Ansoldi"
                    },
                    {
                        "name": "J. M. Antelis"
                    },
                    {
                        "name": "S. Antier"
                    },
                    {
                        "name": "T. Apostolatos"
                    },
                    {
                        "name": "E. Z. Appavuravther"
                    },
                    {
                        "name": "S. Appert"
                    },
                    {
                        "name": "S. K. Apple"
                    },
                    {
                        "name": "K. Arai"
                    },
                    {
                        "name": "A. Araya"
                    },
                    {
                        "name": "M. C. Araya"
                    },
                    {
                        "name": "J. S. Areeda"
                    },
                    {
                        "name": "M. Arène"
                    },
                    {
                        "name": "N. Aritomi"
                    },
                    {
                        "name": "N. Arnaud"
                    },
                    {
                        "name": "M. Arogeti"
                    },
                    {
                        "name": "S. M. Aronson"
                    },
                    {
                        "name": "K. G. Arun"
                    },
                    {
                        "name": "H. Asada"
                    },
                    {
                        "name": "Y. Asali"
                    },
                    {
                        "name": "G. Ashton"
                    },
                    {
                        "name": "Y. Aso"
                    },
                    {
                        "name": "M. Assiduo"
                    },
                    {
                        "name": "S. Assis de Souza Melo"
                    },
                    {
                        "name": "S. M. Aston"
                    },
                    {
                        "name": "P. Astone"
                    },
                    {
                        "name": "F. Aubin"
                    },
                    {
                        "name": "K. AultONeal"
                    },
                    {
                        "name": "C. Austin"
                    },
                    {
                        "name": "S. Babak"
                    },
                    {
                        "name": "F. Badaracco"
                    },
                    {
                        "name": "M. K. M. Bader"
                    },
                    {
                        "name": "C. Badger"
                    },
                    {
                        "name": "S. Bae"
                    },
                    {
                        "name": "Y. Bae"
                    },
                    {
                        "name": "A. M. Baer"
                    },
                    {
                        "name": "S. Bagnasco"
                    },
                    {
                        "name": "Y. Bai"
                    },
                    {
                        "name": "J. Baird"
                    },
                    {
                        "name": "R. Bajpai"
                    },
                    {
                        "name": "T. Baka"
                    },
                    {
                        "name": "M. Ball"
                    },
                    {
                        "name": "G. Ballardin"
                    },
                    {
                        "name": "S. W. Ballmer"
                    },
                    {
                        "name": "A. Balsamo"
                    },
                    {
                        "name": "G. Baltus"
                    },
                    {
                        "name": "S. Banagiri"
                    },
                    {
                        "name": "B. Banerjee"
                    },
                    {
                        "name": "D. Bankar"
                    },
                    {
                        "name": "J. C. Barayoga"
                    },
                    {
                        "name": "C. Barbieri"
                    },
                    {
                        "name": "B. C. Barish"
                    },
                    {
                        "name": "D. Barker"
                    },
                    {
                        "name": "P. Barneo"
                    },
                    {
                        "name": "F. Barone"
                    },
                    {
                        "name": "B. Barr"
                    },
                    {
                        "name": "L. Barsotti"
                    },
                    {
                        "name": "M. Barsuglia"
                    },
                    {
                        "name": "D. Barta"
                    },
                    {
                        "name": "J. Bartlett"
                    },
                    {
                        "name": "M. A. Barton"
                    },
                    {
                        "name": "I. Bartos"
                    },
                    {
                        "name": "S. Basak"
                    },
                    {
                        "name": "R. Bassiri"
                    },
                    {
                        "name": "A. Basti"
                    },
                    {
                        "name": "M. Bawaj"
                    },
                    {
                        "name": "J. C. Bayley"
                    },
                    {
                        "name": "M. Bazzan"
                    },
                    {
                        "name": "B. R. Becher"
                    },
                    {
                        "name": "B. Bécsy"
                    },
                    {
                        "name": "V. M. Bedakihale"
                    },
                    {
                        "name": "F. Beirnaert"
                    },
                    {
                        "name": "M. Bejger"
                    },
                    {
                        "name": "I. Belahcene"
                    },
                    {
                        "name": "V. Benedetto"
                    },
                    {
                        "name": "D. Beniwal"
                    },
                    {
                        "name": "M. G. Benjamin"
                    },
                    {
                        "name": "T. F. Bennett"
                    },
                    {
                        "name": "J. D. Bentley"
                    },
                    {
                        "name": "M. BenYaala"
                    },
                    {
                        "name": "S. Bera"
                    },
                    {
                        "name": "M. Berbel"
                    },
                    {
                        "name": "F. Bergamin"
                    },
                    {
                        "name": "B. K. Berger"
                    },
                    {
                        "name": "S. Bernuzzi"
                    },
                    {
                        "name": "C. P. L. Berry"
                    },
                    {
                        "name": "D. Bersanetti"
                    },
                    {
                        "name": "A. Bertolini"
                    },
                    {
                        "name": "J. Betzwieser"
                    },
                    {
                        "name": "D. Beveridge"
                    },
                    {
                        "name": "R. Bhandare"
                    },
                    {
                        "name": "A. V. Bhandari"
                    },
                    {
                        "name": "U. Bhardwaj"
                    },
                    {
                        "name": "R. Bhatt"
                    },
                    {
                        "name": "D. Bhattacharjee"
                    },
                    {
                        "name": "S. Bhaumik"
                    },
                    {
                        "name": "A. Bianchi"
                    },
                    {
                        "name": "I. A. Bilenko"
                    },
                    {
                        "name": "G. Billingsley"
                    },
                    {
                        "name": "S. Bini"
                    },
                    {
                        "name": "R. Birney"
                    },
                    {
                        "name": "O. Birnholtz"
                    },
                    {
                        "name": "S. Biscans"
                    },
                    {
                        "name": "M. Bischi"
                    },
                    {
                        "name": "S. Biscoveanu"
                    },
                    {
                        "name": "A. Bisht"
                    },
                    {
                        "name": "B. Biswas"
                    },
                    {
                        "name": "M. Bitossi"
                    },
                    {
                        "name": "M. -A. Bizouard"
                    },
                    {
                        "name": "J. K. Blackburn"
                    },
                    {
                        "name": "C. D. Blair"
                    },
                    {
                        "name": "D. G. Blair"
                    },
                    {
                        "name": "R. M. Blair"
                    },
                    {
                        "name": "F. Bobba"
                    },
                    {
                        "name": "N. Bode"
                    },
                    {
                        "name": "M. Boër"
                    },
                    {
                        "name": "G. Bogaert"
                    },
                    {
                        "name": "M. Boldrini"
                    },
                    {
                        "name": "G. N. Bolingbroke"
                    },
                    {
                        "name": "L. D. Bonavena"
                    },
                    {
                        "name": "F. Bondu"
                    },
                    {
                        "name": "E. Bonilla"
                    },
                    {
                        "name": "R. Bonnand"
                    },
                    {
                        "name": "P. Booker"
                    },
                    {
                        "name": "B. A. Boom"
                    },
                    {
                        "name": "R. Bork"
                    },
                    {
                        "name": "V. Boschi"
                    },
                    {
                        "name": "N. Bose"
                    },
                    {
                        "name": "S. Bose"
                    },
                    {
                        "name": "V. Bossilkov"
                    },
                    {
                        "name": "V. Boudart"
                    },
                    {
                        "name": "Y. Bouffanais"
                    },
                    {
                        "name": "A. Bozzi"
                    },
                    {
                        "name": "C. Bradaschia"
                    },
                    {
                        "name": "P. R. Brady"
                    },
                    {
                        "name": "A. Bramley"
                    },
                    {
                        "name": "A. Branch"
                    },
                    {
                        "name": "M. Branchesi"
                    },
                    {
                        "name": "J. E. Brau"
                    },
                    {
                        "name": "M. Breschi"
                    },
                    {
                        "name": "T. Briant"
                    },
                    {
                        "name": "J. H. Briggs"
                    },
                    {
                        "name": "A. Brillet"
                    },
                    {
                        "name": "M. Brinkmann"
                    },
                    {
                        "name": "P. Brockill"
                    },
                    {
                        "name": "A. F. Brooks"
                    },
                    {
                        "name": "J. Brooks"
                    },
                    {
                        "name": "D. D. Brown"
                    },
                    {
                        "name": "S. Brunett"
                    },
                    {
                        "name": "G. Bruno"
                    },
                    {
                        "name": "R. Bruntz"
                    },
                    {
                        "name": "J. Bryant"
                    },
                    {
                        "name": "F. Bucci"
                    },
                    {
                        "name": "T. Bulik"
                    },
                    {
                        "name": "H. J. Bulten"
                    },
                    {
                        "name": "A. Buonanno"
                    },
                    {
                        "name": "K. Burtnyk"
                    },
                    {
                        "name": "R. Buscicchio"
                    },
                    {
                        "name": "D. Buskulic"
                    },
                    {
                        "name": "C. Buy"
                    },
                    {
                        "name": "R. L. Byer"
                    },
                    {
                        "name": "G. S. Cabourn Davies"
                    },
                    {
                        "name": "G. Cabras"
                    },
                    {
                        "name": "R. Cabrita"
                    },
                    {
                        "name": "L. Cadonati"
                    },
                    {
                        "name": "M. Caesar"
                    },
                    {
                        "name": "G. Cagnoli"
                    },
                    {
                        "name": "C. Cahillane"
                    },
                    {
                        "name": "J. Calderón Bustillo"
                    },
                    {
                        "name": "J. D. Callaghan"
                    },
                    {
                        "name": "T. A. Callister"
                    },
                    {
                        "name": "E. Calloni"
                    },
                    {
                        "name": "J. Cameron"
                    },
                    {
                        "name": "J. B. Camp"
                    },
                    {
                        "name": "M. Canepa"
                    },
                    {
                        "name": "S. Canevarolo"
                    },
                    {
                        "name": "M. Cannavacciuolo"
                    },
                    {
                        "name": "K. C. Cannon"
                    },
                    {
                        "name": "H. Cao"
                    },
                    {
                        "name": "Z. Cao"
                    },
                    {
                        "name": "E. Capocasa"
                    },
                    {
                        "name": "E. Capote"
                    },
                    {
                        "name": "G. Carapella"
                    },
                    {
                        "name": "F. Carbognani"
                    },
                    {
                        "name": "M. Carlassara"
                    },
                    {
                        "name": "J. B. Carlin"
                    },
                    {
                        "name": "M. F. Carney"
                    },
                    {
                        "name": "M. Carpinelli"
                    },
                    {
                        "name": "G. Carrillo"
                    },
                    {
                        "name": "G. Carullo"
                    },
                    {
                        "name": "T. L. Carver"
                    },
                    {
                        "name": "J. Casanueva Diaz"
                    },
                    {
                        "name": "C. Casentini"
                    },
                    {
                        "name": "G. Castaldi"
                    },
                    {
                        "name": "S. Caudill"
                    },
                    {
                        "name": "M. Cavaglià"
                    },
                    {
                        "name": "F. Cavalier"
                    },
                    {
                        "name": "R. Cavalieri"
                    },
                    {
                        "name": "G. Cella"
                    },
                    {
                        "name": "P. Cerdá-Durán"
                    },
                    {
                        "name": "E. Cesarini"
                    },
                    {
                        "name": "W. Chaibi"
                    },
                    {
                        "name": "S. Chalathadka Subrahmanya"
                    },
                    {
                        "name": "E. Champion"
                    },
                    {
                        "name": "C. -H. Chan"
                    },
                    {
                        "name": "C. Chan"
                    },
                    {
                        "name": "C. L. Chan"
                    },
                    {
                        "name": "K. Chan"
                    },
                    {
                        "name": "M. Chan"
                    },
                    {
                        "name": "K. Chandra"
                    },
                    {
                        "name": "I. P. Chang"
                    },
                    {
                        "name": "P. Chanial"
                    },
                    {
                        "name": "S. Chao"
                    },
                    {
                        "name": "C. Chapman-Bird"
                    },
                    {
                        "name": "P. Charlton"
                    },
                    {
                        "name": "E. A. Chase"
                    },
                    {
                        "name": "E. Chassande-Mottin"
                    },
                    {
                        "name": "C. Chatterjee"
                    },
                    {
                        "name": "Debarati Chatterjee"
                    },
                    {
                        "name": "Deep Chatterjee"
                    },
                    {
                        "name": "M. Chaturvedi"
                    },
                    {
                        "name": "S. Chaty"
                    },
                    {
                        "name": "K. Chatziioannou"
                    },
                    {
                        "name": "C. Chen"
                    },
                    {
                        "name": "D. Chen"
                    },
                    {
                        "name": "H. Y. Chen"
                    },
                    {
                        "name": "J. Chen"
                    },
                    {
                        "name": "K. Chen"
                    },
                    {
                        "name": "X. Chen"
                    },
                    {
                        "name": "Y. -B. Chen"
                    },
                    {
                        "name": "Y. -R. Chen"
                    },
                    {
                        "name": "Z. Chen"
                    },
                    {
                        "name": "H. Cheng"
                    },
                    {
                        "name": "C. K. Cheong"
                    },
                    {
                        "name": "H. Y. Cheung"
                    },
                    {
                        "name": "H. Y. Chia"
                    },
                    {
                        "name": "F. Chiadini"
                    },
                    {
                        "name": "C-Y. Chiang"
                    },
                    {
                        "name": "G. Chiarini"
                    },
                    {
                        "name": "R. Chierici"
                    },
                    {
                        "name": "A. Chincarini"
                    },
                    {
                        "name": "M. L. Chiofalo"
                    },
                    {
                        "name": "A. Chiummo"
                    },
                    {
                        "name": "R. K. Choudhary"
                    },
                    {
                        "name": "S. Choudhary"
                    },
                    {
                        "name": "N. Christensen"
                    },
                    {
                        "name": "Q. Chu"
                    },
                    {
                        "name": "Y-K. Chu"
                    },
                    {
                        "name": "S. S. Y. Chua"
                    },
                    {
                        "name": "K. W. Chung"
                    },
                    {
                        "name": "G. Ciani"
                    },
                    {
                        "name": "P. Ciecielag"
                    },
                    {
                        "name": "M. Cieślar"
                    },
                    {
                        "name": "M. Cifaldi"
                    },
                    {
                        "name": "A. A. Ciobanu"
                    },
                    {
                        "name": "R. Ciolfi"
                    },
                    {
                        "name": "F. Cipriano"
                    },
                    {
                        "name": "F. Clara"
                    },
                    {
                        "name": "J. A. Clark"
                    },
                    {
                        "name": "P. Clearwater"
                    },
                    {
                        "name": "S. Clesse"
                    },
                    {
                        "name": "F. Cleva"
                    },
                    {
                        "name": "E. Coccia"
                    },
                    {
                        "name": "E. Codazzo"
                    },
                    {
                        "name": "P. -F. Cohadon"
                    },
                    {
                        "name": "D. E. Cohen"
                    },
                    {
                        "name": "M. Colleoni"
                    },
                    {
                        "name": "C. G. Collette"
                    },
                    {
                        "name": "A. Colombo"
                    },
                    {
                        "name": "M. Colpi"
                    },
                    {
                        "name": "C. M. Compton"
                    },
                    {
                        "name": "M. Constancio"
                    },
                    {
                        "name": "L. Conti"
                    },
                    {
                        "name": "S. J. Cooper"
                    },
                    {
                        "name": "P. Corban"
                    },
                    {
                        "name": "T. R. Corbitt"
                    },
                    {
                        "name": "I. Cordero-Carrión"
                    },
                    {
                        "name": "S. Corezzi"
                    },
                    {
                        "name": "K. R. Corley"
                    },
                    {
                        "name": "N. J. Cornish"
                    },
                    {
                        "name": "D. Corre"
                    },
                    {
                        "name": "A. Corsi"
                    },
                    {
                        "name": "S. Cortese"
                    },
                    {
                        "name": "C. A. Costa"
                    },
                    {
                        "name": "R. Cotesta"
                    },
                    {
                        "name": "R. Cottingham"
                    },
                    {
                        "name": "M. W. Coughlin"
                    },
                    {
                        "name": "J. -P. Coulon"
                    },
                    {
                        "name": "S. T. Countryman"
                    },
                    {
                        "name": "B. Cousins"
                    },
                    {
                        "name": "P. Couvares"
                    },
                    {
                        "name": "D. M. Coward"
                    },
                    {
                        "name": "M. J. Cowart"
                    },
                    {
                        "name": "D. C. Coyne"
                    },
                    {
                        "name": "R. Coyne"
                    },
                    {
                        "name": "J. D. E. Creighton"
                    },
                    {
                        "name": "T. D. Creighton"
                    },
                    {
                        "name": "A. W. Criswell"
                    },
                    {
                        "name": "M. Croquette"
                    },
                    {
                        "name": "S. G. Crowder"
                    },
                    {
                        "name": "J. R. Cudell"
                    },
                    {
                        "name": "T. J. Cullen"
                    },
                    {
                        "name": "A. Cumming"
                    },
                    {
                        "name": "R. Cummings"
                    },
                    {
                        "name": "L. Cunningham"
                    },
                    {
                        "name": "E. Cuoco"
                    },
                    {
                        "name": "M. Curyło"
                    },
                    {
                        "name": "P. Dabadie"
                    },
                    {
                        "name": "T. Dal Canton"
                    },
                    {
                        "name": "S. Dall'Osso"
                    },
                    {
                        "name": "G. Dálya"
                    },
                    {
                        "name": "A. Dana"
                    },
                    {
                        "name": "B. D'Angelo"
                    },
                    {
                        "name": "S. Danilishin"
                    },
                    {
                        "name": "S. D'Antonio"
                    },
                    {
                        "name": "K. Danzmann"
                    },
                    {
                        "name": "C. Darsow-Fromm"
                    },
                    {
                        "name": "A. Dasgupta"
                    },
                    {
                        "name": "L. E. H. Datrier"
                    },
                    {
                        "name": "Sayak Datta"
                    },
                    {
                        "name": "Sayantani Datta"
                    },
                    {
                        "name": "V. Dattilo"
                    },
                    {
                        "name": "I. Dave"
                    },
                    {
                        "name": "M. Davier"
                    },
                    {
                        "name": "D. Davis"
                    },
                    {
                        "name": "M. C. Davis"
                    },
                    {
                        "name": "E. J. Daw"
                    },
                    {
                        "name": "R. Dean"
                    },
                    {
                        "name": "D. DeBra"
                    },
                    {
                        "name": "M. Deenadayalan"
                    },
                    {
                        "name": "J. Degallaix"
                    },
                    {
                        "name": "M. De Laurentis"
                    },
                    {
                        "name": "S. Deléglise"
                    },
                    {
                        "name": "V. Del Favero"
                    },
                    {
                        "name": "F. De Lillo"
                    },
                    {
                        "name": "N. De Lillo"
                    },
                    {
                        "name": "D. Dell'Aquila"
                    },
                    {
                        "name": "W. Del Pozzo"
                    },
                    {
                        "name": "L. M. DeMarchi"
                    },
                    {
                        "name": "F. De Matteis"
                    },
                    {
                        "name": "V. D'Emilio"
                    },
                    {
                        "name": "N. Demos"
                    },
                    {
                        "name": "T. Dent"
                    },
                    {
                        "name": "A. Depasse"
                    },
                    {
                        "name": "R. De Pietri"
                    },
                    {
                        "name": "R. De Rosa"
                    },
                    {
                        "name": "C. De Rossi"
                    },
                    {
                        "name": "R. DeSalvo"
                    },
                    {
                        "name": "R. De Simone"
                    },
                    {
                        "name": "S. Dhurandhar"
                    },
                    {
                        "name": "M. C. Díaz"
                    },
                    {
                        "name": "N. A. Didio"
                    },
                    {
                        "name": "T. Dietrich"
                    },
                    {
                        "name": "L. Di Fiore"
                    },
                    {
                        "name": "C. Di Fronzo"
                    },
                    {
                        "name": "C. Di Giorgio"
                    },
                    {
                        "name": "F. Di Giovanni"
                    },
                    {
                        "name": "M. Di Giovanni"
                    },
                    {
                        "name": "T. Di Girolamo"
                    },
                    {
                        "name": "A. Di Lieto"
                    },
                    {
                        "name": "A. Di Michele"
                    },
                    {
                        "name": "B. Ding"
                    },
                    {
                        "name": "S. Di Pace"
                    },
                    {
                        "name": "I. Di Palma"
                    },
                    {
                        "name": "F. Di Renzo"
                    },
                    {
                        "name": "A. K. Divakarla"
                    },
                    {
                        "name": "Divyajyoti"
                    },
                    {
                        "name": "A. Dmitriev"
                    },
                    {
                        "name": "Z. Doctor"
                    },
                    {
                        "name": "L. Donahue"
                    },
                    {
                        "name": "L. D'Onofrio"
                    },
                    {
                        "name": "F. Donovan"
                    },
                    {
                        "name": "K. L. Dooley"
                    },
                    {
                        "name": "S. Doravari"
                    },
                    {
                        "name": "M. Drago"
                    },
                    {
                        "name": "J. C. Driggers"
                    },
                    {
                        "name": "Y. Drori"
                    },
                    {
                        "name": "J. -G. Ducoin"
                    },
                    {
                        "name": "P. Dupej"
                    },
                    {
                        "name": "U. Dupletsa"
                    },
                    {
                        "name": "O. Durante"
                    },
                    {
                        "name": "D. D'Urso"
                    },
                    {
                        "name": "P. -A. Duverne"
                    },
                    {
                        "name": "S. E. Dwyer"
                    },
                    {
                        "name": "C. Eassa"
                    },
                    {
                        "name": "P. J. Easter"
                    },
                    {
                        "name": "M. Ebersold"
                    },
                    {
                        "name": "T. Eckhardt"
                    },
                    {
                        "name": "G. Eddolls"
                    },
                    {
                        "name": "B. Edelman"
                    },
                    {
                        "name": "T. B. Edo"
                    },
                    {
                        "name": "O. Edy"
                    },
                    {
                        "name": "A. Effler"
                    },
                    {
                        "name": "S. Eguchi"
                    },
                    {
                        "name": "J. Eichholz"
                    },
                    {
                        "name": "S. S. Eikenberry"
                    },
                    {
                        "name": "M. Eisenmann"
                    },
                    {
                        "name": "R. A. Eisenstein"
                    },
                    {
                        "name": "A. Ejlli"
                    },
                    {
                        "name": "E. Engelby"
                    },
                    {
                        "name": "Y. Enomoto"
                    },
                    {
                        "name": "L. Errico"
                    },
                    {
                        "name": "R. C. Essick"
                    },
                    {
                        "name": "H. Estellés"
                    },
                    {
                        "name": "D. Estevez"
                    },
                    {
                        "name": "Z. Etienne"
                    },
                    {
                        "name": "T. Etzel"
                    },
                    {
                        "name": "M. Evans"
                    },
                    {
                        "name": "T. M. Evans"
                    },
                    {
                        "name": "T. Evstafyeva"
                    },
                    {
                        "name": "B. E. Ewing"
                    },
                    {
                        "name": "F. Fabrizi"
                    },
                    {
                        "name": "F. Faedi"
                    },
                    {
                        "name": "V. Fafone"
                    },
                    {
                        "name": "H. Fair"
                    },
                    {
                        "name": "S. Fairhurst"
                    },
                    {
                        "name": "P. C. Fan"
                    },
                    {
                        "name": "A. M. Farah"
                    },
                    {
                        "name": "S. Farinon"
                    },
                    {
                        "name": "B. Farr"
                    },
                    {
                        "name": "W. M. Farr"
                    },
                    {
                        "name": "E. J. Fauchon-Jones"
                    },
                    {
                        "name": "G. Favaro"
                    },
                    {
                        "name": "M. Favata"
                    },
                    {
                        "name": "M. Fays"
                    },
                    {
                        "name": "M. Fazio"
                    },
                    {
                        "name": "J. Feicht"
                    },
                    {
                        "name": "M. M. Fejer"
                    },
                    {
                        "name": "E. Fenyvesi"
                    },
                    {
                        "name": "D. L. Ferguson"
                    },
                    {
                        "name": "A. Fernandez-Galiana"
                    },
                    {
                        "name": "I. Ferrante"
                    },
                    {
                        "name": "T. A. Ferreira"
                    },
                    {
                        "name": "F. Fidecaro"
                    },
                    {
                        "name": "P. Figura"
                    },
                    {
                        "name": "A. Fiori"
                    },
                    {
                        "name": "I. Fiori"
                    },
                    {
                        "name": "M. Fishbach"
                    },
                    {
                        "name": "R. P. Fisher"
                    },
                    {
                        "name": "R. Fittipaldi"
                    },
                    {
                        "name": "V. Fiumara"
                    },
                    {
                        "name": "R. Flaminio"
                    },
                    {
                        "name": "E. Floden"
                    },
                    {
                        "name": "H. K. Fong"
                    },
                    {
                        "name": "J. A. Font"
                    },
                    {
                        "name": "B. Fornal"
                    },
                    {
                        "name": "P. W. F. Forsyth"
                    },
                    {
                        "name": "A. Franke"
                    },
                    {
                        "name": "S. Frasca"
                    },
                    {
                        "name": "F. Frasconi"
                    },
                    {
                        "name": "J. P. Freed"
                    },
                    {
                        "name": "Z. Frei"
                    },
                    {
                        "name": "A. Freise"
                    },
                    {
                        "name": "O. Freitas"
                    },
                    {
                        "name": "R. Frey"
                    },
                    {
                        "name": "P. Fritschel"
                    },
                    {
                        "name": "V. V. Frolov"
                    },
                    {
                        "name": "G. G. Fronzé"
                    },
                    {
                        "name": "Y. Fujii"
                    },
                    {
                        "name": "Y. Fujikawa"
                    },
                    {
                        "name": "Y. Fujimoto"
                    },
                    {
                        "name": "P. Fulda"
                    },
                    {
                        "name": "M. Fyffe"
                    },
                    {
                        "name": "H. A. Gabbard"
                    },
                    {
                        "name": "W. E. Gabella"
                    },
                    {
                        "name": "B. U. Gadre"
                    },
                    {
                        "name": "J. R. Gair"
                    },
                    {
                        "name": "J. Gais"
                    },
                    {
                        "name": "S. Galaudage"
                    },
                    {
                        "name": "R. Gamba"
                    },
                    {
                        "name": "D. Ganapathy"
                    },
                    {
                        "name": "A. Ganguly"
                    },
                    {
                        "name": "D. Gao"
                    },
                    {
                        "name": "S. G. Gaonkar"
                    },
                    {
                        "name": "B. Garaventa"
                    },
                    {
                        "name": "C. García Núñez"
                    },
                    {
                        "name": "C. García-Quirós"
                    },
                    {
                        "name": "F. Garufi"
                    },
                    {
                        "name": "B. Gateley"
                    },
                    {
                        "name": "V. Gayathri"
                    },
                    {
                        "name": "G. -G. Ge"
                    },
                    {
                        "name": "G. Gemme"
                    },
                    {
                        "name": "A. Gennai"
                    },
                    {
                        "name": "V. Gennari"
                    },
                    {
                        "name": "J. George"
                    },
                    {
                        "name": "O. Gerberding"
                    },
                    {
                        "name": "L. Gergely"
                    },
                    {
                        "name": "P. Gewecke"
                    },
                    {
                        "name": "S. Ghonge"
                    },
                    {
                        "name": "Abhirup Ghosh"
                    },
                    {
                        "name": "Archisman Ghosh"
                    },
                    {
                        "name": "Shaon Ghosh"
                    },
                    {
                        "name": "Shrobana Ghosh"
                    },
                    {
                        "name": "Tathagata Ghosh"
                    },
                    {
                        "name": "B. Giacomazzo"
                    },
                    {
                        "name": "L. Giacoppo"
                    },
                    {
                        "name": "J. A. Giaime"
                    },
                    {
                        "name": "K. D. Giardina"
                    },
                    {
                        "name": "D. R. Gibson"
                    },
                    {
                        "name": "C. Gier"
                    },
                    {
                        "name": "M. Giesler"
                    },
                    {
                        "name": "P. Giri"
                    },
                    {
                        "name": "F. Gissi"
                    },
                    {
                        "name": "S. Gkaitatzis"
                    },
                    {
                        "name": "J. Glanzer"
                    },
                    {
                        "name": "A. E. Gleckl"
                    },
                    {
                        "name": "P. Godwin"
                    },
                    {
                        "name": "E. Goetz"
                    },
                    {
                        "name": "R. Goetz"
                    },
                    {
                        "name": "N. Gohlke"
                    },
                    {
                        "name": "J. Golomb"
                    },
                    {
                        "name": "B. Goncharov"
                    },
                    {
                        "name": "G. González"
                    },
                    {
                        "name": "M. Gosselin"
                    },
                    {
                        "name": "R. Gouaty"
                    },
                    {
                        "name": "D. W. Gould"
                    },
                    {
                        "name": "S. Goyal"
                    },
                    {
                        "name": "B. Grace"
                    },
                    {
                        "name": "A. Grado"
                    },
                    {
                        "name": "V. Graham"
                    },
                    {
                        "name": "M. Granata"
                    },
                    {
                        "name": "V. Granata"
                    },
                    {
                        "name": "A. Grant"
                    },
                    {
                        "name": "S. Gras"
                    },
                    {
                        "name": "P. Grassia"
                    },
                    {
                        "name": "C. Gray"
                    },
                    {
                        "name": "R. Gray"
                    },
                    {
                        "name": "G. Greco"
                    },
                    {
                        "name": "A. C. Green"
                    },
                    {
                        "name": "R. Green"
                    },
                    {
                        "name": "A. M. Gretarsson"
                    },
                    {
                        "name": "E. M. Gretarsson"
                    },
                    {
                        "name": "D. Griffith"
                    },
                    {
                        "name": "W. L. Griffiths"
                    },
                    {
                        "name": "H. L. Griggs"
                    },
                    {
                        "name": "G. Grignani"
                    },
                    {
                        "name": "A. Grimaldi"
                    },
                    {
                        "name": "E. Grimes"
                    },
                    {
                        "name": "S. J. Grimm"
                    },
                    {
                        "name": "H. Grote"
                    },
                    {
                        "name": "S. Grunewald"
                    },
                    {
                        "name": "P. Gruning"
                    },
                    {
                        "name": "A. S. Gruson"
                    },
                    {
                        "name": "D. Guerra"
                    },
                    {
                        "name": "G. M. Guidi"
                    },
                    {
                        "name": "A. R. Guimaraes"
                    },
                    {
                        "name": "G. Guixé"
                    },
                    {
                        "name": "H. K. Gulati"
                    },
                    {
                        "name": "A. M. Gunny"
                    },
                    {
                        "name": "H. -K. Guo"
                    },
                    {
                        "name": "Y. Guo"
                    },
                    {
                        "name": "Anchal Gupta"
                    },
                    {
                        "name": "Anuradha Gupta"
                    },
                    {
                        "name": "I. M. Gupta"
                    },
                    {
                        "name": "P. Gupta"
                    },
                    {
                        "name": "S. K. Gupta"
                    },
                    {
                        "name": "R. Gustafson"
                    },
                    {
                        "name": "F. Guzman"
                    },
                    {
                        "name": "S. Ha"
                    },
                    {
                        "name": "I. P. W. Hadiputrawan"
                    },
                    {
                        "name": "L. Haegel"
                    },
                    {
                        "name": "S. Haino"
                    },
                    {
                        "name": "O. Halim"
                    },
                    {
                        "name": "E. D. Hall"
                    },
                    {
                        "name": "E. Z. Hamilton"
                    },
                    {
                        "name": "G. Hammond"
                    },
                    {
                        "name": "W. -B. Han"
                    },
                    {
                        "name": "M. Haney"
                    },
                    {
                        "name": "J. Hanks"
                    },
                    {
                        "name": "C. Hanna"
                    },
                    {
                        "name": "M. D. Hannam"
                    },
                    {
                        "name": "O. Hannuksela"
                    },
                    {
                        "name": "H. Hansen"
                    },
                    {
                        "name": "T. J. Hansen"
                    },
                    {
                        "name": "J. Hanson"
                    },
                    {
                        "name": "T. Harder"
                    },
                    {
                        "name": "K. Haris"
                    },
                    {
                        "name": "J. Harms"
                    },
                    {
                        "name": "G. M. Harry"
                    },
                    {
                        "name": "I. W. Harry"
                    },
                    {
                        "name": "D. Hartwig"
                    },
                    {
                        "name": "K. Hasegawa"
                    },
                    {
                        "name": "B. Haskell"
                    },
                    {
                        "name": "C. -J. Haster"
                    },
                    {
                        "name": "J. S. Hathaway"
                    },
                    {
                        "name": "K. Hattori"
                    },
                    {
                        "name": "K. Haughian"
                    },
                    {
                        "name": "H. Hayakawa"
                    },
                    {
                        "name": "K. Hayama"
                    },
                    {
                        "name": "F. J. Hayes"
                    },
                    {
                        "name": "J. Healy"
                    },
                    {
                        "name": "A. Heidmann"
                    },
                    {
                        "name": "A. Heidt"
                    },
                    {
                        "name": "M. C. Heintze"
                    },
                    {
                        "name": "J. Heinze"
                    },
                    {
                        "name": "J. Heinzel"
                    },
                    {
                        "name": "H. Heitmann"
                    },
                    {
                        "name": "F. Hellman"
                    },
                    {
                        "name": "P. Hello"
                    },
                    {
                        "name": "A. F. Helmling-Cornell"
                    },
                    {
                        "name": "G. Hemming"
                    },
                    {
                        "name": "M. Hendry"
                    },
                    {
                        "name": "I. S. Heng"
                    },
                    {
                        "name": "E. Hennes"
                    },
                    {
                        "name": "J. Hennig"
                    },
                    {
                        "name": "M. H. Hennig"
                    },
                    {
                        "name": "C. Henshaw"
                    },
                    {
                        "name": "A. G. Hernandez"
                    },
                    {
                        "name": "F. Hernandez Vivanco"
                    },
                    {
                        "name": "M. Heurs"
                    },
                    {
                        "name": "A. L. Hewitt"
                    },
                    {
                        "name": "S. Higginbotham"
                    },
                    {
                        "name": "S. Hild"
                    },
                    {
                        "name": "P. Hill"
                    },
                    {
                        "name": "Y. Himemoto"
                    },
                    {
                        "name": "A. S. Hines"
                    },
                    {
                        "name": "N. Hirata"
                    },
                    {
                        "name": "C. Hirose"
                    },
                    {
                        "name": "T-C. Ho"
                    },
                    {
                        "name": "S. Hochheim"
                    },
                    {
                        "name": "D. Hofman"
                    },
                    {
                        "name": "J. N. Hohmann"
                    },
                    {
                        "name": "D. G. Holcomb"
                    },
                    {
                        "name": "N. A. Holland"
                    },
                    {
                        "name": "I. J. Hollows"
                    },
                    {
                        "name": "Z. J. Holmes"
                    },
                    {
                        "name": "K. Holt"
                    },
                    {
                        "name": "D. E. Holz"
                    },
                    {
                        "name": "Q. Hong"
                    },
                    {
                        "name": "J. Hough"
                    },
                    {
                        "name": "S. Hourihane"
                    },
                    {
                        "name": "E. J. Howell"
                    },
                    {
                        "name": "C. G. Hoy"
                    },
                    {
                        "name": "D. Hoyland"
                    },
                    {
                        "name": "A. Hreibi"
                    },
                    {
                        "name": "B-H. Hsieh"
                    },
                    {
                        "name": "H-F. Hsieh"
                    },
                    {
                        "name": "C. Hsiung"
                    },
                    {
                        "name": "Y. Hsu"
                    },
                    {
                        "name": "H-Y. Huang"
                    },
                    {
                        "name": "P. Huang"
                    },
                    {
                        "name": "Y-C. Huang"
                    },
                    {
                        "name": "Y. -J. Huang"
                    },
                    {
                        "name": "Yiting Huang"
                    },
                    {
                        "name": "Yiwen Huang"
                    },
                    {
                        "name": "M. T. Hübner"
                    },
                    {
                        "name": "A. D. Huddart"
                    },
                    {
                        "name": "B. Hughey"
                    },
                    {
                        "name": "D. C. Y. Hui"
                    },
                    {
                        "name": "V. Hui"
                    },
                    {
                        "name": "S. Husa"
                    },
                    {
                        "name": "S. H. Huttner"
                    },
                    {
                        "name": "R. Huxford"
                    },
                    {
                        "name": "T. Huynh-Dinh"
                    },
                    {
                        "name": "S. Ide"
                    },
                    {
                        "name": "B. Idzkowski"
                    },
                    {
                        "name": "A. Iess"
                    },
                    {
                        "name": "K. Inayoshi"
                    },
                    {
                        "name": "Y. Inoue"
                    },
                    {
                        "name": "P. Iosif"
                    },
                    {
                        "name": "M. Isi"
                    },
                    {
                        "name": "K. Isleif"
                    },
                    {
                        "name": "K. Ito"
                    },
                    {
                        "name": "Y. Itoh"
                    },
                    {
                        "name": "B. R. Iyer"
                    },
                    {
                        "name": "V. JaberianHamedan"
                    },
                    {
                        "name": "T. Jacqmin"
                    },
                    {
                        "name": "P. -E. Jacquet"
                    },
                    {
                        "name": "S. J. Jadhav"
                    },
                    {
                        "name": "S. P. Jadhav"
                    },
                    {
                        "name": "T. Jain"
                    },
                    {
                        "name": "A. L. James"
                    },
                    {
                        "name": "A. Z. Jan"
                    },
                    {
                        "name": "K. Jani"
                    },
                    {
                        "name": "J. Janquart"
                    },
                    {
                        "name": "K. Janssens"
                    },
                    {
                        "name": "N. N. Janthalur"
                    },
                    {
                        "name": "P. Jaranowski"
                    },
                    {
                        "name": "D. Jariwala"
                    },
                    {
                        "name": "R. Jaume"
                    },
                    {
                        "name": "A. C. Jenkins"
                    },
                    {
                        "name": "K. Jenner"
                    },
                    {
                        "name": "C. Jeon"
                    },
                    {
                        "name": "W. Jia"
                    },
                    {
                        "name": "J. Jiang"
                    },
                    {
                        "name": "H. -B. Jin"
                    },
                    {
                        "name": "G. R. Johns"
                    },
                    {
                        "name": "N. K. Johnson-McDaniel"
                    },
                    {
                        "name": "R. Johnston"
                    },
                    {
                        "name": "A. W. Jones"
                    },
                    {
                        "name": "D. I. Jones"
                    },
                    {
                        "name": "P. Jones"
                    },
                    {
                        "name": "R. Jones"
                    },
                    {
                        "name": "P. Joshi"
                    },
                    {
                        "name": "L. Ju"
                    },
                    {
                        "name": "A. Jue"
                    },
                    {
                        "name": "P. Jung"
                    },
                    {
                        "name": "K. Jung"
                    },
                    {
                        "name": "J. Junker"
                    },
                    {
                        "name": "V. Juste"
                    },
                    {
                        "name": "K. Kaihotsu"
                    },
                    {
                        "name": "T. Kajita"
                    },
                    {
                        "name": "M. Kakizaki"
                    },
                    {
                        "name": "C. V. Kalaghatgi"
                    },
                    {
                        "name": "V. Kalogera"
                    },
                    {
                        "name": "B. Kamai"
                    },
                    {
                        "name": "M. Kamiizumi"
                    },
                    {
                        "name": "N. Kanda"
                    },
                    {
                        "name": "S. Kandhasamy"
                    },
                    {
                        "name": "G. Kang"
                    },
                    {
                        "name": "J. B. Kanner"
                    },
                    {
                        "name": "Y. Kao"
                    },
                    {
                        "name": "S. J. Kapadia"
                    },
                    {
                        "name": "D. P. Kapasi"
                    },
                    {
                        "name": "C. Karathanasis"
                    },
                    {
                        "name": "S. Karki"
                    },
                    {
                        "name": "R. Kashyap"
                    },
                    {
                        "name": "M. Kasprzack"
                    },
                    {
                        "name": "W. Kastaun"
                    },
                    {
                        "name": "T. Kato"
                    },
                    {
                        "name": "S. Katsanevas"
                    },
                    {
                        "name": "E. Katsavounidis"
                    },
                    {
                        "name": "W. Katzman"
                    },
                    {
                        "name": "T. Kaur"
                    },
                    {
                        "name": "K. Kawabe"
                    },
                    {
                        "name": "K. Kawaguchi"
                    },
                    {
                        "name": "F. Kéfélian"
                    },
                    {
                        "name": "D. Keitel"
                    },
                    {
                        "name": "J. S. Key"
                    },
                    {
                        "name": "S. Khadka"
                    },
                    {
                        "name": "F. Y. Khalili"
                    },
                    {
                        "name": "S. Khan"
                    },
                    {
                        "name": "T. Khanam"
                    },
                    {
                        "name": "E. A. Khazanov"
                    },
                    {
                        "name": "N. Khetan"
                    },
                    {
                        "name": "M. Khursheed"
                    },
                    {
                        "name": "N. Kijbunchoo"
                    },
                    {
                        "name": "A. Kim"
                    },
                    {
                        "name": "C. Kim"
                    },
                    {
                        "name": "J. C. Kim"
                    },
                    {
                        "name": "J. Kim"
                    },
                    {
                        "name": "K. Kim"
                    },
                    {
                        "name": "W. S. Kim"
                    },
                    {
                        "name": "Y. -M. Kim"
                    },
                    {
                        "name": "C. Kimball"
                    },
                    {
                        "name": "N. Kimura"
                    },
                    {
                        "name": "M. Kinley-Hanlon"
                    },
                    {
                        "name": "R. Kirchhoff"
                    },
                    {
                        "name": "J. S. Kissel"
                    },
                    {
                        "name": "S. Klimenko"
                    },
                    {
                        "name": "T. Klinger"
                    },
                    {
                        "name": "A. M. Knee"
                    },
                    {
                        "name": "T. D. Knowles"
                    },
                    {
                        "name": "N. Knust"
                    },
                    {
                        "name": "E. Knyazev"
                    },
                    {
                        "name": "Y. Kobayashi"
                    },
                    {
                        "name": "P. Koch"
                    },
                    {
                        "name": "G. Koekoek"
                    },
                    {
                        "name": "K. Kohri"
                    },
                    {
                        "name": "K. Kokeyama"
                    },
                    {
                        "name": "S. Koley"
                    },
                    {
                        "name": "P. Kolitsidou"
                    },
                    {
                        "name": "M. Kolstein"
                    },
                    {
                        "name": "K. Komori"
                    },
                    {
                        "name": "V. Kondrashov"
                    },
                    {
                        "name": "A. K. H. Kong"
                    },
                    {
                        "name": "A. Kontos"
                    },
                    {
                        "name": "N. Koper"
                    },
                    {
                        "name": "M. Korobko"
                    },
                    {
                        "name": "M. Kovalam"
                    },
                    {
                        "name": "N. Koyama"
                    },
                    {
                        "name": "D. B. Kozak"
                    },
                    {
                        "name": "C. Kozakai"
                    },
                    {
                        "name": "V. Kringel"
                    },
                    {
                        "name": "N. V. Krishnendu"
                    },
                    {
                        "name": "A. Królak"
                    },
                    {
                        "name": "G. Kuehn"
                    },
                    {
                        "name": "F. Kuei"
                    },
                    {
                        "name": "P. Kuijer"
                    },
                    {
                        "name": "S. Kulkarni"
                    },
                    {
                        "name": "A. Kumar"
                    },
                    {
                        "name": "Prayush Kumar"
                    },
                    {
                        "name": "Rahul Kumar"
                    },
                    {
                        "name": "Rakesh Kumar"
                    },
                    {
                        "name": "J. Kume"
                    },
                    {
                        "name": "K. Kuns"
                    },
                    {
                        "name": "Y. Kuromiya"
                    },
                    {
                        "name": "S. Kuroyanagi"
                    },
                    {
                        "name": "K. Kwak"
                    },
                    {
                        "name": "G. Lacaille"
                    },
                    {
                        "name": "P. Lagabbe"
                    },
                    {
                        "name": "D. Laghi"
                    },
                    {
                        "name": "E. Lalande"
                    },
                    {
                        "name": "M. Lalleman"
                    },
                    {
                        "name": "T. L. Lam"
                    },
                    {
                        "name": "A. Lamberts"
                    },
                    {
                        "name": "M. Landry"
                    },
                    {
                        "name": "B. B. Lane"
                    },
                    {
                        "name": "R. N. Lang"
                    },
                    {
                        "name": "J. Lange"
                    },
                    {
                        "name": "B. Lantz"
                    },
                    {
                        "name": "I. La Rosa"
                    },
                    {
                        "name": "A. Lartaux-Vollard"
                    },
                    {
                        "name": "P. D. Lasky"
                    },
                    {
                        "name": "M. Laxen"
                    },
                    {
                        "name": "A. Lazzarini"
                    },
                    {
                        "name": "C. Lazzaro"
                    },
                    {
                        "name": "P. Leaci"
                    },
                    {
                        "name": "S. Leavey"
                    },
                    {
                        "name": "S. LeBohec"
                    },
                    {
                        "name": "Y. K. Lecoeuche"
                    },
                    {
                        "name": "E. Lee"
                    },
                    {
                        "name": "H. M. Lee"
                    },
                    {
                        "name": "H. W. Lee"
                    },
                    {
                        "name": "K. Lee"
                    },
                    {
                        "name": "R. Lee"
                    },
                    {
                        "name": "I. N. Legred"
                    },
                    {
                        "name": "J. Lehmann"
                    },
                    {
                        "name": "A. Lemaître"
                    },
                    {
                        "name": "M. Lenti"
                    },
                    {
                        "name": "M. Leonardi"
                    },
                    {
                        "name": "E. Leonova"
                    },
                    {
                        "name": "N. Leroy"
                    },
                    {
                        "name": "N. Letendre"
                    },
                    {
                        "name": "C. Levesque"
                    },
                    {
                        "name": "Y. Levin"
                    },
                    {
                        "name": "J. N. Leviton"
                    },
                    {
                        "name": "K. Leyde"
                    },
                    {
                        "name": "A. K. Y. Li"
                    },
                    {
                        "name": "B. Li"
                    },
                    {
                        "name": "J. Li"
                    },
                    {
                        "name": "K. L. Li"
                    },
                    {
                        "name": "P. Li"
                    },
                    {
                        "name": "T. G. F. Li"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "C-Y. Lin"
                    },
                    {
                        "name": "E. T. Lin"
                    },
                    {
                        "name": "F-K. Lin"
                    },
                    {
                        "name": "F-L. Lin"
                    },
                    {
                        "name": "H. L. Lin"
                    },
                    {
                        "name": "L. C. -C. Lin"
                    },
                    {
                        "name": "F. Linde"
                    },
                    {
                        "name": "S. D. Linker"
                    },
                    {
                        "name": "J. N. Linley"
                    },
                    {
                        "name": "T. B. Littenberg"
                    },
                    {
                        "name": "G. C. Liu"
                    },
                    {
                        "name": "J. Liu"
                    },
                    {
                        "name": "K. Liu"
                    },
                    {
                        "name": "X. Liu"
                    },
                    {
                        "name": "F. Llamas"
                    },
                    {
                        "name": "R. K. L. Lo"
                    },
                    {
                        "name": "T. Lo"
                    },
                    {
                        "name": "L. T. London"
                    },
                    {
                        "name": "A. Longo"
                    },
                    {
                        "name": "D. Lopez"
                    },
                    {
                        "name": "M. Lopez Portilla"
                    },
                    {
                        "name": "M. Lorenzini"
                    },
                    {
                        "name": "V. Loriette"
                    },
                    {
                        "name": "M. Lormand"
                    },
                    {
                        "name": "G. Losurdo"
                    },
                    {
                        "name": "T. P. Lott"
                    },
                    {
                        "name": "J. D. Lough"
                    },
                    {
                        "name": "C. O. Lousto"
                    },
                    {
                        "name": "G. Lovelace"
                    },
                    {
                        "name": "J. F. Lucaccioni"
                    },
                    {
                        "name": "H. Lück"
                    },
                    {
                        "name": "D. Lumaca"
                    },
                    {
                        "name": "A. P. Lundgren"
                    },
                    {
                        "name": "L. -W. Luo"
                    },
                    {
                        "name": "J. E. Lynam"
                    },
                    {
                        "name": "M. Ma'arif"
                    },
                    {
                        "name": "R. Macas"
                    },
                    {
                        "name": "J. B. Machtinger"
                    },
                    {
                        "name": "M. MacInnis"
                    },
                    {
                        "name": "D. M. Macleod"
                    },
                    {
                        "name": "I. A. O. MacMillan"
                    },
                    {
                        "name": "A. Macquet"
                    },
                    {
                        "name": "I. Magaña Hernandez"
                    },
                    {
                        "name": "C. Magazzù"
                    },
                    {
                        "name": "R. M. Magee"
                    },
                    {
                        "name": "E. Maggio"
                    },
                    {
                        "name": "R. Maggiore"
                    },
                    {
                        "name": "M. Magnozzi"
                    },
                    {
                        "name": "S. Mahesh"
                    },
                    {
                        "name": "E. Majorana"
                    },
                    {
                        "name": "I. Maksimovic"
                    },
                    {
                        "name": "S. Maliakal"
                    },
                    {
                        "name": "A. Malik"
                    },
                    {
                        "name": "N. Man"
                    },
                    {
                        "name": "V. Mandic"
                    },
                    {
                        "name": "V. Mangano"
                    },
                    {
                        "name": "G. L. Mansell"
                    },
                    {
                        "name": "M. Manske"
                    },
                    {
                        "name": "M. Mantovani"
                    },
                    {
                        "name": "M. Mapelli"
                    },
                    {
                        "name": "F. Marchesoni"
                    },
                    {
                        "name": "D. Marín Pina"
                    },
                    {
                        "name": "F. Marion"
                    },
                    {
                        "name": "Z. Mark"
                    },
                    {
                        "name": "S. Márka"
                    },
                    {
                        "name": "Z. Márka"
                    },
                    {
                        "name": "C. Markakis"
                    },
                    {
                        "name": "A. S. Markosyan"
                    },
                    {
                        "name": "A. Markowitz"
                    },
                    {
                        "name": "E. Maros"
                    },
                    {
                        "name": "A. Marquina"
                    },
                    {
                        "name": "S. Marsat"
                    },
                    {
                        "name": "F. Martelli"
                    },
                    {
                        "name": "I. W. Martin"
                    },
                    {
                        "name": "R. M. Martin"
                    },
                    {
                        "name": "M. Martinez"
                    },
                    {
                        "name": "V. A. Martinez"
                    },
                    {
                        "name": "V. Martinez"
                    },
                    {
                        "name": "K. Martinovic"
                    },
                    {
                        "name": "D. V. Martynov"
                    },
                    {
                        "name": "E. J. Marx"
                    },
                    {
                        "name": "H. Masalehdan"
                    },
                    {
                        "name": "K. Mason"
                    },
                    {
                        "name": "E. Massera"
                    },
                    {
                        "name": "A. Masserot"
                    },
                    {
                        "name": "M. Masso-Reid"
                    },
                    {
                        "name": "S. Mastrogiovanni"
                    },
                    {
                        "name": "A. Matas"
                    },
                    {
                        "name": "M. Mateu-Lucena"
                    },
                    {
                        "name": "F. Matichard"
                    },
                    {
                        "name": "M. Matiushechkina"
                    },
                    {
                        "name": "N. Mavalvala"
                    },
                    {
                        "name": "J. J. McCann"
                    },
                    {
                        "name": "R. McCarthy"
                    },
                    {
                        "name": "D. E. McClelland"
                    },
                    {
                        "name": "P. K. McClincy"
                    },
                    {
                        "name": "S. McCormick"
                    },
                    {
                        "name": "L. McCuller"
                    },
                    {
                        "name": "G. I. McGhee"
                    },
                    {
                        "name": "S. C. McGuire"
                    },
                    {
                        "name": "C. McIsaac"
                    },
                    {
                        "name": "J. McIver"
                    },
                    {
                        "name": "T. McRae"
                    },
                    {
                        "name": "S. T. McWilliams"
                    },
                    {
                        "name": "D. Meacher"
                    },
                    {
                        "name": "M. Mehmet"
                    },
                    {
                        "name": "A. K. Mehta"
                    },
                    {
                        "name": "Q. Meijer"
                    },
                    {
                        "name": "A. Melatos"
                    },
                    {
                        "name": "D. A. Melchor"
                    },
                    {
                        "name": "G. Mendell"
                    },
                    {
                        "name": "A. Menendez-Vazquez"
                    },
                    {
                        "name": "C. S. Menoni"
                    },
                    {
                        "name": "R. A. Mercer"
                    },
                    {
                        "name": "L. Mereni"
                    },
                    {
                        "name": "K. Merfeld"
                    },
                    {
                        "name": "E. L. Merilh"
                    },
                    {
                        "name": "J. D. Merritt"
                    },
                    {
                        "name": "M. Merzougui"
                    },
                    {
                        "name": "S. Meshkov"
                    },
                    {
                        "name": "C. Messenger"
                    },
                    {
                        "name": "C. Messick"
                    },
                    {
                        "name": "P. M. Meyers"
                    },
                    {
                        "name": "F. Meylahn"
                    },
                    {
                        "name": "A. Mhaske"
                    },
                    {
                        "name": "A. Miani"
                    },
                    {
                        "name": "H. Miao"
                    },
                    {
                        "name": "I. Michaloliakos"
                    },
                    {
                        "name": "C. Michel"
                    },
                    {
                        "name": "Y. Michimura"
                    },
                    {
                        "name": "H. Middleton"
                    },
                    {
                        "name": "D. P. Mihaylov"
                    },
                    {
                        "name": "L. Milano"
                    },
                    {
                        "name": "A. L. Miller"
                    },
                    {
                        "name": "A. Miller"
                    },
                    {
                        "name": "B. Miller"
                    },
                    {
                        "name": "M. Millhouse"
                    },
                    {
                        "name": "J. C. Mills"
                    },
                    {
                        "name": "E. Milotti"
                    },
                    {
                        "name": "Y. Minenkov"
                    },
                    {
                        "name": "N. Mio"
                    },
                    {
                        "name": "Ll. M. Mir"
                    },
                    {
                        "name": "M. Miravet-Tenés"
                    },
                    {
                        "name": "A. Mishkin"
                    },
                    {
                        "name": "C. Mishra"
                    },
                    {
                        "name": "T. Mishra"
                    },
                    {
                        "name": "T. Mistry"
                    },
                    {
                        "name": "S. Mitra"
                    },
                    {
                        "name": "V. P. Mitrofanov"
                    },
                    {
                        "name": "G. Mitselmakher"
                    },
                    {
                        "name": "R. Mittleman"
                    },
                    {
                        "name": "O. Miyakawa"
                    },
                    {
                        "name": "K. Miyo"
                    },
                    {
                        "name": "S. Miyoki"
                    },
                    {
                        "name": "Geoffrey Mo"
                    },
                    {
                        "name": "L. M. Modafferi"
                    },
                    {
                        "name": "E. Moguel"
                    },
                    {
                        "name": "K. Mogushi"
                    },
                    {
                        "name": "S. R. P. Mohapatra"
                    },
                    {
                        "name": "S. R. Mohite"
                    },
                    {
                        "name": "I. Molina"
                    },
                    {
                        "name": "M. Molina-Ruiz"
                    },
                    {
                        "name": "M. Mondin"
                    },
                    {
                        "name": "M. Montani"
                    },
                    {
                        "name": "C. J. Moore"
                    },
                    {
                        "name": "J. Moragues"
                    },
                    {
                        "name": "D. Moraru"
                    },
                    {
                        "name": "F. Morawski"
                    },
                    {
                        "name": "A. More"
                    },
                    {
                        "name": "C. Moreno"
                    },
                    {
                        "name": "G. Moreno"
                    },
                    {
                        "name": "Y. Mori"
                    },
                    {
                        "name": "S. Morisaki"
                    },
                    {
                        "name": "N. Morisue"
                    },
                    {
                        "name": "Y. Moriwaki"
                    },
                    {
                        "name": "B. Mours"
                    },
                    {
                        "name": "C. M. Mow-Lowry"
                    },
                    {
                        "name": "S. Mozzon"
                    },
                    {
                        "name": "F. Muciaccia"
                    },
                    {
                        "name": "Arunava Mukherjee"
                    },
                    {
                        "name": "D. Mukherjee"
                    },
                    {
                        "name": "Soma Mukherjee"
                    },
                    {
                        "name": "Subroto Mukherjee"
                    },
                    {
                        "name": "Suvodip Mukherjee"
                    },
                    {
                        "name": "N. Mukund"
                    },
                    {
                        "name": "A. Mullavey"
                    },
                    {
                        "name": "J. Munch"
                    },
                    {
                        "name": "E. A. Muñiz"
                    },
                    {
                        "name": "P. G. Murray"
                    },
                    {
                        "name": "R. Musenich"
                    },
                    {
                        "name": "S. Muusse"
                    },
                    {
                        "name": "S. L. Nadji"
                    },
                    {
                        "name": "K. Nagano"
                    },
                    {
                        "name": "A. Nagar"
                    },
                    {
                        "name": "K. Nakamura"
                    },
                    {
                        "name": "H. Nakano"
                    },
                    {
                        "name": "M. Nakano"
                    },
                    {
                        "name": "Y. Nakayama"
                    },
                    {
                        "name": "V. Napolano"
                    },
                    {
                        "name": "I. Nardecchia"
                    },
                    {
                        "name": "T. Narikawa"
                    },
                    {
                        "name": "H. Narola"
                    },
                    {
                        "name": "L. Naticchioni"
                    },
                    {
                        "name": "B. Nayak"
                    },
                    {
                        "name": "R. K. Nayak"
                    },
                    {
                        "name": "B. F. Neil"
                    },
                    {
                        "name": "J. Neilson"
                    },
                    {
                        "name": "A. Nelson"
                    },
                    {
                        "name": "T. J. N. Nelson"
                    },
                    {
                        "name": "M. Nery"
                    },
                    {
                        "name": "P. Neubauer"
                    },
                    {
                        "name": "A. Neunzert"
                    },
                    {
                        "name": "K. Y. Ng"
                    },
                    {
                        "name": "S. W. S. Ng"
                    },
                    {
                        "name": "C. Nguyen"
                    },
                    {
                        "name": "P. Nguyen"
                    },
                    {
                        "name": "T. Nguyen"
                    },
                    {
                        "name": "L. Nguyen Quynh"
                    },
                    {
                        "name": "J. Ni"
                    },
                    {
                        "name": "W. -T. Ni"
                    },
                    {
                        "name": "S. A. Nichols"
                    },
                    {
                        "name": "T. Nishimoto"
                    },
                    {
                        "name": "A. Nishizawa"
                    },
                    {
                        "name": "S. Nissanke"
                    },
                    {
                        "name": "E. Nitoglia"
                    },
                    {
                        "name": "F. Nocera"
                    },
                    {
                        "name": "M. Norman"
                    },
                    {
                        "name": "C. North"
                    },
                    {
                        "name": "S. Nozaki"
                    },
                    {
                        "name": "G. Nurbek"
                    },
                    {
                        "name": "L. K. Nuttall"
                    },
                    {
                        "name": "Y. Obayashi"
                    },
                    {
                        "name": "J. Oberling"
                    },
                    {
                        "name": "B. D. O'Brien"
                    },
                    {
                        "name": "J. O'Dell"
                    },
                    {
                        "name": "E. Oelker"
                    },
                    {
                        "name": "W. Ogaki"
                    },
                    {
                        "name": "G. Oganesyan"
                    },
                    {
                        "name": "J. J. Oh"
                    },
                    {
                        "name": "K. Oh"
                    },
                    {
                        "name": "S. H. Oh"
                    },
                    {
                        "name": "M. Ohashi"
                    },
                    {
                        "name": "T. Ohashi"
                    },
                    {
                        "name": "M. Ohkawa"
                    },
                    {
                        "name": "F. Ohme"
                    },
                    {
                        "name": "H. Ohta"
                    },
                    {
                        "name": "M. A. Okada"
                    },
                    {
                        "name": "Y. Okutani"
                    },
                    {
                        "name": "C. Olivetto"
                    },
                    {
                        "name": "K. Oohara"
                    },
                    {
                        "name": "R. Oram"
                    },
                    {
                        "name": "B. O'Reilly"
                    },
                    {
                        "name": "R. G. Ormiston"
                    },
                    {
                        "name": "N. D. Ormsby"
                    },
                    {
                        "name": "R. O'Shaughnessy"
                    },
                    {
                        "name": "E. O'Shea"
                    },
                    {
                        "name": "S. Oshino"
                    },
                    {
                        "name": "S. Ossokine"
                    },
                    {
                        "name": "C. Osthelder"
                    },
                    {
                        "name": "S. Otabe"
                    },
                    {
                        "name": "D. J. Ottaway"
                    },
                    {
                        "name": "H. Overmier"
                    },
                    {
                        "name": "A. E. Pace"
                    },
                    {
                        "name": "G. Pagano"
                    },
                    {
                        "name": "R. Pagano"
                    },
                    {
                        "name": "M. A. Page"
                    },
                    {
                        "name": "G. Pagliaroli"
                    },
                    {
                        "name": "A. Pai"
                    },
                    {
                        "name": "S. A. Pai"
                    },
                    {
                        "name": "S. Pal"
                    },
                    {
                        "name": "J. R. Palamos"
                    },
                    {
                        "name": "O. Palashov"
                    },
                    {
                        "name": "C. Palomba"
                    },
                    {
                        "name": "H. Pan"
                    },
                    {
                        "name": "K. -C. Pan"
                    },
                    {
                        "name": "P. K. Panda"
                    },
                    {
                        "name": "P. T. H. Pang"
                    },
                    {
                        "name": "C. Pankow"
                    },
                    {
                        "name": "F. Pannarale"
                    },
                    {
                        "name": "B. C. Pant"
                    },
                    {
                        "name": "F. H. Panther"
                    },
                    {
                        "name": "F. Paoletti"
                    },
                    {
                        "name": "A. Paoli"
                    },
                    {
                        "name": "A. Paolone"
                    },
                    {
                        "name": "G. Pappas"
                    },
                    {
                        "name": "A. Parisi"
                    },
                    {
                        "name": "H. Park"
                    },
                    {
                        "name": "J. Park"
                    },
                    {
                        "name": "W. Parker"
                    },
                    {
                        "name": "D. Pascucci"
                    },
                    {
                        "name": "A. Pasqualetti"
                    },
                    {
                        "name": "R. Passaquieti"
                    },
                    {
                        "name": "D. Passuello"
                    },
                    {
                        "name": "M. Patel"
                    },
                    {
                        "name": "M. Pathak"
                    },
                    {
                        "name": "B. Patricelli"
                    },
                    {
                        "name": "A. S. Patron"
                    },
                    {
                        "name": "S. Paul"
                    },
                    {
                        "name": "E. Payne"
                    },
                    {
                        "name": "M. Pedraza"
                    },
                    {
                        "name": "R. Pedurand"
                    },
                    {
                        "name": "M. Pegoraro"
                    },
                    {
                        "name": "A. Pele"
                    },
                    {
                        "name": "F. E. Peña Arellano"
                    },
                    {
                        "name": "S. Penano"
                    },
                    {
                        "name": "S. Penn"
                    },
                    {
                        "name": "A. Perego"
                    },
                    {
                        "name": "A. Pereira"
                    },
                    {
                        "name": "T. Pereira"
                    },
                    {
                        "name": "C. J. Perez"
                    },
                    {
                        "name": "C. Périgois"
                    },
                    {
                        "name": "C. C. Perkins"
                    },
                    {
                        "name": "A. Perreca"
                    },
                    {
                        "name": "S. Perriès"
                    },
                    {
                        "name": "D. Pesios"
                    },
                    {
                        "name": "J. Petermann"
                    },
                    {
                        "name": "D. Petterson"
                    },
                    {
                        "name": "H. P. Pfeiffer"
                    },
                    {
                        "name": "H. Pham"
                    },
                    {
                        "name": "K. A. Pham"
                    },
                    {
                        "name": "K. S. Phukon"
                    },
                    {
                        "name": "H. Phurailatpam"
                    },
                    {
                        "name": "O. J. Piccinni"
                    },
                    {
                        "name": "M. Pichot"
                    },
                    {
                        "name": "M. Piendibene"
                    },
                    {
                        "name": "F. Piergiovanni"
                    },
                    {
                        "name": "L. Pierini"
                    },
                    {
                        "name": "V. Pierro"
                    },
                    {
                        "name": "G. Pillant"
                    },
                    {
                        "name": "M. Pillas"
                    },
                    {
                        "name": "F. Pilo"
                    },
                    {
                        "name": "L. Pinard"
                    },
                    {
                        "name": "C. Pineda-Bosque"
                    },
                    {
                        "name": "I. M. Pinto"
                    },
                    {
                        "name": "M. Pinto"
                    },
                    {
                        "name": "B. J. Piotrzkowski"
                    },
                    {
                        "name": "K. Piotrzkowski"
                    },
                    {
                        "name": "M. Pirello"
                    },
                    {
                        "name": "M. D. Pitkin"
                    },
                    {
                        "name": "A. Placidi"
                    },
                    {
                        "name": "E. Placidi"
                    },
                    {
                        "name": "M. L. Planas"
                    },
                    {
                        "name": "W. Plastino"
                    },
                    {
                        "name": "C. Pluchar"
                    },
                    {
                        "name": "R. Poggiani"
                    },
                    {
                        "name": "E. Polini"
                    },
                    {
                        "name": "L. Pompili"
                    },
                    {
                        "name": "D. Y. T. Pong"
                    },
                    {
                        "name": "S. Ponrathnam"
                    },
                    {
                        "name": "E. K. Porter"
                    },
                    {
                        "name": "R. Poulton"
                    },
                    {
                        "name": "A. Poverman"
                    },
                    {
                        "name": "J. Powell"
                    },
                    {
                        "name": "M. Pracchia"
                    },
                    {
                        "name": "T. Pradier"
                    },
                    {
                        "name": "A. K. Prajapati"
                    },
                    {
                        "name": "K. Prasai"
                    },
                    {
                        "name": "R. Prasanna"
                    },
                    {
                        "name": "G. Pratten"
                    },
                    {
                        "name": "M. Principe"
                    },
                    {
                        "name": "G. A. Prodi"
                    },
                    {
                        "name": "L. Prokhorov"
                    },
                    {
                        "name": "P. Prosposito"
                    },
                    {
                        "name": "L. Prudenzi"
                    },
                    {
                        "name": "A. Puecher"
                    },
                    {
                        "name": "M. Punturo"
                    },
                    {
                        "name": "F. Puosi"
                    },
                    {
                        "name": "P. Puppo"
                    },
                    {
                        "name": "M. Pürrer"
                    },
                    {
                        "name": "H. Qi"
                    },
                    {
                        "name": "N. Quartey"
                    },
                    {
                        "name": "V. Quetschke"
                    },
                    {
                        "name": "P. J. Quinonez"
                    },
                    {
                        "name": "R. Quitzow-James"
                    },
                    {
                        "name": "N. Qutob"
                    },
                    {
                        "name": "F. J. Raab"
                    },
                    {
                        "name": "G. Raaijmakers"
                    },
                    {
                        "name": "H. Radkins"
                    },
                    {
                        "name": "N. Radulesco"
                    },
                    {
                        "name": "P. Raffai"
                    },
                    {
                        "name": "S. X. Rail"
                    },
                    {
                        "name": "S. Raja"
                    },
                    {
                        "name": "C. Rajan"
                    },
                    {
                        "name": "K. E. Ramirez"
                    },
                    {
                        "name": "T. D. Ramirez"
                    },
                    {
                        "name": "A. Ramos-Buades"
                    },
                    {
                        "name": "J. Rana"
                    },
                    {
                        "name": "P. Rapagnani"
                    },
                    {
                        "name": "A. Ray"
                    },
                    {
                        "name": "V. Raymond"
                    },
                    {
                        "name": "N. Raza"
                    },
                    {
                        "name": "M. Razzano"
                    },
                    {
                        "name": "J. Read"
                    },
                    {
                        "name": "L. A. Rees"
                    },
                    {
                        "name": "T. Regimbau"
                    },
                    {
                        "name": "L. Rei"
                    },
                    {
                        "name": "S. Reid"
                    },
                    {
                        "name": "S. W. Reid"
                    },
                    {
                        "name": "D. H. Reitze"
                    },
                    {
                        "name": "P. Relton"
                    },
                    {
                        "name": "A. Renzini"
                    },
                    {
                        "name": "P. Rettegno"
                    },
                    {
                        "name": "B. Revenu"
                    },
                    {
                        "name": "A. Reza"
                    },
                    {
                        "name": "M. Rezac"
                    },
                    {
                        "name": "F. Ricci"
                    },
                    {
                        "name": "D. Richards"
                    },
                    {
                        "name": "J. W. Richardson"
                    },
                    {
                        "name": "L. Richardson"
                    },
                    {
                        "name": "G. Riemenschneider"
                    },
                    {
                        "name": "K. Riles"
                    },
                    {
                        "name": "S. Rinaldi"
                    },
                    {
                        "name": "K. Rink"
                    },
                    {
                        "name": "N. A. Robertson"
                    },
                    {
                        "name": "R. Robie"
                    },
                    {
                        "name": "F. Robinet"
                    },
                    {
                        "name": "A. Rocchi"
                    },
                    {
                        "name": "S. Rodriguez"
                    },
                    {
                        "name": "L. Rolland"
                    },
                    {
                        "name": "J. G. Rollins"
                    },
                    {
                        "name": "M. Romanelli"
                    },
                    {
                        "name": "R. Romano"
                    },
                    {
                        "name": "C. L. Romel"
                    },
                    {
                        "name": "A. Romero"
                    },
                    {
                        "name": "I. M. Romero-Shaw"
                    },
                    {
                        "name": "J. H. Romie"
                    },
                    {
                        "name": "S. Ronchini"
                    },
                    {
                        "name": "L. Rosa"
                    },
                    {
                        "name": "C. A. Rose"
                    },
                    {
                        "name": "D. Rosińska"
                    },
                    {
                        "name": "M. P. Ross"
                    },
                    {
                        "name": "S. Rowan"
                    },
                    {
                        "name": "S. J. Rowlinson"
                    },
                    {
                        "name": "S. Roy"
                    },
                    {
                        "name": "Santosh Roy"
                    },
                    {
                        "name": "Soumen Roy"
                    },
                    {
                        "name": "D. Rozza"
                    },
                    {
                        "name": "P. Ruggi"
                    },
                    {
                        "name": "K. Ruiz-Rocha"
                    },
                    {
                        "name": "K. Ryan"
                    },
                    {
                        "name": "S. Sachdev"
                    },
                    {
                        "name": "T. Sadecki"
                    },
                    {
                        "name": "J. Sadiq"
                    },
                    {
                        "name": "S. Saha"
                    },
                    {
                        "name": "Y. Saito"
                    },
                    {
                        "name": "K. Sakai"
                    },
                    {
                        "name": "M. Sakellariadou"
                    },
                    {
                        "name": "S. Sakon"
                    },
                    {
                        "name": "O. S. Salafia"
                    },
                    {
                        "name": "F. Salces-Carcoba"
                    },
                    {
                        "name": "L. Salconi"
                    },
                    {
                        "name": "M. Saleem"
                    },
                    {
                        "name": "F. Salemi"
                    },
                    {
                        "name": "A. Samajdar"
                    },
                    {
                        "name": "E. J. Sanchez"
                    },
                    {
                        "name": "J. H. Sanchez"
                    },
                    {
                        "name": "L. E. Sanchez"
                    },
                    {
                        "name": "N. Sanchis-Gual"
                    },
                    {
                        "name": "J. R. Sanders"
                    },
                    {
                        "name": "A. Sanuy"
                    },
                    {
                        "name": "T. R. Saravanan"
                    },
                    {
                        "name": "N. Sarin"
                    },
                    {
                        "name": "B. Sassolas"
                    },
                    {
                        "name": "H. Satari"
                    },
                    {
                        "name": "B. S. Sathyaprakash"
                    },
                    {
                        "name": "O. Sauter"
                    },
                    {
                        "name": "R. L. Savage"
                    },
                    {
                        "name": "V. Savant"
                    },
                    {
                        "name": "T. Sawada"
                    },
                    {
                        "name": "H. L. Sawant"
                    },
                    {
                        "name": "S. Sayah"
                    },
                    {
                        "name": "D. Schaetzl"
                    },
                    {
                        "name": "M. Scheel"
                    },
                    {
                        "name": "J. Scheuer"
                    },
                    {
                        "name": "M. G. Schiworski"
                    },
                    {
                        "name": "P. Schmidt"
                    },
                    {
                        "name": "S. Schmidt"
                    },
                    {
                        "name": "R. Schnabel"
                    },
                    {
                        "name": "M. Schneewind"
                    },
                    {
                        "name": "R. M. S. Schofield"
                    },
                    {
                        "name": "A. Schönbeck"
                    },
                    {
                        "name": "B. W. Schulte"
                    },
                    {
                        "name": "B. F. Schutz"
                    },
                    {
                        "name": "E. Schwartz"
                    },
                    {
                        "name": "J. Scott"
                    },
                    {
                        "name": "S. M. Scott"
                    },
                    {
                        "name": "M. Seglar-Arroyo"
                    },
                    {
                        "name": "Y. Sekiguchi"
                    },
                    {
                        "name": "D. Sellers"
                    },
                    {
                        "name": "A. S. Sengupta"
                    },
                    {
                        "name": "D. Sentenac"
                    },
                    {
                        "name": "E. G. Seo"
                    },
                    {
                        "name": "V. Sequino"
                    },
                    {
                        "name": "A. Sergeev"
                    },
                    {
                        "name": "Y. Setyawati"
                    },
                    {
                        "name": "T. Shaffer"
                    },
                    {
                        "name": "M. S. Shahriar"
                    },
                    {
                        "name": "M. A. Shaikh"
                    },
                    {
                        "name": "B. Shams"
                    },
                    {
                        "name": "L. Shao"
                    },
                    {
                        "name": "A. Sharma"
                    },
                    {
                        "name": "P. Sharma"
                    },
                    {
                        "name": "P. Shawhan"
                    },
                    {
                        "name": "N. S. Shcheblanov"
                    },
                    {
                        "name": "A. Sheela"
                    },
                    {
                        "name": "Y. Shikano"
                    },
                    {
                        "name": "M. Shikauchi"
                    },
                    {
                        "name": "H. Shimizu"
                    },
                    {
                        "name": "K. Shimode"
                    },
                    {
                        "name": "H. Shinkai"
                    },
                    {
                        "name": "T. Shishido"
                    },
                    {
                        "name": "A. Shoda"
                    },
                    {
                        "name": "D. H. Shoemaker"
                    },
                    {
                        "name": "D. M. Shoemaker"
                    },
                    {
                        "name": "S. ShyamSundar"
                    },
                    {
                        "name": "M. Sieniawska"
                    },
                    {
                        "name": "D. Sigg"
                    },
                    {
                        "name": "L. Silenzi"
                    },
                    {
                        "name": "L. P. Singer"
                    },
                    {
                        "name": "D. Singh"
                    },
                    {
                        "name": "M. K. Singh"
                    },
                    {
                        "name": "N. Singh"
                    },
                    {
                        "name": "A. Singha"
                    },
                    {
                        "name": "A. M. Sintes"
                    },
                    {
                        "name": "V. Sipala"
                    },
                    {
                        "name": "V. Skliris"
                    },
                    {
                        "name": "B. J. J. Slagmolen"
                    },
                    {
                        "name": "T. J. Slaven-Blair"
                    },
                    {
                        "name": "J. Smetana"
                    },
                    {
                        "name": "J. R. Smith"
                    },
                    {
                        "name": "L. Smith"
                    },
                    {
                        "name": "R. J. E. Smith"
                    },
                    {
                        "name": "J. Soldateschi"
                    },
                    {
                        "name": "S. N. Somala"
                    },
                    {
                        "name": "K. Somiya"
                    },
                    {
                        "name": "I. Song"
                    },
                    {
                        "name": "K. Soni"
                    },
                    {
                        "name": "S. Soni"
                    },
                    {
                        "name": "V. Sordini"
                    },
                    {
                        "name": "F. Sorrentino"
                    },
                    {
                        "name": "N. Sorrentino"
                    },
                    {
                        "name": "R. Soulard"
                    },
                    {
                        "name": "T. Souradeep"
                    },
                    {
                        "name": "E. Sowell"
                    },
                    {
                        "name": "V. Spagnuolo"
                    },
                    {
                        "name": "A. P. Spencer"
                    },
                    {
                        "name": "M. Spera"
                    },
                    {
                        "name": "P. Spinicelli"
                    },
                    {
                        "name": "A. K. Srivastava"
                    },
                    {
                        "name": "V. Srivastava"
                    },
                    {
                        "name": "K. Staats"
                    },
                    {
                        "name": "C. Stachie"
                    },
                    {
                        "name": "F. Stachurski"
                    },
                    {
                        "name": "D. A. Steer"
                    },
                    {
                        "name": "J. Steinhoff"
                    },
                    {
                        "name": "J. Steinlechner"
                    },
                    {
                        "name": "S. Steinlechner"
                    },
                    {
                        "name": "N. Stergioulas"
                    },
                    {
                        "name": "D. J. Stops"
                    },
                    {
                        "name": "M. Stover"
                    },
                    {
                        "name": "K. A. Strain"
                    },
                    {
                        "name": "L. C. Strang"
                    },
                    {
                        "name": "G. Stratta"
                    },
                    {
                        "name": "M. D. Strong"
                    },
                    {
                        "name": "A. Strunk"
                    },
                    {
                        "name": "R. Sturani"
                    },
                    {
                        "name": "A. L. Stuver"
                    },
                    {
                        "name": "M. Suchenek"
                    },
                    {
                        "name": "S. Sudhagar"
                    },
                    {
                        "name": "V. Sudhir"
                    },
                    {
                        "name": "R. Sugimoto"
                    },
                    {
                        "name": "H. G. Suh"
                    },
                    {
                        "name": "A. G. Sullivan"
                    },
                    {
                        "name": "J. M. Sullivan"
                    },
                    {
                        "name": "T. Z. Summerscales"
                    },
                    {
                        "name": "L. Sun"
                    },
                    {
                        "name": "S. Sunil"
                    },
                    {
                        "name": "A. Sur"
                    },
                    {
                        "name": "J. Suresh"
                    },
                    {
                        "name": "P. J. Sutton"
                    },
                    {
                        "name": "Takamasa Suzuki"
                    },
                    {
                        "name": "Takanori Suzuki"
                    },
                    {
                        "name": "Toshikazu Suzuki"
                    },
                    {
                        "name": "B. L. Swinkels"
                    },
                    {
                        "name": "M. J. Szczepańczyk"
                    },
                    {
                        "name": "P. Szewczyk"
                    },
                    {
                        "name": "M. Tacca"
                    },
                    {
                        "name": "H. Tagoshi"
                    },
                    {
                        "name": "S. C. Tait"
                    },
                    {
                        "name": "H. Takahashi"
                    },
                    {
                        "name": "R. Takahashi"
                    },
                    {
                        "name": "S. Takano"
                    },
                    {
                        "name": "H. Takeda"
                    },
                    {
                        "name": "M. Takeda"
                    },
                    {
                        "name": "C. J. Talbot"
                    },
                    {
                        "name": "C. Talbot"
                    },
                    {
                        "name": "K. Tanaka"
                    },
                    {
                        "name": "Taiki Tanaka"
                    },
                    {
                        "name": "Takahiro Tanaka"
                    },
                    {
                        "name": "A. J. Tanasijczuk"
                    },
                    {
                        "name": "S. Tanioka"
                    },
                    {
                        "name": "D. B. Tanner"
                    },
                    {
                        "name": "D. Tao"
                    },
                    {
                        "name": "L. Tao"
                    },
                    {
                        "name": "R. D. Tapia"
                    },
                    {
                        "name": "E. N. Tapia San Martín"
                    },
                    {
                        "name": "C. Taranto"
                    },
                    {
                        "name": "A. Taruya"
                    },
                    {
                        "name": "J. D. Tasson"
                    },
                    {
                        "name": "R. Tenorio"
                    },
                    {
                        "name": "J. E. S. Terhune"
                    },
                    {
                        "name": "L. Terkowski"
                    },
                    {
                        "name": "M. P. Thirugnanasambandam"
                    },
                    {
                        "name": "M. Thomas"
                    },
                    {
                        "name": "P. Thomas"
                    },
                    {
                        "name": "E. E. Thompson"
                    },
                    {
                        "name": "J. E. Thompson"
                    },
                    {
                        "name": "S. R. Thondapu"
                    },
                    {
                        "name": "K. A. Thorne"
                    },
                    {
                        "name": "E. Thrane"
                    },
                    {
                        "name": "Shubhanshu Tiwari"
                    },
                    {
                        "name": "Srishti Tiwari"
                    },
                    {
                        "name": "V. Tiwari"
                    },
                    {
                        "name": "A. M. Toivonen"
                    },
                    {
                        "name": "A. E. Tolley"
                    },
                    {
                        "name": "T. Tomaru"
                    },
                    {
                        "name": "T. Tomura"
                    },
                    {
                        "name": "M. Tonelli"
                    },
                    {
                        "name": "Z. Tornasi"
                    },
                    {
                        "name": "A. Torres-Forné"
                    },
                    {
                        "name": "C. I. Torrie"
                    },
                    {
                        "name": "I. Tosta e Melo"
                    },
                    {
                        "name": "D. Töyrä"
                    },
                    {
                        "name": "A. Trapananti"
                    },
                    {
                        "name": "F. Travasso"
                    },
                    {
                        "name": "G. Traylor"
                    },
                    {
                        "name": "M. Trevor"
                    },
                    {
                        "name": "M. C. Tringali"
                    },
                    {
                        "name": "A. Tripathee"
                    },
                    {
                        "name": "L. Troiano"
                    },
                    {
                        "name": "A. Trovato"
                    },
                    {
                        "name": "L. Trozzo"
                    },
                    {
                        "name": "R. J. Trudeau"
                    },
                    {
                        "name": "D. Tsai"
                    },
                    {
                        "name": "K. W. Tsang"
                    },
                    {
                        "name": "T. Tsang"
                    },
                    {
                        "name": "J-S. Tsao"
                    },
                    {
                        "name": "M. Tse"
                    },
                    {
                        "name": "R. Tso"
                    },
                    {
                        "name": "S. Tsuchida"
                    },
                    {
                        "name": "L. Tsukada"
                    },
                    {
                        "name": "D. Tsuna"
                    },
                    {
                        "name": "T. Tsutsui"
                    },
                    {
                        "name": "K. Turbang"
                    },
                    {
                        "name": "M. Turconi"
                    },
                    {
                        "name": "D. Tuyenbayev"
                    },
                    {
                        "name": "A. S. Ubhi"
                    },
                    {
                        "name": "N. Uchikata"
                    },
                    {
                        "name": "T. Uchiyama"
                    },
                    {
                        "name": "R. P. Udall"
                    },
                    {
                        "name": "A. Ueda"
                    },
                    {
                        "name": "T. Uehara"
                    },
                    {
                        "name": "K. Ueno"
                    },
                    {
                        "name": "G. Ueshima"
                    },
                    {
                        "name": "C. S. Unnikrishnan"
                    },
                    {
                        "name": "A. L. Urban"
                    },
                    {
                        "name": "T. Ushiba"
                    },
                    {
                        "name": "A. Utina"
                    },
                    {
                        "name": "G. Vajente"
                    },
                    {
                        "name": "A. Vajpeyi"
                    },
                    {
                        "name": "G. Valdes"
                    },
                    {
                        "name": "M. Valentini"
                    },
                    {
                        "name": "V. Valsan"
                    },
                    {
                        "name": "N. van Bakel"
                    },
                    {
                        "name": "M. van Beuzekom"
                    },
                    {
                        "name": "M. van Dael"
                    },
                    {
                        "name": "J. F. J. van den Brand"
                    },
                    {
                        "name": "C. Van Den Broeck"
                    },
                    {
                        "name": "D. C. Vander-Hyde"
                    },
                    {
                        "name": "H. van Haevermaet"
                    },
                    {
                        "name": "J. V. van Heijningen"
                    },
                    {
                        "name": "M. H. P. M. van Putten"
                    },
                    {
                        "name": "N. van Remortel"
                    },
                    {
                        "name": "M. Vardaro"
                    },
                    {
                        "name": "A. F. Vargas"
                    },
                    {
                        "name": "V. Varma"
                    },
                    {
                        "name": "M. Vasúth"
                    },
                    {
                        "name": "A. Vecchio"
                    },
                    {
                        "name": "G. Vedovato"
                    },
                    {
                        "name": "J. Veitch"
                    },
                    {
                        "name": "P. J. Veitch"
                    },
                    {
                        "name": "J. Venneberg"
                    },
                    {
                        "name": "G. Venugopalan"
                    },
                    {
                        "name": "D. Verkindt"
                    },
                    {
                        "name": "P. Verma"
                    },
                    {
                        "name": "Y. Verma"
                    },
                    {
                        "name": "S. M. Vermeulen"
                    },
                    {
                        "name": "D. Veske"
                    },
                    {
                        "name": "F. Vetrano"
                    },
                    {
                        "name": "A. Viceré"
                    },
                    {
                        "name": "S. Vidyant"
                    },
                    {
                        "name": "A. D. Viets"
                    },
                    {
                        "name": "A. Vijaykumar"
                    },
                    {
                        "name": "V. Villa-Ortega"
                    },
                    {
                        "name": "J. -Y. Vinet"
                    },
                    {
                        "name": "A. Virtuoso"
                    },
                    {
                        "name": "S. Vitale"
                    },
                    {
                        "name": "H. Vocca"
                    },
                    {
                        "name": "E. R. G. von Reis"
                    },
                    {
                        "name": "J. S. A. von Wrangel"
                    },
                    {
                        "name": "C. Vorvick"
                    },
                    {
                        "name": "S. P. Vyatchanin"
                    },
                    {
                        "name": "L. E. Wade"
                    },
                    {
                        "name": "M. Wade"
                    },
                    {
                        "name": "K. J. Wagner"
                    },
                    {
                        "name": "R. Wald"
                    },
                    {
                        "name": "R. C. Walet"
                    },
                    {
                        "name": "M. Walker"
                    },
                    {
                        "name": "G. S. Wallace"
                    },
                    {
                        "name": "L. Wallace"
                    },
                    {
                        "name": "J. Wang"
                    },
                    {
                        "name": "J. Z. Wang"
                    },
                    {
                        "name": "W. H. Wang"
                    },
                    {
                        "name": "R. L. Ward"
                    },
                    {
                        "name": "J. Warner"
                    },
                    {
                        "name": "M. Was"
                    },
                    {
                        "name": "T. Washimi"
                    },
                    {
                        "name": "N. Y. Washington"
                    },
                    {
                        "name": "J. Watchi"
                    },
                    {
                        "name": "B. Weaver"
                    },
                    {
                        "name": "C. R. Weaving"
                    },
                    {
                        "name": "S. A. Webster"
                    },
                    {
                        "name": "M. Weinert"
                    },
                    {
                        "name": "A. J. Weinstein"
                    },
                    {
                        "name": "R. Weiss"
                    },
                    {
                        "name": "C. M. Weller"
                    },
                    {
                        "name": "R. A. Weller"
                    },
                    {
                        "name": "F. Wellmann"
                    },
                    {
                        "name": "L. Wen"
                    },
                    {
                        "name": "P. Weßels"
                    },
                    {
                        "name": "K. Wette"
                    },
                    {
                        "name": "J. T. Whelan"
                    },
                    {
                        "name": "D. D. White"
                    },
                    {
                        "name": "B. F. Whiting"
                    },
                    {
                        "name": "C. Whittle"
                    },
                    {
                        "name": "D. Wilken"
                    },
                    {
                        "name": "D. Williams"
                    },
                    {
                        "name": "M. J. Williams"
                    },
                    {
                        "name": "A. R. Williamson"
                    },
                    {
                        "name": "J. L. Willis"
                    },
                    {
                        "name": "B. Willke"
                    },
                    {
                        "name": "D. J. Wilson"
                    },
                    {
                        "name": "C. C. Wipf"
                    },
                    {
                        "name": "T. Wlodarczyk"
                    },
                    {
                        "name": "G. Woan"
                    },
                    {
                        "name": "J. Woehler"
                    },
                    {
                        "name": "J. K. Wofford"
                    },
                    {
                        "name": "D. Wong"
                    },
                    {
                        "name": "I. C. F. Wong"
                    },
                    {
                        "name": "M. Wright"
                    },
                    {
                        "name": "C. Wu"
                    },
                    {
                        "name": "D. S. Wu"
                    },
                    {
                        "name": "H. Wu"
                    },
                    {
                        "name": "D. M. Wysocki"
                    },
                    {
                        "name": "L. Xiao"
                    },
                    {
                        "name": "T. Yamada"
                    },
                    {
                        "name": "H. Yamamoto"
                    },
                    {
                        "name": "K. Yamamoto"
                    },
                    {
                        "name": "T. Yamamoto"
                    },
                    {
                        "name": "K. Yamashita"
                    },
                    {
                        "name": "R. Yamazaki"
                    },
                    {
                        "name": "F. W. Yang"
                    },
                    {
                        "name": "K. Z. Yang"
                    },
                    {
                        "name": "L. Yang"
                    },
                    {
                        "name": "Y. -C. Yang"
                    },
                    {
                        "name": "Y. Yang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "M. J. Yap"
                    },
                    {
                        "name": "D. W. Yeeles"
                    },
                    {
                        "name": "S. -W. Yeh"
                    },
                    {
                        "name": "A. B. Yelikar"
                    },
                    {
                        "name": "M. Ying"
                    },
                    {
                        "name": "J. Yokoyama"
                    },
                    {
                        "name": "T. Yokozawa"
                    },
                    {
                        "name": "J. Yoo"
                    },
                    {
                        "name": "T. Yoshioka"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Haocun Yu"
                    },
                    {
                        "name": "H. Yuzurihara"
                    },
                    {
                        "name": "A. Zadrożny"
                    },
                    {
                        "name": "M. Zanolin"
                    },
                    {
                        "name": "S. Zeidler"
                    },
                    {
                        "name": "T. Zelenova"
                    },
                    {
                        "name": "J. -P. Zendri"
                    },
                    {
                        "name": "M. Zevin"
                    },
                    {
                        "name": "M. Zhan"
                    },
                    {
                        "name": "H. Zhang"
                    },
                    {
                        "name": "J. Zhang"
                    },
                    {
                        "name": "L. Zhang"
                    },
                    {
                        "name": "R. Zhang"
                    },
                    {
                        "name": "T. Zhang"
                    },
                    {
                        "name": "Y. Zhang"
                    },
                    {
                        "name": "C. Zhao"
                    },
                    {
                        "name": "G. Zhao"
                    },
                    {
                        "name": "Y. Zhao"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "R. Zhou"
                    },
                    {
                        "name": "Z. Zhou"
                    },
                    {
                        "name": "X. J. Zhu"
                    },
                    {
                        "name": "Z. -H. Zhu"
                    },
                    {
                        "name": "A. B. Zimmerman"
                    },
                    {
                        "name": "M. E. Zucker"
                    },
                    {
                        "name": "J. Zweizig"
                    }
                ],
                "author_detail": {
                    "name": "J. Zweizig"
                },
                "author": "J. Zweizig",
                "arxiv_doi": "10.1103/PhysRevD.112.084080"
            },
            {
                "id": "http://arxiv.org/abs/2509.16364v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.16364v2",
                "title": "Extracting Higgs Self-Coupling Constraints through Triple Higgs Boson Production at Future Hadron Colliders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Higgs Self-Coupling Constraints through Triple Higgs Boson Production at Future Hadron Colliders"
                },
                "updated": "2025-11-17T15:04:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    4,
                    16,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.16364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.16364v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1140/epjc/s10052-025-15051-7",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "We present a systematic study of triple Higgs boson production at future high-energy hadron colliders, using the six-$b$-jet final state as a probe of the Higgs self-interactions. We conduct, under realistic detector smearing assumptions, both a traditional cut-based analysis, and a multivariate one using gradient boosting. The multivariate strategy is found to enhance sensitivity to beyond the Standard Model effects on the Higgs boson's self-couplings, while preserving large signal event yields, thus enabling more robust statistical inference. This allows us to assess the impact of detector effects, systematic uncertainties, background normalisation, as well as different truncation choices in an effective-field-theory description of the new physics effects possibly affecting the Higgs boson's self-interactions. Our results demonstrate that statistically-meaningful and perturbative-unitarity-compatible constraints on the trilinear and quartic Higgs boson self-couplings can be achieved, provided that systematic uncertainties are controlled at the few-percent level. Finally, we extrapolate our results to various collider energies and luminosities, demonstrating in particular that an 85 TeV proton-proton collider performs comparably to a 100 TeV machine. Altogether, our findings therefore establish the six-$b$ channel as a viable probe of the Higgs self-interactions at most future hadron collider options currently being examined by the high-energy physics community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a systematic study of triple Higgs boson production at future high-energy hadron colliders, using the six-$b$-jet final state as a probe of the Higgs self-interactions. We conduct, under realistic detector smearing assumptions, both a traditional cut-based analysis, and a multivariate one using gradient boosting. The multivariate strategy is found to enhance sensitivity to beyond the Standard Model effects on the Higgs boson's self-couplings, while preserving large signal event yields, thus enabling more robust statistical inference. This allows us to assess the impact of detector effects, systematic uncertainties, background normalisation, as well as different truncation choices in an effective-field-theory description of the new physics effects possibly affecting the Higgs boson's self-interactions. Our results demonstrate that statistically-meaningful and perturbative-unitarity-compatible constraints on the trilinear and quartic Higgs boson self-couplings can be achieved, provided that systematic uncertainties are controlled at the few-percent level. Finally, we extrapolate our results to various collider energies and luminosities, demonstrating in particular that an 85 TeV proton-proton collider performs comparably to a 100 TeV machine. Altogether, our findings therefore establish the six-$b$ channel as a viable probe of the Higgs self-interactions at most future hadron collider options currently being examined by the high-energy physics community."
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-19T19:12:33Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    19,
                    12,
                    33,
                    4,
                    262,
                    0
                ],
                "arxiv_comment": "15 pages, 10 figures, 9 tables, two-column format. Matches published version",
                "arxiv_primary_category": {
                    "term": "hep-ph"
                },
                "arxiv_journal_ref": "Eur. Phys. J. C 85, 1309 (2025)",
                "authors": [
                    {
                        "name": "Benjamin Fuks"
                    },
                    {
                        "name": "Andreas Papaefstathiou"
                    },
                    {
                        "name": "Gilberto Tetlalmatzi-Xolocotzi"
                    }
                ],
                "author_detail": {
                    "name": "Gilberto Tetlalmatzi-Xolocotzi"
                },
                "author": "Gilberto Tetlalmatzi-Xolocotzi",
                "arxiv_doi": "10.1140/epjc/s10052-025-15051-7"
            },
            {
                "id": "http://arxiv.org/abs/2511.04486v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04486v2",
                "title": "EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits"
                },
                "updated": "2025-11-17T15:03:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    3,
                    36,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04486v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Instructed code editing, where LLMs directly modify a developer's existing code based on a user instruction, is becoming a widely used interaction mode in AI coding assistants. However, few benchmarks directly evaluate this capability and current datasets often rely on artificial sources. We introduce EDIT-Bench, a benchmark for evaluating LLM code editing capabilities grounded in real-world usage, i.e., user instructions and code contexts collected in the wild. EDIT-Bench comprises of 540 problems, multiple natural and programming languages, and a diverse set of real-world use cases, ranging from resolving errors to adding features. EDIT-Bench introduces context-dependent problems that require the model to understand code context, highlighted code, and cursor position in addition to the user instruction. We evaluate 40 diverse LLMs and observe that EDIT-Bench is a challenging set of problems where only 1 model scores over 60%. We find that model performance varies across different categories of user instructions. Further, we find that varying levels of contextual information greatly affect task success rate, with performance varying up to 11%, indicating the importance of evaluating with realistic context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instructed code editing, where LLMs directly modify a developer's existing code based on a user instruction, is becoming a widely used interaction mode in AI coding assistants. However, few benchmarks directly evaluate this capability and current datasets often rely on artificial sources. We introduce EDIT-Bench, a benchmark for evaluating LLM code editing capabilities grounded in real-world usage, i.e., user instructions and code contexts collected in the wild. EDIT-Bench comprises of 540 problems, multiple natural and programming languages, and a diverse set of real-world use cases, ranging from resolving errors to adding features. EDIT-Bench introduces context-dependent problems that require the model to understand code context, highlighted code, and cursor position in addition to the user instruction. We evaluate 40 diverse LLMs and observe that EDIT-Bench is a challenging set of problems where only 1 model scores over 60%. We find that model performance varies across different categories of user instructions. Further, we find that varying levels of contextual information greatly affect task success rate, with performance varying up to 11%, indicating the importance of evaluating with realistic context."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T16:05:28Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    5,
                    28,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Wayne Chi"
                    },
                    {
                        "name": "Valerie Chen"
                    },
                    {
                        "name": "Ryan Shar"
                    },
                    {
                        "name": "Aditya Mittal"
                    },
                    {
                        "name": "Jenny Liang"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Anastasios Nikolas Angelopoulos"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Chris Donahue"
                    }
                ],
                "author_detail": {
                    "name": "Chris Donahue"
                },
                "author": "Chris Donahue"
            },
            {
                "id": "http://arxiv.org/abs/2507.16124v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.16124v3",
                "title": "Benchmarking LLM Privacy Recognition for Social Robot Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM Privacy Recognition for Social Robot Decision Making"
                },
                "updated": "2025-11-17T15:01:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    1,
                    43,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.16124v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.16124v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While robots have previously utilized rule-based systems or probabilistic models for user interaction, the rapid evolution of large language models (LLMs) presents new opportunities to develop LLM-powered robots for enhanced human-robot interaction (HRI). To fully realize these capabilities, however, robots need to collect data such as audio, fine-grained images, video, and locations. As a result, LLMs often process sensitive personal information, particularly within private environments, such as homes. Given the tension between utility and privacy risks, evaluating how current LLMs manage sensitive data is critical. Specifically, we aim to explore the extent to which out-of-the-box LLMs are privacy-aware in the context of household robots. In this work, we present a set of privacy-relevant scenarios developed using the Contextual Integrity (CI) framework. We first surveyed users' privacy preferences regarding in-home robot behaviors and then examined how their privacy orientations affected their choices of these behaviors (N = 450). We then provided the same set of scenarios and questions to state-of-the-art LLMs (N = 10) and found that the agreement between humans and LLMs was generally low. To further investigate the capabilities of LLMs as potential privacy controllers, we implemented four additional prompting strategies and compared their results. We discuss the performance of the evaluated models as well as the implications and potential of AI privacy awareness in human-robot interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While robots have previously utilized rule-based systems or probabilistic models for user interaction, the rapid evolution of large language models (LLMs) presents new opportunities to develop LLM-powered robots for enhanced human-robot interaction (HRI). To fully realize these capabilities, however, robots need to collect data such as audio, fine-grained images, video, and locations. As a result, LLMs often process sensitive personal information, particularly within private environments, such as homes. Given the tension between utility and privacy risks, evaluating how current LLMs manage sensitive data is critical. Specifically, we aim to explore the extent to which out-of-the-box LLMs are privacy-aware in the context of household robots. In this work, we present a set of privacy-relevant scenarios developed using the Contextual Integrity (CI) framework. We first surveyed users' privacy preferences regarding in-home robot behaviors and then examined how their privacy orientations affected their choices of these behaviors (N = 450). We then provided the same set of scenarios and questions to state-of-the-art LLMs (N = 10) and found that the agreement between humans and LLMs was generally low. To further investigate the capabilities of LLMs as potential privacy controllers, we implemented four additional prompting strategies and compared their results. We discuss the performance of the evaluated models as well as the implications and potential of AI privacy awareness in human-robot interaction."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-22T00:36:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    0,
                    36,
                    59,
                    1,
                    203,
                    0
                ],
                "arxiv_comment": "18 pages, 7 figures. Dakota Sullivan and Shirley Zhang contributed equally to this work",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Dakota Sullivan"
                    },
                    {
                        "name": "Shirley Zhang"
                    },
                    {
                        "name": "Jennica Li"
                    },
                    {
                        "name": "Heather Kirkorian"
                    },
                    {
                        "name": "Bilge Mutlu"
                    },
                    {
                        "name": "Kassem Fawaz"
                    }
                ],
                "author_detail": {
                    "name": "Kassem Fawaz"
                },
                "author": "Kassem Fawaz"
            },
            {
                "id": "http://arxiv.org/abs/2511.13453v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13453v1",
                "title": "Hardware optimization on Android for inference of AI models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware optimization on Android for inference of AI models"
                },
                "updated": "2025-11-17T14:58:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    58,
                    15,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13453v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:58:15Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    58,
                    15,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "8 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Iulius Gherasim"
                    },
                    {
                        "name": "Carlos García Sánchez"
                    }
                ],
                "author_detail": {
                    "name": "Carlos García Sánchez"
                },
                "author": "Carlos García Sánchez"
            },
            {
                "id": "http://arxiv.org/abs/2505.20782v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.20782v2",
                "title": "Towards Cross-Domain Multi-Targeted Adversarial Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Cross-Domain Multi-Targeted Adversarial Attacks"
                },
                "updated": "2025-11-17T14:56:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    56,
                    37,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.20782v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.20782v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-targeted adversarial attacks aim to mislead classifiers toward specific target classes using a single perturbation generator with a conditional input specifying the desired target class. Existing methods face two key limitations: (1) a single generator supports only a limited number of predefined target classes, and (2) it requires access to the victim model's training data to learn target class semantics. This dependency raises data leakage concerns in practical black-box scenarios where the training data is typically private. To address these limitations, we propose a novel Cross-Domain Multi-Targeted Attack (CD-MTA) that can generate perturbations toward arbitrary target classes, even those that do not exist in the attacker's training data. CD-MTA is trained on a single public dataset but can perform targeted attacks on black-box models trained on different datasets with disjoint and unknown class sets. Our method requires only a single example image that visually represents the desired target class, without relying its label, class distribution or pretrained embeddings. We achieve this through a Feature Injection Module (FIM) and class-agnostic objectives which guide the generator to extract transferable, fine-grained features from the target image without inferring class semantics. Experiments on ImageNet and seven additional datasets show that CD-MTA outperforms existing multi-targeted attack methods on unseen target classes in black-box and cross-domain scenarios. The code is available at https://github.com/tgoncalv/CD-MTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-targeted adversarial attacks aim to mislead classifiers toward specific target classes using a single perturbation generator with a conditional input specifying the desired target class. Existing methods face two key limitations: (1) a single generator supports only a limited number of predefined target classes, and (2) it requires access to the victim model's training data to learn target class semantics. This dependency raises data leakage concerns in practical black-box scenarios where the training data is typically private. To address these limitations, we propose a novel Cross-Domain Multi-Targeted Attack (CD-MTA) that can generate perturbations toward arbitrary target classes, even those that do not exist in the attacker's training data. CD-MTA is trained on a single public dataset but can perform targeted attacks on black-box models trained on different datasets with disjoint and unknown class sets. Our method requires only a single example image that visually represents the desired target class, without relying its label, class distribution or pretrained embeddings. We achieve this through a Feature Injection Module (FIM) and class-agnostic objectives which guide the generator to extract transferable, fine-grained features from the target image without inferring class semantics. Experiments on ImageNet and seven additional datasets show that CD-MTA outperforms existing multi-targeted attack methods on unseen target classes in black-box and cross-domain scenarios. The code is available at https://github.com/tgoncalv/CD-MTA."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-27T06:39:29Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    39,
                    29,
                    1,
                    147,
                    0
                ],
                "arxiv_comment": "Under review",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Taïga Gonçalves"
                    },
                    {
                        "name": "Tomo Miyazaki"
                    },
                    {
                        "name": "Shinichiro Omachi"
                    }
                ],
                "author_detail": {
                    "name": "Shinichiro Omachi"
                },
                "author": "Shinichiro Omachi"
            },
            {
                "id": "http://arxiv.org/abs/2504.10950v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.10950v2",
                "title": "Unveiling Challenges for LLMs in Enterprise Data Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Challenges for LLMs in Enterprise Data Engineering"
                },
                "updated": "2025-11-17T14:54:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    54,
                    12,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.10950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.10950v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.14778/3773749.3773758",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Large Language Models (LLMs) promise to automate data engineering on tabular data, offering enterprises a valuable opportunity to cut the high costs of manual data handling. But the enterprise domain comes with unique challenges that existing LLM-based approaches for data engineering often overlook, such as large table sizes, more complex tasks, and the need for internal knowledge. To bridge these gaps, we identify key enterprise-specific challenges related to data, tasks, and background knowledge and extensively evaluate how they affect data engineering with LLMs. Our analysis reveals that LLMs face substantial limitations in real-world enterprise scenarios, with accuracy declining sharply. Our findings contribute to a systematic understanding of LLMs for enterprise data engineering to support their adoption in industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) promise to automate data engineering on tabular data, offering enterprises a valuable opportunity to cut the high costs of manual data handling. But the enterprise domain comes with unique challenges that existing LLM-based approaches for data engineering often overlook, such as large table sizes, more complex tasks, and the need for internal knowledge. To bridge these gaps, we identify key enterprise-specific challenges related to data, tasks, and background knowledge and extensively evaluate how they affect data engineering with LLMs. Our analysis reveals that LLMs face substantial limitations in real-world enterprise scenarios, with accuracy declining sharply. Our findings contribute to a systematic understanding of LLMs for enterprise data engineering to support their adoption in industry."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-15T07:57:05Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    7,
                    57,
                    5,
                    1,
                    105,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Jan-Micha Bodensohn"
                    },
                    {
                        "name": "Ulf Brackmann"
                    },
                    {
                        "name": "Liane Vogel"
                    },
                    {
                        "name": "Anupam Sanghi"
                    },
                    {
                        "name": "Carsten Binnig"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Binnig"
                },
                "author": "Carsten Binnig",
                "arxiv_doi": "10.14778/3773749.3773758"
            },
            {
                "id": "http://arxiv.org/abs/2511.13442v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13442v1",
                "title": "Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline"
                },
                "updated": "2025-11-17T14:49:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    49,
                    57,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13442v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:49:57Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    49,
                    57,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Rui Zuo"
                    },
                    {
                        "name": "Qinyue Tong"
                    },
                    {
                        "name": "Zhe-Ming Lu"
                    },
                    {
                        "name": "Ziqian Lu"
                    }
                ],
                "author_detail": {
                    "name": "Ziqian Lu"
                },
                "author": "Ziqian Lu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13440v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13440v1",
                "title": "Random sets from the perspective of metric statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random sets from the perspective of metric statistics"
                },
                "updated": "2025-11-17T14:49:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    49,
                    30,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13440v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Since the seminal work by Beresteanu and Molinari(2008), the random set theory and related inference methods have been widely applied in partially identified econometric models. Meanwhile, there is an emerging field in statistics for studying random objects in metric spaces, called metric statistics. This paper clarifies a relationship between two fundamental concepts in these literatures, the Aumann and Fréchet means, and presents some applications of metric statistics to econometric problems involving random sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the seminal work by Beresteanu and Molinari(2008), the random set theory and related inference methods have been widely applied in partially identified econometric models. Meanwhile, there is an emerging field in statistics for studying random objects in metric spaces, called metric statistics. This paper clarifies a relationship between two fundamental concepts in these literatures, the Aumann and Fréchet means, and presents some applications of metric statistics to econometric problems involving random sets."
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:49:30Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    49,
                    30,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "22 pages, 2 figures",
                "arxiv_primary_category": {
                    "term": "math.ST"
                },
                "authors": [
                    {
                        "name": "Daisuke Kurisu"
                    },
                    {
                        "name": "Yuta Okamoto"
                    },
                    {
                        "name": "Taisuke Otsu"
                    }
                ],
                "author_detail": {
                    "name": "Taisuke Otsu"
                },
                "author": "Taisuke Otsu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13436v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13436v1",
                "title": "Shaping the Mantle: The Role of Superheated Cores After Giant Impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shaping the Mantle: The Role of Superheated Cores After Giant Impacts"
                },
                "updated": "2025-11-17T14:46:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    46,
                    32,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13436v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Moon-forming giant impact significantly influenced the initial thermal state of Earth's mantle by generating a global magma ocean, marking the onset of mantle evolution. Recent Smoothed Particle Hydrodynamics (SPH) simulations indicate that such a collision would produce a superheated core, whose cooling would strongly influence subsequent mantle dynamics. Here, we present systematic SPH simulations of diverse giant-impact scenarios and show that the superheated core formed after the impact can trigger secondary mantle melting, thereby governing the final state of the magma ocean. To further quantify this effect, we employ a parameterized mantle-melting model to evaluate the influence of secondary melting on the lower mantle. Our results suggest three possible outcomes: complete mantle melting, the formation of a basal melt layer, or the initiation of an early superplume. Combined with recent two-phase magma-ocean solidification models, we infer that all three scenarios would result in basal melt layers of varying thickness, partially retaining the thermal energy of the superheated core. In the canonical Moon-forming scenario, the superheated core would rapidly transfer heat to Earth's lower mantle, causing secondary mantle melting within approximately 277--5983 years and generating either a basal melt layer or a fully molten mantle. Both outcomes would effectively erase primordial heterogeneities in the lower mantle and impose distinct pathways for its subsequent thermal evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Moon-forming giant impact significantly influenced the initial thermal state of Earth's mantle by generating a global magma ocean, marking the onset of mantle evolution. Recent Smoothed Particle Hydrodynamics (SPH) simulations indicate that such a collision would produce a superheated core, whose cooling would strongly influence subsequent mantle dynamics. Here, we present systematic SPH simulations of diverse giant-impact scenarios and show that the superheated core formed after the impact can trigger secondary mantle melting, thereby governing the final state of the magma ocean. To further quantify this effect, we employ a parameterized mantle-melting model to evaluate the influence of secondary melting on the lower mantle. Our results suggest three possible outcomes: complete mantle melting, the formation of a basal melt layer, or the initiation of an early superplume. Combined with recent two-phase magma-ocean solidification models, we infer that all three scenarios would result in basal melt layers of varying thickness, partially retaining the thermal energy of the superheated core. In the canonical Moon-forming scenario, the superheated core would rapidly transfer heat to Earth's lower mantle, causing secondary mantle melting within approximately 277--5983 years and generating either a basal melt layer or a fully molten mantle. Both outcomes would effectively erase primordial heterogeneities in the lower mantle and impose distinct pathways for its subsequent thermal evolution."
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:46:32Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    46,
                    32,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "26 pages, 7 figures, 2 tables, and 5 supplementary figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP"
                },
                "authors": [
                    {
                        "name": "You Zhou"
                    }
                ],
                "author_detail": {
                    "name": "You Zhou"
                },
                "author": "You Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2508.04663v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.04663v2",
                "title": "HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models"
                },
                "updated": "2025-11-17T14:38:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    38,
                    19,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.04663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.04663v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "State-of-the-art text-to-image diffusion models (DMs) achieve remarkable quality, yet their massive parameter scale (8-11B) poses significant challenges for inferences on resource-constrained devices. In this paper, we present HierarchicalPrune, a novel compression framework grounded in a key observation: DM blocks exhibit distinct functional hierarchies, where early blocks establish semantic structures while later blocks handle texture refinements. HierarchicalPrune synergistically combines three techniques: (1) Hierarchical Position Pruning, which identifies and removes less essential later blocks based on position hierarchy; (2) Positional Weight Preservation, which systematically protects early model portions that are essential for semantic structural integrity; and (3) Sensitivity-Guided Distillation, which adjusts knowledge-transfer intensity based on our discovery of block-wise sensitivity variations. As a result, our framework brings billion-scale diffusion models into a range more suitable for on-device inference, while preserving the quality of the output images. Specifically, combined with INT4 weight quantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction (e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on server and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score and 7% in HPSv2 score compared to the original model. Finally, our comprehensive user study with 85 participants demonstrates that HierarchicalPrune maintains perceptual quality comparable to the original model while significantly outperforming prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art text-to-image diffusion models (DMs) achieve remarkable quality, yet their massive parameter scale (8-11B) poses significant challenges for inferences on resource-constrained devices. In this paper, we present HierarchicalPrune, a novel compression framework grounded in a key observation: DM blocks exhibit distinct functional hierarchies, where early blocks establish semantic structures while later blocks handle texture refinements. HierarchicalPrune synergistically combines three techniques: (1) Hierarchical Position Pruning, which identifies and removes less essential later blocks based on position hierarchy; (2) Positional Weight Preservation, which systematically protects early model portions that are essential for semantic structural integrity; and (3) Sensitivity-Guided Distillation, which adjusts knowledge-transfer intensity based on our discovery of block-wise sensitivity variations. As a result, our framework brings billion-scale diffusion models into a range more suitable for on-device inference, while preserving the quality of the output images. Specifically, combined with INT4 weight quantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction (e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on server and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score and 7% in HPSv2 score compared to the original model. Finally, our comprehensive user study with 85 participants demonstrates that HierarchicalPrune maintains perceptual quality comparable to the original model while significantly outperforming prior works."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-06T17:30:44Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    30,
                    44,
                    2,
                    218,
                    0
                ],
                "arxiv_comment": "Accepted at AAAI 2026 (Main Technical Track)",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Young D. Kwon"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Sijia Li"
                    },
                    {
                        "name": "Da Li"
                    },
                    {
                        "name": "Sourav Bhattacharya"
                    },
                    {
                        "name": "Stylianos I. Venieris"
                    }
                ],
                "author_detail": {
                    "name": "Stylianos I. Venieris"
                },
                "author": "Stylianos I. Venieris"
            },
            {
                "id": "http://arxiv.org/abs/2511.13421v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13421v1",
                "title": "Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression"
                },
                "updated": "2025-11-17T14:34:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    34,
                    3,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13421v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \\textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \\approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \\textit{i.e.}, $E(K, N) \\approx K$ for $K \\le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \\approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \\textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \\approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \\textit{i.e.}, $E(K, N) \\approx K$ for $K \\le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \\approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:34:03Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    34,
                    3,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Tingkai Yan"
                    },
                    {
                        "name": "Haodong Wen"
                    },
                    {
                        "name": "Binghui Li"
                    },
                    {
                        "name": "Kairong Luo"
                    },
                    {
                        "name": "Wenguang Chen"
                    },
                    {
                        "name": "Kaifeng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Kaifeng Lyu"
                },
                "author": "Kaifeng Lyu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13417v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13417v1",
                "title": "Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source"
                },
                "updated": "2025-11-17T14:30:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    30,
                    43,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13417v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:30:43Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    30,
                    43,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Mykola Lavreniuk"
                    },
                    {
                        "name": "Nataliia Kussul"
                    },
                    {
                        "name": "Andrii Shelestov"
                    },
                    {
                        "name": "Yevhenii Salii"
                    },
                    {
                        "name": "Volodymyr Kuzin"
                    },
                    {
                        "name": "Sergii Skakun"
                    },
                    {
                        "name": "Zoltan Szantoi"
                    }
                ],
                "author_detail": {
                    "name": "Zoltan Szantoi"
                },
                "author": "Zoltan Szantoi"
            },
            {
                "id": "http://arxiv.org/abs/2511.13410v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13410v1",
                "title": "Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction"
                },
                "updated": "2025-11-17T14:22:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    22,
                    32,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13410v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:22:32Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    22,
                    32,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026 (Oral)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhaopei Huang"
                    },
                    {
                        "name": "Qifeng Dai"
                    },
                    {
                        "name": "Guozheng Wu"
                    },
                    {
                        "name": "Xiaopeng Wu"
                    },
                    {
                        "name": "Kehan Chen"
                    },
                    {
                        "name": "Chuan Yu"
                    },
                    {
                        "name": "Xubin Li"
                    },
                    {
                        "name": "Tiezheng Ge"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Qin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Qin Jin"
                },
                "author": "Qin Jin"
            },
            {
                "id": "http://arxiv.org/abs/2412.00796v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.00796v2",
                "title": "Gaussian quasi-likelihood analysis for non-Gaussian linear mixed-effects model with system noise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian quasi-likelihood analysis for non-Gaussian linear mixed-effects model with system noise"
                },
                "updated": "2025-11-17T14:09:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    9,
                    0,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.00796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.00796v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We consider statistical inference for a class of mixed-effects models with system noise described by a non-Gaussian integrated Ornstein-Uhlenbeck process. Under the asymptotics where the number of individuals goes to infinity with possibly unbalanced sampling frequency across individuals, we prove some theoretical properties of the Gaussian quasi-likelihood function, followed by the asymptotic normality and the tail-probability estimate of the associated estimator. In addition to the joint inference, we propose and investigate the three-stage inference strategy, revealing that they are first-order equivalent while quantitatively different in the second-order terms. Numerical experiments are given to illustrate the theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider statistical inference for a class of mixed-effects models with system noise described by a non-Gaussian integrated Ornstein-Uhlenbeck process. Under the asymptotics where the number of individuals goes to infinity with possibly unbalanced sampling frequency across individuals, we prove some theoretical properties of the Gaussian quasi-likelihood function, followed by the asymptotic normality and the tail-probability estimate of the associated estimator. In addition to the joint inference, we propose and investigate the three-stage inference strategy, revealing that they are first-order equivalent while quantitatively different in the second-order terms. Numerical experiments are given to illustrate the theoretical results."
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-01T12:42:29Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    12,
                    42,
                    29,
                    6,
                    336,
                    0
                ],
                "arxiv_comment": "26 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "math.ST"
                },
                "authors": [
                    {
                        "name": "Takumi Imamura"
                    },
                    {
                        "name": "Hiroki Masuda"
                    }
                ],
                "author_detail": {
                    "name": "Hiroki Masuda"
                },
                "author": "Hiroki Masuda"
            },
            {
                "id": "http://arxiv.org/abs/2511.13394v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13394v1",
                "title": "Fast and Robust Simulation-Based Inference With Optimization Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Robust Simulation-Based Inference With Optimization Monte Carlo"
                },
                "updated": "2025-11-17T14:07:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    7,
                    36,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13394v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:07:36Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    7,
                    36,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Vasilis Gkolemis"
                    },
                    {
                        "name": "Christos Diou"
                    },
                    {
                        "name": "Michael Gutmann"
                    }
                ],
                "author_detail": {
                    "name": "Michael Gutmann"
                },
                "author": "Michael Gutmann"
            },
            {
                "id": "http://arxiv.org/abs/2511.13393v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13393v1",
                "title": "Learning Cosmology from Nearest Neighbour Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cosmology from Nearest Neighbour Statistics"
                },
                "updated": "2025-11-17T14:06:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    6,
                    53,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13393v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Extracting cosmological parameters from galaxy/halo catalogues with sub-percent level accuracy is an important aspect of modern cosmology, especially in view of ongoing and upcoming surveys such as Euclid, DESI, and LSST. While traditional two-point statistics have been known to be suboptimal for this task, recently proposed k-Nearest Neighbour (kNN) based summary statistics have demonstrated tighter constraining power. Building on the kNN statistics, we introduce a new field-level representation of discrete halo catalogues - NN distance maps. We employ this technique on the halo catalogues obtained from Quijote N-body simulation suites. By combining these maps with kNN-based summary statistics, we train a hybrid neural network to infer cosmological parameters, showing that the resulting constraints achieve state-of-the-art, if not the best, accuracy. In addition, our hybrid framework is 5-10 times more computationally efficient than some of the existing point-cloud-based ML methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting cosmological parameters from galaxy/halo catalogues with sub-percent level accuracy is an important aspect of modern cosmology, especially in view of ongoing and upcoming surveys such as Euclid, DESI, and LSST. While traditional two-point statistics have been known to be suboptimal for this task, recently proposed k-Nearest Neighbour (kNN) based summary statistics have demonstrated tighter constraining power. Building on the kNN statistics, we introduce a new field-level representation of discrete halo catalogues - NN distance maps. We employ this technique on the halo catalogues obtained from Quijote N-body simulation suites. By combining these maps with kNN-based summary statistics, we train a hybrid neural network to infer cosmological parameters, showing that the resulting constraints achieve state-of-the-art, if not the best, accuracy. In addition, our hybrid framework is 5-10 times more computationally efficient than some of the existing point-cloud-based ML methods."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:06:53Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    6,
                    53,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Submitted for publication to A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Atrideb Chatterjee"
                    },
                    {
                        "name": "Arka Banerjee"
                    },
                    {
                        "name": "Francisco Villaescusa-Navarro"
                    },
                    {
                        "name": "Tom Abel"
                    }
                ],
                "author_detail": {
                    "name": "Tom Abel"
                },
                "author": "Tom Abel"
            },
            {
                "id": "http://arxiv.org/abs/2505.23734v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.23734v4",
                "title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS"
                },
                "updated": "2025-11-17T14:03:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    3,
                    46,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.23734v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.23734v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a promising solution for novel view synthesis, enabling one-pass inference without the need for per-scene 3DGS optimization. However, their scalability is fundamentally constrained by the limited capacity of their models, leading to degraded performance or excessive memory consumption as the number of input views increases. In this work, we analyze feed-forward 3DGS frameworks through the lens of the Information Bottleneck principle and introduce ZPressor, a lightweight architecture-agnostic module that enables efficient compression of multi-view inputs into a compact latent state $Z$ that retains essential scene information while discarding redundancy. Concretely, ZPressor enables existing feed-forward 3DGS models to scale to over 100 input views at 480P resolution on an 80GB GPU, by partitioning the views into anchor and support sets and using cross attention to compress the information from the support views into anchor views, forming the compressed latent state $Z$. We show that integrating ZPressor into several state-of-the-art feed-forward 3DGS models consistently improves performance under moderate input views and enhances robustness under dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K. The video results, code and trained models are available on our project page: https://lhmd.top/zpressor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a promising solution for novel view synthesis, enabling one-pass inference without the need for per-scene 3DGS optimization. However, their scalability is fundamentally constrained by the limited capacity of their models, leading to degraded performance or excessive memory consumption as the number of input views increases. In this work, we analyze feed-forward 3DGS frameworks through the lens of the Information Bottleneck principle and introduce ZPressor, a lightweight architecture-agnostic module that enables efficient compression of multi-view inputs into a compact latent state $Z$ that retains essential scene information while discarding redundancy. Concretely, ZPressor enables existing feed-forward 3DGS models to scale to over 100 input views at 480P resolution on an 80GB GPU, by partitioning the views into anchor and support sets and using cross attention to compress the information from the support views into anchor views, forming the compressed latent state $Z$. We show that integrating ZPressor into several state-of-the-art feed-forward 3DGS models consistently improves performance under moderate input views and enhances robustness under dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K. The video results, code and trained models are available on our project page: https://lhmd.top/zpressor."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-29T17:57:04Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    57,
                    4,
                    3,
                    149,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025, Project Page: https://lhmd.top/zpressor, Code: https://github.com/ziplab/ZPressor",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Donny Y. Chen"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Duochao Shi"
                    },
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang"
            },
            {
                "id": "http://arxiv.org/abs/2511.13389v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13389v1",
                "title": "Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference"
                },
                "updated": "2025-11-17T14:00:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    0,
                    0,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13389v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:00:00Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    0,
                    0,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted by the Energy Informatics.Academy Conference 2025 (EI.A 2025)",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Zhipeng Ma"
                    },
                    {
                        "name": "Bo Nørregaard Jørgensen"
                    },
                    {
                        "name": "Zheng Grace Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Grace Ma"
                },
                "author": "Zheng Grace Ma"
            },
            {
                "id": "http://arxiv.org/abs/2503.09095v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2503.09095v2",
                "title": "Backdooring CLIP through Concept Confusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdooring CLIP through Concept Confusion"
                },
                "updated": "2025-11-17T13:57:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    57,
                    4,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2503.09095v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2503.09095v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Backdoor attacks pose a serious threat to deep learning models by allowing adversaries to implant hidden behaviors that remain dormant on clean inputs but are maliciously triggered at inference. Existing backdoor attack methods typically rely on explicit triggers such as image patches or pixel perturbations, which makes them easier to detect and limits their applicability in complex settings. To address this limitation, we take a different perspective by analyzing backdoor attacks through the lens of concept-level reasoning, drawing on insights from interpretable AI. We show that traditional attacks can be viewed as implicitly manipulating the concepts activated within a model's latent space. This motivates a natural question: can backdoors be built by directly manipulating concepts? To answer this, we propose the Concept Confusion Attack (CCA), a novel framework that designates human-understandable concepts as internal triggers, eliminating the need for explicit input modifications. By relabeling images that strongly exhibit a chosen concept and fine-tuning on this mixed dataset, CCA teaches the model to associate the concept itself with the attacker's target label. Consequently, the presence of the concept alone is sufficient to activate the backdoor, making the attack stealthier and more resistant to existing defenses. Using CLIP as a case study, we show that CCA achieves high attack success rates while preserving clean-task accuracy and evading state-of-the-art defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor attacks pose a serious threat to deep learning models by allowing adversaries to implant hidden behaviors that remain dormant on clean inputs but are maliciously triggered at inference. Existing backdoor attack methods typically rely on explicit triggers such as image patches or pixel perturbations, which makes them easier to detect and limits their applicability in complex settings. To address this limitation, we take a different perspective by analyzing backdoor attacks through the lens of concept-level reasoning, drawing on insights from interpretable AI. We show that traditional attacks can be viewed as implicitly manipulating the concepts activated within a model's latent space. This motivates a natural question: can backdoors be built by directly manipulating concepts? To answer this, we propose the Concept Confusion Attack (CCA), a novel framework that designates human-understandable concepts as internal triggers, eliminating the need for explicit input modifications. By relabeling images that strongly exhibit a chosen concept and fine-tuning on this mixed dataset, CCA teaches the model to associate the concept itself with the attacker's target label. Consequently, the presence of the concept alone is sufficient to activate the backdoor, making the attack stealthier and more resistant to existing defenses. Using CLIP as a case study, we show that CCA achieves high attack success rates while preserving clean-task accuracy and evading state-of-the-art defenses."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-03-12T06:17:12Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    6,
                    17,
                    12,
                    2,
                    71,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Lijie Hu"
                    },
                    {
                        "name": "Junchi Liao"
                    },
                    {
                        "name": "Weimin Lyu"
                    },
                    {
                        "name": "Shaopeng Fu"
                    },
                    {
                        "name": "Tianhao Huang"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang"
            },
            {
                "id": "http://arxiv.org/abs/2509.07864v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.07864v2",
                "title": "Tracing and Mitigating Hallucinations in Multimodal LLMs via Dynamic Attention Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing and Mitigating Hallucinations in Multimodal LLMs via Dynamic Attention Localization"
                },
                "updated": "2025-11-17T13:57:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    57,
                    4,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.07864v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal Large Language Models (MLLMs) achieve strong performance on tasks like image captioning and visual question answering, but remain prone to hallucinations, where generated text conflicts with the visual input. Prior work links this partly to insufficient visual attention, but existing attention-based detectors and mitigation typically apply uniform adjustments across layers and heads, obscuring where errors originate. In this paper, we first show these methods fail to accurately localize problematic layers. Then, we introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags anomalous layers, and Image Attention Focus (IAF) which scores attention heads within those layers. Analysis shows that LIAE pinpoints faulty layers and IAF reliably ranks heads that warrant correction. Guided by these signals, we propose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a task-agnostic, attention-guided method that dynamically localizes and corrects errors during inference with negligible overhead. Furthermore, by establishing a connection between D-LEAF and DPO, we provide theoretical justification for the effectiveness of D-LEAF. Results show our D-LEAF delivers a 53\\% relative improvement on standard captioning benchmarks, and on VQA both accuracy and F1-score improve by approximately 4\\%, substantially suppressing hallucinations while preserving efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) achieve strong performance on tasks like image captioning and visual question answering, but remain prone to hallucinations, where generated text conflicts with the visual input. Prior work links this partly to insufficient visual attention, but existing attention-based detectors and mitigation typically apply uniform adjustments across layers and heads, obscuring where errors originate. In this paper, we first show these methods fail to accurately localize problematic layers. Then, we introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags anomalous layers, and Image Attention Focus (IAF) which scores attention heads within those layers. Analysis shows that LIAE pinpoints faulty layers and IAF reliably ranks heads that warrant correction. Guided by these signals, we propose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a task-agnostic, attention-guided method that dynamically localizes and corrects errors during inference with negligible overhead. Furthermore, by establishing a connection between D-LEAF and DPO, we provide theoretical justification for the effectiveness of D-LEAF. Results show our D-LEAF delivers a 53\\% relative improvement on standard captioning benchmarks, and on VQA both accuracy and F1-score improve by approximately 4\\%, substantially suppressing hallucinations while preserving efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-09T15:51:15Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    51,
                    15,
                    1,
                    252,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tiancheng Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Jiaye Lin"
                    },
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Lijie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Hu"
                },
                "author": "Lijie Hu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13381v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13381v1",
                "title": "Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts"
                },
                "updated": "2025-11-17T13:54:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    54,
                    0,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13381v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:54:00Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    54,
                    0,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Siyu Zhu"
                    },
                    {
                        "name": "Mouxiao Bian"
                    },
                    {
                        "name": "Yue Xie"
                    },
                    {
                        "name": "Yongyu Tang"
                    },
                    {
                        "name": "Zhikang Yu"
                    },
                    {
                        "name": "Tianbin Li"
                    },
                    {
                        "name": "Pengcheng Chen"
                    },
                    {
                        "name": "Bing Han"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Xiaoyan Dong"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyan Dong"
                },
                "author": "Xiaoyan Dong"
            },
            {
                "id": "http://arxiv.org/abs/2511.13373v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13373v1",
                "title": "A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs"
                },
                "updated": "2025-11-17T13:47:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    47,
                    27,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13373v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:47:27Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    47,
                    27,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Prakrit Timilsina"
                    },
                    {
                        "name": "Anuj Nepal"
                    },
                    {
                        "name": "Rajan Kadel"
                    },
                    {
                        "name": "Robin Doss"
                    }
                ],
                "author_detail": {
                    "name": "Robin Doss"
                },
                "author": "Robin Doss"
            },
            {
                "id": "http://arxiv.org/abs/2511.07046v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.07046v3",
                "title": "Learning Quantized Continuous Controllers for Integer Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Quantized Continuous Controllers for Integer Hardware"
                },
                "updated": "2025-11-17T13:47:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    47,
                    4,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.07046v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.07046v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deploying continuous-control reinforcement learning policies on embedded hardware requires meeting tight latency and power budgets. Small FPGAs can deliver these, but only if costly floating point pipelines are avoided. We study quantization-aware training (QAT) of policies for integer inference and we present a learning-to-hardware pipeline that automatically selects low-bit policies and synthesizes them to an Artix-7 FPGA. Across five MuJoCo tasks, we obtain policy networks that are competitive with full precision (FP32) policies but require as few as 3 or even only 2 bits per weight, and per internal activation value, as long as input precision is chosen carefully. On the target hardware, the selected policies achieve inference latencies on the order of microseconds and consume microjoules per action, favorably comparing to a quantized reference. Last, we observe that the quantized policies exhibit increased input noise robustness compared to the floating-point baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying continuous-control reinforcement learning policies on embedded hardware requires meeting tight latency and power budgets. Small FPGAs can deliver these, but only if costly floating point pipelines are avoided. We study quantization-aware training (QAT) of policies for integer inference and we present a learning-to-hardware pipeline that automatically selects low-bit policies and synthesizes them to an Artix-7 FPGA. Across five MuJoCo tasks, we obtain policy networks that are competitive with full precision (FP32) policies but require as few as 3 or even only 2 bits per weight, and per internal activation value, as long as input precision is chosen carefully. On the target hardware, the selected policies achieve inference latencies on the order of microseconds and consume microjoules per action, favorably comparing to a quantized reference. Last, we observe that the quantized policies exhibit increased input noise robustness compared to the floating-point baseline."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T12:39:14Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    12,
                    39,
                    14,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "17 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Fabian Kresse"
                    },
                    {
                        "name": "Christoph H. Lampert"
                    }
                ],
                "author_detail": {
                    "name": "Christoph H. Lampert"
                },
                "author": "Christoph H. Lampert"
            },
            {
                "id": "http://arxiv.org/abs/2511.13371v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13371v1",
                "title": "Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning"
                },
                "updated": "2025-11-17T13:46:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    46,
                    19,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13371v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:46:19Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    46,
                    19,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Caroline Baumgartner"
                    },
                    {
                        "name": "Eleanor Spens"
                    },
                    {
                        "name": "Neil Burgess"
                    },
                    {
                        "name": "Petru Manescu"
                    }
                ],
                "author_detail": {
                    "name": "Petru Manescu"
                },
                "author": "Petru Manescu"
            },
            {
                "id": "http://arxiv.org/abs/2510.04108v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.04108v2",
                "title": "Can Linear Probes Measure LLM Uncertainty?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Linear Probes Measure LLM Uncertainty?"
                },
                "updated": "2025-11-17T13:43:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    43,
                    43,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.04108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.04108v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Effective Uncertainty Quantification (UQ) represents a key aspect for reliable deployment of Large Language Models (LLMs) in automated decision-making and beyond. Yet, for LLM generation with multiple choice structure, the state-of-the-art in UQ is still dominated by the naive baseline given by the maximum softmax score. To address this shortcoming, we demonstrate that taking a principled approach via Bayesian statistics leads to improved performance despite leveraging the simplest possible model, namely linear regression. More precisely, we propose to train multiple Bayesian linear models, each predicting the output of a layer given the output of the previous one. Based on the obtained layer-level posterior distributions, we infer the global uncertainty level of the LLM by identifying a sparse combination of distributional features, leading to an efficient UQ scheme. Numerical experiments on various LLMs show consistent improvement over state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Uncertainty Quantification (UQ) represents a key aspect for reliable deployment of Large Language Models (LLMs) in automated decision-making and beyond. Yet, for LLM generation with multiple choice structure, the state-of-the-art in UQ is still dominated by the naive baseline given by the maximum softmax score. To address this shortcoming, we demonstrate that taking a principled approach via Bayesian statistics leads to improved performance despite leveraging the simplest possible model, namely linear regression. More precisely, we propose to train multiple Bayesian linear models, each predicting the output of a layer given the output of the previous one. Based on the obtained layer-level posterior distributions, we infer the global uncertainty level of the LLM by identifying a sparse combination of distributional features, leading to an efficient UQ scheme. Numerical experiments on various LLMs show consistent improvement over state-of-the-art baselines."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-05T09:14:57Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    9,
                    14,
                    57,
                    6,
                    278,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ramzi Dakhmouche"
                    },
                    {
                        "name": "Adrien Letellier"
                    },
                    {
                        "name": "Hossein Gorji"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Gorji"
                },
                "author": "Hossein Gorji"
            },
            {
                "id": "http://arxiv.org/abs/2511.13369v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13369v1",
                "title": "Unifying points of interest taxonomies: mapping OpenStreetMap tags to the Foursquare category system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying points of interest taxonomies: mapping OpenStreetMap tags to the Foursquare category system"
                },
                "updated": "2025-11-17T13:43:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    43,
                    25,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13369v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The heterogeneity of Point of Interest (POI) taxonomies is a persistent challenge for the integration of urban datasets and the development of location-based services. OpenStreetMap (OSM) adopts a flexible, community-driven tagging system, while Foursquare (FS) relies on a curated hierarchical structure. Here we present an openly available benchmark and mapping framework that aligns OSM tags with the FS taxonomy. This resource integrates the richness of community-driven OSM data with the hierarchical structure of FS, enabling reproducible and interoperable urban analytics. The dataset is complemented by an evaluation of embedding and LLM-based alignment strategies and a pipeline that supports scalable updates as OSM evolves. Together, these elements provide both a robust reference resource and a practical tool for the community. Our approach is structured around three components: the construction of a manually curated benchmark as a gold standard, the evaluation of pretrained text embedding models for semantic alignment between OSM tags and FS categories, and an LLM-based refinement stage that enhances robustness and adaptability. The proposed methodology provides a scalable and reproducible solution for taxonomy unification, with direct applications to urban analytics, mobility studies, and smart city services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The heterogeneity of Point of Interest (POI) taxonomies is a persistent challenge for the integration of urban datasets and the development of location-based services. OpenStreetMap (OSM) adopts a flexible, community-driven tagging system, while Foursquare (FS) relies on a curated hierarchical structure. Here we present an openly available benchmark and mapping framework that aligns OSM tags with the FS taxonomy. This resource integrates the richness of community-driven OSM data with the hierarchical structure of FS, enabling reproducible and interoperable urban analytics. The dataset is complemented by an evaluation of embedding and LLM-based alignment strategies and a pipeline that supports scalable updates as OSM evolves. Together, these elements provide both a robust reference resource and a practical tool for the community. Our approach is structured around three components: the construction of a manually curated benchmark as a gold standard, the evaluation of pretrained text embedding models for semantic alignment between OSM tags and FS categories, and an LLM-based refinement stage that enhances robustness and adaptability. The proposed methodology provides a scalable and reproducible solution for taxonomy unification, with direct applications to urban analytics, mobility studies, and smart city services."
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:43:25Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    43,
                    25,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI"
                },
                "authors": [
                    {
                        "name": "Lilou Soulas"
                    },
                    {
                        "name": "Lorenzo Lucchini"
                    },
                    {
                        "name": "Maurizio Napolitano"
                    },
                    {
                        "name": "Sebastiano Bontorin"
                    },
                    {
                        "name": "Simone Centellegher"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Riccardo Gallotti"
                    },
                    {
                        "name": "Eleonora Andreotti"
                    }
                ],
                "author_detail": {
                    "name": "Eleonora Andreotti"
                },
                "author": "Eleonora Andreotti"
            },
            {
                "id": "http://arxiv.org/abs/2502.11928v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.11928v2",
                "title": "Exploring the BSM parameter space with Neural Network aided Simulation-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the BSM parameter space with Neural Network aided Simulation-Based Inference"
                },
                "updated": "2025-11-17T13:42:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    42,
                    20,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.11928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.11928v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Some of the issues that make sampling parameter spaces of various beyond the Standard Model (BSM) scenarios computationally expensive are the high dimensionality of the input parameter space, complex likelihoods, and stringent experimental constraints. In this work, we explore likelihood-free approaches, leveraging neural network-aided Simulation-Based Inference (SBI) to alleviate this issue. We focus on three amortized SBI methods: Neural Posterior Estimation (NPE), Neural Likelihood Estimation (NLE), and Neural Ratio Estimation (NRE) and perform a comparative analysis through the validation test known as the \\textit{ Test of Accuracy with Random Points} (TARP), as well as through posterior sample efficiency and computational time. As an example, we focus on the scalar sector of the phenomenological minimal supersymmetric SM (pMSSM) and observe that the NPE method outperforms the others and generates correct posterior distributions of the parameters with a minimal number of samples. The efficacy of this framework is tested on 5 parameter pMSSM with Higgs and flavor physics data and its performance is compared with the MCMC method. We further add dark matter (DM) observables to make the task more challenging and consider a 9 parameter pMSSM. We observe that even though the efficiency factor drops, the amortized SBI method still produces faithful posterior distributions. SBI predicted points satisfying DM constraints are mostly bino-dominated upto $\\sim$ 1.5 TeV, and are mostly wino-dominated within the 1.5 - 2 TeV range.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some of the issues that make sampling parameter spaces of various beyond the Standard Model (BSM) scenarios computationally expensive are the high dimensionality of the input parameter space, complex likelihoods, and stringent experimental constraints. In this work, we explore likelihood-free approaches, leveraging neural network-aided Simulation-Based Inference (SBI) to alleviate this issue. We focus on three amortized SBI methods: Neural Posterior Estimation (NPE), Neural Likelihood Estimation (NLE), and Neural Ratio Estimation (NRE) and perform a comparative analysis through the validation test known as the \\textit{ Test of Accuracy with Random Points} (TARP), as well as through posterior sample efficiency and computational time. As an example, we focus on the scalar sector of the phenomenological minimal supersymmetric SM (pMSSM) and observe that the NPE method outperforms the others and generates correct posterior distributions of the parameters with a minimal number of samples. The efficacy of this framework is tested on 5 parameter pMSSM with Higgs and flavor physics data and its performance is compared with the MCMC method. We further add dark matter (DM) observables to make the task more challenging and consider a 9 parameter pMSSM. We observe that even though the efficiency factor drops, the amortized SBI method still produces faithful posterior distributions. SBI predicted points satisfying DM constraints are mostly bino-dominated upto $\\sim$ 1.5 TeV, and are mostly wino-dominated within the 1.5 - 2 TeV range."
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-17T15:41:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    15,
                    41,
                    25,
                    0,
                    48,
                    0
                ],
                "arxiv_comment": "Accepted by JHEP for publication",
                "arxiv_primary_category": {
                    "term": "hep-ph"
                },
                "authors": [
                    {
                        "name": "Atrideb Chatterjee"
                    },
                    {
                        "name": "Arghya Choudhury"
                    },
                    {
                        "name": "Sourav Mitra"
                    },
                    {
                        "name": "Arpita Mondal"
                    },
                    {
                        "name": "Subhadeep Mondal"
                    }
                ],
                "author_detail": {
                    "name": "Subhadeep Mondal"
                },
                "author": "Subhadeep Mondal"
            },
            {
                "id": "http://arxiv.org/abs/2511.13368v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13368v1",
                "title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning"
                },
                "updated": "2025-11-17T13:41:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    41,
                    31,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13368v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:41:31Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    41,
                    31,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kajetan Dymkiewicz"
                    },
                    {
                        "name": "Ivan Vulic"
                    },
                    {
                        "name": "Helen Yannakoudakis"
                    },
                    {
                        "name": "Eilam Shapira"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Anna Korhonen"
                    }
                ],
                "author_detail": {
                    "name": "Anna Korhonen"
                },
                "author": "Anna Korhonen"
            },
            {
                "id": "http://arxiv.org/abs/2309.06706v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2309.06706v3",
                "title": "Simultaneous Machine Translation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Machine Translation with Large Language Models"
                },
                "updated": "2025-11-17T13:41:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    41,
                    30,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2309.06706v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2309.06706v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Real-world simultaneous machine translation (SimulMT) systems face more challenges than just the quality-latency trade-off. They also need to address issues related to robustness with noisy input, processing long contexts, and flexibility for knowledge injection. These challenges demand models with strong language understanding and generation capabilities which may not often equipped by dedicated MT models. In this paper, we investigate the possibility of applying Large Language Models (LLM) to SimulMT tasks by using existing incremental-decoding methods with a newly proposed RALCP algorithm for latency reduction. We conducted experiments using the \\texttt{Llama2-7b-chat} model on nine different languages from the MUST-C dataset. The results show that LLM outperforms dedicated MT models in terms of BLEU and LAAL metrics. Further analysis indicates that LLM has advantages in terms of tuning efficiency and robustness. However, it is important to note that the computational cost of LLM remains a significant obstacle to its application in SimulMT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world simultaneous machine translation (SimulMT) systems face more challenges than just the quality-latency trade-off. They also need to address issues related to robustness with noisy input, processing long contexts, and flexibility for knowledge injection. These challenges demand models with strong language understanding and generation capabilities which may not often equipped by dedicated MT models. In this paper, we investigate the possibility of applying Large Language Models (LLM) to SimulMT tasks by using existing incremental-decoding methods with a newly proposed RALCP algorithm for latency reduction. We conducted experiments using the \\texttt{Llama2-7b-chat} model on nine different languages from the MUST-C dataset. The results show that LLM outperforms dedicated MT models in terms of BLEU and LAAL metrics. Further analysis indicates that LLM has advantages in terms of tuning efficiency and robustness. However, it is important to note that the computational cost of LLM remains a significant obstacle to its application in SimulMT."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-09-13T04:06:47Z",
                "published_parsed": [
                    2023,
                    9,
                    13,
                    4,
                    6,
                    47,
                    2,
                    256,
                    0
                ],
                "arxiv_comment": "Accepted to ALTA 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Jinming Zhao"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Fatemeh Shiri"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari"
            },
            {
                "id": "http://arxiv.org/abs/2511.13366v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13366v1",
                "title": "Local asymptotic normality for discretely observed McKean-Vlasov diffusions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local asymptotic normality for discretely observed McKean-Vlasov diffusions"
                },
                "updated": "2025-11-17T13:41:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    41,
                    16,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13366v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study the local asymptotic normality (LAN) property for the likelihood function associated with discretely observed $d$-dimensional McKean-Vlasov stochastic differential equations over a fixed time interval. The model involves a joint parameter in both the drift and diffusion coefficients, introducing challenges due to its dependence on the process distribution. We derive a stochastic expansion of the log-likelihood ratio using Malliavin calculus techniques and establish the LAN property under appropriate conditions. The main technical challenge arises from the implicit nature of the transition densities, which we address through integration by parts and Gaussian-type bounds. This work extends existing LAN results for interacting particle systems to the mean-field regime, contributing to statistical inference in non-linear stochastic models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the local asymptotic normality (LAN) property for the likelihood function associated with discretely observed $d$-dimensional McKean-Vlasov stochastic differential equations over a fixed time interval. The model involves a joint parameter in both the drift and diffusion coefficients, introducing challenges due to its dependence on the process distribution. We derive a stochastic expansion of the log-likelihood ratio using Malliavin calculus techniques and establish the LAN property under appropriate conditions. The main technical challenge arises from the implicit nature of the transition densities, which we address through integration by parts and Gaussian-type bounds. This work extends existing LAN results for interacting particle systems to the mean-field regime, contributing to statistical inference in non-linear stochastic models"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:41:16Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    41,
                    16,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.ST"
                },
                "authors": [
                    {
                        "name": "Akram Heidari"
                    },
                    {
                        "name": "Mark Podolskij"
                    }
                ],
                "author_detail": {
                    "name": "Mark Podolskij"
                },
                "author": "Mark Podolskij"
            },
            {
                "id": "http://arxiv.org/abs/2508.03294v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.03294v2",
                "title": "NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty"
                },
                "updated": "2025-11-17T13:37:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    37,
                    20,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.03294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.03294v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Estimating the difficulty of exam questions is essential for developing good exams, but professors are not always good at this task. We compare various Large Language Model-based methods with three professors in their ability to estimate what percentage of students will give correct answers on True/False exam questions in the areas of Neural Networks and Machine Learning. Our results show that the professors have limited ability to distinguish between easy and difficult questions and that they are outperformed by directly asking Gemini 2.5 to solve this task. Yet, we obtained even better results using uncertainties of the LLMs solving the questions in a supervised learning setting, using only 42 training samples. We conclude that supervised learning using LLM uncertainty can help professors better estimate the difficulty of exam questions, improving the quality of assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the difficulty of exam questions is essential for developing good exams, but professors are not always good at this task. We compare various Large Language Model-based methods with three professors in their ability to estimate what percentage of students will give correct answers on True/False exam questions in the areas of Neural Networks and Machine Learning. Our results show that the professors have limited ability to distinguish between easy and difficult questions and that they are outperformed by directly asking Gemini 2.5 to solve this task. Yet, we obtained even better results using uncertainties of the LLMs solving the questions in a supervised learning setting, using only 42 training samples. We conclude that supervised learning using LLM uncertainty can help professors better estimate the difficulty of exam questions, improving the quality of assessment."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-05T10:12:38Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    10,
                    12,
                    38,
                    1,
                    217,
                    0
                ],
                "arxiv_comment": "10 pages, 2 figures, presented at ECAI 2025 at the 2nd International Workshop on AI in Society, Education and Educational Research (AISEER)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Leonidas Zotos"
                    },
                    {
                        "name": "Ivo Pascal de Jong"
                    },
                    {
                        "name": "Matias Valdenegro-Toro"
                    },
                    {
                        "name": "Andreea Ioana Sburlea"
                    },
                    {
                        "name": "Malvina Nissim"
                    },
                    {
                        "name": "Hedderik van Rijn"
                    }
                ],
                "author_detail": {
                    "name": "Hedderik van Rijn"
                },
                "author": "Hedderik van Rijn"
            },
            {
                "id": "http://arxiv.org/abs/2511.13365v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13365v1",
                "title": "InfoDecom: Decomposing Information for Defending against Privacy Leakage in Split Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfoDecom: Decomposing Information for Defending against Privacy Leakage in Split Inference"
                },
                "updated": "2025-11-17T13:36:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    36,
                    40,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13365v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Split inference (SI) enables users to access deep learning (DL) services without directly transmitting raw data. However, recent studies reveal that data reconstruction attacks (DRAs) can recover the original inputs from the smashed data sent from the client to the server, leading to significant privacy leakage. While various defenses have been proposed, they often result in substantial utility degradation, particularly when the client-side model is shallow. We identify a key cause of this trade-off: existing defenses apply excessive perturbation to redundant information in the smashed data. To address this issue in computer vision tasks, we propose InfoDecom, a defense framework that first decomposes and removes redundant information and then injects noise calibrated to provide theoretically guaranteed privacy. Experiments demonstrate that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines. The code and the appendix are available at https://github.com/SASA-cloud/InfoDecom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split inference (SI) enables users to access deep learning (DL) services without directly transmitting raw data. However, recent studies reveal that data reconstruction attacks (DRAs) can recover the original inputs from the smashed data sent from the client to the server, leading to significant privacy leakage. While various defenses have been proposed, they often result in substantial utility degradation, particularly when the client-side model is shallow. We identify a key cause of this trade-off: existing defenses apply excessive perturbation to redundant information in the smashed data. To address this issue in computer vision tasks, we propose InfoDecom, a defense framework that first decomposes and removes redundant information and then injects noise calibrated to provide theoretically guaranteed privacy. Experiments demonstrate that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines. The code and the appendix are available at https://github.com/SASA-cloud/InfoDecom."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:36:40Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    36,
                    40,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Ruijun Deng"
                    },
                    {
                        "name": "Zhihui Lu"
                    },
                    {
                        "name": "Qiang Duan"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Duan"
                },
                "author": "Qiang Duan"
            },
            {
                "id": "http://arxiv.org/abs/2402.10552v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2402.10552v4",
                "title": "Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models"
                },
                "updated": "2025-11-17T13:33:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    33,
                    37,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2402.10552v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2402.10552v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.18653/v1/2025.iwslt-1.8",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Simultaneous machine translation (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency. In this paper, we propose a conversational SimulMT framework to enhance the inference efficiency of LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of LLM in translation quality while achieving comparable computational latency to specialized SimulMT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous machine translation (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency. In this paper, we propose a conversational SimulMT framework to enhance the inference efficiency of LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of LLM in translation quality while achieving comparable computational latency to specialized SimulMT models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-02-16T10:32:16Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    10,
                    32,
                    16,
                    4,
                    47,
                    0
                ],
                "arxiv_comment": "Accepted to IWSLT 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "arxiv_doi": "10.18653/v1/2025.iwslt-1.8"
            },
            {
                "id": "http://arxiv.org/abs/2511.13361v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13361v1",
                "title": "MedDCR: Learning to Design Agentic Workflows for Medical Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedDCR: Learning to Design Agentic Workflows for Medical Coding"
                },
                "updated": "2025-11-17T13:30:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    30,
                    51,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13361v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:30:51Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    30,
                    51,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jiyang Zheng"
                    },
                    {
                        "name": "Islam Nassar"
                    },
                    {
                        "name": "Thanh Vu"
                    },
                    {
                        "name": "Xu Zhong"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Long Duong"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuan-Fang Li"
                },
                "author": "Yuan-Fang Li"
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.13717v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13717v1",
                "title": "TZ-LLM: Protecting On-Device Large Language Models with Arm TrustZone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TZ-LLM: Protecting On-Device Large Language Models with Arm TrustZone"
                },
                "updated": "2025-11-17T18:59:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    59,
                    20,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13717v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) deployed on mobile devices offer benefits like user privacy and reduced network latency, but introduce a significant security risk: the leakage of proprietary models to end users.\n  To mitigate this risk, we propose a system design for protecting on-device LLMs using Arm Trusted Execution Environment (TEE), TrustZone. Our system addresses two primary challenges: (1) The dilemma between memory efficiency and fast inference (caching model parameters within TEE memory). (2) The lack of efficient and secure Neural Processing Unit (NPU) time-sharing between Rich Execution Environment (REE) and TEE.\n  Our approach incorporates two key innovations. First, we employ pipelined restoration, leveraging the deterministic memory access patterns of LLM inference to prefetch parameters on demand, hiding memory allocation, I/O and decryption latency under computation time. Second, we introduce a co-driver design, creating a minimal data plane NPU driver in the TEE that collaborates with the full-fledged REE driver. This reduces the TEE TCB size and eliminates control plane reinitialization overhead during NPU world switches.\n  We implemented our system on the emerging OpenHarmony OS and the llama.cpp inference framework, and evaluated it with various LLMs on an Arm Rockchip device. Compared to a strawman TEE baseline lacking our optimizations, our system reduces TTFT by up to 90.9% and increases decoding speed by up to 23.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) deployed on mobile devices offer benefits like user privacy and reduced network latency, but introduce a significant security risk: the leakage of proprietary models to end users.\n  To mitigate this risk, we propose a system design for protecting on-device LLMs using Arm Trusted Execution Environment (TEE), TrustZone. Our system addresses two primary challenges: (1) The dilemma between memory efficiency and fast inference (caching model parameters within TEE memory). (2) The lack of efficient and secure Neural Processing Unit (NPU) time-sharing between Rich Execution Environment (REE) and TEE.\n  Our approach incorporates two key innovations. First, we employ pipelined restoration, leveraging the deterministic memory access patterns of LLM inference to prefetch parameters on demand, hiding memory allocation, I/O and decryption latency under computation time. Second, we introduce a co-driver design, creating a minimal data plane NPU driver in the TEE that collaborates with the full-fledged REE driver. This reduces the TEE TCB size and eliminates control plane reinitialization overhead during NPU world switches.\n  We implemented our system on the emerging OpenHarmony OS and the llama.cpp inference framework, and evaluated it with various LLMs on an Arm Rockchip device. Compared to a strawman TEE baseline lacking our optimizations, our system reduces TTFT by up to 90.9% and increases decoding speed by up to 23.2%."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:59:20Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    59,
                    20,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Xunjie Wang"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Zhichao Hua"
                    },
                    {
                        "name": "Jinyu Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinyu Gu"
                },
                "author": "Jinyu Gu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13704v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13704v1",
                "title": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models"
                },
                "updated": "2025-11-17T18:52:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    52,
                    44,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13704v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:52:44Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    52,
                    44,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Project: https://haroldchen19.github.io/TiViBench-Page/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Harold Haodong Chen"
                    },
                    {
                        "name": "Disen Lan"
                    },
                    {
                        "name": "Wen-Jie Shu"
                    },
                    {
                        "name": "Qingyang Liu"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Sirui Chen"
                    },
                    {
                        "name": "Wenkai Cheng"
                    },
                    {
                        "name": "Kanghao Chen"
                    },
                    {
                        "name": "Hongfei Zhang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Rongjin Guo"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Cong Chen"
                },
                "author": "Ying-Cong Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.13703v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13703v1",
                "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations"
                },
                "updated": "2025-11-17T18:52:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    52,
                    22,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13703v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:52:22Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    52,
                    22,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Lavender Y. Jiang"
                    },
                    {
                        "name": "Angelica Chen"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Xujin Chris Liu"
                    },
                    {
                        "name": "Radhika Dua"
                    },
                    {
                        "name": "Kevin Eaton"
                    },
                    {
                        "name": "Frederick Wolff"
                    },
                    {
                        "name": "Robert Steele"
                    },
                    {
                        "name": "Jeff Zhang"
                    },
                    {
                        "name": "Anton Alyakin"
                    },
                    {
                        "name": "Qingkai Pan"
                    },
                    {
                        "name": "Yanbing Chen"
                    },
                    {
                        "name": "Karl L. Sangwon"
                    },
                    {
                        "name": "Daniel A. Alber"
                    },
                    {
                        "name": "Jaden Stryker"
                    },
                    {
                        "name": "Jin Vivian Lee"
                    },
                    {
                        "name": "Yindalon Aphinyanaphongs"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    },
                    {
                        "name": "Eric Karl Oermann"
                    }
                ],
                "author_detail": {
                    "name": "Eric Karl Oermann"
                },
                "author": "Eric Karl Oermann"
            },
            {
                "id": "http://arxiv.org/abs/2511.13689v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13689v1",
                "title": "Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation"
                },
                "updated": "2025-11-17T18:41:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    41,
                    16,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13689v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:41:16Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    41,
                    16,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Sofia Jamil"
                    },
                    {
                        "name": "Kotla Sai Charan"
                    },
                    {
                        "name": "Sriparna Saha"
                    },
                    {
                        "name": "Koustava Goswami"
                    },
                    {
                        "name": "Joseph K J"
                    }
                ],
                "author_detail": {
                    "name": "Joseph K J"
                },
                "author": "Joseph K J"
            },
            {
                "id": "http://arxiv.org/abs/2510.03898v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.03898v2",
                "title": "Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles"
                },
                "updated": "2025-11-17T18:39:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    39,
                    10,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.03898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.03898v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Detecting media bias is crucial, specifically in the South Asian region. Despite this, annotated datasets and computational studies for Bangla political bias research remain scarce. Crucially because, political stance detection in Bangla news requires understanding of linguistic cues, cultural context, subtle biases, rhetorical strategies, code-switching, implicit sentiment, and socio-political background. To address this, we introduce the first benchmark dataset of 200 politically significant and highly debated Bangla news articles, labeled for government-leaning, government-critique, and neutral stances, alongside diagnostic analyses for evaluating large language models (LLMs). Our comprehensive evaluation of 28 proprietary and open-source LLMs shows strong performance in detecting government-critique content (F1 up to 0.83) but substantial difficulty with neutral articles (F1 as low as 0.00). Models also tend to over-predict government-leaning stances, often misinterpreting ambiguous narratives. This dataset and its associated diagnostics provide a foundation for advancing stance detection in Bangla media research and offer insights for improving LLM performance in low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting media bias is crucial, specifically in the South Asian region. Despite this, annotated datasets and computational studies for Bangla political bias research remain scarce. Crucially because, political stance detection in Bangla news requires understanding of linguistic cues, cultural context, subtle biases, rhetorical strategies, code-switching, implicit sentiment, and socio-political background. To address this, we introduce the first benchmark dataset of 200 politically significant and highly debated Bangla news articles, labeled for government-leaning, government-critique, and neutral stances, alongside diagnostic analyses for evaluating large language models (LLMs). Our comprehensive evaluation of 28 proprietary and open-source LLMs shows strong performance in detecting government-critique content (F1 up to 0.83) but substantial difficulty with neutral articles (F1 as low as 0.00). Models also tend to over-predict government-leaning stances, often misinterpreting ambiguous narratives. This dataset and its associated diagnostics provide a foundation for advancing stance detection in Bangla media research and offer insights for improving LLM performance in low-resource languages."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-04T18:34:34Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    18,
                    34,
                    34,
                    5,
                    277,
                    0
                ],
                "arxiv_comment": "Accepted to BLP at AACL-IJCNLP 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Nusrat Jahan Lia"
                    },
                    {
                        "name": "Shubhashis Roy Dipta"
                    },
                    {
                        "name": "Abdullah Khan Zehady"
                    },
                    {
                        "name": "Naymul Islam"
                    },
                    {
                        "name": "Madhusodan Chakraborty"
                    },
                    {
                        "name": "Abdullah Al Wasif"
                    }
                ],
                "author_detail": {
                    "name": "Abdullah Al Wasif"
                },
                "author": "Abdullah Al Wasif"
            },
            {
                "id": "http://arxiv.org/abs/2511.13676v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13676v1",
                "title": "T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization"
                },
                "updated": "2025-11-17T18:32:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    32,
                    3,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13676v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:32:03Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    32,
                    3,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted to DATE 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Hyunwoo Oh"
                    },
                    {
                        "name": "KyungIn Nam"
                    },
                    {
                        "name": "Rajat Bhattacharjya"
                    },
                    {
                        "name": "Hanning Chen"
                    },
                    {
                        "name": "Tamoghno Das"
                    },
                    {
                        "name": "Sanggeon Yun"
                    },
                    {
                        "name": "Suyeon Jang"
                    },
                    {
                        "name": "Andrew Ding"
                    },
                    {
                        "name": "Nikil Dutt"
                    },
                    {
                        "name": "Mohsen Imani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Imani"
                },
                "author": "Mohsen Imani"
            },
            {
                "id": "http://arxiv.org/abs/2406.18966v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2406.18966v5",
                "title": "DataGen: Unified Synthetic Dataset Generation via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataGen: Unified Synthetic Dataset Generation via Large Language Models"
                },
                "updated": "2025-11-17T18:22:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    22,
                    49,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2406.18966v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2406.18966v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents DataGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. DataGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, DataGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by DataGen, and each module within DataGen plays a critical role in this enhancement. Additionally, DataGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that DataGen effectively supports dynamic and evolving benchmarking and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents DataGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. DataGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, DataGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by DataGen, and each module within DataGen plays a critical role in this enhancement. Additionally, DataGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that DataGen effectively supports dynamic and evolving benchmarking and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-06-27T07:56:44Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    7,
                    56,
                    44,
                    3,
                    179,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.13663v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13663v1",
                "title": "Cost-Driven Synthesis of Sound Abstract Interpreters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Driven Synthesis of Sound Abstract Interpreters"
                },
                "updated": "2025-11-17T18:16:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    16,
                    36,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13663v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:16:36Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    16,
                    36,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "37 pages, 20 figures",
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Qiuhan Gu"
                    },
                    {
                        "name": "Avaljot Singh"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh"
            },
            {
                "id": "http://arxiv.org/abs/2511.13658v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13658v1",
                "title": "Why is \"Chicago\" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why is \"Chicago\" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues"
                },
                "updated": "2025-11-17T18:15:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    15,
                    13,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13658v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:15:13Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    15,
                    13,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jiaming Qu"
                    },
                    {
                        "name": "Mengtian Guo"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.13654v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13654v1",
                "title": "Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning"
                },
                "updated": "2025-11-17T18:03:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    3,
                    49,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13654v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T18:03:49Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    18,
                    3,
                    49,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "To appear in the Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Pascal Zimmer"
                    },
                    {
                        "name": "Ghassan Karame"
                    }
                ],
                "author_detail": {
                    "name": "Ghassan Karame"
                },
                "author": "Ghassan Karame"
            },
            {
                "id": "http://arxiv.org/abs/2511.13646v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13646v1",
                "title": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?"
                },
                "updated": "2025-11-17T17:58:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    58,
                    18,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13646v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:58:18Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    58,
                    18,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Chunqiu Steven Xia"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Yan Yang"
                    },
                    {
                        "name": "Yuxiang Wei"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.13641v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13641v1",
                "title": "It's a Feature, Not a Bug: Secure and Auditable State Rollback for Confidential Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It's a Feature, Not a Bug: Secure and Auditable State Rollback for Confidential Cloud Applications"
                },
                "updated": "2025-11-17T17:53:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    53,
                    47,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13641v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Replay and rollback attacks threaten cloud application integrity by reintroducing authentic yet stale data through an untrusted storage interface to compromise application decision-making. Prior security frameworks mitigate these attacks by enforcing forward-only state transitions (state continuity) with hardware-backed mechanisms, but they categorically treat all rollback as malicious and thus preclude legitimate rollbacks used for operational recovery from corruption or misconfiguration. We present Rebound, a general-purpose security framework that preserves rollback protection while enabling policy-authorized legitimate rollbacks of application binaries, configuration, and data. Key to Rebound is a reference monitor that mediates state transitions, enforces authorization policy, guarantees atomicity of state updates and rollbacks, and emits a tamper-evident log that provides transparency to applications and auditors. We formally prove Rebound's security properties and show through an application case study -- with software deployment workflows in GitLab CI -- that it enables robust control over binary, configuration, and raw data versioning with low end-to-end overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Replay and rollback attacks threaten cloud application integrity by reintroducing authentic yet stale data through an untrusted storage interface to compromise application decision-making. Prior security frameworks mitigate these attacks by enforcing forward-only state transitions (state continuity) with hardware-backed mechanisms, but they categorically treat all rollback as malicious and thus preclude legitimate rollbacks used for operational recovery from corruption or misconfiguration. We present Rebound, a general-purpose security framework that preserves rollback protection while enabling policy-authorized legitimate rollbacks of application binaries, configuration, and data. Key to Rebound is a reference monitor that mediates state transitions, enforces authorization policy, guarantees atomicity of state updates and rollbacks, and emits a tamper-evident log that provides transparency to applications and auditors. We formally prove Rebound's security properties and show through an application case study -- with software deployment workflows in GitLab CI -- that it enables robust control over binary, configuration, and raw data versioning with low end-to-end overhead."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:53:47Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    53,
                    47,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Quinn Burke"
                    },
                    {
                        "name": "Anjo Vahldiek-Oberwagner"
                    },
                    {
                        "name": "Michael Swift"
                    },
                    {
                        "name": "Patrick McDaniel"
                    }
                ],
                "author_detail": {
                    "name": "Patrick McDaniel"
                },
                "author": "Patrick McDaniel"
            },
            {
                "id": "http://arxiv.org/abs/2511.13640v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13640v1",
                "title": "Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures"
                },
                "updated": "2025-11-17T17:53:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    53,
                    12,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13640v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:53:12Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    53,
                    12,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Haohui Wang"
                    },
                    {
                        "name": "Jingyuan Qi"
                    },
                    {
                        "name": "Jianpeng Chen"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Lifu Huang"
                    },
                    {
                        "name": "Lecheng Zheng"
                    },
                    {
                        "name": "Kevin Choi"
                    },
                    {
                        "name": "Balaji Veeramani"
                    },
                    {
                        "name": "Edward Bowen"
                    },
                    {
                        "name": "Alison Hu"
                    },
                    {
                        "name": "Tyler Cody"
                    },
                    {
                        "name": "Dawei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Zhou"
                },
                "author": "Dawei Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2511.13630v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13630v1",
                "title": "Beyond Mimicry: Preference Coherence in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Mimicry: Preference Coherence in LLMs"
                },
                "updated": "2025-11-17T17:41:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    41,
                    48,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13630v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:41:48Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    41,
                    48,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Luhan Mikaelson"
                    },
                    {
                        "name": "Derek Shiller"
                    },
                    {
                        "name": "Hayley Clatterbuck"
                    }
                ],
                "author_detail": {
                    "name": "Hayley Clatterbuck"
                },
                "author": "Hayley Clatterbuck"
            },
            {
                "id": "http://arxiv.org/abs/2508.21323v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.21323v2",
                "title": "LLM-driven Provenance Forensics for Threat Investigation and Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-driven Provenance Forensics for Threat Investigation and Detection"
                },
                "updated": "2025-11-17T17:40:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    40,
                    34,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.21323v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.21323v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce PROVSEEK, an LLM-powered agentic framework for automated provenance-driven forensic analysis and threat intelligence extraction. PROVSEEK employs specialized toolchains to dynamically retrieve relevant context by generating precise, context-aware queries that fuse knowledge from threat reports with evidence from system provenance data. The framework resolves provenance queries, orchestrates multiple role-specific agents, and synthesizes structured, ground-truth verifiable forensic summaries. By combining agent orchestration with Retrieval-Augmented Generation (RAG) and chain-of-thought (CoT) reasoning, data-guided filtration using a behavioral model, PROVSEEK enables adaptive multi-step analysis that iteratively refines hypotheses, verifies supporting evidence, and produces scalable, interpretable forensic explanations of attack behaviors. PROVSEEK is designed for automated threat investigation without task-specific training data, enabling forensic-style investigation even when no prior knowledge of the environment. We conduct a comprehensive evaluation on publicly available DARPA datasets, demonstrating that PROVSEEK outperforms retrieval-based methods for the intelligence extraction task, achieving a 34% improvement in contextual precision/recall; and for threat detection task, PROVSEEK achieves 22%/29% higher precision/recall compared to both a baseline agent approach and State-Of-The-Art (SOTA) Provenance-based Intrusion Detection System (PIDS). In our scalability study, we show that PROVSEEK increases token usage by 1.42x and latency by 1.63x as the database size increases 50x, making it optimal for large-scale deployment. We also conducted an ablation and error analysis study to show how different components of PROVSEEK affect the detection performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PROVSEEK, an LLM-powered agentic framework for automated provenance-driven forensic analysis and threat intelligence extraction. PROVSEEK employs specialized toolchains to dynamically retrieve relevant context by generating precise, context-aware queries that fuse knowledge from threat reports with evidence from system provenance data. The framework resolves provenance queries, orchestrates multiple role-specific agents, and synthesizes structured, ground-truth verifiable forensic summaries. By combining agent orchestration with Retrieval-Augmented Generation (RAG) and chain-of-thought (CoT) reasoning, data-guided filtration using a behavioral model, PROVSEEK enables adaptive multi-step analysis that iteratively refines hypotheses, verifies supporting evidence, and produces scalable, interpretable forensic explanations of attack behaviors. PROVSEEK is designed for automated threat investigation without task-specific training data, enabling forensic-style investigation even when no prior knowledge of the environment. We conduct a comprehensive evaluation on publicly available DARPA datasets, demonstrating that PROVSEEK outperforms retrieval-based methods for the intelligence extraction task, achieving a 34% improvement in contextual precision/recall; and for threat detection task, PROVSEEK achieves 22%/29% higher precision/recall compared to both a baseline agent approach and State-Of-The-Art (SOTA) Provenance-based Intrusion Detection System (PIDS). In our scalability study, we show that PROVSEEK increases token usage by 1.42x and latency by 1.63x as the database size increases 50x, making it optimal for large-scale deployment. We also conducted an ablation and error analysis study to show how different components of PROVSEEK affect the detection performance."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-29T04:39:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    4,
                    39,
                    52,
                    4,
                    241,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Kunal Mukherjee"
                    },
                    {
                        "name": "Murat Kantarcioglu"
                    }
                ],
                "author_detail": {
                    "name": "Murat Kantarcioglu"
                },
                "author": "Murat Kantarcioglu"
            },
            {
                "id": "http://arxiv.org/abs/2504.03784v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.03784v5",
                "title": "Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning"
                },
                "updated": "2025-11-17T17:33:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    33,
                    15,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.03784v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.03784v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset. The code is available at https:// github.com/ VRPO/ VRPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset. The code is available at https:// github.com/ VRPO/ VRPO."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-03T16:16:35Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    16,
                    35,
                    3,
                    93,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Kai Ye"
                    },
                    {
                        "name": "Hongyi Zhou"
                    },
                    {
                        "name": "Jin Zhu"
                    },
                    {
                        "name": "Francesco Quinzan"
                    },
                    {
                        "name": "Chengchun Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chengchun Shi"
                },
                "author": "Chengchun Shi"
            },
            {
                "id": "http://arxiv.org/abs/2510.27176v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.27176v3",
                "title": "Glia: A Human-Inspired AI for Automated Systems Design and Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glia: A Human-Inspired AI for Automated Systems Design and Optimization"
                },
                "updated": "2025-11-17T17:29:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    29,
                    30,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.27176v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.27176v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that grounds abstract reasoning in empirical feedback. Unlike prior ML-for-systems methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for LLM inference, it produces new algorithms for request routing, scheduling, and auto-scaling that perform at human-expert levels in significantly less time, while yielding novel insights into workload behavior. Our results suggest that by combining reasoning LLMs with structured experimentation, an AI can produce creative and understandable designs for complex systems problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that grounds abstract reasoning in empirical feedback. Unlike prior ML-for-systems methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for LLM inference, it produces new algorithms for request routing, scheduling, and auto-scaling that perform at human-expert levels in significantly less time, while yielding novel insights into workload behavior. Our results suggest that by combining reasoning LLMs with structured experimentation, an AI can produce creative and understandable designs for complex systems problems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-31T04:58:00Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    58,
                    0,
                    4,
                    304,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Pantea Karimi"
                    },
                    {
                        "name": "Arash Nasr-Esfahany"
                    },
                    {
                        "name": "Kimia Noorbakhsh"
                    },
                    {
                        "name": "Joseph Chandler"
                    },
                    {
                        "name": "Ali ParandehGheibi"
                    },
                    {
                        "name": "Mohammad Alizadeh"
                    },
                    {
                        "name": "Hari Balakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Hari Balakrishnan"
                },
                "author": "Hari Balakrishnan"
            },
            {
                "id": "http://arxiv.org/abs/2511.13614v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13614v1",
                "title": "Market-Dependent Communication in Multi-Agent Alpha Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Market-Dependent Communication in Multi-Agent Alpha Generation"
                },
                "updated": "2025-11-17T17:19:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    19,
                    56,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13614v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-strategy hedge funds face a fundamental organizational choice: should analysts generating trading strategies communicate, and if so, how? We investigate this using 5-agent LLM-based trading systems across 450 experiments spanning 21 months, comparing five organizational structures from isolated baseline to collaborative and competitive conversation. We show that communication improves performance, but optimal communication design depends on market characteristics. Competitive conversation excels in volatile technology stocks, while collaborative conversation dominates stable general stocks. Finance stocks resist all communication interventions. Surprisingly, all structures, including isolated agents, converge to similar strategy alignments, challenging assumptions that transparency causes harmful diversity loss. Performance differences stem from behavioral mechanisms: competitive agents focus on stock-level allocation while collaborative agents develop technical frameworks. Conversation quality scores show zero correlation with returns. These findings demonstrate that optimal communication design must match market volatility characteristics, and sophisticated discussions don't guarantee better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-strategy hedge funds face a fundamental organizational choice: should analysts generating trading strategies communicate, and if so, how? We investigate this using 5-agent LLM-based trading systems across 450 experiments spanning 21 months, comparing five organizational structures from isolated baseline to collaborative and competitive conversation. We show that communication improves performance, but optimal communication design depends on market characteristics. Competitive conversation excels in volatile technology stocks, while collaborative conversation dominates stable general stocks. Finance stocks resist all communication interventions. Surprisingly, all structures, including isolated agents, converge to similar strategy alignments, challenging assumptions that transparency causes harmful diversity loss. Performance differences stem from behavioral mechanisms: competitive agents focus on stock-level allocation while collaborative agents develop technical frameworks. Conversation quality scores show zero correlation with returns. These findings demonstrate that optimal communication design must match market volatility characteristics, and sophisticated discussions don't guarantee better performance."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:19:56Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    19,
                    56,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Jerick Shi"
                    },
                    {
                        "name": "Burton Hollifield"
                    }
                ],
                "author_detail": {
                    "name": "Burton Hollifield"
                },
                "author": "Burton Hollifield"
            },
            {
                "id": "http://arxiv.org/abs/2511.13612v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13612v1",
                "title": "P1: Mastering Physics Olympiads with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P1: Mastering Physics Olympiads with Reinforcement Learning"
                },
                "updated": "2025-11-17T17:18:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    18,
                    13,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13612v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T17:18:13Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    18,
                    13,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jiacheng Chen"
                    },
                    {
                        "name": "Qianjia Cheng"
                    },
                    {
                        "name": "Fangchen Yu"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Shenghe Zheng"
                    },
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Qingyang Zhang"
                    },
                    {
                        "name": "Haonan He"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Yufeng Zhao"
                    },
                    {
                        "name": "Futing Wang"
                    },
                    {
                        "name": "Li Sheng"
                    },
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Yizhuo Li"
                    },
                    {
                        "name": "Wenxauan Zeng"
                    },
                    {
                        "name": "Yulun Wu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Ganqu Cui"
                    }
                ],
                "author_detail": {
                    "name": "Ganqu Cui"
                },
                "author": "Ganqu Cui"
            },
            {
                "id": "http://arxiv.org/abs/2511.11177v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11177v2",
                "title": "Viper-F1: Fast and Fine-Grained Multimodal Understanding with Cross-Modal State-Space Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viper-F1: Fast and Fine-Grained Multimodal Understanding with Cross-Modal State-Space Modulation"
                },
                "updated": "2025-11-17T17:08:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    17,
                    8,
                    31,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11177v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11177v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in multimodal large language models (MLLMs) have enabled impressive progress in vision-language understanding, yet their high computational cost limits deployment in resource-constrained scenarios such as robotic manipulation, personal assistants, and smart cameras. Most existing methods rely on Transformer-based cross-attention, whose quadratic complexity hinders efficiency. Moreover, small vision-language models often struggle to precisely capture fine-grained, task-relevant visual regions, leading to degraded performance on fine-grained reasoning tasks that limit their effectiveness in the real world. To address these issues, we introduce Viper-F1, a Hybrid State-Space Vision-Language Model that replaces attention with efficient Liquid State-Space Dynamics. To further enhance visual grounding, we propose a Token-Grid Correlation Module, which computes lightweight correlations between text tokens and image patches and modulates the state-space dynamics via FiLM conditioning. This enables the model to selectively emphasize visual regions relevant to the textual prompt while maintaining linear-time inference. Experimental results across multiple benchmarks demonstrate that Viper-F1 achieves accurate, fine-grained understanding with significantly improved efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal large language models (MLLMs) have enabled impressive progress in vision-language understanding, yet their high computational cost limits deployment in resource-constrained scenarios such as robotic manipulation, personal assistants, and smart cameras. Most existing methods rely on Transformer-based cross-attention, whose quadratic complexity hinders efficiency. Moreover, small vision-language models often struggle to precisely capture fine-grained, task-relevant visual regions, leading to degraded performance on fine-grained reasoning tasks that limit their effectiveness in the real world. To address these issues, we introduce Viper-F1, a Hybrid State-Space Vision-Language Model that replaces attention with efficient Liquid State-Space Dynamics. To further enhance visual grounding, we propose a Token-Grid Correlation Module, which computes lightweight correlations between text tokens and image patches and modulates the state-space dynamics via FiLM conditioning. This enables the model to selectively emphasize visual regions relevant to the textual prompt while maintaining linear-time inference. Experimental results across multiple benchmarks demonstrate that Viper-F1 achieves accurate, fine-grained understanding with significantly improved efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T11:21:48Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    11,
                    21,
                    48,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Quoc-Huy Trinh"
                    },
                    {
                        "name": "Mustapha Abdullahi"
                    },
                    {
                        "name": "Do Duy Hung Trinh"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Debesh Jha"
                    }
                ],
                "author_detail": {
                    "name": "Debesh Jha"
                },
                "author": "Debesh Jha"
            },
            {
                "id": "http://arxiv.org/abs/2502.19662v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.19662v3",
                "title": "HALO: Hardware-aware quantization with low critical-path-delay weights for LLM acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HALO: Hardware-aware quantization with low critical-path-delay weights for LLM acceleration"
                },
                "updated": "2025-11-17T16:57:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    57,
                    58,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.19662v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.19662v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Quantization is critical for efficiently deploying large language models (LLMs). Yet conventional methods remain hardware-agnostic, limited to bit-width constraints, and do not account for intrinsic circuit characteristics such as the timing behaviors and energy profiles of Multiply-Accumulate (MAC) units. This disconnect from circuit-level behavior limits the ability to exploit available timing margins and energy-saving opportunities, reducing the overall efficiency of deployment on modern accelerators.\n  To address these limitations, we propose HALO, a versatile framework for Hardware-Aware Post-Training Quantization (PTQ). Unlike traditional methods, HALO explicitly incorporates detailed hardware characteristics, including critical-path timing and power consumption, into its quantization approach. HALO strategically selects weights with low critical-path-delays enabling higher operational frequencies and dynamic frequency scaling without disrupting the architecture's dataflow. Remarkably, HALO achieves these improvements with only a few dynamic voltage and frequency scaling (DVFS) adjustments, ensuring simplicity and practicality in deployment. Additionally, by reducing switching activity within the MAC units, HALO effectively lowers energy consumption. Evaluations on accelerators such as Tensor Processing Units (TPUs) and Graphics Processing Units (GPUs) demonstrate that HALO significantly enhances inference efficiency, achieving average performance improvements of 270% and energy savings of 51% over baseline quantization methods, all with minimal impact on accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is critical for efficiently deploying large language models (LLMs). Yet conventional methods remain hardware-agnostic, limited to bit-width constraints, and do not account for intrinsic circuit characteristics such as the timing behaviors and energy profiles of Multiply-Accumulate (MAC) units. This disconnect from circuit-level behavior limits the ability to exploit available timing margins and energy-saving opportunities, reducing the overall efficiency of deployment on modern accelerators.\n  To address these limitations, we propose HALO, a versatile framework for Hardware-Aware Post-Training Quantization (PTQ). Unlike traditional methods, HALO explicitly incorporates detailed hardware characteristics, including critical-path timing and power consumption, into its quantization approach. HALO strategically selects weights with low critical-path-delays enabling higher operational frequencies and dynamic frequency scaling without disrupting the architecture's dataflow. Remarkably, HALO achieves these improvements with only a few dynamic voltage and frequency scaling (DVFS) adjustments, ensuring simplicity and practicality in deployment. Additionally, by reducing switching activity within the MAC units, HALO effectively lowers energy consumption. Evaluations on accelerators such as Tensor Processing Units (TPUs) and Graphics Processing Units (GPUs) demonstrate that HALO significantly enhances inference efficiency, achieving average performance improvements of 270% and energy savings of 51% over baseline quantization methods, all with minimal impact on accuracy."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-27T01:08:33Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    1,
                    8,
                    33,
                    3,
                    58,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Rohan Juneja"
                    },
                    {
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Tulika Mitra"
                    },
                    {
                        "name": "Li-Shiuan Peh"
                    }
                ],
                "author_detail": {
                    "name": "Li-Shiuan Peh"
                },
                "author": "Li-Shiuan Peh"
            },
            {
                "id": "http://arxiv.org/abs/2511.13593v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13593v1",
                "title": "Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents"
                },
                "updated": "2025-11-17T16:55:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    55,
                    19,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13593v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:55:19Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    55,
                    19,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Piaohong Wang"
                    },
                    {
                        "name": "Motong Tian"
                    },
                    {
                        "name": "Jiaxian Li"
                    },
                    {
                        "name": "Yuan Liang"
                    },
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Qianben Chen"
                    },
                    {
                        "name": "Tiannan Wang"
                    },
                    {
                        "name": "Zhicong Lu"
                    },
                    {
                        "name": "Jiawei Ma"
                    },
                    {
                        "name": "Yuchen Eleanor Jiang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wangchunshu Zhou"
                },
                "author": "Wangchunshu Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2511.13590v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13590v1",
                "title": "Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation"
                },
                "updated": "2025-11-17T16:52:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    52,
                    19,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13590v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:52:19Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    52,
                    19,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Xiaoming Yin"
                    },
                    {
                        "name": "Xing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xing Chen"
                },
                "author": "Xing Chen"
            },
            {
                "id": "http://arxiv.org/abs/2508.14031v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.14031v2",
                "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation"
                },
                "updated": "2025-11-17T16:48:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    48,
                    6,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.14031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.14031v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-19T17:53:35Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    53,
                    35,
                    1,
                    231,
                    0
                ],
                "arxiv_comment": "Accepted at AAAI 2026 AI Alignment Track, Source code: https://github.com/HahmDY/agentic-ft-safety",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dongyoon Hahm"
                    },
                    {
                        "name": "Taywon Min"
                    },
                    {
                        "name": "Woogyeol Jin"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee"
            },
            {
                "id": "http://arxiv.org/abs/2511.13580v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13580v1",
                "title": "Development of a Low-Cost, Autonomous Pulse Amplitude Modulated (PAM) Chlorophyll Fluorometer for In-Situ Monitoring of Photosystem II Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Low-Cost, Autonomous Pulse Amplitude Modulated (PAM) Chlorophyll Fluorometer for In-Situ Monitoring of Photosystem II Efficiency"
                },
                "updated": "2025-11-17T16:41:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    41,
                    31,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13580v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The quantum yield efficiency of photosystem II (PhiPSII) is an important parameter for assessing the photosynthetic performance and stress status of plants. Commercial PAM fluorometers can measure this parameter, but they are often expensive, bulky, or lack autonomous operation. This work presents the development of an autonomous PAM fluorometer designed to address these limitations and enable large-scale deployment. It supports high spatio-temporal monitoring of PhiPSII in forest canopies under a wide range of ambient light conditions.\n  The prototype costs approximately 150 EUR, has dimensions of 3 cm x 6 cm x 2 cm, and weighs about 50 g. In side-by-side tests across three plant species, it achieved measurement accuracy comparable to state-of-the-art commercial sensors, with a correlation factor of R^2 = 0.95.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quantum yield efficiency of photosystem II (PhiPSII) is an important parameter for assessing the photosynthetic performance and stress status of plants. Commercial PAM fluorometers can measure this parameter, but they are often expensive, bulky, or lack autonomous operation. This work presents the development of an autonomous PAM fluorometer designed to address these limitations and enable large-scale deployment. It supports high spatio-temporal monitoring of PhiPSII in forest canopies under a wide range of ambient light conditions.\n  The prototype costs approximately 150 EUR, has dimensions of 3 cm x 6 cm x 2 cm, and weighs about 50 g. In side-by-side tests across three plant species, it achieved measurement accuracy comparable to state-of-the-art commercial sensors, with a correlation factor of R^2 = 0.95."
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:41:31Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    41,
                    31,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Author's preprint. The manuscript is being prepared for submission to a journal",
                "arxiv_primary_category": {
                    "term": "physics.ins-det"
                },
                "authors": [
                    {
                        "name": "Samaneh Baghbani"
                    },
                    {
                        "name": "Uygar Akkoc"
                    },
                    {
                        "name": "Clara Stock"
                    },
                    {
                        "name": "Christiane Werner"
                    },
                    {
                        "name": "Stefan J. Rupitsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan J. Rupitsch"
                },
                "author": "Stefan J. Rupitsch"
            },
            {
                "id": "http://arxiv.org/abs/2508.10501v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.10501v3",
                "title": "PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning"
                },
                "updated": "2025-11-17T16:36:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    36,
                    12,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.10501v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.10501v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-14T10:03:47Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    3,
                    47,
                    3,
                    226,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yushi Feng"
                    },
                    {
                        "name": "Junye Du"
                    },
                    {
                        "name": "Yingying Hong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Lequan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lequan Yu"
                },
                "author": "Lequan Yu"
            },
            {
                "id": "http://arxiv.org/abs/2511.06390v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06390v2",
                "title": "Ghost in the Transformer: Tracing LLM Lineage with SVD-Fingerprint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ghost in the Transformer: Tracing LLM Lineage with SVD-Fingerprint"
                },
                "updated": "2025-11-17T16:20:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    20,
                    58,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06390v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06390v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have rapidly advanced and are widely adopted across diverse fields. Due to the substantial computational cost and data requirements of training from scratch, many developers choose to fine-tune or modify existing open-source models. While most adhere to open-source licenses, some falsely claim original training despite clear derivation from public models. This raises pressing concerns about intellectual property protection and highlights the need for reliable methods to verify model provenance. In this paper, we propose GhostSpec, a lightweight yet effective method for verifying LLM lineage without access to training data or modification of model behavior. Our approach constructs compact and robust fingerprints by applying singular value decomposition (SVD) to invariant products of internal attention weight matrices, effectively capturing the structural identity of a model. Unlike watermarking or output-based methods, GhostSpec is fully data-free, non-invasive, and computationally efficient. It demonstrates strong robustness to sequential fine-tuning, pruning, block expansion, and even adversarial transformations. Extensive experiments show that GhostSpec can reliably trace the lineage of transformed models with minimal overhead. By offering a practical solution for model verification and reuse tracking, our method contributes to the protection of intellectual property and fosters a transparent, trustworthy ecosystem for large-scale language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have rapidly advanced and are widely adopted across diverse fields. Due to the substantial computational cost and data requirements of training from scratch, many developers choose to fine-tune or modify existing open-source models. While most adhere to open-source licenses, some falsely claim original training despite clear derivation from public models. This raises pressing concerns about intellectual property protection and highlights the need for reliable methods to verify model provenance. In this paper, we propose GhostSpec, a lightweight yet effective method for verifying LLM lineage without access to training data or modification of model behavior. Our approach constructs compact and robust fingerprints by applying singular value decomposition (SVD) to invariant products of internal attention weight matrices, effectively capturing the structural identity of a model. Unlike watermarking or output-based methods, GhostSpec is fully data-free, non-invasive, and computationally efficient. It demonstrates strong robustness to sequential fine-tuning, pruning, block expansion, and even adversarial transformations. Extensive experiments show that GhostSpec can reliably trace the lineage of transformed models with minimal overhead. By offering a practical solution for model verification and reuse tracking, our method contributes to the protection of intellectual property and fosters a transparent, trustworthy ecosystem for large-scale language models."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-09T13:57:59Z",
                "published_parsed": [
                    2025,
                    11,
                    9,
                    13,
                    57,
                    59,
                    6,
                    313,
                    0
                ],
                "arxiv_comment": "Accepted at AAAI 2026 (Oral)",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Suqing Wang"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Zuchao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zuchao Li"
                },
                "author": "Zuchao Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.13548v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13548v1",
                "title": "ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models"
                },
                "updated": "2025-11-17T16:19:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    19,
                    21,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13548v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \\textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \\textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:19:21Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    19,
                    21,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Siyang Cheng"
                    },
                    {
                        "name": "Gaotian Liu"
                    },
                    {
                        "name": "Rui Mei"
                    },
                    {
                        "name": "Yilin Wang"
                    },
                    {
                        "name": "Kejia Zhang"
                    },
                    {
                        "name": "Kaishuo Wei"
                    },
                    {
                        "name": "Yuqi Yu"
                    },
                    {
                        "name": "Weiping Wen"
                    },
                    {
                        "name": "Xiaojie Wu"
                    },
                    {
                        "name": "Junhua Liu"
                    }
                ],
                "author_detail": {
                    "name": "Junhua Liu"
                },
                "author": "Junhua Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.01668v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.01668v2",
                "title": "Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics"
                },
                "updated": "2025-11-17T16:17:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    17,
                    55,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.01668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.01668v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As artificial intelligence permeates judicial forensics, ensuring the veracity and traceability of legal question answering (QA) has become critical. Conventional large language models (LLMs) are prone to hallucination, risking misleading guidance in legal consultation, while static knowledge bases struggle to keep pace with frequently updated statutes and case law. We present a hybrid legal QA agent tailored for judicial settings that integrates retrieval-augmented generation (RAG) with multi-model ensembling to deliver reliable, auditable, and continuously updatable counsel. The system prioritizes retrieval over generation: when a trusted legal repository yields relevant evidence, answers are produced via RAG; otherwise, multiple LLMs generate candidates that are scored by a specialized selector, with the top-ranked answer returned. High-quality outputs then undergo human review before being written back to the repository, enabling dynamic knowledge evolution and provenance tracking. Experiments on the Law\\_QA dataset show that our hybrid approach significantly outperforms both a single-model baseline and a vanilla RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm the complementary contributions of retrieval prioritization, model ensembling, and the human-in-the-loop update mechanism. The proposed system demonstrably reduces hallucination while improving answer quality and legal compliance, advancing the practical landing of media forensics technologies in judicial scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As artificial intelligence permeates judicial forensics, ensuring the veracity and traceability of legal question answering (QA) has become critical. Conventional large language models (LLMs) are prone to hallucination, risking misleading guidance in legal consultation, while static knowledge bases struggle to keep pace with frequently updated statutes and case law. We present a hybrid legal QA agent tailored for judicial settings that integrates retrieval-augmented generation (RAG) with multi-model ensembling to deliver reliable, auditable, and continuously updatable counsel. The system prioritizes retrieval over generation: when a trusted legal repository yields relevant evidence, answers are produced via RAG; otherwise, multiple LLMs generate candidates that are scored by a specialized selector, with the top-ranked answer returned. High-quality outputs then undergo human review before being written back to the repository, enabling dynamic knowledge evolution and provenance tracking. Experiments on the Law\\_QA dataset show that our hybrid approach significantly outperforms both a single-model baseline and a vanilla RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm the complementary contributions of retrieval prioritization, model ensembling, and the human-in-the-loop update mechanism. The proposed system demonstrably reduces hallucination while improving answer quality and legal compliance, advancing the practical landing of media forensics technologies in judicial scenarios."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T15:30:58Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    15,
                    30,
                    58,
                    0,
                    307,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yueqing Xi"
                    },
                    {
                        "name": "Yifan Bai"
                    },
                    {
                        "name": "Huasen Luo"
                    },
                    {
                        "name": "Weiliang Wen"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Haoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Haoliang Li"
                },
                "author": "Haoliang Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.13542v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13542v1",
                "title": "Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop"
                },
                "updated": "2025-11-17T16:15:50Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    15,
                    50,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13542v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale."
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:15:50Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    15,
                    50,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE"
                },
                "authors": [
                    {
                        "name": "Amirreza Mehrabi"
                    },
                    {
                        "name": "Jason Wade Morphew"
                    },
                    {
                        "name": "Breejha Quezada"
                    },
                    {
                        "name": "N. Sanjay Rebello"
                    }
                ],
                "author_detail": {
                    "name": "N. Sanjay Rebello"
                },
                "author": "N. Sanjay Rebello"
            },
            {
                "id": "http://arxiv.org/abs/2409.14507v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2409.14507v6",
                "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders"
                },
                "updated": "2025-11-17T16:10:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    10,
                    6,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2409.14507v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2409.14507v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features (\"math\" may split into \"algebra\", \"geometry\", etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get \"absorbed\" into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features (\"math\" may split into \"algebra\", \"geometry\", etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get \"absorbed\" into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-09-22T16:11:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    11,
                    2,
                    6,
                    266,
                    0
                ],
                "arxiv_comment": "Accepted at NeurIPS 2025 (Oral)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "James Wilken-Smith"
                    },
                    {
                        "name": "Tomáš Dulka"
                    },
                    {
                        "name": "Hardik Bhatnagar"
                    },
                    {
                        "name": "Satvik Golechha"
                    },
                    {
                        "name": "Joseph Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Bloom"
                },
                "author": "Joseph Bloom"
            },
            {
                "id": "http://arxiv.org/abs/2409.19100v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2409.19100v2",
                "title": "Personalizing Prostate Cancer Education for Patients Using an EHR-Integrated LLM Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalizing Prostate Cancer Education for Patients Using an EHR-Integrated LLM Agent"
                },
                "updated": "2025-11-17T16:06:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    6,
                    12,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2409.19100v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2409.19100v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cancer patients often lack timely education and personalized support due to clinician workload. This quality improvement study develops and evaluates a Large Language Model (LLM) agent, MedEduChat, which is integrated with the clinic's electronic health records (EHR) and designed to enhance prostate cancer patient education. Fifteen non-metastatic prostate cancer patients and three clinicians recruited from the Mayo Clinic interacted with the agent between May 2024 and April 2025. Findings showed that MedEduChat has a high usability score (UMUX 83.7 out of 100) and improves patients' health confidence (Health Confidence Score rose from 9.9 to 13.9). Clinicians evaluated the patient-chat interaction history and rated MedEduChat as highly correct (2.9 out of 3), complete (2.7 out of 3), and safe (2.7 out of 3), with moderate personalization (2.3 out of 3). This study highlights the potential of LLM agents to improve patient engagement and health education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer patients often lack timely education and personalized support due to clinician workload. This quality improvement study develops and evaluates a Large Language Model (LLM) agent, MedEduChat, which is integrated with the clinic's electronic health records (EHR) and designed to enhance prostate cancer patient education. Fifteen non-metastatic prostate cancer patients and three clinicians recruited from the Mayo Clinic interacted with the agent between May 2024 and April 2025. Findings showed that MedEduChat has a high usability score (UMUX 83.7 out of 100) and improves patients' health confidence (Health Confidence Score rose from 9.9 to 13.9). Clinicians evaluated the patient-chat interaction history and rated MedEduChat as highly correct (2.9 out of 3), complete (2.7 out of 3), and safe (2.7 out of 3), with moderate personalization (2.3 out of 3). This study highlights the potential of LLM agents to improve patient engagement and health education."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-09-27T19:04:11Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    19,
                    4,
                    11,
                    4,
                    271,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "arxiv_journal_ref": "npj Digital Medicine 2025",
                "authors": [
                    {
                        "name": "Yuexing Hao"
                    },
                    {
                        "name": "Jason Holmes"
                    },
                    {
                        "name": "Mark R. Waddle"
                    },
                    {
                        "name": "Brian J. Davis"
                    },
                    {
                        "name": "Nathan Y. Yu"
                    },
                    {
                        "name": "Kristin Vickers"
                    },
                    {
                        "name": "Heather Preston"
                    },
                    {
                        "name": "Drew Margolin"
                    },
                    {
                        "name": "Corinna E. Lockenhoff"
                    },
                    {
                        "name": "Aditya Vashistha"
                    },
                    {
                        "name": "Saleh Kalantari"
                    },
                    {
                        "name": "Marzyeh Ghassemi"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13526v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13526v1",
                "title": "Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models"
                },
                "updated": "2025-11-17T16:00:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    0,
                    42,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13526v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T16:00:42Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    16,
                    0,
                    42,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "5 pages, 1 figure, 1 table. Accepted at AI4RWC@WI-IAT 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zhengda Wang"
                    },
                    {
                        "name": "Daqian Shi"
                    },
                    {
                        "name": "Jingyi Zhao"
                    },
                    {
                        "name": "Xiaolei Diao"
                    },
                    {
                        "name": "Xiongfeng Tang"
                    },
                    {
                        "name": "Yanguo Qin"
                    }
                ],
                "author_detail": {
                    "name": "Yanguo Qin"
                },
                "author": "Yanguo Qin"
            },
            {
                "id": "http://arxiv.org/abs/2511.13524v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13524v1",
                "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI"
                },
                "updated": "2025-11-17T15:58:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    58,
                    46,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13524v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T15:58:46Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    58,
                    46,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "9 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "arxiv_journal_ref": "AAAI 2026 Oral",
                "authors": [
                    {
                        "name": "Yuhang Peng"
                    },
                    {
                        "name": "Yizhou Pan"
                    },
                    {
                        "name": "Xinning He"
                    },
                    {
                        "name": "Jihaoyu Yang"
                    },
                    {
                        "name": "Xinyu Yin"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Xiaoji Zheng"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Jiangtao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiangtao Gong"
                },
                "author": "Jiangtao Gong"
            },
            {
                "id": "http://arxiv.org/abs/2504.19838v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.19838v3",
                "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects"
                },
                "updated": "2025-11-17T15:51:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    51,
                    58,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.19838v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.19838v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents. The collection of papers reviewed in this survey will be hosted and regularly updated on the GitHub repository: https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents. The collection of papers reviewed in this survey will be hosted and regularly updated on the GitHub repository: https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-28T14:39:25Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    39,
                    25,
                    0,
                    118,
                    0
                ],
                "arxiv_comment": "Paper accepted to TMLR 2025, Project Homepage: https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Guangyi Liu"
                    },
                    {
                        "name": "Pengxiang Zhao"
                    },
                    {
                        "name": "Yaozhen Liang"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Yaxuan Guo"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Weifeng Lin"
                    },
                    {
                        "name": "Yuxiang Chai"
                    },
                    {
                        "name": "Yue Han"
                    },
                    {
                        "name": "Shuai Ren"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaoyu Liang"
                    },
                    {
                        "name": "WenHao Wang"
                    },
                    {
                        "name": "Tianze Wu"
                    },
                    {
                        "name": "Zhengxi Lu"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "LiLinghao"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Guanjing Xiong"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.13517v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13517v1",
                "title": "Interpretable Ransomware Detection Using Hybrid Large Language Models: A Comparative Analysis of BERT, RoBERTa, and DeBERTa Through LIME and SHAP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Ransomware Detection Using Hybrid Large Language Models: A Comparative Analysis of BERT, RoBERTa, and DeBERTa Through LIME and SHAP"
                },
                "updated": "2025-11-17T15:51:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    51,
                    36,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13517v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Ransomware continues to evolve in complexity, making early and explainable detection a critical requirement for modern cybersecurity systems. This study presents a comparative analysis of three Transformer-based Large Language Models (LLMs) (BERT, RoBERTa, and DeBERTa) for ransomware detection using two structured datasets: UGRansome and Process Memory (PM). Since LLMs are primarily designed for natural language processing (NLP), numerical and categorical ransomware features were transformed into textual sequences using KBinsDiscretizer and token-based encoding. This enabled the models to learn behavioural patterns from system activity and network traffic through contextual embeddings. The models were fine-tuned on approximately 2,500 labelled samples and evaluated using accuracy, F1 score, and ROC-AUC. To ensure transparent decision-making in this high-stakes domain, two explainable AI techniques (LIME and SHAP) were applied to interpret feature contributions. The results show that the models learn distinct ransomware-related cues: BERT relies heavily on dominant file-operation features, RoBERTa demonstrates balanced reliance on network and financial signals, while DeBERTa exhibits strong sensitivity to financial and network-traffic indicators. Visualisation of embeddings further reveals structural differences in token representation, with RoBERTa producing more isotropic embeddings and DeBERTa capturing highly directional, disentangled patterns. In general, RoBERTa achieved the strongest F1-score, while BERT yielded the highest ROC-AUC performance. The integration of LLMs with XAI provides a transparent framework capable of identifying feature-level evidence behind ransomware predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ransomware continues to evolve in complexity, making early and explainable detection a critical requirement for modern cybersecurity systems. This study presents a comparative analysis of three Transformer-based Large Language Models (LLMs) (BERT, RoBERTa, and DeBERTa) for ransomware detection using two structured datasets: UGRansome and Process Memory (PM). Since LLMs are primarily designed for natural language processing (NLP), numerical and categorical ransomware features were transformed into textual sequences using KBinsDiscretizer and token-based encoding. This enabled the models to learn behavioural patterns from system activity and network traffic through contextual embeddings. The models were fine-tuned on approximately 2,500 labelled samples and evaluated using accuracy, F1 score, and ROC-AUC. To ensure transparent decision-making in this high-stakes domain, two explainable AI techniques (LIME and SHAP) were applied to interpret feature contributions. The results show that the models learn distinct ransomware-related cues: BERT relies heavily on dominant file-operation features, RoBERTa demonstrates balanced reliance on network and financial signals, while DeBERTa exhibits strong sensitivity to financial and network-traffic indicators. Visualisation of embeddings further reveals structural differences in token representation, with RoBERTa producing more isotropic embeddings and DeBERTa capturing highly directional, disentangled patterns. In general, RoBERTa achieved the strongest F1-score, while BERT yielded the highest ROC-AUC performance. The integration of LLMs with XAI provides a transparent framework capable of identifying feature-level evidence behind ransomware predictions."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T15:51:36Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    51,
                    36,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Elodie Mutombo Ngoie"
                    },
                    {
                        "name": "Mike Nkongolo Wa Nkongolo"
                    },
                    {
                        "name": "Peace Azugo"
                    },
                    {
                        "name": "Mahmut Tokmak"
                    }
                ],
                "author_detail": {
                    "name": "Mahmut Tokmak"
                },
                "author": "Mahmut Tokmak"
            },
            {
                "id": "http://arxiv.org/abs/2507.10800v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.10800v2",
                "title": "ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference"
                },
                "updated": "2025-11-17T15:43:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    43,
                    33,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.10800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.10800v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "ViTs deliver SOTA performance, yet their fixed computational budget prevents scalable deployment across heterogeneous hardware. Recent Matryoshka-style Transformer architectures mitigate this by embedding nested subnetworks within a single model to enable scalable inference. However, these models allocate the same amount of compute to all inputs, regardless of their complexity, which leads to inefficiencies. To address this, we introduce ThinkingViT, a nested ViT architecture that employs progressive thinking stages to dynamically adjust inference computation based on input difficulty. ThinkingViT first activates a small subset of the most important attention heads to produce an initial prediction. If the prediction confidence exceeds a predefined threshold, inference terminates early. Otherwise, within the same backbone, it activates a larger subset of attention heads and conducts a new forward pass. This process continues iteratively until the model reaches the predefined confidence level or exhausts its maximum capacity. To boost the performance of subsequent rounds, we introduce a Token Recycling approach that fuses the input embeddings with the embeddings from the previous stage. Experiments show that ThinkingViT surpasses nested baselines by up to 2.0 percentage points (p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs on ImageNet-1K. We show that the backbone-preserving design of ThinkingViT allows it to serve as a plug-in upgrade for ViTs in downstream tasks such as semantic segmentation. We also demonstrate that ThinkingViT transfers effectively to other architectures such as Swin. The source code is available at https://github.com/ds-kiel/ThinkingViT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViTs deliver SOTA performance, yet their fixed computational budget prevents scalable deployment across heterogeneous hardware. Recent Matryoshka-style Transformer architectures mitigate this by embedding nested subnetworks within a single model to enable scalable inference. However, these models allocate the same amount of compute to all inputs, regardless of their complexity, which leads to inefficiencies. To address this, we introduce ThinkingViT, a nested ViT architecture that employs progressive thinking stages to dynamically adjust inference computation based on input difficulty. ThinkingViT first activates a small subset of the most important attention heads to produce an initial prediction. If the prediction confidence exceeds a predefined threshold, inference terminates early. Otherwise, within the same backbone, it activates a larger subset of attention heads and conducts a new forward pass. This process continues iteratively until the model reaches the predefined confidence level or exhausts its maximum capacity. To boost the performance of subsequent rounds, we introduce a Token Recycling approach that fuses the input embeddings with the embeddings from the previous stage. Experiments show that ThinkingViT surpasses nested baselines by up to 2.0 percentage points (p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs on ImageNet-1K. We show that the backbone-preserving design of ThinkingViT allows it to serve as a plug-in upgrade for ViTs in downstream tasks such as semantic segmentation. We also demonstrate that ThinkingViT transfers effectively to other architectures such as Swin. The source code is available at https://github.com/ds-kiel/ThinkingViT."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-14T20:54:41Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    54,
                    41,
                    0,
                    195,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ali Hojjat"
                    },
                    {
                        "name": "Janek Haberer"
                    },
                    {
                        "name": "Soren Pirk"
                    },
                    {
                        "name": "Olaf Landsiedel"
                    }
                ],
                "author_detail": {
                    "name": "Olaf Landsiedel"
                },
                "author": "Olaf Landsiedel"
            },
            {
                "id": "http://arxiv.org/abs/2511.13505v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13505v1",
                "title": "Applying Large Language Models to Characterize Public Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying Large Language Models to Characterize Public Narratives"
                },
                "updated": "2025-11-17T15:41:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    41,
                    55,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13505v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T15:41:55Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    41,
                    55,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Elinor Poole-Dayan"
                    },
                    {
                        "name": "Daniel T Kessler"
                    },
                    {
                        "name": "Hannah Chiou"
                    },
                    {
                        "name": "Margaret Hughes"
                    },
                    {
                        "name": "Emily S Lin"
                    },
                    {
                        "name": "Marshall Ganz"
                    },
                    {
                        "name": "Deb Roy"
                    }
                ],
                "author_detail": {
                    "name": "Deb Roy"
                },
                "author": "Deb Roy"
            },
            {
                "id": "http://arxiv.org/abs/2511.13502v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13502v1",
                "title": "Tight and Practical Privacy Auditing for Differentially Private In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight and Practical Privacy Auditing for Differentially Private In-Context Learning"
                },
                "updated": "2025-11-17T15:39:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    39,
                    54,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13502v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) perform in-context learning (ICL) by adapting to tasks from prompt demonstrations, which in practice often contain private or proprietary data. Although differential privacy (DP) with private voting is a pragmatic mitigation, DP-ICL implementations are error-prone, and worst-case DP bounds may substantially overestimate actual leakage, calling for practical auditing tools. We present a tight and efficient privacy auditing framework for DP-ICL systems that runs membership inference attacks and translates their success rates into empirical privacy guarantees using Gaussian DP. Our analysis of the private voting mechanism identifies vote configurations that maximize the auditing signal, guiding the design of audit queries that reliably reveal whether a canary demonstration is present in the context. The framework supports both black-box (API-only) and white-box (internal vote) threat models, and unifies auditing for classification and generation by reducing both to a binary decision problem. Experiments on standard text classification and generation benchmarks show that our empirical leakage estimates closely match theoretical DP budgets on classification tasks and are consistently lower on generation tasks due to conservative embedding-sensitivity bounds, making our framework a practical privacy auditor and verifier for real-world DP-ICL deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) perform in-context learning (ICL) by adapting to tasks from prompt demonstrations, which in practice often contain private or proprietary data. Although differential privacy (DP) with private voting is a pragmatic mitigation, DP-ICL implementations are error-prone, and worst-case DP bounds may substantially overestimate actual leakage, calling for practical auditing tools. We present a tight and efficient privacy auditing framework for DP-ICL systems that runs membership inference attacks and translates their success rates into empirical privacy guarantees using Gaussian DP. Our analysis of the private voting mechanism identifies vote configurations that maximize the auditing signal, guiding the design of audit queries that reliably reveal whether a canary demonstration is present in the context. The framework supports both black-box (API-only) and white-box (internal vote) threat models, and unifies auditing for classification and generation by reducing both to a binary decision problem. Experiments on standard text classification and generation benchmarks show that our empirical leakage estimates closely match theoretical DP budgets on classification tasks and are consistently lower on generation tasks due to conservative embedding-sensitivity bounds, making our framework a practical privacy auditor and verifier for real-world DP-ICL deployments."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T15:39:54Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    39,
                    54,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Yuyang Xia"
                    },
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Li Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiong"
                },
                "author": "Li Xiong"
            },
            {
                "id": "http://arxiv.org/abs/2506.00210v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.00210v2",
                "title": "REIC: RAG-Enhanced Intent Classification at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REIC: RAG-Enhanced Intent Classification at Scale"
                },
                "updated": "2025-11-17T15:21:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    21,
                    31,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.00210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.00210v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate intent classification is critical for efficient routing in customer service, ensuring customers are connected with the most suitable agents while reducing handling times and operational costs. However, as companies expand their product lines, intent classification faces scalability challenges due to the increasing number of intents and variations in taxonomy across different verticals. In this paper, we introduce REIC, a Retrieval-augmented generation Enhanced Intent Classification approach, which addresses these challenges effectively. REIC leverages retrieval-augmented generation (RAG) to dynamically incorporate relevant knowledge, enabling precise classification without the need for frequent retraining. Through extensive experiments on real-world datasets, we demonstrate that REIC outperforms traditional fine-tuning, zero-shot, and few-shot methods in large-scale customer service settings. Our results highlight its effectiveness in both in-domain and out-of-domain scenarios, demonstrating its potential for real-world deployment in adaptive and large-scale intent classification systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate intent classification is critical for efficient routing in customer service, ensuring customers are connected with the most suitable agents while reducing handling times and operational costs. However, as companies expand their product lines, intent classification faces scalability challenges due to the increasing number of intents and variations in taxonomy across different verticals. In this paper, we introduce REIC, a Retrieval-augmented generation Enhanced Intent Classification approach, which addresses these challenges effectively. REIC leverages retrieval-augmented generation (RAG) to dynamically incorporate relevant knowledge, enabling precise classification without the need for frequent retraining. Through extensive experiments on real-world datasets, we demonstrate that REIC outperforms traditional fine-tuning, zero-shot, and few-shot methods in large-scale customer service settings. Our results highlight its effectiveness in both in-domain and out-of-domain scenarios, demonstrating its potential for real-world deployment in adaptive and large-scale intent classification systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-30T20:32:10Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    20,
                    32,
                    10,
                    4,
                    150,
                    0
                ],
                "arxiv_comment": "Accepted by EMNLP 2025 (Industry Track)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ziji Zhang"
                    },
                    {
                        "name": "Michael Yang"
                    },
                    {
                        "name": "Zhiyu Chen"
                    },
                    {
                        "name": "Yingying Zhuang"
                    },
                    {
                        "name": "Shu-Ting Pi"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Rajashekar Maragoud"
                    },
                    {
                        "name": "Vy Nguyen"
                    },
                    {
                        "name": "Anurag Beniwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Beniwal"
                },
                "author": "Anurag Beniwal"
            },
            {
                "id": "http://arxiv.org/abs/2511.13476v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13476v1",
                "title": "Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation"
                },
                "updated": "2025-11-17T15:14:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    14,
                    17,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13476v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.3390/app152111619",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T15:14:17Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    14,
                    17,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "arxiv_journal_ref": "Applied Sciences, 2025, 15(21), 11619",
                "authors": [
                    {
                        "name": "Zhipeng Ma"
                    },
                    {
                        "name": "Ali Rida Bahja"
                    },
                    {
                        "name": "Andreas Burgdorf"
                    },
                    {
                        "name": "André Pomp"
                    },
                    {
                        "name": "Tobias Meisen"
                    },
                    {
                        "name": "Bo Nørregaard Jørgensen"
                    },
                    {
                        "name": "Zheng Grace Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Grace Ma"
                },
                "author": "Zheng Grace Ma",
                "arxiv_doi": "10.3390/app152111619"
            },
            {
                "id": "http://arxiv.org/abs/2511.13458v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13458v1",
                "title": "Trust in Vision-Language Models: Insights from a Participatory User Workshop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust in Vision-Language Models: Insights from a Participatory User Workshop"
                },
                "updated": "2025-11-17T15:04:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    4,
                    59,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13458v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T15:04:59Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    4,
                    59,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "arxiv_journal_ref": "Proceedings of the The European Workshop on Trustworthy AI (Trust-AI) at ECAI 2025",
                "authors": [
                    {
                        "name": "Agnese Chiatti"
                    },
                    {
                        "name": "Lara Piccolo"
                    },
                    {
                        "name": "Sara Bernardini"
                    },
                    {
                        "name": "Matteo Matteucci"
                    },
                    {
                        "name": "Viola Schiaffonati"
                    }
                ],
                "author_detail": {
                    "name": "Viola Schiaffonati"
                },
                "author": "Viola Schiaffonati"
            },
            {
                "id": "http://arxiv.org/abs/2511.04486v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.04486v2",
                "title": "EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits"
                },
                "updated": "2025-11-17T15:03:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    3,
                    36,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.04486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.04486v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Instructed code editing, where LLMs directly modify a developer's existing code based on a user instruction, is becoming a widely used interaction mode in AI coding assistants. However, few benchmarks directly evaluate this capability and current datasets often rely on artificial sources. We introduce EDIT-Bench, a benchmark for evaluating LLM code editing capabilities grounded in real-world usage, i.e., user instructions and code contexts collected in the wild. EDIT-Bench comprises of 540 problems, multiple natural and programming languages, and a diverse set of real-world use cases, ranging from resolving errors to adding features. EDIT-Bench introduces context-dependent problems that require the model to understand code context, highlighted code, and cursor position in addition to the user instruction. We evaluate 40 diverse LLMs and observe that EDIT-Bench is a challenging set of problems where only 1 model scores over 60%. We find that model performance varies across different categories of user instructions. Further, we find that varying levels of contextual information greatly affect task success rate, with performance varying up to 11%, indicating the importance of evaluating with realistic context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instructed code editing, where LLMs directly modify a developer's existing code based on a user instruction, is becoming a widely used interaction mode in AI coding assistants. However, few benchmarks directly evaluate this capability and current datasets often rely on artificial sources. We introduce EDIT-Bench, a benchmark for evaluating LLM code editing capabilities grounded in real-world usage, i.e., user instructions and code contexts collected in the wild. EDIT-Bench comprises of 540 problems, multiple natural and programming languages, and a diverse set of real-world use cases, ranging from resolving errors to adding features. EDIT-Bench introduces context-dependent problems that require the model to understand code context, highlighted code, and cursor position in addition to the user instruction. We evaluate 40 diverse LLMs and observe that EDIT-Bench is a challenging set of problems where only 1 model scores over 60%. We find that model performance varies across different categories of user instructions. Further, we find that varying levels of contextual information greatly affect task success rate, with performance varying up to 11%, indicating the importance of evaluating with realistic context."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-06T16:05:28Z",
                "published_parsed": [
                    2025,
                    11,
                    6,
                    16,
                    5,
                    28,
                    3,
                    310,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Wayne Chi"
                    },
                    {
                        "name": "Valerie Chen"
                    },
                    {
                        "name": "Ryan Shar"
                    },
                    {
                        "name": "Aditya Mittal"
                    },
                    {
                        "name": "Jenny Liang"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Anastasios Nikolas Angelopoulos"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Chris Donahue"
                    }
                ],
                "author_detail": {
                    "name": "Chris Donahue"
                },
                "author": "Chris Donahue"
            },
            {
                "id": "http://arxiv.org/abs/2507.16124v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.16124v3",
                "title": "Benchmarking LLM Privacy Recognition for Social Robot Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM Privacy Recognition for Social Robot Decision Making"
                },
                "updated": "2025-11-17T15:01:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    15,
                    1,
                    43,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.16124v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.16124v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While robots have previously utilized rule-based systems or probabilistic models for user interaction, the rapid evolution of large language models (LLMs) presents new opportunities to develop LLM-powered robots for enhanced human-robot interaction (HRI). To fully realize these capabilities, however, robots need to collect data such as audio, fine-grained images, video, and locations. As a result, LLMs often process sensitive personal information, particularly within private environments, such as homes. Given the tension between utility and privacy risks, evaluating how current LLMs manage sensitive data is critical. Specifically, we aim to explore the extent to which out-of-the-box LLMs are privacy-aware in the context of household robots. In this work, we present a set of privacy-relevant scenarios developed using the Contextual Integrity (CI) framework. We first surveyed users' privacy preferences regarding in-home robot behaviors and then examined how their privacy orientations affected their choices of these behaviors (N = 450). We then provided the same set of scenarios and questions to state-of-the-art LLMs (N = 10) and found that the agreement between humans and LLMs was generally low. To further investigate the capabilities of LLMs as potential privacy controllers, we implemented four additional prompting strategies and compared their results. We discuss the performance of the evaluated models as well as the implications and potential of AI privacy awareness in human-robot interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While robots have previously utilized rule-based systems or probabilistic models for user interaction, the rapid evolution of large language models (LLMs) presents new opportunities to develop LLM-powered robots for enhanced human-robot interaction (HRI). To fully realize these capabilities, however, robots need to collect data such as audio, fine-grained images, video, and locations. As a result, LLMs often process sensitive personal information, particularly within private environments, such as homes. Given the tension between utility and privacy risks, evaluating how current LLMs manage sensitive data is critical. Specifically, we aim to explore the extent to which out-of-the-box LLMs are privacy-aware in the context of household robots. In this work, we present a set of privacy-relevant scenarios developed using the Contextual Integrity (CI) framework. We first surveyed users' privacy preferences regarding in-home robot behaviors and then examined how their privacy orientations affected their choices of these behaviors (N = 450). We then provided the same set of scenarios and questions to state-of-the-art LLMs (N = 10) and found that the agreement between humans and LLMs was generally low. To further investigate the capabilities of LLMs as potential privacy controllers, we implemented four additional prompting strategies and compared their results. We discuss the performance of the evaluated models as well as the implications and potential of AI privacy awareness in human-robot interaction."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-22T00:36:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    0,
                    36,
                    59,
                    1,
                    203,
                    0
                ],
                "arxiv_comment": "18 pages, 7 figures. Dakota Sullivan and Shirley Zhang contributed equally to this work",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Dakota Sullivan"
                    },
                    {
                        "name": "Shirley Zhang"
                    },
                    {
                        "name": "Jennica Li"
                    },
                    {
                        "name": "Heather Kirkorian"
                    },
                    {
                        "name": "Bilge Mutlu"
                    },
                    {
                        "name": "Kassem Fawaz"
                    }
                ],
                "author_detail": {
                    "name": "Kassem Fawaz"
                },
                "author": "Kassem Fawaz"
            },
            {
                "id": "http://arxiv.org/abs/2504.10950v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.10950v2",
                "title": "Unveiling Challenges for LLMs in Enterprise Data Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Challenges for LLMs in Enterprise Data Engineering"
                },
                "updated": "2025-11-17T14:54:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    54,
                    12,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.10950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.10950v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.14778/3773749.3773758",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Large Language Models (LLMs) promise to automate data engineering on tabular data, offering enterprises a valuable opportunity to cut the high costs of manual data handling. But the enterprise domain comes with unique challenges that existing LLM-based approaches for data engineering often overlook, such as large table sizes, more complex tasks, and the need for internal knowledge. To bridge these gaps, we identify key enterprise-specific challenges related to data, tasks, and background knowledge and extensively evaluate how they affect data engineering with LLMs. Our analysis reveals that LLMs face substantial limitations in real-world enterprise scenarios, with accuracy declining sharply. Our findings contribute to a systematic understanding of LLMs for enterprise data engineering to support their adoption in industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) promise to automate data engineering on tabular data, offering enterprises a valuable opportunity to cut the high costs of manual data handling. But the enterprise domain comes with unique challenges that existing LLM-based approaches for data engineering often overlook, such as large table sizes, more complex tasks, and the need for internal knowledge. To bridge these gaps, we identify key enterprise-specific challenges related to data, tasks, and background knowledge and extensively evaluate how they affect data engineering with LLMs. Our analysis reveals that LLMs face substantial limitations in real-world enterprise scenarios, with accuracy declining sharply. Our findings contribute to a systematic understanding of LLMs for enterprise data engineering to support their adoption in industry."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-15T07:57:05Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    7,
                    57,
                    5,
                    1,
                    105,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Jan-Micha Bodensohn"
                    },
                    {
                        "name": "Ulf Brackmann"
                    },
                    {
                        "name": "Liane Vogel"
                    },
                    {
                        "name": "Anupam Sanghi"
                    },
                    {
                        "name": "Carsten Binnig"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Binnig"
                },
                "author": "Carsten Binnig",
                "arxiv_doi": "10.14778/3773749.3773758"
            },
            {
                "id": "http://arxiv.org/abs/2511.13421v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13421v1",
                "title": "Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression"
                },
                "updated": "2025-11-17T14:34:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    34,
                    3,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13421v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \\textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \\approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \\textit{i.e.}, $E(K, N) \\approx K$ for $K \\le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \\approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \\textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \\approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \\textit{i.e.}, $E(K, N) \\approx K$ for $K \\le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \\approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:34:03Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    34,
                    3,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Tingkai Yan"
                    },
                    {
                        "name": "Haodong Wen"
                    },
                    {
                        "name": "Binghui Li"
                    },
                    {
                        "name": "Kairong Luo"
                    },
                    {
                        "name": "Wenguang Chen"
                    },
                    {
                        "name": "Kaifeng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Kaifeng Lyu"
                },
                "author": "Kaifeng Lyu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13410v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13410v1",
                "title": "Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction"
                },
                "updated": "2025-11-17T14:22:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    22,
                    32,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13410v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T14:22:32Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    14,
                    22,
                    32,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026 (Oral)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhaopei Huang"
                    },
                    {
                        "name": "Qifeng Dai"
                    },
                    {
                        "name": "Guozheng Wu"
                    },
                    {
                        "name": "Xiaopeng Wu"
                    },
                    {
                        "name": "Kehan Chen"
                    },
                    {
                        "name": "Chuan Yu"
                    },
                    {
                        "name": "Xubin Li"
                    },
                    {
                        "name": "Tiezheng Ge"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Qin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Qin Jin"
                },
                "author": "Qin Jin"
            },
            {
                "id": "http://arxiv.org/abs/2509.07864v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.07864v2",
                "title": "Tracing and Mitigating Hallucinations in Multimodal LLMs via Dynamic Attention Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing and Mitigating Hallucinations in Multimodal LLMs via Dynamic Attention Localization"
                },
                "updated": "2025-11-17T13:57:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    57,
                    4,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.07864v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal Large Language Models (MLLMs) achieve strong performance on tasks like image captioning and visual question answering, but remain prone to hallucinations, where generated text conflicts with the visual input. Prior work links this partly to insufficient visual attention, but existing attention-based detectors and mitigation typically apply uniform adjustments across layers and heads, obscuring where errors originate. In this paper, we first show these methods fail to accurately localize problematic layers. Then, we introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags anomalous layers, and Image Attention Focus (IAF) which scores attention heads within those layers. Analysis shows that LIAE pinpoints faulty layers and IAF reliably ranks heads that warrant correction. Guided by these signals, we propose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a task-agnostic, attention-guided method that dynamically localizes and corrects errors during inference with negligible overhead. Furthermore, by establishing a connection between D-LEAF and DPO, we provide theoretical justification for the effectiveness of D-LEAF. Results show our D-LEAF delivers a 53\\% relative improvement on standard captioning benchmarks, and on VQA both accuracy and F1-score improve by approximately 4\\%, substantially suppressing hallucinations while preserving efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) achieve strong performance on tasks like image captioning and visual question answering, but remain prone to hallucinations, where generated text conflicts with the visual input. Prior work links this partly to insufficient visual attention, but existing attention-based detectors and mitigation typically apply uniform adjustments across layers and heads, obscuring where errors originate. In this paper, we first show these methods fail to accurately localize problematic layers. Then, we introduce two diagnostics: Layer Image Attention Entropy (LIAE) which flags anomalous layers, and Image Attention Focus (IAF) which scores attention heads within those layers. Analysis shows that LIAE pinpoints faulty layers and IAF reliably ranks heads that warrant correction. Guided by these signals, we propose Dynamic Layer-wise Entropy and Attention Fusion (D-LEAF), a task-agnostic, attention-guided method that dynamically localizes and corrects errors during inference with negligible overhead. Furthermore, by establishing a connection between D-LEAF and DPO, we provide theoretical justification for the effectiveness of D-LEAF. Results show our D-LEAF delivers a 53\\% relative improvement on standard captioning benchmarks, and on VQA both accuracy and F1-score improve by approximately 4\\%, substantially suppressing hallucinations while preserving efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-09T15:51:15Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    15,
                    51,
                    15,
                    1,
                    252,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tiancheng Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Jiaye Lin"
                    },
                    {
                        "name": "Guimin Hu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Lijie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Lijie Hu"
                },
                "author": "Lijie Hu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13381v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13381v1",
                "title": "Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts"
                },
                "updated": "2025-11-17T13:54:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    54,
                    0,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13381v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:54:00Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    54,
                    0,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Siyu Zhu"
                    },
                    {
                        "name": "Mouxiao Bian"
                    },
                    {
                        "name": "Yue Xie"
                    },
                    {
                        "name": "Yongyu Tang"
                    },
                    {
                        "name": "Zhikang Yu"
                    },
                    {
                        "name": "Tianbin Li"
                    },
                    {
                        "name": "Pengcheng Chen"
                    },
                    {
                        "name": "Bing Han"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Xiaoyan Dong"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyan Dong"
                },
                "author": "Xiaoyan Dong"
            },
            {
                "id": "http://arxiv.org/abs/2511.13373v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13373v1",
                "title": "A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs"
                },
                "updated": "2025-11-17T13:47:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    47,
                    27,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13373v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:47:27Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    47,
                    27,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Prakrit Timilsina"
                    },
                    {
                        "name": "Anuj Nepal"
                    },
                    {
                        "name": "Rajan Kadel"
                    },
                    {
                        "name": "Robin Doss"
                    }
                ],
                "author_detail": {
                    "name": "Robin Doss"
                },
                "author": "Robin Doss"
            },
            {
                "id": "http://arxiv.org/abs/2511.11560v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11560v2",
                "title": "A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication"
                },
                "updated": "2025-11-17T13:43:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    43,
                    56,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11560v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In semi-decentralized federated learning, devices primarily rely on device-to-device communication but occasionally interact with a central server. Periodically, a sampled subset of devices uploads their local models to the server, which computes an aggregate model. The server can then either (i) share this aggregate model only with the sampled clients (sampled-to-sampled, S2S) or (ii) broadcast it to all clients (sampled-to-all, S2A). Despite their practical significance, a rigorous theoretical and empirical comparison of these two strategies remains absent. We address this gap by analyzing S2S and S2A within a unified convergence framework that accounts for key system parameters: sampling rate, server aggregation frequency, and network connectivity. Our results, both analytical and experimental, reveal distinct regimes where one strategy outperforms the other, depending primarily on the degree of data heterogeneity across devices. These insights lead to concrete design guidelines for practical semi-decentralized FL deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In semi-decentralized federated learning, devices primarily rely on device-to-device communication but occasionally interact with a central server. Periodically, a sampled subset of devices uploads their local models to the server, which computes an aggregate model. The server can then either (i) share this aggregate model only with the sampled clients (sampled-to-sampled, S2S) or (ii) broadcast it to all clients (sampled-to-all, S2A). Despite their practical significance, a rigorous theoretical and empirical comparison of these two strategies remains absent. We address this gap by analyzing S2S and S2A within a unified convergence framework that accounts for key system parameters: sampling rate, server aggregation frequency, and network connectivity. Our results, both analytical and experimental, reveal distinct regimes where one strategy outperforms the other, depending primarily on the degree of data heterogeneity across devices. These insights lead to concrete design guidelines for practical semi-decentralized FL deployments."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T18:53:37Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    18,
                    53,
                    37,
                    4,
                    318,
                    0
                ],
                "arxiv_comment": "Accepted as a conference paper at AAAI 2026 (oral presentation). This is the extended version including the appendix",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Angelo Rodio"
                    },
                    {
                        "name": "Giovanni Neglia"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Erik G. Larsson"
                    }
                ],
                "author_detail": {
                    "name": "Erik G. Larsson"
                },
                "author": "Erik G. Larsson"
            },
            {
                "id": "http://arxiv.org/abs/2510.04108v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.04108v2",
                "title": "Can Linear Probes Measure LLM Uncertainty?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Linear Probes Measure LLM Uncertainty?"
                },
                "updated": "2025-11-17T13:43:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    43,
                    43,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.04108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.04108v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Effective Uncertainty Quantification (UQ) represents a key aspect for reliable deployment of Large Language Models (LLMs) in automated decision-making and beyond. Yet, for LLM generation with multiple choice structure, the state-of-the-art in UQ is still dominated by the naive baseline given by the maximum softmax score. To address this shortcoming, we demonstrate that taking a principled approach via Bayesian statistics leads to improved performance despite leveraging the simplest possible model, namely linear regression. More precisely, we propose to train multiple Bayesian linear models, each predicting the output of a layer given the output of the previous one. Based on the obtained layer-level posterior distributions, we infer the global uncertainty level of the LLM by identifying a sparse combination of distributional features, leading to an efficient UQ scheme. Numerical experiments on various LLMs show consistent improvement over state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Uncertainty Quantification (UQ) represents a key aspect for reliable deployment of Large Language Models (LLMs) in automated decision-making and beyond. Yet, for LLM generation with multiple choice structure, the state-of-the-art in UQ is still dominated by the naive baseline given by the maximum softmax score. To address this shortcoming, we demonstrate that taking a principled approach via Bayesian statistics leads to improved performance despite leveraging the simplest possible model, namely linear regression. More precisely, we propose to train multiple Bayesian linear models, each predicting the output of a layer given the output of the previous one. Based on the obtained layer-level posterior distributions, we infer the global uncertainty level of the LLM by identifying a sparse combination of distributional features, leading to an efficient UQ scheme. Numerical experiments on various LLMs show consistent improvement over state-of-the-art baselines."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-05T09:14:57Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    9,
                    14,
                    57,
                    6,
                    278,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ramzi Dakhmouche"
                    },
                    {
                        "name": "Adrien Letellier"
                    },
                    {
                        "name": "Hossein Gorji"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Gorji"
                },
                "author": "Hossein Gorji"
            },
            {
                "id": "http://arxiv.org/abs/2511.13369v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13369v1",
                "title": "Unifying points of interest taxonomies: mapping OpenStreetMap tags to the Foursquare category system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying points of interest taxonomies: mapping OpenStreetMap tags to the Foursquare category system"
                },
                "updated": "2025-11-17T13:43:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    43,
                    25,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13369v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The heterogeneity of Point of Interest (POI) taxonomies is a persistent challenge for the integration of urban datasets and the development of location-based services. OpenStreetMap (OSM) adopts a flexible, community-driven tagging system, while Foursquare (FS) relies on a curated hierarchical structure. Here we present an openly available benchmark and mapping framework that aligns OSM tags with the FS taxonomy. This resource integrates the richness of community-driven OSM data with the hierarchical structure of FS, enabling reproducible and interoperable urban analytics. The dataset is complemented by an evaluation of embedding and LLM-based alignment strategies and a pipeline that supports scalable updates as OSM evolves. Together, these elements provide both a robust reference resource and a practical tool for the community. Our approach is structured around three components: the construction of a manually curated benchmark as a gold standard, the evaluation of pretrained text embedding models for semantic alignment between OSM tags and FS categories, and an LLM-based refinement stage that enhances robustness and adaptability. The proposed methodology provides a scalable and reproducible solution for taxonomy unification, with direct applications to urban analytics, mobility studies, and smart city services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The heterogeneity of Point of Interest (POI) taxonomies is a persistent challenge for the integration of urban datasets and the development of location-based services. OpenStreetMap (OSM) adopts a flexible, community-driven tagging system, while Foursquare (FS) relies on a curated hierarchical structure. Here we present an openly available benchmark and mapping framework that aligns OSM tags with the FS taxonomy. This resource integrates the richness of community-driven OSM data with the hierarchical structure of FS, enabling reproducible and interoperable urban analytics. The dataset is complemented by an evaluation of embedding and LLM-based alignment strategies and a pipeline that supports scalable updates as OSM evolves. Together, these elements provide both a robust reference resource and a practical tool for the community. Our approach is structured around three components: the construction of a manually curated benchmark as a gold standard, the evaluation of pretrained text embedding models for semantic alignment between OSM tags and FS categories, and an LLM-based refinement stage that enhances robustness and adaptability. The proposed methodology provides a scalable and reproducible solution for taxonomy unification, with direct applications to urban analytics, mobility studies, and smart city services."
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:43:25Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    43,
                    25,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI"
                },
                "authors": [
                    {
                        "name": "Lilou Soulas"
                    },
                    {
                        "name": "Lorenzo Lucchini"
                    },
                    {
                        "name": "Maurizio Napolitano"
                    },
                    {
                        "name": "Sebastiano Bontorin"
                    },
                    {
                        "name": "Simone Centellegher"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Riccardo Gallotti"
                    },
                    {
                        "name": "Eleonora Andreotti"
                    }
                ],
                "author_detail": {
                    "name": "Eleonora Andreotti"
                },
                "author": "Eleonora Andreotti"
            },
            {
                "id": "http://arxiv.org/abs/2511.13368v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13368v1",
                "title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning"
                },
                "updated": "2025-11-17T13:41:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    41,
                    31,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13368v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:41:31Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    41,
                    31,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kajetan Dymkiewicz"
                    },
                    {
                        "name": "Ivan Vulic"
                    },
                    {
                        "name": "Helen Yannakoudakis"
                    },
                    {
                        "name": "Eilam Shapira"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Anna Korhonen"
                    }
                ],
                "author_detail": {
                    "name": "Anna Korhonen"
                },
                "author": "Anna Korhonen"
            },
            {
                "id": "http://arxiv.org/abs/2309.06706v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2309.06706v3",
                "title": "Simultaneous Machine Translation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Machine Translation with Large Language Models"
                },
                "updated": "2025-11-17T13:41:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    41,
                    30,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2309.06706v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2309.06706v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Real-world simultaneous machine translation (SimulMT) systems face more challenges than just the quality-latency trade-off. They also need to address issues related to robustness with noisy input, processing long contexts, and flexibility for knowledge injection. These challenges demand models with strong language understanding and generation capabilities which may not often equipped by dedicated MT models. In this paper, we investigate the possibility of applying Large Language Models (LLM) to SimulMT tasks by using existing incremental-decoding methods with a newly proposed RALCP algorithm for latency reduction. We conducted experiments using the \\texttt{Llama2-7b-chat} model on nine different languages from the MUST-C dataset. The results show that LLM outperforms dedicated MT models in terms of BLEU and LAAL metrics. Further analysis indicates that LLM has advantages in terms of tuning efficiency and robustness. However, it is important to note that the computational cost of LLM remains a significant obstacle to its application in SimulMT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world simultaneous machine translation (SimulMT) systems face more challenges than just the quality-latency trade-off. They also need to address issues related to robustness with noisy input, processing long contexts, and flexibility for knowledge injection. These challenges demand models with strong language understanding and generation capabilities which may not often equipped by dedicated MT models. In this paper, we investigate the possibility of applying Large Language Models (LLM) to SimulMT tasks by using existing incremental-decoding methods with a newly proposed RALCP algorithm for latency reduction. We conducted experiments using the \\texttt{Llama2-7b-chat} model on nine different languages from the MUST-C dataset. The results show that LLM outperforms dedicated MT models in terms of BLEU and LAAL metrics. Further analysis indicates that LLM has advantages in terms of tuning efficiency and robustness. However, it is important to note that the computational cost of LLM remains a significant obstacle to its application in SimulMT."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-09-13T04:06:47Z",
                "published_parsed": [
                    2023,
                    9,
                    13,
                    4,
                    6,
                    47,
                    2,
                    256,
                    0
                ],
                "arxiv_comment": "Accepted to ALTA 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Jinming Zhao"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Fatemeh Shiri"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari"
            },
            {
                "id": "http://arxiv.org/abs/2508.03294v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.03294v2",
                "title": "NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty"
                },
                "updated": "2025-11-17T13:37:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    37,
                    20,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.03294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.03294v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Estimating the difficulty of exam questions is essential for developing good exams, but professors are not always good at this task. We compare various Large Language Model-based methods with three professors in their ability to estimate what percentage of students will give correct answers on True/False exam questions in the areas of Neural Networks and Machine Learning. Our results show that the professors have limited ability to distinguish between easy and difficult questions and that they are outperformed by directly asking Gemini 2.5 to solve this task. Yet, we obtained even better results using uncertainties of the LLMs solving the questions in a supervised learning setting, using only 42 training samples. We conclude that supervised learning using LLM uncertainty can help professors better estimate the difficulty of exam questions, improving the quality of assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the difficulty of exam questions is essential for developing good exams, but professors are not always good at this task. We compare various Large Language Model-based methods with three professors in their ability to estimate what percentage of students will give correct answers on True/False exam questions in the areas of Neural Networks and Machine Learning. Our results show that the professors have limited ability to distinguish between easy and difficult questions and that they are outperformed by directly asking Gemini 2.5 to solve this task. Yet, we obtained even better results using uncertainties of the LLMs solving the questions in a supervised learning setting, using only 42 training samples. We conclude that supervised learning using LLM uncertainty can help professors better estimate the difficulty of exam questions, improving the quality of assessment."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-05T10:12:38Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    10,
                    12,
                    38,
                    1,
                    217,
                    0
                ],
                "arxiv_comment": "10 pages, 2 figures, presented at ECAI 2025 at the 2nd International Workshop on AI in Society, Education and Educational Research (AISEER)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Leonidas Zotos"
                    },
                    {
                        "name": "Ivo Pascal de Jong"
                    },
                    {
                        "name": "Matias Valdenegro-Toro"
                    },
                    {
                        "name": "Andreea Ioana Sburlea"
                    },
                    {
                        "name": "Malvina Nissim"
                    },
                    {
                        "name": "Hedderik van Rijn"
                    }
                ],
                "author_detail": {
                    "name": "Hedderik van Rijn"
                },
                "author": "Hedderik van Rijn"
            },
            {
                "id": "http://arxiv.org/abs/2402.10552v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2402.10552v4",
                "title": "Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models"
                },
                "updated": "2025-11-17T13:33:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    33,
                    37,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2402.10552v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2402.10552v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.18653/v1/2025.iwslt-1.8",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Simultaneous machine translation (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency. In this paper, we propose a conversational SimulMT framework to enhance the inference efficiency of LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of LLM in translation quality while achieving comparable computational latency to specialized SimulMT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous machine translation (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency. In this paper, we propose a conversational SimulMT framework to enhance the inference efficiency of LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of LLM in translation quality while achieving comparable computational latency to specialized SimulMT models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-02-16T10:32:16Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    10,
                    32,
                    16,
                    4,
                    47,
                    0
                ],
                "arxiv_comment": "Accepted to IWSLT 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "arxiv_doi": "10.18653/v1/2025.iwslt-1.8"
            },
            {
                "id": "http://arxiv.org/abs/2511.13361v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13361v1",
                "title": "MedDCR: Learning to Design Agentic Workflows for Medical Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedDCR: Learning to Design Agentic Workflows for Medical Coding"
                },
                "updated": "2025-11-17T13:30:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    30,
                    51,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13361v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:30:51Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    30,
                    51,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jiyang Zheng"
                    },
                    {
                        "name": "Islam Nassar"
                    },
                    {
                        "name": "Thanh Vu"
                    },
                    {
                        "name": "Xu Zhong"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Long Duong"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuan-Fang Li"
                },
                "author": "Yuan-Fang Li"
            },
            {
                "id": "http://arxiv.org/abs/2505.20334v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.20334v2",
                "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query"
                },
                "updated": "2025-11-17T13:29:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    29,
                    25,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.20334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.20334v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-24T10:34:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    34,
                    38,
                    5,
                    144,
                    0
                ],
                "arxiv_comment": "Accepted by EMNLP 2025 Main",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che"
            },
            {
                "id": "http://arxiv.org/abs/2511.13357v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13357v1",
                "title": "FLOWER: Flow-Oriented Entity-Relationship Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLOWER: Flow-Oriented Entity-Relationship Tool"
                },
                "updated": "2025-11-17T13:23:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    23,
                    24,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13357v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:23:24Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    23,
                    24,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Dmitry Moskalev"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Moskalev"
                },
                "author": "Dmitry Moskalev"
            },
            {
                "id": "http://arxiv.org/abs/2509.11947v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.11947v2",
                "title": "A GPU-Accelerated RAG-Based Telegram Assistant for Supporting Parallel Processing Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A GPU-Accelerated RAG-Based Telegram Assistant for Supporting Parallel Processing Students"
                },
                "updated": "2025-11-17T13:19:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    19,
                    12,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.11947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.11947v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This project addresses a critical pedagogical need: offering students continuous, on-demand academic assistance beyond conventional reception hours. I present a domain-specific Retrieval-Augmented Generation (RAG) system powered by a quantized Mistral-7B Instruct model and deployed as a Telegram bot. The assistant enhances learning by delivering real-time, personalized responses aligned with the \"Introduction to Parallel Processing\" course materials. GPU acceleration significantly improves inference latency, enabling practical deployment on consumer hardware. This approach demonstrates how consumer GPUs can enable affordable, private, and effective AI tutoring for HPC education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This project addresses a critical pedagogical need: offering students continuous, on-demand academic assistance beyond conventional reception hours. I present a domain-specific Retrieval-Augmented Generation (RAG) system powered by a quantized Mistral-7B Instruct model and deployed as a Telegram bot. The assistant enhances learning by delivering real-time, personalized responses aligned with the \"Introduction to Parallel Processing\" course materials. GPU acceleration significantly improves inference latency, enabling practical deployment on consumer hardware. This approach demonstrates how consumer GPUs can enable affordable, private, and effective AI tutoring for HPC education."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-15T14:06:09Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    6,
                    9,
                    0,
                    258,
                    0
                ],
                "arxiv_comment": "9 pages",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Guy Tel-Zur"
                    }
                ],
                "author_detail": {
                    "name": "Guy Tel-Zur"
                },
                "author": "Guy Tel-Zur"
            },
            {
                "id": "http://arxiv.org/abs/2511.13347v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13347v1",
                "title": "Joint Transmit Beamforming and Reflection Optimization for Beyond Diagonal RIS Aided Multi-Cell MIMO Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Transmit Beamforming and Reflection Optimization for Beyond Diagonal RIS Aided Multi-Cell MIMO Communication"
                },
                "updated": "2025-11-17T13:12:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    12,
                    54,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13347v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The sixth-generation (6G) wireless networks will rely on ultra-dense multi-cell deployment to meet the high rate and connectivity demands. However, frequency reuse leads to severe inter-cell interference, particularly for cell-edge users, which limits the communication performance. To overcome this challenge, we investigate a beyond diagonal reconfigurable intelligent surface (BD-RIS) aided multi-cell multi-user downlink MIMO communication system, where a BD-RIS is deployed to enhance desired signals and suppress both intra-cell and inter-cell interference.We formulate the joint optimization problem of the transmit beamforming matrices at the BSs and the BD-RIS reflection matrix to maximize the weighted sum rate of all users, subject to the challenging unitary constraint of the BD-RIS reflection matrix and transmit power constraints at the BSs. To tackle this non-convex and difficult problem, we apply the weighted minimum mean squared error (WMMSE) method to transform the problem into an equivalent tractable form, and propose an efficient alternating optimization (AO) based algorithm to iteratively update the transmit beamforming and BD-RIS reflection using Lagrange duality theory and manifold optimization. Numerical results demonstrate the superiority of the proposed design over various benchmark schemes, and provide useful practical insights on the BD-RIS deployment strategy for multi-cell systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sixth-generation (6G) wireless networks will rely on ultra-dense multi-cell deployment to meet the high rate and connectivity demands. However, frequency reuse leads to severe inter-cell interference, particularly for cell-edge users, which limits the communication performance. To overcome this challenge, we investigate a beyond diagonal reconfigurable intelligent surface (BD-RIS) aided multi-cell multi-user downlink MIMO communication system, where a BD-RIS is deployed to enhance desired signals and suppress both intra-cell and inter-cell interference.We formulate the joint optimization problem of the transmit beamforming matrices at the BSs and the BD-RIS reflection matrix to maximize the weighted sum rate of all users, subject to the challenging unitary constraint of the BD-RIS reflection matrix and transmit power constraints at the BSs. To tackle this non-convex and difficult problem, we apply the weighted minimum mean squared error (WMMSE) method to transform the problem into an equivalent tractable form, and propose an efficient alternating optimization (AO) based algorithm to iteratively update the transmit beamforming and BD-RIS reflection using Lagrange duality theory and manifold optimization. Numerical results demonstrate the superiority of the proposed design over various benchmark schemes, and provide useful practical insights on the BD-RIS deployment strategy for multi-cell systems."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:12:54Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    12,
                    54,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "submitted for possible publication",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Shuo Zheng"
                    },
                    {
                        "name": "Shuowen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shuowen Zhang"
                },
                "author": "Shuowen Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.13341v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13341v1",
                "title": "An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains"
                },
                "updated": "2025-11-17T13:10:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    10,
                    36,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13341v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:10:36Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    10,
                    36,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "7 figures, 4 tables, conference",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Zihe Yan"
                    },
                    {
                        "name": "Kai Luo"
                    },
                    {
                        "name": "Haoyu Yang"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Guancheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Guancheng Li"
                },
                "author": "Guancheng Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.13333v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13333v1",
                "title": "AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research"
                },
                "updated": "2025-11-17T13:05:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    5,
                    25,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13333v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:05:25Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    5,
                    25,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted at AAAI 2026 (oral)",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Alexandru-Mihai Apostu"
                    },
                    {
                        "name": "Andrei Preda"
                    },
                    {
                        "name": "Alexandra Daniela Damir"
                    },
                    {
                        "name": "Diana Bolocan"
                    },
                    {
                        "name": "Radu Tudor Ionescu"
                    },
                    {
                        "name": "Ioana Croitoru"
                    },
                    {
                        "name": "Mihaela Gaman"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela Gaman"
                },
                "author": "Mihaela Gaman"
            },
            {
                "id": "http://arxiv.org/abs/2511.13329v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13329v1",
                "title": "RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection"
                },
                "updated": "2025-11-17T13:04:36Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    4,
                    36,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13329v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \\textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \\textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T13:04:36Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    4,
                    36,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shufan Yang"
                    },
                    {
                        "name": "Zifeng Cheng"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Yafeng Yin"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Shiping Ge"
                    },
                    {
                        "name": "Yuchen Fu"
                    },
                    {
                        "name": "Qing Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qing Gu"
                },
                "author": "Qing Gu"
            },
            {
                "id": "http://arxiv.org/abs/2509.11864v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.11864v2",
                "title": "NeuroStrike: Neuron-Level Attacks on Aligned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroStrike: Neuron-Level Attacks on Aligned LLMs"
                },
                "updated": "2025-11-17T13:02:51Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    13,
                    2,
                    51,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.11864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.11864v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.14722/ndss.2026.230660",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Safety alignment is critical for the ethical deployment of large language models (LLMs), guiding them to avoid generating harmful or unethical content. Current alignment techniques, such as supervised fine-tuning and reinforcement learning from human feedback, remain fragile and can be bypassed by carefully crafted adversarial prompts. Unfortunately, such attacks rely on trial and error, lack generalizability across models, and are constrained by scalability and reliability.\n  This paper presents NeuroStrike, a novel and generalizable attack framework that exploits a fundamental vulnerability introduced by alignment techniques: the reliance on sparse, specialized safety neurons responsible for detecting and suppressing harmful inputs. We apply NeuroStrike to both white-box and black-box settings: In the white-box setting, NeuroStrike identifies safety neurons through feedforward activation analysis and prunes them during inference to disable safety mechanisms. In the black-box setting, we propose the first LLM profiling attack, which leverages safety neuron transferability by training adversarial prompt generators on open-weight surrogate models and then deploying them against black-box and proprietary targets. We evaluate NeuroStrike on over 20 open-weight LLMs from major LLM developers. By removing less than 0.6% of neurons in targeted layers, NeuroStrike achieves an average attack success rate (ASR) of 76.9% using only vanilla malicious prompts. Moreover, Neurostrike generalizes to four multimodal LLMs with 100% ASR on unsafe image inputs. Safety neurons transfer effectively across architectures, raising ASR to 78.5% on 11 fine-tuned models and 77.7% on five distilled models. The black-box LLM profiling attack achieves an average ASR of 63.7% across five black-box models, including the Google Gemini family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment is critical for the ethical deployment of large language models (LLMs), guiding them to avoid generating harmful or unethical content. Current alignment techniques, such as supervised fine-tuning and reinforcement learning from human feedback, remain fragile and can be bypassed by carefully crafted adversarial prompts. Unfortunately, such attacks rely on trial and error, lack generalizability across models, and are constrained by scalability and reliability.\n  This paper presents NeuroStrike, a novel and generalizable attack framework that exploits a fundamental vulnerability introduced by alignment techniques: the reliance on sparse, specialized safety neurons responsible for detecting and suppressing harmful inputs. We apply NeuroStrike to both white-box and black-box settings: In the white-box setting, NeuroStrike identifies safety neurons through feedforward activation analysis and prunes them during inference to disable safety mechanisms. In the black-box setting, we propose the first LLM profiling attack, which leverages safety neuron transferability by training adversarial prompt generators on open-weight surrogate models and then deploying them against black-box and proprietary targets. We evaluate NeuroStrike on over 20 open-weight LLMs from major LLM developers. By removing less than 0.6% of neurons in targeted layers, NeuroStrike achieves an average attack success rate (ASR) of 76.9% using only vanilla malicious prompts. Moreover, Neurostrike generalizes to four multimodal LLMs with 100% ASR on unsafe image inputs. Safety neurons transfer effectively across architectures, raising ASR to 78.5% on 11 fine-tuned models and 77.7% on five distilled models. The black-box LLM profiling attack achieves an average ASR of 63.7% across five black-box models, including the Google Gemini family."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-15T12:38:39Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    12,
                    38,
                    39,
                    0,
                    258,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Lichao Wu"
                    },
                    {
                        "name": "Sasha Behrouzi"
                    },
                    {
                        "name": "Mohamadreza Rostami"
                    },
                    {
                        "name": "Maximilian Thang"
                    },
                    {
                        "name": "Stjepan Picek"
                    },
                    {
                        "name": "Ahmad-Reza Sadeghi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad-Reza Sadeghi"
                },
                "author": "Ahmad-Reza Sadeghi",
                "arxiv_doi": "10.14722/ndss.2026.230660"
            },
            {
                "id": "http://arxiv.org/abs/2511.13319v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13319v1",
                "title": "Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs"
                },
                "updated": "2025-11-17T12:56:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    56,
                    33,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13319v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.\n  In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $ε$-local differential privacy ($ε$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.\n  In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $ε$-local differential privacy ($ε$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T12:56:33Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    56,
                    33,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Chelsea McMurray"
                    },
                    {
                        "name": "Hayder Tirmazi"
                    }
                ],
                "author_detail": {
                    "name": "Hayder Tirmazi"
                },
                "author": "Hayder Tirmazi"
            },
            {
                "id": "http://arxiv.org/abs/2511.13305v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13305v1",
                "title": "SAINT: Service-level Integration Test Generation with Program Analysis and LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAINT: Service-level Integration Test Generation with Program Analysis and LLM-based Agents"
                },
                "updated": "2025-11-17T12:29:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    29,
                    42,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13305v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Enterprise applications are typically tested at multiple levels, with service-level testing playing an important role in validating application functionality. Existing service-level testing tools, especially for RESTful APIs, often employ fuzzing and/or depend on OpenAPI specifications which are not readily available in real-world enterprise codebases. Moreover, these tools are limited in their ability to generate functional tests that effectively exercise meaningful scenarios. In this work, we present SAINT, a novel white-box testing approach for service-level testing of enterprise Java applications. SAINT combines static analysis, large language models (LLMs), and LLM-based agents to automatically generate endpoint and scenario-based tests. The approach builds two key models: an endpoint model, capturing syntactic and semantic information about service endpoints, and an operation dependency graph, capturing inter-endpoint ordering constraints. SAINT then employs LLM-based agents to generate tests. Endpoint-focused tests aim to maximize code and database interaction coverage. Scenario-based tests are synthesized by extracting application use cases from code and refining them into executable tests via planning, action, and reflection phases of the agentic loop. We evaluated SAINT on eight Java applications, including a proprietary enterprise application. Our results illustrate the effectiveness of SAINT in coverage, fault detection, and scenario generation. Moreover, a developer survey provides strong endorsement of the scenario-based tests generated by SAINT. Overall, our work shows that combining static analysis with agentic LLM workflows enables more effective, functional, and developer-aligned service-level test generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise applications are typically tested at multiple levels, with service-level testing playing an important role in validating application functionality. Existing service-level testing tools, especially for RESTful APIs, often employ fuzzing and/or depend on OpenAPI specifications which are not readily available in real-world enterprise codebases. Moreover, these tools are limited in their ability to generate functional tests that effectively exercise meaningful scenarios. In this work, we present SAINT, a novel white-box testing approach for service-level testing of enterprise Java applications. SAINT combines static analysis, large language models (LLMs), and LLM-based agents to automatically generate endpoint and scenario-based tests. The approach builds two key models: an endpoint model, capturing syntactic and semantic information about service endpoints, and an operation dependency graph, capturing inter-endpoint ordering constraints. SAINT then employs LLM-based agents to generate tests. Endpoint-focused tests aim to maximize code and database interaction coverage. Scenario-based tests are synthesized by extracting application use cases from code and refining them into executable tests via planning, action, and reflection phases of the agentic loop. We evaluated SAINT on eight Java applications, including a proprietary enterprise application. Our results illustrate the effectiveness of SAINT in coverage, fault detection, and scenario generation. Moreover, a developer survey provides strong endorsement of the scenario-based tests generated by SAINT. Overall, our work shows that combining static analysis with agentic LLM workflows enables more effective, functional, and developer-aligned service-level test generation."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T12:29:42Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    29,
                    42,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted at ICSE'26",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Rangeet Pan"
                    },
                    {
                        "name": "Raju Pavuluri"
                    },
                    {
                        "name": "Ruikai Huang"
                    },
                    {
                        "name": "Rahul Krishna"
                    },
                    {
                        "name": "Tyler Stennett"
                    },
                    {
                        "name": "Alessandro Orso"
                    },
                    {
                        "name": "Saurabh SInha"
                    }
                ],
                "author_detail": {
                    "name": "Saurabh SInha"
                },
                "author": "Saurabh SInha"
            },
            {
                "id": "http://arxiv.org/abs/2507.01769v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.01769v2",
                "title": "Scalable Satellite Swarm Deployment via Distance-based Orbital Transition Under $J_2$ Perturbation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Satellite Swarm Deployment via Distance-based Orbital Transition Under $J_2$ Perturbation"
                },
                "updated": "2025-11-17T12:29:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    29,
                    4,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.01769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.01769v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents an autonomous guidance and control strategy for a satellite swarm that enables scalable distributed space structures for innovative science and business opportunities. The averaged $J_2$ orbital parameters that describe the drift and periodic orbital motion were derived along with their target values to achieve a distributed space structure in a decentralized manner. This enabled the design of a distance-based orbital stabilizer to ensure autonomous deployment into a monolithic formation of a coplanar equidistant configuration on a user-defined orbital plane. Continuous formation control was assumed to be achieved through fuel-free actuation, such as satellite magnetic field interaction and differential aerodynamic forces, thereby maintaining long-term formation stability without thruster usage. A major challenge for such actuation systems is the potential loss of control capability due to increasing inter-satellite distances resulting from unstable orbital dynamics, particularly for autonomous satellite swarms. To mitigate this risk, our decentralized deployment controller minimized drift distance during unexpected communication outages. As a case study, we consider the deployment of palm-sized satellites into a coplanar equidistant formation in a $J_2$-perturbed orbit. Moreover, centralized grouping strategies are presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an autonomous guidance and control strategy for a satellite swarm that enables scalable distributed space structures for innovative science and business opportunities. The averaged $J_2$ orbital parameters that describe the drift and periodic orbital motion were derived along with their target values to achieve a distributed space structure in a decentralized manner. This enabled the design of a distance-based orbital stabilizer to ensure autonomous deployment into a monolithic formation of a coplanar equidistant configuration on a user-defined orbital plane. Continuous formation control was assumed to be achieved through fuel-free actuation, such as satellite magnetic field interaction and differential aerodynamic forces, thereby maintaining long-term formation stability without thruster usage. A major challenge for such actuation systems is the potential loss of control capability due to increasing inter-satellite distances resulting from unstable orbital dynamics, particularly for autonomous satellite swarms. To mitigate this risk, our decentralized deployment controller minimized drift distance during unexpected communication outages. As a case study, we consider the deployment of palm-sized satellites into a coplanar equidistant formation in a $J_2$-perturbed orbit. Moreover, centralized grouping strategies are presented."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-02T14:50:21Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    14,
                    50,
                    21,
                    2,
                    183,
                    0
                ],
                "arxiv_comment": "Submitted to AIAA Journal of Guidance, Control, and Dynamics",
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Yuta Takahashi"
                    },
                    {
                        "name": "Shin-ichiro Sakai"
                    }
                ],
                "author_detail": {
                    "name": "Shin-ichiro Sakai"
                },
                "author": "Shin-ichiro Sakai"
            },
            {
                "id": "http://arxiv.org/abs/2507.02962v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.02962v5",
                "title": "RAG-R1: Incentivizing the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-R1: Incentivizing the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism"
                },
                "updated": "2025-11-17T12:23:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    23,
                    38,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.02962v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.02962v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs), despite their remarkable capabilities, are prone to generating hallucinated or outdated content due to their static internal knowledge. While Retrieval-Augmented Generation (RAG) integrated with Reinforcement Learning (RL) offers a solution, these methods are fundamentally constrained by a single-query mode, leading to prohibitive latency and inherent brittleness. To overcome these limitations, we introduce RAG-R1, a novel two-stage training framework centered around multi-query parallelism. Our framework enables LLMs to adaptively leverage internal and external knowledge during the reasoning process while transitioning from the single-query mode to multi-query parallelism. This architectural shift bolsters reasoning robustness while significantly reducing inference latency. Extensive experiments on seven question-answering benchmarks confirm the superiority of our method, which outperforms the strongest baseline by up to 13.7% and decreases inference time by 11.1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite their remarkable capabilities, are prone to generating hallucinated or outdated content due to their static internal knowledge. While Retrieval-Augmented Generation (RAG) integrated with Reinforcement Learning (RL) offers a solution, these methods are fundamentally constrained by a single-query mode, leading to prohibitive latency and inherent brittleness. To overcome these limitations, we introduce RAG-R1, a novel two-stage training framework centered around multi-query parallelism. Our framework enables LLMs to adaptively leverage internal and external knowledge during the reasoning process while transitioning from the single-query mode to multi-query parallelism. This architectural shift bolsters reasoning robustness while significantly reducing inference latency. Extensive experiments on seven question-answering benchmarks confirm the superiority of our method, which outperforms the strongest baseline by up to 13.7% and decreases inference time by 11.1%."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-30T09:02:45Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    9,
                    2,
                    45,
                    0,
                    181,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhiwen Tan"
                    },
                    {
                        "name": "Jiaming Huang"
                    },
                    {
                        "name": "Qintong Wu"
                    },
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Gu"
                },
                "author": "Jinjie Gu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13293v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13293v1",
                "title": "Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval"
                },
                "updated": "2025-11-17T12:15:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    15,
                    46,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13293v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \\underline{g}enerative \\underline{h}ierarchical \\underline{a}gentic \\underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \\underline{g}enerative \\underline{h}ierarchical \\underline{a}gentic \\underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T12:15:46Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    15,
                    46,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Chuang Zhao"
                    },
                    {
                        "name": "Hui Tang"
                    },
                    {
                        "name": "Hongke Zhao"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    },
                    {
                        "name": "Xiaomeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaomeng Li"
                },
                "author": "Xiaomeng Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.13290v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13290v1",
                "title": "Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment"
                },
                "updated": "2025-11-17T12:13:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    13,
                    15,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13290v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via \"dropout\" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via \"dropout\" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T12:13:15Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    13,
                    15,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jea Kwon"
                    },
                    {
                        "name": "Luiz Felipe Vecchietti"
                    },
                    {
                        "name": "Sungwon Park"
                    },
                    {
                        "name": "Meeyoung Cha"
                    }
                ],
                "author_detail": {
                    "name": "Meeyoung Cha"
                },
                "author": "Meeyoung Cha"
            },
            {
                "id": "http://arxiv.org/abs/2510.25506v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.25506v3",
                "title": "Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies"
                },
                "updated": "2025-11-17T12:06:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    6,
                    48,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.25506v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.25506v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3744916.3773207",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Large Language Models have gained remarkable interest in industry and academia. The increasing interest in LLMs in academia is also reflected in the number of publications on this topic over the last years. For instance, alone 78 of the around 425 publications at ICSE 2024 performed experiments with LLMs. Conducting empirical studies with LLMs remains challenging and raises questions on how to achieve reproducible results, for both researchers and practitioners. One important step towards excelling in empirical research on LLM and their application is to first understand to what extent current research results are eventually reproducible and what factors may impede reproducibility. This investigation is within the scope of our work. We contribute an analysis of the reproducibility of LLM-centric studies, provide insights into the factors impeding reproducibility, and discuss suggestions on how to improve the current state. In particular, we studied the 85 articles describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 85 articles, 18 provided research artefacts and used OpenAI models. We attempted to replicate those 18 studies. Of the 18 studies, only five were sufficiently complete and executable. For none of the five studies, we were able to fully reproduce the results. Two studies seemed to be partially reproducible, and three studies did not seem to be reproducible. Our results highlight not only the need for stricter research artefact evaluations but also for more robust study designs to ensure the reproducible value of future publications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have gained remarkable interest in industry and academia. The increasing interest in LLMs in academia is also reflected in the number of publications on this topic over the last years. For instance, alone 78 of the around 425 publications at ICSE 2024 performed experiments with LLMs. Conducting empirical studies with LLMs remains challenging and raises questions on how to achieve reproducible results, for both researchers and practitioners. One important step towards excelling in empirical research on LLM and their application is to first understand to what extent current research results are eventually reproducible and what factors may impede reproducibility. This investigation is within the scope of our work. We contribute an analysis of the reproducibility of LLM-centric studies, provide insights into the factors impeding reproducibility, and discuss suggestions on how to improve the current state. In particular, we studied the 85 articles describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 85 articles, 18 provided research artefacts and used OpenAI models. We attempted to replicate those 18 studies. Of the 18 studies, only five were sufficiently complete and executable. For none of the five studies, we were able to fully reproduce the results. Two studies seemed to be partially reproducible, and three studies did not seem to be reproducible. Our results highlight not only the need for stricter research artefact evaluations but also for more robust study designs to ensure the reproducible value of future publications."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-29T13:31:32Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    31,
                    32,
                    2,
                    302,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Florian Angermeir"
                    },
                    {
                        "name": "Maximilian Amougou"
                    },
                    {
                        "name": "Mark Kreitz"
                    },
                    {
                        "name": "Andreas Bauer"
                    },
                    {
                        "name": "Matthias Linhuber"
                    },
                    {
                        "name": "Davide Fucci"
                    },
                    {
                        "name": "Fabiola Moyón C."
                    },
                    {
                        "name": "Daniel Mendez"
                    },
                    {
                        "name": "Tony Gorschek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Gorschek"
                },
                "author": "Tony Gorschek",
                "arxiv_doi": "10.1145/3744916.3773207"
            },
            {
                "id": "http://arxiv.org/abs/2511.13288v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13288v1",
                "title": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO"
                },
                "updated": "2025-11-17T12:06:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    6,
                    30,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13288v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T12:06:30Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    12,
                    6,
                    30,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Haoyang Hong"
                    },
                    {
                        "name": "Jiajun Yin"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Jingnan Liu"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Ailing Yu"
                    },
                    {
                        "name": "Ji Li"
                    },
                    {
                        "name": "Zhiling Ye"
                    },
                    {
                        "name": "Hansong Xiao"
                    },
                    {
                        "name": "Yefei Chen"
                    },
                    {
                        "name": "Hualei Zhou"
                    },
                    {
                        "name": "Yun Yue"
                    },
                    {
                        "name": "Minghui Yang"
                    },
                    {
                        "name": "Chunxiao Guo"
                    },
                    {
                        "name": "Junwei Liu"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Jinjie Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Gu"
                },
                "author": "Jinjie Gu"
            },
            {
                "id": "http://arxiv.org/abs/2410.22041v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2410.22041v3",
                "title": "An LLM-based Simulation Framework for Embodied Conversational Agents in Psychological Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-based Simulation Framework for Embodied Conversational Agents in Psychological Counseling"
                },
                "updated": "2025-11-17T11:54:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    54,
                    40,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2410.22041v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2410.22041v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Due to privacy concerns, open dialogue datasets for mental health are primarily generated through human or AI synthesis methods. However, the inherent implicit nature of psychological processes, particularly those of clients, poses challenges to the authenticity and diversity of synthetic data. In this paper, we propose ECAs (short for Embodied Conversational Agents), a framework for embodied agent simulation based on Large Language Models (LLMs) that incorporates multiple psychological theoretical principles.Using simulation, we expand real counseling case data into a nuanced embodied cognitive memory space and generate dialogue data based on high-frequency counseling questions.We validated our framework using the D4 dataset. First, we created a public ECAs dataset through batch simulations based on D4. Licensed counselors evaluated our method, demonstrating that it significantly outperforms baselines in simulation authenticity and necessity. Additionally, two LLM-based automated evaluation methods were employed to confirm the higher quality of the generated dialogues compared to the baselines. The source code and dataset are available at https://github.com/AIR-DISCOVER/ECAs-Dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to privacy concerns, open dialogue datasets for mental health are primarily generated through human or AI synthesis methods. However, the inherent implicit nature of psychological processes, particularly those of clients, poses challenges to the authenticity and diversity of synthetic data. In this paper, we propose ECAs (short for Embodied Conversational Agents), a framework for embodied agent simulation based on Large Language Models (LLMs) that incorporates multiple psychological theoretical principles.Using simulation, we expand real counseling case data into a nuanced embodied cognitive memory space and generate dialogue data based on high-frequency counseling questions.We validated our framework using the D4 dataset. First, we created a public ECAs dataset through batch simulations based on D4. Licensed counselors evaluated our method, demonstrating that it significantly outperforms baselines in simulation authenticity and necessity. Additionally, two LLM-based automated evaluation methods were employed to confirm the higher quality of the generated dialogues compared to the baselines. The source code and dataset are available at https://github.com/AIR-DISCOVER/ECAs-Dataset."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-10-29T13:46:52Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    46,
                    52,
                    1,
                    303,
                    0
                ],
                "arxiv_comment": "Accepted to AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Lixiu Wu"
                    },
                    {
                        "name": "Yuanrong Tang"
                    },
                    {
                        "name": "Qisen Pan"
                    },
                    {
                        "name": "Xianyang Zhan"
                    },
                    {
                        "name": "Yucheng Han"
                    },
                    {
                        "name": "Lanxi Xiao"
                    },
                    {
                        "name": "Tianhong Wang"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Jiangtao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiangtao Gong"
                },
                "author": "Jiangtao Gong"
            },
            {
                "id": "http://arxiv.org/abs/2511.10962v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10962v2",
                "title": "LEMUR: Large scale End-to-end MUltimodal Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEMUR: Large scale End-to-end MUltimodal Recommendation"
                },
                "updated": "2025-11-17T11:54:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    54,
                    7,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10962v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditional ID-based recommender systems often struggle with cold-start and generalization challenges. Multimodal recommendation systems, which leverage textual and visual data, offer a promising solution to mitigate these issues. However, existing industrial approaches typically adopt a two-stage training paradigm: first pretraining a multimodal model, then applying its frozen representations to train the recommendation model. This decoupled framework suffers from misalignment between multimodal learning and recommendation objectives, as well as an inability to adapt dynamically to new data. To address these limitations, we propose LEMUR, the first large-scale multimodal recommender system trained end-to-end from raw data. By jointly optimizing both the multimodal and recommendation components, LEMUR ensures tighter alignment with downstream objectives while enabling real-time parameter updates. Constructing multimodal sequential representations from user history often entails prohibitively high computational costs. To alleviate this bottleneck, we propose a novel memory bank mechanism that incrementally accumulates historical multimodal representations throughout the training process. After one month of deployment in Douyin Search, LEMUR has led to a 0.843% reduction in query change rate decay and a 0.81% improvement in QAUC. Additionally, LEMUR has shown significant gains across key offline metrics for Douyin Advertisement. Our results validate the superiority of end-to-end multimodal recommendation in real-world industrial scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional ID-based recommender systems often struggle with cold-start and generalization challenges. Multimodal recommendation systems, which leverage textual and visual data, offer a promising solution to mitigate these issues. However, existing industrial approaches typically adopt a two-stage training paradigm: first pretraining a multimodal model, then applying its frozen representations to train the recommendation model. This decoupled framework suffers from misalignment between multimodal learning and recommendation objectives, as well as an inability to adapt dynamically to new data. To address these limitations, we propose LEMUR, the first large-scale multimodal recommender system trained end-to-end from raw data. By jointly optimizing both the multimodal and recommendation components, LEMUR ensures tighter alignment with downstream objectives while enabling real-time parameter updates. Constructing multimodal sequential representations from user history often entails prohibitively high computational costs. To alleviate this bottleneck, we propose a novel memory bank mechanism that incrementally accumulates historical multimodal representations throughout the training process. After one month of deployment in Douyin Search, LEMUR has led to a 0.843% reduction in query change rate decay and a 0.81% improvement in QAUC. Additionally, LEMUR has shown significant gains across key offline metrics for Douyin Advertisement. Our results validate the superiority of end-to-end multimodal recommendation in real-world industrial scenarios."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T05:15:15Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    5,
                    15,
                    15,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Xintian Han"
                    },
                    {
                        "name": "Honggang Chen"
                    },
                    {
                        "name": "Quan Lin"
                    },
                    {
                        "name": "Jingyue Gao"
                    },
                    {
                        "name": "Xiangyuan Ren"
                    },
                    {
                        "name": "Lifei Zhu"
                    },
                    {
                        "name": "Zhisheng Ye"
                    },
                    {
                        "name": "Shikang Wu"
                    },
                    {
                        "name": "XiongHang Xie"
                    },
                    {
                        "name": "Xiaochu Gan"
                    },
                    {
                        "name": "Bingzheng Wei"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Jingjian Lin"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Junfeng Ge"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Ge"
                },
                "author": "Junfeng Ge"
            },
            {
                "id": "http://arxiv.org/abs/2511.13274v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13274v1",
                "title": "KForge: Program Synthesis for Diverse AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KForge: Program Synthesis for Diverse AI Hardware Accelerators"
                },
                "updated": "2025-11-17T11:46:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    46,
                    43,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13274v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.\n  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.\n  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T11:46:43Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    46,
                    43,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Under review at MLSys 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Taras Sereda"
                    },
                    {
                        "name": "Tom St. John"
                    },
                    {
                        "name": "Burak Bartan"
                    },
                    {
                        "name": "Natalie Serrino"
                    },
                    {
                        "name": "Sachin Katti"
                    },
                    {
                        "name": "Zain Asgar"
                    }
                ],
                "author_detail": {
                    "name": "Zain Asgar"
                },
                "author": "Zain Asgar"
            },
            {
                "id": "http://arxiv.org/abs/2511.13273v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13273v1",
                "title": "Spatial Blind Spot: Auditory Motion Perception Deficits in Audio LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Blind Spot: Auditory Motion Perception Deficits in Audio LLMs"
                },
                "updated": "2025-11-17T11:45:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    45,
                    41,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13273v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AMPBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AMPBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AMPBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AMPBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T11:45:41Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    45,
                    41,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Zhe Sun"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Jiayu Yao"
                    },
                    {
                        "name": "Yiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwei Wang"
                },
                "author": "Yiwei Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.13272v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13272v1",
                "title": "Pinching-Antenna-Enabled Cognitive Radio Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pinching-Antenna-Enabled Cognitive Radio Networks"
                },
                "updated": "2025-11-17T11:43:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    43,
                    27,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13272v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper investigates a pinching-antenna (PA)-enabled cognitive radio network, where both the primary transmitter (PT) and secondary transmitter (ST) are equipped with a single waveguide and multiple PAs to facilitate simultaneous spectrum sharing. Under a general Ricean fading channel model, a closed-form analytical expression for the average spectral efficiency (SE) achieved by PAs is first derived. Based on this, a sum-SE maximization problem is formulated to jointly optimize the primary and secondary pinching beamforming, subject to system constraints on the transmission power budgets, minimum antenna separation requirements, and feasible PA deployment regions. To address this non-convex problem, a three-stage optimization algorithm is developed to sequentially optimize both the PT and ST pinching beamforming, and the ST power control. For the PT and ST pinching beamforming optimization, the coarse positions of PA are first determined at the waveguide-level. Then, wavelength-level refinements achieve constructive signal combination at the intended user and destructive superposition at the unintended user. For the ST power control, a closed-form solution is derived. Simulation results demonstrate that i) PAs can achieve significant SE improvements over conventional fixed-position antennas; ii) the proposed pinching beamforming design achieves effective interference suppression and superior performance for both even and odd numbers of PAs; and iii) the developed three-stage optimization algorithm enables nearly orthogonal transmission between the primary and secondary networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates a pinching-antenna (PA)-enabled cognitive radio network, where both the primary transmitter (PT) and secondary transmitter (ST) are equipped with a single waveguide and multiple PAs to facilitate simultaneous spectrum sharing. Under a general Ricean fading channel model, a closed-form analytical expression for the average spectral efficiency (SE) achieved by PAs is first derived. Based on this, a sum-SE maximization problem is formulated to jointly optimize the primary and secondary pinching beamforming, subject to system constraints on the transmission power budgets, minimum antenna separation requirements, and feasible PA deployment regions. To address this non-convex problem, a three-stage optimization algorithm is developed to sequentially optimize both the PT and ST pinching beamforming, and the ST power control. For the PT and ST pinching beamforming optimization, the coarse positions of PA are first determined at the waveguide-level. Then, wavelength-level refinements achieve constructive signal combination at the intended user and destructive superposition at the unintended user. For the ST power control, a closed-form solution is derived. Simulation results demonstrate that i) PAs can achieve significant SE improvements over conventional fixed-position antennas; ii) the proposed pinching beamforming design achieves effective interference suppression and superior performance for both even and odd numbers of PAs; and iii) the developed three-stage optimization algorithm enables nearly orthogonal transmission between the primary and secondary networks."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T11:43:27Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    43,
                    27,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "13 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Zeyang Sun"
                    },
                    {
                        "name": "Xidong Mu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Sai Xu"
                    },
                    {
                        "name": "Michail Matthaiou"
                    }
                ],
                "author_detail": {
                    "name": "Michail Matthaiou"
                },
                "author": "Michail Matthaiou"
            },
            {
                "id": "http://arxiv.org/abs/2508.06248v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.06248v3",
                "title": "Deepfake Detection that Generalizes Across Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deepfake Detection that Generalizes Across Benchmarks"
                },
                "updated": "2025-11-17T11:41:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    41,
                    58,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.06248v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.06248v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The generalization of deepfake detectors to unseen manipulation techniques remains a challenge for practical deployment. Although many approaches adapt foundation models by introducing significant architectural complexity, this work demonstrates that robust generalization is achievable through a parameter-efficient adaptation of one of the foundational pre-trained vision encoders. The proposed method, GenD, fine-tunes only the Layer Normalization parameters (0.03% of the total) and enhances generalization by enforcing a hyperspherical feature manifold using L2 normalization and metric learning on it.\n  We conducted an extensive evaluation on 14 benchmark datasets spanning from 2019 to 2025. The proposed method achieves state-of-the-art performance, outperforming more complex, recent approaches in average cross-dataset AUROC. Our analysis yields two primary findings for the field: 1) training on paired real-fake data from the same source video is essential for mitigating shortcut learning and improving generalization, and 2) detection difficulty on academic datasets has not strictly increased over time, with models trained on older, diverse datasets showing strong generalization capabilities.\n  This work delivers a computationally efficient and reproducible method, proving that state-of-the-art generalization is attainable by making targeted, minimal changes to a pre-trained foundational image encoder model. The code is at: https://github.com/yermandy/GenD",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generalization of deepfake detectors to unseen manipulation techniques remains a challenge for practical deployment. Although many approaches adapt foundation models by introducing significant architectural complexity, this work demonstrates that robust generalization is achievable through a parameter-efficient adaptation of one of the foundational pre-trained vision encoders. The proposed method, GenD, fine-tunes only the Layer Normalization parameters (0.03% of the total) and enhances generalization by enforcing a hyperspherical feature manifold using L2 normalization and metric learning on it.\n  We conducted an extensive evaluation on 14 benchmark datasets spanning from 2019 to 2025. The proposed method achieves state-of-the-art performance, outperforming more complex, recent approaches in average cross-dataset AUROC. Our analysis yields two primary findings for the field: 1) training on paired real-fake data from the same source video is essential for mitigating shortcut learning and improving generalization, and 2) detection difficulty on academic datasets has not strictly increased over time, with models trained on older, diverse datasets showing strong generalization capabilities.\n  This work delivers a computationally efficient and reproducible method, proving that state-of-the-art generalization is attainable by making targeted, minimal changes to a pre-trained foundational image encoder model. The code is at: https://github.com/yermandy/GenD"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-08T12:03:56Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    3,
                    56,
                    4,
                    220,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Andrii Yermakov"
                    },
                    {
                        "name": "Jan Cech"
                    },
                    {
                        "name": "Jiri Matas"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz"
            },
            {
                "id": "http://arxiv.org/abs/2510.01223v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.01223v2",
                "title": "Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge"
                },
                "updated": "2025-11-17T11:40:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    40,
                    43,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.01223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.01223v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks. However, they remain exposed to jailbreak attacks, eliciting harmful responses. The nested scenario strategy has been increasingly adopted across various methods, demonstrating immense potential. Nevertheless, these methods are easily detectable due to their prominent malicious intentions. In this work, we are the first to find and systematically verify that LLMs' alignment defenses are not sensitive to nested scenarios, where these scenarios are highly semantically relevant to the queries and incorporate targeted toxic knowledge. This is a crucial yet insufficiently explored direction. Based on this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs' alignment. By building scenarios highly relevant to the queries and integrating targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs. Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful queries, leading to outstanding concealment. Extensive experiments demonstrate that RTS-Attack exhibits superior performance in both efficiency and universality compared to the baselines across diverse advanced LLMs, including GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available at https://github.com/nercode/Work. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL CONTENT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks. However, they remain exposed to jailbreak attacks, eliciting harmful responses. The nested scenario strategy has been increasingly adopted across various methods, demonstrating immense potential. Nevertheless, these methods are easily detectable due to their prominent malicious intentions. In this work, we are the first to find and systematically verify that LLMs' alignment defenses are not sensitive to nested scenarios, where these scenarios are highly semantically relevant to the queries and incorporate targeted toxic knowledge. This is a crucial yet insufficiently explored direction. Based on this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs' alignment. By building scenarios highly relevant to the queries and integrating targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs. Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful queries, leading to outstanding concealment. Extensive experiments demonstrate that RTS-Attack exhibits superior performance in both efficiency and universality compared to the baselines across diverse advanced LLMs, including GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available at https://github.com/nercode/Work. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL CONTENT."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-22T12:37:07Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    37,
                    7,
                    0,
                    265,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Ning Xu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Hui Dou"
                    }
                ],
                "author_detail": {
                    "name": "Hui Dou"
                },
                "author": "Hui Dou"
            },
            {
                "id": "http://arxiv.org/abs/2511.10011v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10011v2",
                "title": "Reinforcing Trustworthiness in Multimodal Emotional Support Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcing Trustworthiness in Multimodal Emotional Support Systems"
                },
                "updated": "2025-11-17T11:26:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    26,
                    54,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10011v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10011v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In today's world, emotional support is increasingly essential, yet it remains challenging for both those seeking help and those offering it. Multimodal approaches to emotional support show great promise by integrating diverse data sources to provide empathetic, contextually relevant responses, fostering more effective interactions. However, current methods have notable limitations, often relying solely on text or converting other data types into text, or providing emotion recognition only, thus overlooking the full potential of multimodal inputs. Moreover, many studies prioritize response generation without accurately identifying critical emotional support elements or ensuring the reliability of outputs. To overcome these issues, we introduce \\textsc{ MultiMood}, a new framework that (i) leverages multimodal embeddings from video, audio, and text to predict emotional components and to produce responses responses aligned with professional therapeutic standards. To improve trustworthiness, we (ii) incorporate novel psychological criteria and apply Reinforcement Learning (RL) to optimize large language models (LLMs) for consistent adherence to these standards. We also (iii) analyze several advanced LLMs to assess their multimodal emotional support capabilities. Experimental results show that MultiMood achieves state-of-the-art on MESC and DFEW datasets while RL-driven trustworthiness improvements are validated through human and LLM evaluations, demonstrating its superior capability in applying a multimodal framework in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In today's world, emotional support is increasingly essential, yet it remains challenging for both those seeking help and those offering it. Multimodal approaches to emotional support show great promise by integrating diverse data sources to provide empathetic, contextually relevant responses, fostering more effective interactions. However, current methods have notable limitations, often relying solely on text or converting other data types into text, or providing emotion recognition only, thus overlooking the full potential of multimodal inputs. Moreover, many studies prioritize response generation without accurately identifying critical emotional support elements or ensuring the reliability of outputs. To overcome these issues, we introduce \\textsc{ MultiMood}, a new framework that (i) leverages multimodal embeddings from video, audio, and text to predict emotional components and to produce responses responses aligned with professional therapeutic standards. To improve trustworthiness, we (ii) incorporate novel psychological criteria and apply Reinforcement Learning (RL) to optimize large language models (LLMs) for consistent adherence to these standards. We also (iii) analyze several advanced LLMs to assess their multimodal emotional support capabilities. Experimental results show that MultiMood achieves state-of-the-art on MESC and DFEW datasets while RL-driven trustworthiness improvements are validated through human and LLM evaluations, demonstrating its superior capability in applying a multimodal framework in this domain."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T06:28:07Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    6,
                    28,
                    7,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Huy M. Le"
                    },
                    {
                        "name": "Dat Tien Nguyen"
                    },
                    {
                        "name": "Ngan T. T. Vo"
                    },
                    {
                        "name": "Tuan D. Q. Nguyen"
                    },
                    {
                        "name": "Nguyen Binh Le"
                    },
                    {
                        "name": "Duy Minh Ho Nguyen"
                    },
                    {
                        "name": "Daniel Sonntag"
                    },
                    {
                        "name": "Lizi Liao"
                    },
                    {
                        "name": "Binh T. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Binh T. Nguyen"
                },
                "author": "Binh T. Nguyen"
            },
            {
                "id": "http://arxiv.org/abs/2511.13254v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13254v1",
                "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance"
                },
                "updated": "2025-11-17T11:13:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    13,
                    34,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13254v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T11:13:34Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    13,
                    34,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shalini Maiti"
                    },
                    {
                        "name": "Amar Budhiraja"
                    },
                    {
                        "name": "Bhavul Gauri"
                    },
                    {
                        "name": "Gaurav Chaurasia"
                    },
                    {
                        "name": "Anton Protopopov"
                    },
                    {
                        "name": "Alexis Audran-Reiss"
                    },
                    {
                        "name": "Michael Slater"
                    },
                    {
                        "name": "Despoina Magka"
                    },
                    {
                        "name": "Tatiana Shavrina"
                    },
                    {
                        "name": "Roberta Raileanu"
                    },
                    {
                        "name": "Yoram Bachrach"
                    }
                ],
                "author_detail": {
                    "name": "Yoram Bachrach"
                },
                "author": "Yoram Bachrach"
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.06261v4",
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "updated": "2025-11-17T11:11:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    11,
                    28,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.06261v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.06261v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the LLM instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's memory in the concurrent KV cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's memory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the LLM instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's memory in the concurrent KV cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's memory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "arxiv_comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Erik Schultheis"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh"
            },
            {
                "id": "http://arxiv.org/abs/2507.04842v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.04842v2",
                "title": "Efficient SAR Vessel Detection for FPGA-Based On-Satellite Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient SAR Vessel Detection for FPGA-Based On-Satellite Sensing"
                },
                "updated": "2025-11-17T11:06:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    6,
                    45,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.04842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.04842v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3769102.3772713",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Rapid analysis of satellite imagery within minutes-to-hours of acquisition is increasingly vital for many remote sensing applications, and is an essential component for developing next-generation autonomous and distributed satellite systems. On-satellite machine learning (ML) has the potential for such rapid analysis, by overcoming latency associated with intermittent satellite connectivity to ground stations or relay satellites, but state-of-the-art models are often too large or power-hungry for on-board deployment. Vessel detection using Synthetic Aperture Radar (SAR) is a critical time-sensitive application in maritime security that exemplifies this challenge. SAR vessel detection has previously been demonstrated only by ML models that either are too large for satellite deployment, have not been developed for sufficiently low-power hardware, or have only been tested on small SAR datasets that do not sufficiently represent the difficulty of the real-world task. Here we systematically explore a suite of architectural adaptations to develop a novel YOLOv8 architecture optimized for this task and FPGA-based processing. We deploy our model on a Kria KV260 MPSoC, and show it can analyze a ~700 megapixel SAR image in less than a minute, within common satellite power constraints (<10W). Our model has detection and classification performance only ~2% and 3% lower than values from state-of-the-art GPU-based models on the largest and most diverse open SAR vessel dataset, xView3-SAR, despite being ~50 and ~2500 times more computationally efficient. This work represents a key contribution towards on-satellite ML for time-critical SAR analysis, and more autonomous, scalable satellites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid analysis of satellite imagery within minutes-to-hours of acquisition is increasingly vital for many remote sensing applications, and is an essential component for developing next-generation autonomous and distributed satellite systems. On-satellite machine learning (ML) has the potential for such rapid analysis, by overcoming latency associated with intermittent satellite connectivity to ground stations or relay satellites, but state-of-the-art models are often too large or power-hungry for on-board deployment. Vessel detection using Synthetic Aperture Radar (SAR) is a critical time-sensitive application in maritime security that exemplifies this challenge. SAR vessel detection has previously been demonstrated only by ML models that either are too large for satellite deployment, have not been developed for sufficiently low-power hardware, or have only been tested on small SAR datasets that do not sufficiently represent the difficulty of the real-world task. Here we systematically explore a suite of architectural adaptations to develop a novel YOLOv8 architecture optimized for this task and FPGA-based processing. We deploy our model on a Kria KV260 MPSoC, and show it can analyze a ~700 megapixel SAR image in less than a minute, within common satellite power constraints (<10W). Our model has detection and classification performance only ~2% and 3% lower than values from state-of-the-art GPU-based models on the largest and most diverse open SAR vessel dataset, xView3-SAR, despite being ~50 and ~2500 times more computationally efficient. This work represents a key contribution towards on-satellite ML for time-critical SAR analysis, and more autonomous, scalable satellites."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-07T10:03:31Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    10,
                    3,
                    31,
                    0,
                    188,
                    0
                ],
                "arxiv_comment": "17 pages, 7 figures, 6 tables. To be presented in the 10th ACM/IEEE Symposium on Edge Computing (SEC '25)",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Colin Laganier"
                    },
                    {
                        "name": "Liam Fletcher"
                    },
                    {
                        "name": "Elim Kwan"
                    },
                    {
                        "name": "Richard Walters"
                    },
                    {
                        "name": "Victoria Nockles"
                    }
                ],
                "author_detail": {
                    "name": "Victoria Nockles"
                },
                "author": "Victoria Nockles",
                "arxiv_doi": "10.1145/3769102.3772713"
            },
            {
                "id": "http://arxiv.org/abs/2511.13240v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13240v1",
                "title": "Incoherent Beliefs & Inconsistent Actions in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incoherent Beliefs & Inconsistent Actions in Large Language Models"
                },
                "updated": "2025-11-17T11:04:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    4,
                    0,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13240v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T11:04:00Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    4,
                    0,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Arka Pal"
                    },
                    {
                        "name": "Teo Kitanovski"
                    },
                    {
                        "name": "Arthur Liang"
                    },
                    {
                        "name": "Akilesh Potti"
                    },
                    {
                        "name": "Micah Goldblum"
                    }
                ],
                "author_detail": {
                    "name": "Micah Goldblum"
                },
                "author": "Micah Goldblum"
            },
            {
                "id": "http://arxiv.org/abs/2511.13238v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13238v1",
                "title": "Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms"
                },
                "updated": "2025-11-17T11:01:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    1,
                    9,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13238v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T11:01:09Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    11,
                    1,
                    9,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "46 pages, 8 figures, 2 tables, accepted for publication in Quality & Quantity",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Patrick Parschan"
                    },
                    {
                        "name": "Charlott Jakob"
                    }
                ],
                "author_detail": {
                    "name": "Charlott Jakob"
                },
                "author": "Charlott Jakob"
            },
            {
                "id": "http://arxiv.org/abs/2511.13233v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13233v1",
                "title": "LLM-based Multi-Agent System for Simulating Strategic and Goal-Oriented Data Marketplaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Multi-Agent System for Simulating Strategic and Goal-Oriented Data Marketplaces"
                },
                "updated": "2025-11-17T10:53:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    53,
                    4,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13233v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Data marketplaces, which mediate the purchase and exchange of data from third parties, have attracted growing attention for reducing the cost and effort of data collection while enabling the trading of diverse datasets. However, a systematic understanding of the interactions between market participants, data, and regulations remains limited. To address this gap, we propose a Large Language Model-based Multi-Agent System (LLM-MAS) for data marketplaces. In our framework, buyer and seller agents powered by LLMs operate with explicit objectives and autonomously perform strategic actions, such as planning, searching, purchasing, pricing, and updating data. These agents can reason about market dynamics, forecast future demand, and adjust strategies accordingly. Unlike conventional model-based simulations, which are typically constrained to predefined rules, LLM-MAS supports broader and more adaptive behavior selection through natural language reasoning. We evaluated the framework via simulation experiments using three distribution-based metrics: (1) the number of purchases per dataset, (2) the number of purchases per buyer, and (3) the number of repeated purchases of the same dataset. The results demonstrate that LLM-MAS more faithfully reproduces trading patterns observed in real data marketplaces compared to traditional approaches, and further captures the emergence and evolution of market trends.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data marketplaces, which mediate the purchase and exchange of data from third parties, have attracted growing attention for reducing the cost and effort of data collection while enabling the trading of diverse datasets. However, a systematic understanding of the interactions between market participants, data, and regulations remains limited. To address this gap, we propose a Large Language Model-based Multi-Agent System (LLM-MAS) for data marketplaces. In our framework, buyer and seller agents powered by LLMs operate with explicit objectives and autonomously perform strategic actions, such as planning, searching, purchasing, pricing, and updating data. These agents can reason about market dynamics, forecast future demand, and adjust strategies accordingly. Unlike conventional model-based simulations, which are typically constrained to predefined rules, LLM-MAS supports broader and more adaptive behavior selection through natural language reasoning. We evaluated the framework via simulation experiments using three distribution-based metrics: (1) the number of purchases per dataset, (2) the number of purchases per buyer, and (3) the number of repeated purchases of the same dataset. The results demonstrate that LLM-MAS more faithfully reproduces trading patterns observed in real data marketplaces compared to traditional approaches, and further captures the emergence and evolution of market trends."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T10:53:04Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    53,
                    4,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "10 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Jun Sashihara"
                    },
                    {
                        "name": "Yukihisa Fujita"
                    },
                    {
                        "name": "Kota Nakamura"
                    },
                    {
                        "name": "Masahiro Kuwahara"
                    },
                    {
                        "name": "Teruaki Hayashi"
                    }
                ],
                "author_detail": {
                    "name": "Teruaki Hayashi"
                },
                "author": "Teruaki Hayashi"
            },
            {
                "id": "http://arxiv.org/abs/2408.14398v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.14398v4",
                "title": "On the Limitations of Language Targeted Pruning: Investigating the Calibration Language Impact in Multilingual LLM Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Limitations of Language Targeted Pruning: Investigating the Calibration Language Impact in Multilingual LLM Pruning"
                },
                "updated": "2025-11-17T10:48:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    48,
                    59,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.14398v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.14398v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in large language model (LLM) pruning have shown state-of-the-art (SotA) compression results in post-training and retraining-free settings while maintaining high predictive performance. However, previous research mainly considered calibrating based on English text, despite the multilingual nature of modern LLMs and their frequent use in non-English languages. This analysis paper conducts an in-depth investigation of the performance and internal representation changes associated with pruning multilingual language models for monolingual applications. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse languages, tasks, models, and SotA pruning techniques. We further analyze the latent subspaces, pruning masks, and individual neurons within pruned models. Our results reveal that while calibration on the target language effectively retains perplexity and yields high signal-to-noise ratios, it does not consistently improve downstream task performance. Further analysis of internal representations at three different levels highlights broader limitations of current pruning approaches: While they effectively preserve dominant information like language-specific features, this is insufficient to counteract the loss of nuanced, language-agnostic features that are crucial for knowledge retention and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language model (LLM) pruning have shown state-of-the-art (SotA) compression results in post-training and retraining-free settings while maintaining high predictive performance. However, previous research mainly considered calibrating based on English text, despite the multilingual nature of modern LLMs and their frequent use in non-English languages. This analysis paper conducts an in-depth investigation of the performance and internal representation changes associated with pruning multilingual language models for monolingual applications. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse languages, tasks, models, and SotA pruning techniques. We further analyze the latent subspaces, pruning masks, and individual neurons within pruned models. Our results reveal that while calibration on the target language effectively retains perplexity and yields high signal-to-noise ratios, it does not consistently improve downstream task performance. Further analysis of internal representations at three different levels highlights broader limitations of current pruning approaches: While they effectively preserve dominant information like language-specific features, this is insufficient to counteract the loss of nuanced, language-agnostic features that are crucial for knowledge retention and reasoning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-26T16:29:13Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    16,
                    29,
                    13,
                    0,
                    239,
                    0
                ],
                "arxiv_comment": "Accepted for publication in TACL",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Simon Kurz"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Zhixue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhixue Zhao"
                },
                "author": "Zhixue Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2511.10150v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10150v2",
                "title": "Decoupling Bias, Aligning Distributions: Synergistic Fairness Optimization for Deepfake Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling Bias, Aligning Distributions: Synergistic Fairness Optimization for Deepfake Detection"
                },
                "updated": "2025-11-17T10:45:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    45,
                    19,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10150v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fairness is a core element in the trustworthy deployment of deepfake detection models, especially in the field of digital identity security. Biases in detection models toward different demographic groups, such as gender and race, may lead to systemic misjudgments, exacerbating the digital divide and social inequities. However, current fairness-enhanced detectors often improve fairness at the cost of detection accuracy. To address this challenge, we propose a dual-mechanism collaborative optimization framework. Our proposed method innovatively integrates structural fairness decoupling and global distribution alignment: decoupling channels sensitive to demographic groups at the model architectural level, and subsequently reducing the distance between the overall sample distribution and the distributions corresponding to each demographic group at the feature level. Experimental results demonstrate that, compared with other methods, our framework improves both inter-group and intra-group fairness while maintaining overall detection accuracy across domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness is a core element in the trustworthy deployment of deepfake detection models, especially in the field of digital identity security. Biases in detection models toward different demographic groups, such as gender and race, may lead to systemic misjudgments, exacerbating the digital divide and social inequities. However, current fairness-enhanced detectors often improve fairness at the cost of detection accuracy. To address this challenge, we propose a dual-mechanism collaborative optimization framework. Our proposed method innovatively integrates structural fairness decoupling and global distribution alignment: decoupling channels sensitive to demographic groups at the model architectural level, and subsequently reducing the distance between the overall sample distribution and the distributions corresponding to each demographic group at the feature level. Experimental results demonstrate that, compared with other methods, our framework improves both inter-group and intra-group fairness while maintaining overall detection accuracy across domains."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T10:04:45Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    10,
                    4,
                    45,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Feng Ding"
                    },
                    {
                        "name": "Wenhui Yi"
                    },
                    {
                        "name": "Yunpeng Zhou"
                    },
                    {
                        "name": "Xinan He"
                    },
                    {
                        "name": "Hong Rao"
                    },
                    {
                        "name": "Shu Hu"
                    }
                ],
                "author_detail": {
                    "name": "Shu Hu"
                },
                "author": "Shu Hu"
            },
            {
                "id": "http://arxiv.org/abs/2511.13225v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13225v1",
                "title": "Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms"
                },
                "updated": "2025-11-17T10:41:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    41,
                    7,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13225v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T10:41:07Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    41,
                    7,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted to IJCNLP-AACL 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "Joseph James"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin"
            },
            {
                "id": "http://arxiv.org/abs/2511.13223v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13223v1",
                "title": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs"
                },
                "updated": "2025-11-17T10:38:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    38,
                    56,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13223v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T10:38:56Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    38,
                    56,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Zhengxu Yu"
                    },
                    {
                        "name": "Weihang Pan"
                    },
                    {
                        "name": "Zhongming Jin"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Binbin Lin"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye"
            },
            {
                "id": "http://arxiv.org/abs/2511.13216v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13216v1",
                "title": "GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry"
                },
                "updated": "2025-11-17T10:29:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    29,
                    31,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13216v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms. In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping. Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation. Existing methods incorporate exteroceptive sensors such as LiDAR or cameras. Further enhancement has been introduced by leveraging gravity vector estimation to add additional observations on roll and pitch, thereby increasing the accuracy of vertical pose estimation. However, these approaches tend to degrade in feature-sparse or repetitive scenes and are prone to errors from double-integrated IMU acceleration. To address these challenges, we propose GaRLILEO, a novel gravity-aligned continuous-time radar-leg-inertial odometry framework. GaRLILEO decouples velocity from the IMU by building a continuous-time ego-velocity spline from SoC radar Doppler and leg kinematics information, enabling seamless sensor fusion which mitigates odometry distortion. In addition, GaRLILEO can reliably capture accurate gravity vectors leveraging a novel soft S2-constrained gravity factor, improving vertical pose accuracy without relying on LiDAR or cameras. Evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories, GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes. We open-source both our dataset and algorithm to foster further research in legged robot odometry and SLAM. https://garlileo.github.io/GaRLILEO",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms. In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping. Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation. Existing methods incorporate exteroceptive sensors such as LiDAR or cameras. Further enhancement has been introduced by leveraging gravity vector estimation to add additional observations on roll and pitch, thereby increasing the accuracy of vertical pose estimation. However, these approaches tend to degrade in feature-sparse or repetitive scenes and are prone to errors from double-integrated IMU acceleration. To address these challenges, we propose GaRLILEO, a novel gravity-aligned continuous-time radar-leg-inertial odometry framework. GaRLILEO decouples velocity from the IMU by building a continuous-time ego-velocity spline from SoC radar Doppler and leg kinematics information, enabling seamless sensor fusion which mitigates odometry distortion. In addition, GaRLILEO can reliably capture accurate gravity vectors leveraging a novel soft S2-constrained gravity factor, improving vertical pose accuracy without relying on LiDAR or cameras. Evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories, GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes. We open-source both our dataset and algorithm to foster further research in legged robot odometry and SLAM. https://garlileo.github.io/GaRLILEO"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T10:29:31Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    29,
                    31,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Chiyun Noh"
                    },
                    {
                        "name": "Sangwoo Jung"
                    },
                    {
                        "name": "Hanjun Kim"
                    },
                    {
                        "name": "Yafei Hu"
                    },
                    {
                        "name": "Laura Herlant"
                    },
                    {
                        "name": "Ayoung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Ayoung Kim"
                },
                "author": "Ayoung Kim"
            },
            {
                "id": "http://arxiv.org/abs/2412.08098v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.08098v3",
                "title": "What You See Is Not Always What You Get: Evaluating GPT's Comprehension of Source Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What You See Is Not Always What You Get: Evaluating GPT's Comprehension of Source Code"
                },
                "updated": "2025-11-17T10:13:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    13,
                    22,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.08098v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.08098v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent studies have demonstrated outstanding capabilities of large language models (LLMs) in software engineering tasks, including code generation and comprehension. While LLMs have shown significant potential in assisting with coding, LLMs are vulnerable to adversarial attacks. In this paper, we investigate the vulnerability of LLMs to imperceptible attacks. This class of attacks manipulate source code at the character level, which renders the changes invisible to human reviewers yet effective in misleading LLMs' behaviour. We devise these attacks into four distinct categories and analyse their impacts on code analysis and comprehension tasks. These four types of imperceptible character attacks include coding reordering, invisible coding characters, code deletions, and code homoglyphs. To assess the robustness of state-of-the-art LLMs, we present a systematic evaluation across multiple models using both perturbed and clean code snippets. Two evaluation metrics, model confidence using log probabilities of response and response correctness, are introduced. The results reveal that LLMs are susceptible to imperceptible coding perturbations, with varying degrees of degradation highlighted across different LLMs. Furthermore, we observe a consistent negative correlation between perturbation magnitude and model performance. These results highlight the urgent need for robust LLMs capable of manoeuvring behaviours under imperceptible adversarial conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have demonstrated outstanding capabilities of large language models (LLMs) in software engineering tasks, including code generation and comprehension. While LLMs have shown significant potential in assisting with coding, LLMs are vulnerable to adversarial attacks. In this paper, we investigate the vulnerability of LLMs to imperceptible attacks. This class of attacks manipulate source code at the character level, which renders the changes invisible to human reviewers yet effective in misleading LLMs' behaviour. We devise these attacks into four distinct categories and analyse their impacts on code analysis and comprehension tasks. These four types of imperceptible character attacks include coding reordering, invisible coding characters, code deletions, and code homoglyphs. To assess the robustness of state-of-the-art LLMs, we present a systematic evaluation across multiple models using both perturbed and clean code snippets. Two evaluation metrics, model confidence using log probabilities of response and response correctness, are introduced. The results reveal that LLMs are susceptible to imperceptible coding perturbations, with varying degrees of degradation highlighted across different LLMs. Furthermore, we observe a consistent negative correlation between perturbation magnitude and model performance. These results highlight the urgent need for robust LLMs capable of manoeuvring behaviours under imperceptible adversarial conditions."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-11T04:52:41Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    52,
                    41,
                    2,
                    346,
                    0
                ],
                "arxiv_comment": "This work has been accepted at APSEC 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Jiawen Wen"
                    },
                    {
                        "name": "Bangshuo Zhu"
                    },
                    {
                        "name": "Huaming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huaming Chen"
                },
                "author": "Huaming Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.13201v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13201v1",
                "title": "Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation"
                },
                "updated": "2025-11-17T10:10:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    10,
                    33,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13201v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T10:10:33Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    10,
                    33,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026 main conference",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "arxiv_journal_ref": "AAAI 2026",
                "authors": [
                    {
                        "name": "Hao Hu"
                    },
                    {
                        "name": "Yifan Feng"
                    },
                    {
                        "name": "Ruoxue Li"
                    },
                    {
                        "name": "Rundong Xue"
                    },
                    {
                        "name": "Xingliang Hou"
                    },
                    {
                        "name": "Zhiqiang Tian"
                    },
                    {
                        "name": "Yue Gao"
                    },
                    {
                        "name": "Shaoyi Du"
                    }
                ],
                "author_detail": {
                    "name": "Shaoyi Du"
                },
                "author": "Shaoyi Du"
            },
            {
                "id": "http://arxiv.org/abs/2511.13198v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13198v1",
                "title": "ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer"
                },
                "updated": "2025-11-17T10:08:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    8,
                    24,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13198v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T10:08:24Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    8,
                    24,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zhixin Ou"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Jianchen Han"
                    },
                    {
                        "name": "Baihui Liu"
                    },
                    {
                        "name": "Linbo Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Linbo Qiao"
                },
                "author": "Linbo Qiao"
            },
            {
                "id": "http://arxiv.org/abs/2511.13193v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13193v1",
                "title": "Cost-Effective Communication: An Auction-based Method for Language Agent Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Effective Communication: An Auction-based Method for Language Agent Interaction"
                },
                "updated": "2025-11-17T10:00:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    0,
                    20,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13193v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient \"free-for-all\" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that \"free\" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient \"free-for-all\" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that \"free\" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T10:00:20Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    10,
                    0,
                    20,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yijia Fan"
                    },
                    {
                        "name": "Jusheng Zhang"
                    },
                    {
                        "name": "Kaitong Cai"
                    },
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Chengpei Tang"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.13185v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13185v1",
                "title": "Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction"
                },
                "updated": "2025-11-17T09:46:15Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    9,
                    46,
                    15,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13185v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T09:46:15Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    9,
                    46,
                    15,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "EurIPS DiffSys workshop 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Aishwarya Venkataramanan"
                    },
                    {
                        "name": "Sai Karthikeya Vemuri"
                    },
                    {
                        "name": "Adithya Ashok Chalain Valapil"
                    },
                    {
                        "name": "Joachim Denzler"
                    }
                ],
                "author_detail": {
                    "name": "Joachim Denzler"
                },
                "author": "Joachim Denzler"
            },
            {
                "id": "http://arxiv.org/abs/2405.18921v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2405.18921v3",
                "title": "GLANCE: Global Actions in a Nutshell for Counterfactual Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLANCE: Global Actions in a Nutshell for Counterfactual Explainability"
                },
                "updated": "2025-11-17T09:44:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    9,
                    44,
                    58,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2405.18921v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2405.18921v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The widespread deployment of machine learning systems in critical real-world decision-making applications has highlighted the urgent need for counterfactual explainability methods that operate effectively. Global counterfactual explanations, expressed as actions to offer recourse, aim to provide succinct explanations and insights applicable to large population subgroups. High effectiveness, measured by the fraction of the population that is provided recourse, ensures that the actions benefit as many individuals as possible. Keeping the cost of actions low ensures the proposed recourse actions remain practical and actionable. Limiting the number of actions that provide global counterfactuals is essential to maximizing interpretability. The primary challenge, therefore, is to balance these trade-offs--maximizing effectiveness, minimizing cost, while maintaining a small number of actions. We introduce $\\texttt{GLANCE}$, a versatile and adaptive algorithm that employs a novel agglomerative approach, jointly considering both the feature space and the space of counterfactual actions, thereby accounting for the distribution of points in a way that aligns with the model's structure. This design enables the careful balancing of the trade-offs among the three key objectives, with the size objective functioning as a tunable parameter to keep the actions few and easy to interpret. Our extensive experimental evaluation demonstrates that $\\texttt{GLANCE}$ consistently shows greater robustness and performance compared to existing methods across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of machine learning systems in critical real-world decision-making applications has highlighted the urgent need for counterfactual explainability methods that operate effectively. Global counterfactual explanations, expressed as actions to offer recourse, aim to provide succinct explanations and insights applicable to large population subgroups. High effectiveness, measured by the fraction of the population that is provided recourse, ensures that the actions benefit as many individuals as possible. Keeping the cost of actions low ensures the proposed recourse actions remain practical and actionable. Limiting the number of actions that provide global counterfactuals is essential to maximizing interpretability. The primary challenge, therefore, is to balance these trade-offs--maximizing effectiveness, minimizing cost, while maintaining a small number of actions. We introduce $\\texttt{GLANCE}$, a versatile and adaptive algorithm that employs a novel agglomerative approach, jointly considering both the feature space and the space of counterfactual actions, thereby accounting for the distribution of points in a way that aligns with the model's structure. This design enables the careful balancing of the trade-offs among the three key objectives, with the size objective functioning as a tunable parameter to keep the actions few and easy to interpret. Our extensive experimental evaluation demonstrates that $\\texttt{GLANCE}$ consistently shows greater robustness and performance compared to existing methods across various datasets and models."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-05-29T09:24:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    24,
                    25,
                    2,
                    150,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Loukas Kavouras"
                    },
                    {
                        "name": "Eleni Psaroudaki"
                    },
                    {
                        "name": "Konstantinos Tsopelas"
                    },
                    {
                        "name": "Dimitrios Rontogiannis"
                    },
                    {
                        "name": "Nikolaos Theologitis"
                    },
                    {
                        "name": "Dimitris Sacharidis"
                    },
                    {
                        "name": "Giorgos Giannopoulos"
                    },
                    {
                        "name": "Dimitrios Tomaras"
                    },
                    {
                        "name": "Kleopatra Markou"
                    },
                    {
                        "name": "Dimitrios Gunopulos"
                    },
                    {
                        "name": "Dimitris Fotakis"
                    },
                    {
                        "name": "Ioannis Emiris"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Emiris"
                },
                "author": "Ioannis Emiris"
            },
            {
                "id": "http://arxiv.org/abs/2511.13182v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.13182v1",
                "title": "Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study"
                },
                "updated": "2025-11-17T09:43:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    17,
                    9,
                    43,
                    54,
                    0,
                    321,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.13182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.13182v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T09:43:54Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    9,
                    43,
                    54,
                    0,
                    321,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mihai Dan Nadas"
                    },
                    {
                        "name": "Laura Diosan"
                    }
                ],
                "author_detail": {
                    "name": "Laura Diosan"
                },
                "author": "Laura Diosan"
            }
        ]
    }
]