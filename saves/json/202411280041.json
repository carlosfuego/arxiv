[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17685v1",
                "updated": "2024-11-26T18:52:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:52:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Attamba: Attending To Multi-Token States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attamba: Attending To Multi-Token States"
                },
                "summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v1",
                "updated": "2024-11-26T17:28:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Cheng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Yu"
                },
                "author": "Cheng Yu",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v3",
                "updated": "2024-11-26T17:28:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17559v1",
                "updated": "2024-11-26T16:21:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:21:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces"
                },
                "summary": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest."
                },
                "authors": [
                    {
                        "name": "Abolfazl Changizi"
                    },
                    {
                        "name": "Ali H. Abdollahi Bafghi"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v1",
                "updated": "2024-11-26T14:23:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v1",
                "updated": "2024-11-26T05:10:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v1",
                "updated": "2024-11-26T04:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v1",
                "updated": "2024-11-25T13:33:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v2",
                "updated": "2024-11-25T12:14:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v2",
                "updated": "2024-11-24T21:57:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    57,
                    29,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v1",
                "updated": "2024-11-24T11:30:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper proposes a method for building large language models with\npredefined Key-Value (KV) cache capacity, particularly suitable for the\nattention layers in Transformer decode-only architectures. This method\nintroduces fixed-length KV caches to address the issue of excessive memory\nconsumption in traditional KV caches when handling infinite contexts. By\ndynamically updating the key-value vector sequences, it achieves efficient\ninference within limited cache capacity, significantly reducing memory usage\nwhile maintaining model performance and system throughput. Experimental results\nshow that this method significantly reduces memory usage while maintaining the\nmodel's inference quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a method for building large language models with\npredefined Key-Value (KV) cache capacity, particularly suitable for the\nattention layers in Transformer decode-only architectures. This method\nintroduces fixed-length KV caches to address the issue of excessive memory\nconsumption in traditional KV caches when handling infinite contexts. By\ndynamically updating the key-value vector sequences, it achieves efficient\ninference within limited cache capacity, significantly reducing memory usage\nwhile maintaining model performance and system throughput. Experimental results\nshow that this method significantly reduces memory usage while maintaining the\nmodel's inference quality."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15735v1",
                "updated": "2024-11-24T06:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T06:43:38Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "title": "Test-time Alignment-Enhanced Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Alignment-Enhanced Adapter for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA."
                },
                "authors": [
                    {
                        "name": "Baoshun Tong"
                    },
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v2",
                "updated": "2024-11-23T22:11:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    22,
                    11,
                    42,
                    5,
                    328,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05396v3",
                "updated": "2024-11-23T10:42:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    10,
                    42,
                    11,
                    5,
                    328,
                    0
                ],
                "published": "2024-02-08T04:16:35Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    16,
                    35,
                    3,
                    39,
                    0
                ],
                "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning"
                },
                "summary": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime."
                },
                "authors": [
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Christopher Leung"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IPDPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02109v2",
                "updated": "2024-11-23T01:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    1,
                    44,
                    0,
                    5,
                    328,
                    0
                ],
                "published": "2024-07-02T09:51:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    51,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images"
                },
                "summary": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA."
                },
                "authors": [
                    {
                        "name": "You Huang"
                    },
                    {
                        "name": "Wenbin Lai"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Liujuan Cao"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15322v1",
                "updated": "2024-11-22T19:30:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T19:30:40Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "title": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy"
                },
                "summary": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sugandima Weragoda"
                    },
                    {
                        "name": "Ping Xia"
                    },
                    {
                        "name": "Kevin Stephans"
                    },
                    {
                        "name": "Neil Woody"
                    },
                    {
                        "name": "Michael Martens"
                    },
                    {
                        "name": "Robert Brown"
                    },
                    {
                        "name": "Bingqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Bingqi Guo"
                },
                "author": "Bingqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v1",
                "updated": "2024-11-22T15:55:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v5",
                "updated": "2024-11-21T05:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    55,
                    43,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13854v1",
                "updated": "2024-11-21T05:26:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:26:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Static Reuse Profile Estimation for Array Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Reuse Profile Estimation for Array Applications"
                },
                "summary": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "Accepted in The International Symposium on Memory Systems (MEMSYS\n  24), September 30 to October 03, 2024, Washington, DC, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02243v3",
                "updated": "2024-11-21T04:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    12,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-04T03:06:37Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    3,
                    6,
                    37,
                    6,
                    155,
                    0
                ],
                "title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"
                },
                "summary": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks."
                },
                "authors": [
                    {
                        "name": "Jintao Rong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Linlin Ou"
                    },
                    {
                        "name": "Tianxiao Chen"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Yifan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Liu"
                },
                "author": "Yifan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v2",
                "updated": "2024-11-21T03:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    34,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Nadra Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Nadra Guizani"
                },
                "author": "Nadra Guizani",
                "arxiv_comment": "Model architecture changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13786v1",
                "updated": "2024-11-21T02:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Adaptable Embeddings Network (AEN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable Embeddings Network (AEN)"
                },
                "summary": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems."
                },
                "authors": [
                    {
                        "name": "Stan Loosmore"
                    },
                    {
                        "name": "Alexander Titus"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Titus"
                },
                "author": "Alexander Titus",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13676v1",
                "updated": "2024-11-20T19:51:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:51:25Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymba: A Hybrid-head Architecture for Small Language Models"
                },
                "summary": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Matthijs Van Keirsbilck"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Yingyan Lin"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "20 pages, models are available on huggingface",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "Sébastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13588v1",
                "updated": "2024-11-18T02:49:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T02:49:23Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "title": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study"
                },
                "summary": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis."
                },
                "authors": [
                    {
                        "name": "Xibo Sun"
                    },
                    {
                        "name": "Jiarui Fang"
                    },
                    {
                        "name": "Aoyu Li"
                    },
                    {
                        "name": "Jinzhe Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhe Pan"
                },
                "author": "Jinzhe Pan",
                "arxiv_comment": "9 pages including reference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v3",
                "updated": "2024-11-17T12:56:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    12,
                    56,
                    16,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v1",
                "updated": "2024-11-16T20:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13112v3",
                "updated": "2024-11-16T20:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    39,
                    46,
                    5,
                    321,
                    0
                ],
                "published": "2024-03-19T19:27:23Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    19,
                    27,
                    23,
                    1,
                    79,
                    0
                ],
                "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks"
                },
                "summary": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance."
                },
                "authors": [
                    {
                        "name": "Bo-Ru Lu"
                    },
                    {
                        "name": "Nikita Haduong"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Mari Ostendorf"
                    }
                ],
                "author_detail": {
                    "name": "Mari Ostendorf"
                },
                "author": "Mari Ostendorf",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10803v1",
                "updated": "2024-11-16T13:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T13:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model"
                },
                "summary": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "8 pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01733v2",
                "updated": "2024-11-16T07:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    43,
                    28,
                    5,
                    321,
                    0
                ],
                "published": "2024-06-03T18:49:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    49,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching"
                },
                "summary": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache"
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Michael Bi Mi"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v1",
                "updated": "2024-11-16T01:39:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v4",
                "updated": "2024-11-15T22:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    37,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v2",
                "updated": "2024-11-15T22:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    30,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v1",
                "updated": "2024-11-15T16:24:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v2",
                "updated": "2024-11-15T07:25:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    25,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v1",
                "updated": "2024-11-15T00:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "RuQing G. Xu"
                    },
                    {
                        "name": "Devin A. Matthews"
                    }
                ],
                "author_detail": {
                    "name": "Devin A. Matthews"
                },
                "author": "Devin A. Matthews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09812v1",
                "updated": "2024-11-14T21:01:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:01:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments"
                },
                "summary": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v2",
                "updated": "2024-11-14T17:46:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    46,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v1",
                "updated": "2024-11-14T08:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Pkd-tree: Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pkd-tree: Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08672v1",
                "updated": "2024-11-13T15:07:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T15:07:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks"
                },
                "summary": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Zhibin Gao"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "conference paper with 6 pages and 5 figures. arXiv admin note: text\n  overlap with arXiv:2411.01458",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08312v1",
                "updated": "2024-11-13T03:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T03:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "title": "A Novel Extensible Simulation Framework for CXL-Enabled Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Extensible Simulation Framework for CXL-Enabled Systems"
                },
                "summary": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3."
                },
                "authors": [
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Mingzhe Zhang"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "arxiv_affiliation": "Peking University",
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08203v1",
                "updated": "2024-11-12T21:50:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T21:50:03Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "title": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing"
                },
                "summary": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads."
                },
                "authors": [
                    {
                        "name": "Jacopo Tagliabue"
                    },
                    {
                        "name": "Ryan Curtin"
                    },
                    {
                        "name": "Ciro Greco"
                    }
                ],
                "author_detail": {
                    "name": "Ciro Greco"
                },
                "author": "Ciro Greco",
                "arxiv_comment": "Pre-print of the paper accepted at DEMAI@IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06219v3",
                "updated": "2024-11-12T08:18:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    18,
                    45,
                    1,
                    317,
                    0
                ],
                "published": "2024-05-10T03:06:24Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    3,
                    6,
                    24,
                    4,
                    131,
                    0
                ],
                "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding."
                },
                "authors": [
                    {
                        "name": "Haojie Duanmu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Jiangfei Duan"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v1",
                "updated": "2024-11-12T08:17:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06659v1",
                "updated": "2024-11-11T01:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:53:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning"
                },
                "summary": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git ."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "arxiv_comment": "16 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v2",
                "updated": "2024-11-10T23:04:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    23,
                    4,
                    12,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v4",
                "updated": "2024-11-10T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    58,
                    7,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04873v2",
                "updated": "2024-11-10T10:08:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    10,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "published": "2024-06-07T12:12:25Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    12,
                    12,
                    25,
                    4,
                    159,
                    0
                ],
                "title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior"
                },
                "summary": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main"
                },
                "authors": [
                    {
                        "name": "Tanvir Mahmud"
                    },
                    {
                        "name": "Mustafa Munir"
                    },
                    {
                        "name": "Radu Marculescu"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "arxiv_comment": "Accepted in WACV 2025. Project page:\n  https://tanvir-utexas.github.io/AdaVE_Demo/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v1",
                "updated": "2024-11-10T05:12:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02542v2",
                "updated": "2024-11-07T18:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    58,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    58,
                    3,
                    1,
                    156,
                    0
                ],
                "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loki: Low-rank Keys for Efficient Sparse Attention"
                },
                "summary": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods."
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "Proceedings of the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (Main Conference Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v1",
                "updated": "2024-11-07T14:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.04997v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04997v3",
                "updated": "2024-11-26T18:59:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    59,
                    28,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-07T18:59:16Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    16,
                    3,
                    312,
                    0
                ],
                "title": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation"
                },
                "summary": "CLIP is a foundational multimodal model that aligns image and text features\ninto a shared space using contrastive learning on large-scale image-text pairs.\nIts strength lies in leveraging natural language as a rich supervisory signal.\nWith the rapid progress of large language models (LLMs), we explore their\npotential to further enhance CLIP's multimodal representation learning. This\nwork introduces a fine-tuning approach that integrates LLMs with the pretrained\nCLIP visual encoder, leveraging LLMs' advanced text understanding and\nopen-world knowledge to improve CLIP's ability to process long and complex\ncaptions. To address the challenge of LLMs' autoregressive nature, we propose a\ncaption-to-caption contrastive learning framework to enhance the discriminative\npower of their outputs. Our method achieves substantial performance gains on\nvarious downstream tasks, demonstrating the effectiveness of combining LLMs\nwith CLIP for enhanced multimodal learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP is a foundational multimodal model that aligns image and text features\ninto a shared space using contrastive learning on large-scale image-text pairs.\nIts strength lies in leveraging natural language as a rich supervisory signal.\nWith the rapid progress of large language models (LLMs), we explore their\npotential to further enhance CLIP's multimodal representation learning. This\nwork introduces a fine-tuning approach that integrates LLMs with the pretrained\nCLIP visual encoder, leveraging LLMs' advanced text understanding and\nopen-world knowledge to improve CLIP's ability to process long and complex\ncaptions. To address the challenge of LLMs' autoregressive nature, we propose a\ncaption-to-caption contrastive learning framework to enhance the discriminative\npower of their outputs. Our method achieves substantial performance gains on\nvarious downstream tasks, demonstrating the effectiveness of combining LLMs\nwith CLIP for enhanced multimodal learning."
                },
                "authors": [
                    {
                        "name": "Weiquan Huang"
                    },
                    {
                        "name": "Aoqi Wu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Liang Hu"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Xiyang Dai"
                    },
                    {
                        "name": "Dongdong Chen"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04997v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04997v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17697v1",
                "updated": "2024-11-26T18:59:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    59,
                    22,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:59:22Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    59,
                    22,
                    1,
                    331,
                    0
                ],
                "title": "StableAnimator: High-Quality Identity-Preserving Human Image Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableAnimator: High-Quality Identity-Preserving Human Image Animation"
                },
                "summary": "Current diffusion models for human image animation struggle to ensure\nidentity (ID) consistency. This paper presents StableAnimator, the first\nend-to-end ID-preserving video diffusion framework, which synthesizes\nhigh-quality videos without any post-processing, conditioned on a reference\nimage and a sequence of poses. Building upon a video diffusion model,\nStableAnimator contains carefully designed modules for both training and\ninference striving for identity consistency. In particular, StableAnimator\nbegins by computing image and face embeddings with off-the-shelf extractors,\nrespectively and face embeddings are further refined by interacting with image\nembeddings using a global content-aware Face Encoder. Then, StableAnimator\nintroduces a novel distribution-aware ID Adapter that prevents interference\ncaused by temporal layers while preserving ID via alignment. During inference,\nwe propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to\nfurther enhance the face quality. We demonstrate that solving the HJB equation\ncan be integrated into the diffusion denoising process, and the resulting\nsolution constrains the denoising path and thus benefits ID preservation.\nExperiments on multiple benchmarks show the effectiveness of StableAnimator\nboth qualitatively and quantitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current diffusion models for human image animation struggle to ensure\nidentity (ID) consistency. This paper presents StableAnimator, the first\nend-to-end ID-preserving video diffusion framework, which synthesizes\nhigh-quality videos without any post-processing, conditioned on a reference\nimage and a sequence of poses. Building upon a video diffusion model,\nStableAnimator contains carefully designed modules for both training and\ninference striving for identity consistency. In particular, StableAnimator\nbegins by computing image and face embeddings with off-the-shelf extractors,\nrespectively and face embeddings are further refined by interacting with image\nembeddings using a global content-aware Face Encoder. Then, StableAnimator\nintroduces a novel distribution-aware ID Adapter that prevents interference\ncaused by temporal layers while preserving ID via alignment. During inference,\nwe propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to\nfurther enhance the face quality. We demonstrate that solving the HJB equation\ncan be integrated into the diffusion denoising process, and the resulting\nsolution constrains the denoising path and thus benefits ID preservation.\nExperiments on multiple benchmarks show the effectiveness of StableAnimator\nboth qualitatively and quantitatively."
                },
                "authors": [
                    {
                        "name": "Shuyuan Tu"
                    },
                    {
                        "name": "Zhen Xing"
                    },
                    {
                        "name": "Xintong Han"
                    },
                    {
                        "name": "Zhi-Qi Cheng"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17693v1",
                "updated": "2024-11-26T18:58:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    58,
                    20,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:58:20Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    58,
                    20,
                    1,
                    331,
                    0
                ],
                "title": "Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats"
                },
                "summary": "As large language models (LLMs) become increasingly capable, it is prudent to\nassess whether safety measures remain effective even if LLMs intentionally try\nto bypass them. Previous work introduced control evaluations, an adversarial\nframework for testing deployment strategies of untrusted models (i.e., models\nwhich might be trying to bypass safety measures). While prior work treats a\nsingle failure as unacceptable, we perform control evaluations in a\n\"distributed threat setting\" -- a setting where no single action is\ncatastrophic and no single action provides overwhelming evidence of\nmisalignment. We approach this problem with a two-level deployment framework\nthat uses an adaptive macro-protocol to choose between micro-protocols.\nMicro-protocols operate on a single task, using a less capable, but extensively\ntested (trusted) model to harness and monitor the untrusted model. Meanwhile,\nthe macro-protocol maintains an adaptive credence on the untrusted model's\nalignment based on its past actions, using it to pick between safer and riskier\nmicro-protocols. We evaluate our method in a code generation testbed where a\nred team attempts to generate subtly backdoored code with an LLM whose\ndeployment is safeguarded by a blue team. We plot Pareto frontiers of safety (#\nof non-backdoored solutions) and usefulness (# of correct solutions). At a\ngiven level of usefulness, our adaptive deployment strategy reduces the number\nof backdoors by 80% compared to non-adaptive baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly capable, it is prudent to\nassess whether safety measures remain effective even if LLMs intentionally try\nto bypass them. Previous work introduced control evaluations, an adversarial\nframework for testing deployment strategies of untrusted models (i.e., models\nwhich might be trying to bypass safety measures). While prior work treats a\nsingle failure as unacceptable, we perform control evaluations in a\n\"distributed threat setting\" -- a setting where no single action is\ncatastrophic and no single action provides overwhelming evidence of\nmisalignment. We approach this problem with a two-level deployment framework\nthat uses an adaptive macro-protocol to choose between micro-protocols.\nMicro-protocols operate on a single task, using a less capable, but extensively\ntested (trusted) model to harness and monitor the untrusted model. Meanwhile,\nthe macro-protocol maintains an adaptive credence on the untrusted model's\nalignment based on its past actions, using it to pick between safer and riskier\nmicro-protocols. We evaluate our method in a code generation testbed where a\nred team attempts to generate subtly backdoored code with an LLM whose\ndeployment is safeguarded by a blue team. We plot Pareto frontiers of safety (#\nof non-backdoored solutions) and usefulness (# of correct solutions). At a\ngiven level of usefulness, our adaptive deployment strategy reduces the number\nof backdoors by 80% compared to non-adaptive baselines."
                },
                "authors": [
                    {
                        "name": "Jiaxin Wen"
                    },
                    {
                        "name": "Vivek Hebbar"
                    },
                    {
                        "name": "Caleb Larson"
                    },
                    {
                        "name": "Aryan Bhatt"
                    },
                    {
                        "name": "Ansh Radhakrishnan"
                    },
                    {
                        "name": "Mrinank Sharma"
                    },
                    {
                        "name": "Henry Sleight"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "He He"
                    },
                    {
                        "name": "Ethan Perez"
                    },
                    {
                        "name": "Buck Shlegeris"
                    },
                    {
                        "name": "Akbir Khan"
                    }
                ],
                "author_detail": {
                    "name": "Akbir Khan"
                },
                "author": "Akbir Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17691v1",
                "updated": "2024-11-26T18:57:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    57,
                    58,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:57:58Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    57,
                    58,
                    1,
                    331,
                    0
                ],
                "title": "Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for\n  Quantized LLMs with 100T Training Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for\n  Quantized LLMs with 100T Training Tokens"
                },
                "summary": "We reveal that low-bit quantization favors undertrained large language models\n(LLMs) by observing that models with larger sizes or fewer training tokens\nexperience less quantization-induced degradation (QiD) when applying low-bit\nquantization, whereas smaller models with extensive training tokens suffer\nsignificant QiD. To gain deeper insights into this trend, we study over 1500\nquantized LLM checkpoints of various sizes and at different training levels\n(undertrained or fully trained) in a controlled setting, deriving scaling laws\nfor understanding the relationship between QiD and factors such as the number\nof training tokens, model size and bit width.\n  With the derived scaling laws, we propose a novel perspective that we can use\nQiD to measure an LLM's training levels and determine the number of training\ntokens required for fully training LLMs of various sizes. Moreover, we use the\nscaling laws to predict the quantization performance of different-sized LLMs\ntrained with 100 trillion tokens. Our projection shows that the low-bit\nquantization performance of future models, which are expected to be trained\nwith over 100 trillion tokens, may NOT be desirable. This poses a potential\nchallenge for low-bit quantization in the future and highlights the need for\nawareness of a model's training level when evaluating low-bit quantization\nresearch. To facilitate future research on this problem, we release all the\n1500+ quantized checkpoints used in this work at\nhttps://huggingface.co/Xu-Ouyang.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We reveal that low-bit quantization favors undertrained large language models\n(LLMs) by observing that models with larger sizes or fewer training tokens\nexperience less quantization-induced degradation (QiD) when applying low-bit\nquantization, whereas smaller models with extensive training tokens suffer\nsignificant QiD. To gain deeper insights into this trend, we study over 1500\nquantized LLM checkpoints of various sizes and at different training levels\n(undertrained or fully trained) in a controlled setting, deriving scaling laws\nfor understanding the relationship between QiD and factors such as the number\nof training tokens, model size and bit width.\n  With the derived scaling laws, we propose a novel perspective that we can use\nQiD to measure an LLM's training levels and determine the number of training\ntokens required for fully training LLMs of various sizes. Moreover, we use the\nscaling laws to predict the quantization performance of different-sized LLMs\ntrained with 100 trillion tokens. Our projection shows that the low-bit\nquantization performance of future models, which are expected to be trained\nwith over 100 trillion tokens, may NOT be desirable. This poses a potential\nchallenge for low-bit quantization in the future and highlights the need for\nawareness of a model's training level when evaluating low-bit quantization\nresearch. To facilitate future research on this problem, we release all the\n1500+ quantized checkpoints used in this work at\nhttps://huggingface.co/Xu-Ouyang."
                },
                "authors": [
                    {
                        "name": "Xu Ouyang"
                    },
                    {
                        "name": "Tao Ge"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "Work in progress; Please note that Figure 1's gray areas may not be\n  displayed properly using Chrome (maybe due to bugs in Chrome)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17686v1",
                "updated": "2024-11-26T18:53:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    53,
                    51,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:53:51Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    53,
                    51,
                    1,
                    331,
                    0
                ],
                "title": "Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for\n  Training-Free Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for\n  Training-Free Acceleration"
                },
                "summary": "To accelerate the inference of heavy Multimodal Large Language Models\n(MLLMs), this study rethinks the current landscape of training-free token\nreduction research. We regret to find that the critical components of existing\nmethods are tightly intertwined, with their interconnections and effects\nremaining unclear for comparison, transfer, and expansion. Therefore, we\npropose a unified ''filter-correlate-compress'' paradigm that decomposes the\ntoken reduction into three distinct stages within a pipeline, maintaining\nconsistent design objectives and elements while allowing for unique\nimplementations. We additionally demystify the popular works and subsume them\ninto our paradigm to showcase its universality. Finally, we offer a suite of\nmethods grounded in the paradigm, striking a balance between speed and accuracy\nthroughout different phases of the inference. Experimental results across 10\nbenchmarks indicate that our methods can achieve up to an 82.4% reduction in\nFLOPs with a minimal impact on performance, simultaneously surpassing\nstate-of-the-art training-free methods. Our project page is at\nhttps://ficoco-accelerate.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To accelerate the inference of heavy Multimodal Large Language Models\n(MLLMs), this study rethinks the current landscape of training-free token\nreduction research. We regret to find that the critical components of existing\nmethods are tightly intertwined, with their interconnections and effects\nremaining unclear for comparison, transfer, and expansion. Therefore, we\npropose a unified ''filter-correlate-compress'' paradigm that decomposes the\ntoken reduction into three distinct stages within a pipeline, maintaining\nconsistent design objectives and elements while allowing for unique\nimplementations. We additionally demystify the popular works and subsume them\ninto our paradigm to showcase its universality. Finally, we offer a suite of\nmethods grounded in the paradigm, striking a balance between speed and accuracy\nthroughout different phases of the inference. Experimental results across 10\nbenchmarks indicate that our methods can achieve up to an 82.4% reduction in\nFLOPs with a minimal impact on performance, simultaneously surpassing\nstate-of-the-art training-free methods. Our project page is at\nhttps://ficoco-accelerate.github.io/."
                },
                "authors": [
                    {
                        "name": "Yuhang Han"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Honggang Chen"
                    },
                    {
                        "name": "Qingsen Yan"
                    },
                    {
                        "name": "Siteng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Siteng Huang"
                },
                "author": "Siteng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02611v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02611v3",
                "updated": "2024-11-26T18:51:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    51,
                    57,
                    1,
                    331,
                    0
                ],
                "published": "2024-06-03T07:56:58Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    7,
                    56,
                    58,
                    0,
                    155,
                    0
                ],
                "title": "LOLA: LLM-Assisted Online Learning Algorithm for Content Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LOLA: LLM-Assisted Online Learning Algorithm for Content Experiments"
                },
                "summary": "Modern media firms require automated and efficient methods to identify\ncontent that is most engaging and appealing to users. Leveraging a large-scale\ndataset from Upworthy (a news publisher), which includes 17,681 headline A/B\ntests, we first investigate the ability of three pure-LLM approaches to\nidentify the catchiest headline: prompt-based methods, embedding-based methods,\nand fine-tuned open-source LLMs. Prompt-based approaches perform poorly, while\nboth OpenAI-embedding-based models and the fine-tuned Llama-3-8B achieve\nmarginally higher accuracy than random predictions. In sum, none of the\npure-LLM-based methods can predict the best-performing headline with high\naccuracy. We then introduce the LLM-Assisted Online Learning Algorithm (LOLA),\na novel framework that integrates Large Language Models (LLMs) with adaptive\nexperimentation to optimize content delivery. LOLA combines the best pure-LLM\napproach with the Upper Confidence Bound algorithm to allocate traffic and\nmaximize clicks adaptively. Our numerical experiments on Upworthy data show\nthat LOLA outperforms the standard A/B test method (the current status quo at\nUpworthy), pure bandit algorithms, and pure-LLM approaches, particularly in\nscenarios with limited experimental traffic. Our approach is scalable and\napplicable to content experiments across various settings where firms seek to\noptimize user engagement, including digital advertising and social media\nrecommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern media firms require automated and efficient methods to identify\ncontent that is most engaging and appealing to users. Leveraging a large-scale\ndataset from Upworthy (a news publisher), which includes 17,681 headline A/B\ntests, we first investigate the ability of three pure-LLM approaches to\nidentify the catchiest headline: prompt-based methods, embedding-based methods,\nand fine-tuned open-source LLMs. Prompt-based approaches perform poorly, while\nboth OpenAI-embedding-based models and the fine-tuned Llama-3-8B achieve\nmarginally higher accuracy than random predictions. In sum, none of the\npure-LLM-based methods can predict the best-performing headline with high\naccuracy. We then introduce the LLM-Assisted Online Learning Algorithm (LOLA),\na novel framework that integrates Large Language Models (LLMs) with adaptive\nexperimentation to optimize content delivery. LOLA combines the best pure-LLM\napproach with the Upper Confidence Bound algorithm to allocate traffic and\nmaximize clicks adaptively. Our numerical experiments on Upworthy data show\nthat LOLA outperforms the standard A/B test method (the current status quo at\nUpworthy), pure bandit algorithms, and pure-LLM approaches, particularly in\nscenarios with limited experimental traffic. Our approach is scalable and\napplicable to content experiments across various settings where firms seek to\noptimize user engagement, including digital advertising and social media\nrecommendations."
                },
                "authors": [
                    {
                        "name": "Zikun Ye"
                    },
                    {
                        "name": "Hema Yoganarasimhan"
                    },
                    {
                        "name": "Yufeng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yufeng Zheng"
                },
                "author": "Yufeng Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02611v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02611v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.10748v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.10748v3",
                "updated": "2024-11-26T18:45:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    45,
                    8,
                    1,
                    331,
                    0
                ],
                "published": "2023-01-25T18:11:02Z",
                "published_parsed": [
                    2023,
                    1,
                    25,
                    18,
                    11,
                    2,
                    2,
                    25,
                    0
                ],
                "title": "Individualized prescriptive inference in ischaemic stroke",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Individualized prescriptive inference in ischaemic stroke"
                },
                "summary": "The gold standard in the treatment of ischaemic stroke is set by evidence\nfrom randomized controlled trials, based on simple descriptions of\npresumptively homogeneous populations. Yet the manifest complexity of the\nbrain's functional, connective, and vascular architectures introduces\nheterogeneities that violate the underlying statistical premisses, potentially\nleading to substantial errors at both individual and population levels. The\ncounterfactual nature of interventional inference renders quantifying the\nimpact of this defect difficult. Here we conduct a comprehensive series of\nsemi-synthetic, biologically plausible, virtual interventional trials across\n100M+ distinct simulations. We generate empirically grounded virtual trial data\nfrom large-scale meta-analytic connective, functional, genetic expression, and\nreceptor distribution data, with high-resolution maps of 4K+ acute ischaemic\nlesions. Within each trial, we estimate treatment effects using models varying\nin complexity, in the presence of increasingly confounded outcomes and noisy\ntreatment responses. Individualized prescriptions inferred from simple models,\nfitted to unconfounded data, were less accurate than those from complex models,\nfitted to confounded data. Our results indicate that complex modelling with\nrichly represented lesion data is critical to individualized prescriptive\ninference in ischaemic stroke.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The gold standard in the treatment of ischaemic stroke is set by evidence\nfrom randomized controlled trials, based on simple descriptions of\npresumptively homogeneous populations. Yet the manifest complexity of the\nbrain's functional, connective, and vascular architectures introduces\nheterogeneities that violate the underlying statistical premisses, potentially\nleading to substantial errors at both individual and population levels. The\ncounterfactual nature of interventional inference renders quantifying the\nimpact of this defect difficult. Here we conduct a comprehensive series of\nsemi-synthetic, biologically plausible, virtual interventional trials across\n100M+ distinct simulations. We generate empirically grounded virtual trial data\nfrom large-scale meta-analytic connective, functional, genetic expression, and\nreceptor distribution data, with high-resolution maps of 4K+ acute ischaemic\nlesions. Within each trial, we estimate treatment effects using models varying\nin complexity, in the presence of increasingly confounded outcomes and noisy\ntreatment responses. Individualized prescriptions inferred from simple models,\nfitted to unconfounded data, were less accurate than those from complex models,\nfitted to confounded data. Our results indicate that complex modelling with\nrichly represented lesion data is critical to individualized prescriptive\ninference in ischaemic stroke."
                },
                "authors": [
                    {
                        "name": "Dominic Giles"
                    },
                    {
                        "name": "Robert Gray"
                    },
                    {
                        "name": "Chris Foulon"
                    },
                    {
                        "name": "Guilherme Pombo"
                    },
                    {
                        "name": "James K. Ruffle"
                    },
                    {
                        "name": "Tianbo Xu"
                    },
                    {
                        "name": "H. Rolf Jäger"
                    },
                    {
                        "name": "Jorge Cardoso"
                    },
                    {
                        "name": "Sebastien Ourselin"
                    },
                    {
                        "name": "Geraint Rees"
                    },
                    {
                        "name": "Ashwani Jha"
                    },
                    {
                        "name": "Parashkev Nachev"
                    }
                ],
                "author_detail": {
                    "name": "Parashkev Nachev"
                },
                "author": "Parashkev Nachev",
                "arxiv_comment": "131 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.10748v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.10748v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17679v1",
                "updated": "2024-11-26T18:44:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    44,
                    39,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:44:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    44,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "Enhancing Character-Level Understanding in LLMs through Token Internal\n  Structure Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Character-Level Understanding in LLMs through Token Internal\n  Structure Learning"
                },
                "summary": "Tokenization techniques such as Byte-Pair Encoding (BPE) and Byte-Level BPE\n(BBPE) have significantly improved the computational efficiency and vocabulary\nrepresentation stability of large language models (LLMs) by segmenting text\ninto tokens. However, this segmentation often obscures the internal character\nstructures and sequences within tokens, preventing models from fully learning\nthese intricate details during training. Consequently, LLMs struggle to\ncomprehend the character compositions and positional relationships within\ntokens, especially when fine-tuned on downstream tasks with limited data. In\nthis paper, we introduce Token Internal Position Awareness (TIPA), a novel\napproach that enhances LLMs' understanding of internal token structures by\ntraining them on reverse character prediction tasks using the tokenizer's own\nvocabulary. This method enables models to effectively learn and generalize\ncharacter positions and internal structures. Experimental results demonstrate\nthat LLMs trained with TIPA outperform baseline models in predicting character\npositions at the token level. Furthermore, when applied to the downstream task\nof Chinese Spelling Correction (CSC), TIPA not only accelerates model\nconvergence but also significantly improves task performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization techniques such as Byte-Pair Encoding (BPE) and Byte-Level BPE\n(BBPE) have significantly improved the computational efficiency and vocabulary\nrepresentation stability of large language models (LLMs) by segmenting text\ninto tokens. However, this segmentation often obscures the internal character\nstructures and sequences within tokens, preventing models from fully learning\nthese intricate details during training. Consequently, LLMs struggle to\ncomprehend the character compositions and positional relationships within\ntokens, especially when fine-tuned on downstream tasks with limited data. In\nthis paper, we introduce Token Internal Position Awareness (TIPA), a novel\napproach that enhances LLMs' understanding of internal token structures by\ntraining them on reverse character prediction tasks using the tokenizer's own\nvocabulary. This method enables models to effectively learn and generalize\ncharacter positions and internal structures. Experimental results demonstrate\nthat LLMs trained with TIPA outperform baseline models in predicting character\npositions at the token level. Furthermore, when applied to the downstream task\nof Chinese Spelling Correction (CSC), TIPA not only accelerates model\nconvergence but also significantly improves task performance."
                },
                "authors": [
                    {
                        "name": "Zhu Xu"
                    },
                    {
                        "name": "Zhiqiang Zhao"
                    },
                    {
                        "name": "Zihan Zhang"
                    },
                    {
                        "name": "Yuchi Liu"
                    },
                    {
                        "name": "Quanwei Shen"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Yu Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Kuang"
                },
                "author": "Yu Kuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17674v1",
                "updated": "2024-11-26T18:35:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    35,
                    24,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:35:24Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    35,
                    24,
                    1,
                    331,
                    0
                ],
                "title": "Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with\n  Receptive-Field-Aware Attention Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with\n  Receptive-Field-Aware Attention Weighting"
                },
                "summary": "Understanding the emotions in a dialogue usually requires external knowledge\nto accurately understand the contents. As the LLMs become more and more\npowerful, we do not want to settle on the limited ability of the pre-trained\nlanguage model. However, the LLMs either can only process text modality or are\ntoo expensive to process the multimedia information. We aim to utilize both the\npower of LLMs and the supplementary features from the multimedia modalities. In\nthis paper, we present a framework, Lantern, that can improve the performance\nof a certain vanilla model by prompting large language models with\nreceptive-field-aware attention weighting. This framework trained a multi-task\nvanilla model to produce probabilities of emotion classes and dimension scores.\nThese predictions are fed into the LLMs as references to adjust the predicted\nprobabilities of each emotion class with its external knowledge and contextual\nunderstanding. We slice the dialogue into different receptive fields, and each\nsample is included in exactly t receptive fields. Finally, the predictions of\nLLMs are merged with a receptive-field-aware attention-driven weighting module.\nIn the experiments, vanilla models CORECT and SDT are deployed in Lantern with\nGPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way\nsettings demonstrated that the Lantern can significantly improve the\nperformance of current vanilla models by up to 1.23% and 1.80%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the emotions in a dialogue usually requires external knowledge\nto accurately understand the contents. As the LLMs become more and more\npowerful, we do not want to settle on the limited ability of the pre-trained\nlanguage model. However, the LLMs either can only process text modality or are\ntoo expensive to process the multimedia information. We aim to utilize both the\npower of LLMs and the supplementary features from the multimedia modalities. In\nthis paper, we present a framework, Lantern, that can improve the performance\nof a certain vanilla model by prompting large language models with\nreceptive-field-aware attention weighting. This framework trained a multi-task\nvanilla model to produce probabilities of emotion classes and dimension scores.\nThese predictions are fed into the LLMs as references to adjust the predicted\nprobabilities of each emotion class with its external knowledge and contextual\nunderstanding. We slice the dialogue into different receptive fields, and each\nsample is included in exactly t receptive fields. Finally, the predictions of\nLLMs are merged with a receptive-field-aware attention-driven weighting module.\nIn the experiments, vanilla models CORECT and SDT are deployed in Lantern with\nGPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way\nsettings demonstrated that the Lantern can significantly improve the\nperformance of current vanilla models by up to 1.23% and 1.80%."
                },
                "authors": [
                    {
                        "name": "Liyun Zhang"
                    },
                    {
                        "name": "Dian Ding"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Yi-Chao Chen"
                    },
                    {
                        "name": "Guangtao Xue"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Xue"
                },
                "author": "Guangtao Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17673v1",
                "updated": "2024-11-26T18:32:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    32,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:32:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    32,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "SketchAgent: Language-Driven Sequential Sketch Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SketchAgent: Language-Driven Sequential Sketch Generation"
                },
                "summary": "Sketching serves as a versatile tool for externalizing ideas, enabling rapid\nexploration and visual communication that spans various disciplines. While\nartificial systems have driven substantial advances in content creation and\nhuman-computer interaction, capturing the dynamic and abstract nature of human\nsketching remains challenging. In this work, we introduce SketchAgent, a\nlanguage-driven, sequential sketch generation method that enables users to\ncreate, modify, and refine sketches through dynamic, conversational\ninteractions. Our approach requires no training or fine-tuning. Instead, we\nleverage the sequential nature and rich prior knowledge of off-the-shelf\nmultimodal large language models (LLMs). We present an intuitive sketching\nlanguage, introduced to the model through in-context examples, enabling it to\n\"draw\" using string-based actions. These are processed into vector graphics and\nthen rendered to create a sketch on a pixel canvas, which can be accessed again\nfor further tasks. By drawing stroke by stroke, our agent captures the\nevolving, dynamic qualities intrinsic to sketching. We demonstrate that\nSketchAgent can generate sketches from diverse prompts, engage in\ndialogue-driven drawing, and collaborate meaningfully with human users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sketching serves as a versatile tool for externalizing ideas, enabling rapid\nexploration and visual communication that spans various disciplines. While\nartificial systems have driven substantial advances in content creation and\nhuman-computer interaction, capturing the dynamic and abstract nature of human\nsketching remains challenging. In this work, we introduce SketchAgent, a\nlanguage-driven, sequential sketch generation method that enables users to\ncreate, modify, and refine sketches through dynamic, conversational\ninteractions. Our approach requires no training or fine-tuning. Instead, we\nleverage the sequential nature and rich prior knowledge of off-the-shelf\nmultimodal large language models (LLMs). We present an intuitive sketching\nlanguage, introduced to the model through in-context examples, enabling it to\n\"draw\" using string-based actions. These are processed into vector graphics and\nthen rendered to create a sketch on a pixel canvas, which can be accessed again\nfor further tasks. By drawing stroke by stroke, our agent captures the\nevolving, dynamic qualities intrinsic to sketching. We demonstrate that\nSketchAgent can generate sketches from diverse prompts, engage in\ndialogue-driven drawing, and collaborate meaningfully with human users."
                },
                "authors": [
                    {
                        "name": "Yael Vinker"
                    },
                    {
                        "name": "Tamar Rott Shaham"
                    },
                    {
                        "name": "Kristine Zheng"
                    },
                    {
                        "name": "Alex Zhao"
                    },
                    {
                        "name": "Judith E Fan"
                    },
                    {
                        "name": "Antonio Torralba"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Torralba"
                },
                "author": "Antonio Torralba",
                "arxiv_comment": "project page: https://sketch-agent.csail.mit.edu/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17672v1",
                "updated": "2024-11-26T18:31:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    31,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:31:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    31,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Synthetic Data Generation with LLM for Improved Depression Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Data Generation with LLM for Improved Depression Prediction"
                },
                "summary": "Automatic detection of depression is a rapidly growing field of research at\nthe intersection of psychology and machine learning. However, with its\nexponential interest comes a growing concern for data privacy and scarcity due\nto the sensitivity of such a topic. In this paper, we propose a pipeline for\nLarge Language Models (LLMs) to generate synthetic data to improve the\nperformance of depression prediction models. Starting from unstructured,\nnaturalistic text data from recorded transcripts of clinical interviews, we\nutilize an open-source LLM to generate synthetic data through chain-of-thought\nprompting. This pipeline involves two key steps: the first step is the\ngeneration of the synopsis and sentiment analysis based on the original\ntranscript and depression score, while the second is the generation of the\nsynthetic synopsis/sentiment analysis based on the summaries generated in the\nfirst step and a new depression score. Not only was the synthetic data\nsatisfactory in terms of fidelity and privacy-preserving metrics, it also\nbalanced the distribution of severity in the training dataset, thereby\nsignificantly enhancing the model's capability in predicting the intensity of\nthe patient's depression. By leveraging LLMs to generate synthetic data that\ncan be augmented to limited and imbalanced real-world datasets, we demonstrate\na novel approach to addressing data scarcity and privacy concerns commonly\nfaced in automatic depression detection, all while maintaining the statistical\nintegrity of the original dataset. This approach offers a robust framework for\nfuture mental health research and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic detection of depression is a rapidly growing field of research at\nthe intersection of psychology and machine learning. However, with its\nexponential interest comes a growing concern for data privacy and scarcity due\nto the sensitivity of such a topic. In this paper, we propose a pipeline for\nLarge Language Models (LLMs) to generate synthetic data to improve the\nperformance of depression prediction models. Starting from unstructured,\nnaturalistic text data from recorded transcripts of clinical interviews, we\nutilize an open-source LLM to generate synthetic data through chain-of-thought\nprompting. This pipeline involves two key steps: the first step is the\ngeneration of the synopsis and sentiment analysis based on the original\ntranscript and depression score, while the second is the generation of the\nsynthetic synopsis/sentiment analysis based on the summaries generated in the\nfirst step and a new depression score. Not only was the synthetic data\nsatisfactory in terms of fidelity and privacy-preserving metrics, it also\nbalanced the distribution of severity in the training dataset, thereby\nsignificantly enhancing the model's capability in predicting the intensity of\nthe patient's depression. By leveraging LLMs to generate synthetic data that\ncan be augmented to limited and imbalanced real-world datasets, we demonstrate\na novel approach to addressing data scarcity and privacy concerns commonly\nfaced in automatic depression detection, all while maintaining the statistical\nintegrity of the original dataset. This approach offers a robust framework for\nfuture mental health research and applications."
                },
                "authors": [
                    {
                        "name": "Andrea Kang"
                    },
                    {
                        "name": "Jun Yu Chen"
                    },
                    {
                        "name": "Zoe Lee-Youngzie"
                    },
                    {
                        "name": "Shuhao Fu"
                    }
                ],
                "author_detail": {
                    "name": "Shuhao Fu"
                },
                "author": "Shuhao Fu",
                "arxiv_comment": "6 pages excluding references and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17663v1",
                "updated": "2024-11-26T18:26:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    26,
                    20,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:26:20Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    26,
                    20,
                    1,
                    331,
                    0
                ],
                "title": "Accelerated nested sampling with $β$-flows for gravitational waves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated nested sampling with $β$-flows for gravitational waves"
                },
                "summary": "There is an ever-growing need in the gravitational wave community for fast\nand reliable inference methods, accompanied by an informative error bar. Nested\nsampling satisfies the last two requirements, but its computational cost can\nbecome prohibitive when using the most accurate waveform models. In this paper,\nwe demonstrate the acceleration of nested sampling using a technique called\nposterior repartitioning. This method leverages nested sampling's unique\nability to separate prior and likelihood contributions at the algorithmic\nlevel. Specifically, we define a `repartitioned prior' informed by the\nposterior from a low-resolution run. To construct this repartitioned prior, we\nuse a $\\beta$-flow, a novel type of conditional normalizing flow designed to\nbetter learn deep tail probabilities. $\\beta$-flows are trained on the entire\nnested sampling run and conditioned on an inverse temperature $\\beta$. Applying\nour methods to simulated and real binary black hole mergers, we demonstrate how\nthey can reduce the number of likelihood evaluations required for convergence\nby up to an order of magnitude, enabling faster model comparison and parameter\nestimation. Furthermore, we highlight the robustness of using $\\beta$-flows\nover standard normalizing flows to accelerate nested sampling. Notably,\n$\\beta$-flows successfully recover the same posteriors and evidences as\ntraditional nested sampling, even in cases where standard normalizing flows\nfail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an ever-growing need in the gravitational wave community for fast\nand reliable inference methods, accompanied by an informative error bar. Nested\nsampling satisfies the last two requirements, but its computational cost can\nbecome prohibitive when using the most accurate waveform models. In this paper,\nwe demonstrate the acceleration of nested sampling using a technique called\nposterior repartitioning. This method leverages nested sampling's unique\nability to separate prior and likelihood contributions at the algorithmic\nlevel. Specifically, we define a `repartitioned prior' informed by the\nposterior from a low-resolution run. To construct this repartitioned prior, we\nuse a $\\beta$-flow, a novel type of conditional normalizing flow designed to\nbetter learn deep tail probabilities. $\\beta$-flows are trained on the entire\nnested sampling run and conditioned on an inverse temperature $\\beta$. Applying\nour methods to simulated and real binary black hole mergers, we demonstrate how\nthey can reduce the number of likelihood evaluations required for convergence\nby up to an order of magnitude, enabling faster model comparison and parameter\nestimation. Furthermore, we highlight the robustness of using $\\beta$-flows\nover standard normalizing flows to accelerate nested sampling. Notably,\n$\\beta$-flows successfully recover the same posteriors and evidences as\ntraditional nested sampling, even in cases where standard normalizing flows\nfail."
                },
                "authors": [
                    {
                        "name": "Metha Prathaban"
                    },
                    {
                        "name": "Harry Bevins"
                    },
                    {
                        "name": "Will Handley"
                    }
                ],
                "author_detail": {
                    "name": "Will Handley"
                },
                "author": "Will Handley",
                "arxiv_comment": "12 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17662v1",
                "updated": "2024-11-26T18:26:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    26,
                    17,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:26:17Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    26,
                    17,
                    1,
                    331,
                    0
                ],
                "title": "RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through\n  Embedding Predictive Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through\n  Embedding Predictive Pre-Training"
                },
                "summary": "Vision-based pose estimation of articulated robots with unknown joint angles\nhas applications in collaborative robotics and human-robot interaction tasks.\nCurrent frameworks use neural network encoders to extract image features and\ndownstream layers to predict joint angles and robot pose. While images of\nrobots inherently contain rich information about the robot's physical\nstructures, existing methods often fail to leverage it fully; therefore,\nlimiting performance under occlusions and truncations. To address this, we\nintroduce RoboPEPP, a method that fuses information about the robot's physical\nmodel into the encoder using a masking-based self-supervised\nembedding-predictive architecture. Specifically, we mask the robot's joints and\npre-train an encoder-predictor model to infer the joints' embeddings from\nsurrounding unmasked regions, enhancing the encoder's understanding of the\nrobot's physical model. The pre-trained encoder-predictor pair, along with\njoint angle and keypoint prediction networks, is then fine-tuned for pose and\njoint angle estimation. Random masking of input during fine-tuning and keypoint\nfiltering during evaluation further improves robustness. Our method, evaluated\non several datasets, achieves the best results in robot pose and joint angle\nestimation while being the least sensitive to occlusions and requiring the\nlowest execution time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-based pose estimation of articulated robots with unknown joint angles\nhas applications in collaborative robotics and human-robot interaction tasks.\nCurrent frameworks use neural network encoders to extract image features and\ndownstream layers to predict joint angles and robot pose. While images of\nrobots inherently contain rich information about the robot's physical\nstructures, existing methods often fail to leverage it fully; therefore,\nlimiting performance under occlusions and truncations. To address this, we\nintroduce RoboPEPP, a method that fuses information about the robot's physical\nmodel into the encoder using a masking-based self-supervised\nembedding-predictive architecture. Specifically, we mask the robot's joints and\npre-train an encoder-predictor model to infer the joints' embeddings from\nsurrounding unmasked regions, enhancing the encoder's understanding of the\nrobot's physical model. The pre-trained encoder-predictor pair, along with\njoint angle and keypoint prediction networks, is then fine-tuned for pose and\njoint angle estimation. Random masking of input during fine-tuning and keypoint\nfiltering during evaluation further improves robustness. Our method, evaluated\non several datasets, achieves the best results in robot pose and joint angle\nestimation while being the least sensitive to occlusions and requiring the\nlowest execution time."
                },
                "authors": [
                    {
                        "name": "Raktim Gautam Goswami"
                    },
                    {
                        "name": "Prashanth Krishnamurthy"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Farshad Khorrami"
                    }
                ],
                "author_detail": {
                    "name": "Farshad Khorrami"
                },
                "author": "Farshad Khorrami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17660v1",
                "updated": "2024-11-26T18:25:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    25,
                    51,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:25:51Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    25,
                    51,
                    1,
                    331,
                    0
                ],
                "title": "DROID-Splat: Combining end-to-end SLAM with 3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DROID-Splat: Combining end-to-end SLAM with 3D Gaussian Splatting"
                },
                "summary": "Recent progress in scene synthesis makes standalone SLAM systems purely based\non optimizing hyperprimitives with a Rendering objective possible\n\\cite{monogs}.\n  However, the tracking performance still lacks behind traditional\n\\cite{orbslam} and end-to-end SLAM systems \\cite{droid}.\n  An optimal trade-off between robustness, speed and accuracy has not yet been\nreached, especially for monocular video.\n  In this paper, we introduce a SLAM system based on an end-to-end Tracker and\nextend it with a Renderer based on recent 3D Gaussian Splatting techniques.\n  Our framework \\textbf{DroidSplat} achieves both SotA tracking and rendering\nresults on common SLAM benchmarks.\n  We implemented multiple building blocks of modern SLAM systems to run in\nparallel, allowing for fast inference on common consumer GPU's.\n  Recent progress in monocular depth prediction and camera calibration allows\nour system to achieve strong results even on in-the-wild data without known\ncamera intrinsics.\n  Code will be available at \\url{https://github.com/ChenHoy/DROID-Splat}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in scene synthesis makes standalone SLAM systems purely based\non optimizing hyperprimitives with a Rendering objective possible\n\\cite{monogs}.\n  However, the tracking performance still lacks behind traditional\n\\cite{orbslam} and end-to-end SLAM systems \\cite{droid}.\n  An optimal trade-off between robustness, speed and accuracy has not yet been\nreached, especially for monocular video.\n  In this paper, we introduce a SLAM system based on an end-to-end Tracker and\nextend it with a Renderer based on recent 3D Gaussian Splatting techniques.\n  Our framework \\textbf{DroidSplat} achieves both SotA tracking and rendering\nresults on common SLAM benchmarks.\n  We implemented multiple building blocks of modern SLAM systems to run in\nparallel, allowing for fast inference on common consumer GPU's.\n  Recent progress in monocular depth prediction and camera calibration allows\nour system to achieve strong results even on in-the-wild data without known\ncamera intrinsics.\n  Code will be available at \\url{https://github.com/ChenHoy/DROID-Splat}."
                },
                "authors": [
                    {
                        "name": "Christian Homeyer"
                    },
                    {
                        "name": "Leon Begiristain"
                    },
                    {
                        "name": "Christoph Schnörr"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Schnörr"
                },
                "author": "Christoph Schnörr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17651v1",
                "updated": "2024-11-26T18:16:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    16,
                    56,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:16:56Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    16,
                    56,
                    1,
                    331,
                    0
                ],
                "title": "Toward High-Performance LLM Serving: A Simulation-Based Approach for\n  Identifying Optimal Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward High-Performance LLM Serving: A Simulation-Based Approach for\n  Identifying Optimal Parallelism"
                },
                "summary": "Serving Large Language Models (LLMs) efficiently has become crucial. LLMs are\noften served with multiple devices using techniques like data, pipeline, and\ntensor parallelisms. Each parallelism presents trade-offs between computation,\nmemory, and communication overhead, making it challenging to determine the\noptimal parallel execution plan. Moreover, input workloads also impact\nparallelism strategies. Tasks with long prompts like article summarization are\ncompute-intensive, while tasks with long generation lengths like code\ngeneration are often memory-intensive; these differing characteristics result\nin distinct optimal execution plans. Since searching for the optimal plan via\nactual deployment is prohibitively expensive, we propose APEX, an LLM serving\nsystem simulator that efficiently identifies an optimal parallel execution\nplan. APEX captures the complex characteristics of iteration-level batching, a\ntechnique widely used in SOTA LLM serving systems. APEX leverages the\nrepetitive structure of LLMs to reduce design space, maintaining a similar\nsimulation overhead, even when scaling to trillion scale models. APEX supports\na wide range of LLMs, device clusters, etc., and it can be easily extended\nthrough its high-level templates. We run APEX simulations using a CPU and\nevaluate the identified optimal plans using 8 H100 GPUs, encompassing a wide\nrange of LLMs and input workloads. We show that APEX can find optimal execution\nplans that are up to 4.42x faster than heuristic plans in terms of end-to-end\nserving latency. APEX also reports a set of metrics used in LLM serving\nsystems, such as time per output token and time to first token. Furthermore,\nAPEX can identify an optimal parallel execution plan within 15 minutes using a\nCPU. This is 71x faster and 1234x more cost-effective than actual deployment on\na GPU cluster using cloud services. APEX will be open-sourced upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) efficiently has become crucial. LLMs are\noften served with multiple devices using techniques like data, pipeline, and\ntensor parallelisms. Each parallelism presents trade-offs between computation,\nmemory, and communication overhead, making it challenging to determine the\noptimal parallel execution plan. Moreover, input workloads also impact\nparallelism strategies. Tasks with long prompts like article summarization are\ncompute-intensive, while tasks with long generation lengths like code\ngeneration are often memory-intensive; these differing characteristics result\nin distinct optimal execution plans. Since searching for the optimal plan via\nactual deployment is prohibitively expensive, we propose APEX, an LLM serving\nsystem simulator that efficiently identifies an optimal parallel execution\nplan. APEX captures the complex characteristics of iteration-level batching, a\ntechnique widely used in SOTA LLM serving systems. APEX leverages the\nrepetitive structure of LLMs to reduce design space, maintaining a similar\nsimulation overhead, even when scaling to trillion scale models. APEX supports\na wide range of LLMs, device clusters, etc., and it can be easily extended\nthrough its high-level templates. We run APEX simulations using a CPU and\nevaluate the identified optimal plans using 8 H100 GPUs, encompassing a wide\nrange of LLMs and input workloads. We show that APEX can find optimal execution\nplans that are up to 4.42x faster than heuristic plans in terms of end-to-end\nserving latency. APEX also reports a set of metrics used in LLM serving\nsystems, such as time per output token and time to first token. Furthermore,\nAPEX can identify an optimal parallel execution plan within 15 minutes using a\nCPU. This is 71x faster and 1234x more cost-effective than actual deployment on\na GPU cluster using cloud services. APEX will be open-sourced upon acceptance."
                },
                "authors": [
                    {
                        "name": "Yi-Chien Lin"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Ronald Pineda"
                    },
                    {
                        "name": "Fanny Nina Paravecino"
                    }
                ],
                "author_detail": {
                    "name": "Fanny Nina Paravecino"
                },
                "author": "Fanny Nina Paravecino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12240v2",
                "updated": "2024-11-26T18:14:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    14,
                    50,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-19T05:37:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    37,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "Evaluating Tokenizer Performance of Large Language Models Across\n  Official Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Tokenizer Performance of Large Language Models Across\n  Official Indian Languages"
                },
                "summary": "Large Language Models (LLMs) based on transformer architectures have\nrevolutionized a variety of domains, with tokenization playing a pivotal role\nin their pre-processing and fine-tuning stages. In multilingual models,\nparticularly those tailored for Indic languages, effective tokenization is\ncrucial for optimizing performance. This paper presents a comprehensive\nevaluation of tokenizers used by 12 LLMs across all 22 official languages of\nIndia, with a focus on comparing the efficiency of their tokenization\nprocesses. We employed the Normalized Sequence Length (NSL) as a key metric in\nour analysis. Our findings reveal that the SUTRA tokenizer outperforms all\nother models, including several Indic-specific models, excelling in 14\nlanguages. Notable insights include the SUTRA tokenizer's superior handling of\nIndic languages, GPT-4o's advancement over its predecessor GPT-4 in processing\nIndian languages, and the limited performance of Project Indus in certain\nlanguages. This study underscores the critical importance of developing\ntargeted tokenization strategies for multilingual and Indic-centric models,\nlaying the groundwork for future improvements in tokenizer design to enhance\nlinguistic coverage and model efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on transformer architectures have\nrevolutionized a variety of domains, with tokenization playing a pivotal role\nin their pre-processing and fine-tuning stages. In multilingual models,\nparticularly those tailored for Indic languages, effective tokenization is\ncrucial for optimizing performance. This paper presents a comprehensive\nevaluation of tokenizers used by 12 LLMs across all 22 official languages of\nIndia, with a focus on comparing the efficiency of their tokenization\nprocesses. We employed the Normalized Sequence Length (NSL) as a key metric in\nour analysis. Our findings reveal that the SUTRA tokenizer outperforms all\nother models, including several Indic-specific models, excelling in 14\nlanguages. Notable insights include the SUTRA tokenizer's superior handling of\nIndic languages, GPT-4o's advancement over its predecessor GPT-4 in processing\nIndian languages, and the limited performance of Project Indus in certain\nlanguages. This study underscores the critical importance of developing\ntargeted tokenization strategies for multilingual and Indic-centric models,\nlaying the groundwork for future improvements in tokenizer design to enhance\nlinguistic coverage and model efficiency."
                },
                "authors": [
                    {
                        "name": "S. Tamang"
                    },
                    {
                        "name": "D. J. Bora"
                    }
                ],
                "author_detail": {
                    "name": "D. J. Bora"
                },
                "author": "D. J. Bora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07837v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07837v3",
                "updated": "2024-11-26T18:13:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    13,
                    57,
                    1,
                    331,
                    0
                ],
                "published": "2024-07-10T16:58:31Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    16,
                    58,
                    31,
                    2,
                    192,
                    0
                ],
                "title": "Probe and Prejudice: Classification of compact objects and model\n  comparison using EOS knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probe and Prejudice: Classification of compact objects and model\n  comparison using EOS knowledge"
                },
                "summary": "Nuclear theory and experiments, alongside astrophysical observations,\nconstrain the equation of state (EOS) of supranuclear-dense matter. Conversely,\nknowledge of the EOS allows an improved interpretation of nuclear or\nastrophysical data. In this article, we use several established constraints on\nthe EOS and the new NICER measurement of PSR J0437-4715 to comment on the\nnature of the primary companion in GW230529 and the companion of PSR\nJ0514-4002E. We find that, with a probability of $\\gtrsim 84\\%$ and $\\gtrsim\n68\\%$, respectively, both objects are black holes. These likelihoods increase\nto above $95\\%$ when one uses GW170817's remnant as an upper limit on the TOV\nmass. We also demonstrate that the current knowledge of the EOS substantially\ndisfavors high masses and radii for PSR J0030+0451, inferred recently when\ncombining NICER with XMM-Newton background data and using particular hot-spot\nmodels. Finally, we also use our obtained EOS knowledge to comment on\nmeasurements of the nuclear symmetry energy, finding that the large value\npredicted by the PREX-II measurement displays some mild tension with other\nconstraints on the EOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nuclear theory and experiments, alongside astrophysical observations,\nconstrain the equation of state (EOS) of supranuclear-dense matter. Conversely,\nknowledge of the EOS allows an improved interpretation of nuclear or\nastrophysical data. In this article, we use several established constraints on\nthe EOS and the new NICER measurement of PSR J0437-4715 to comment on the\nnature of the primary companion in GW230529 and the companion of PSR\nJ0514-4002E. We find that, with a probability of $\\gtrsim 84\\%$ and $\\gtrsim\n68\\%$, respectively, both objects are black holes. These likelihoods increase\nto above $95\\%$ when one uses GW170817's remnant as an upper limit on the TOV\nmass. We also demonstrate that the current knowledge of the EOS substantially\ndisfavors high masses and radii for PSR J0030+0451, inferred recently when\ncombining NICER with XMM-Newton background data and using particular hot-spot\nmodels. Finally, we also use our obtained EOS knowledge to comment on\nmeasurements of the nuclear symmetry energy, finding that the large value\npredicted by the PREX-II measurement displays some mild tension with other\nconstraints on the EOS."
                },
                "authors": [
                    {
                        "name": "Hauke Koehn"
                    },
                    {
                        "name": "Thibeau Wouters"
                    },
                    {
                        "name": "Henrik Rose"
                    },
                    {
                        "name": "Peter T. H. Pang"
                    },
                    {
                        "name": "Rahul Somasundaram"
                    },
                    {
                        "name": "Ingo Tews"
                    },
                    {
                        "name": "Tim Dietrich"
                    }
                ],
                "author_detail": {
                    "name": "Tim Dietrich"
                },
                "author": "Tim Dietrich",
                "arxiv_doi": "10.1103/PhysRevD.110.103015",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.103015",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.07837v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07837v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 5 figures, published in PRD, v3 contains minor correction\n  for the Pb-208 dipole",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17639v1",
                "updated": "2024-11-26T18:00:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    0,
                    22,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:00:22Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    0,
                    22,
                    1,
                    331,
                    0
                ],
                "title": "Intrepid MCMC: Metropolis-Hastings with Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrepid MCMC: Metropolis-Hastings with Exploration"
                },
                "summary": "In engineering examples, one often encounters the need to sample from\nunnormalized distributions with complex shapes that may also be implicitly\ndefined through a physical or numerical simulation model, making it\ncomputationally expensive to evaluate the associated density function. For such\ncases, MCMC has proven to be an invaluable tool. Random-walk Metropolis Methods\n(also known as Metropolis-Hastings (MH)), in particular, are highly popular for\ntheir simplicity, flexibility, and ease of implementation. However, most MH\nalgorithms suffer from significant limitations when attempting to sample from\ndistributions with multiple modes (particularly disconnected ones). In this\npaper, we present Intrepid MCMC - a novel MH scheme that utilizes a simple\ncoordinate transformation to significantly improve the mode-finding ability and\nconvergence rate to the target distribution of random-walk Markov chains while\nretaining most of the simplicity of the vanilla MH paradigm. Through multiple\nexamples, we showcase the improvement in the performance of Intrepid MCMC over\nvanilla MH for a wide variety of target distribution shapes. We also provide an\nanalysis of the mixing behavior of the Intrepid Markov chain, as well as the\nefficiency of our algorithm for increasing dimensions. A thorough discussion is\npresented on the practical implementation of the Intrepid MCMC algorithm.\nFinally, its utility is highlighted through a Bayesian parameter inference\nproblem for a two-degree-of-freedom oscillator under free vibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In engineering examples, one often encounters the need to sample from\nunnormalized distributions with complex shapes that may also be implicitly\ndefined through a physical or numerical simulation model, making it\ncomputationally expensive to evaluate the associated density function. For such\ncases, MCMC has proven to be an invaluable tool. Random-walk Metropolis Methods\n(also known as Metropolis-Hastings (MH)), in particular, are highly popular for\ntheir simplicity, flexibility, and ease of implementation. However, most MH\nalgorithms suffer from significant limitations when attempting to sample from\ndistributions with multiple modes (particularly disconnected ones). In this\npaper, we present Intrepid MCMC - a novel MH scheme that utilizes a simple\ncoordinate transformation to significantly improve the mode-finding ability and\nconvergence rate to the target distribution of random-walk Markov chains while\nretaining most of the simplicity of the vanilla MH paradigm. Through multiple\nexamples, we showcase the improvement in the performance of Intrepid MCMC over\nvanilla MH for a wide variety of target distribution shapes. We also provide an\nanalysis of the mixing behavior of the Intrepid Markov chain, as well as the\nefficiency of our algorithm for increasing dimensions. A thorough discussion is\npresented on the practical implementation of the Intrepid MCMC algorithm.\nFinally, its utility is highlighted through a Bayesian parameter inference\nproblem for a two-degree-of-freedom oscillator under free vibration."
                },
                "authors": [
                    {
                        "name": "Promit Chakroborty"
                    },
                    {
                        "name": "Michael D. Shields"
                    }
                ],
                "author_detail": {
                    "name": "Michael D. Shields"
                },
                "author": "Michael D. Shields",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17637v1",
                "updated": "2024-11-26T17:55:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    55,
                    37,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:55:37Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    55,
                    37,
                    1,
                    331,
                    0
                ],
                "title": "On Limitations of LLM as Annotator for Low Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Limitations of LLM as Annotator for Low Resource Languages"
                },
                "summary": "Low-resource languages face significant challenges due to the lack of\nsufficient linguistic data, resources, and tools for tasks such as supervised\nlearning, annotation, and classification. This shortage hinders the development\nof accurate models and datasets, making it difficult to perform critical NLP\ntasks like sentiment analysis or hate speech detection. To bridge this gap,\nLarge Language Models (LLMs) present an opportunity for potential annotators,\ncapable of generating datasets and resources for these underrepresented\nlanguages. In this paper, we focus on Marathi, a low-resource language, and\nevaluate the performance of both closed-source and open-source LLMs as\nannotators. We assess models such as GPT-4o and Gemini 1.0 Pro, Gemma 2 (2B and\n9B), and Llama 3.1 (8B) on classification tasks including sentiment analysis,\nnews classification, and hate speech detection. Our findings reveal that while\nLLMs excel in annotation tasks for high-resource languages like English, they\nstill fall short when applied to Marathi. Even advanced closed models like\nGemini and GPT underperform in comparison to BERT-based baselines, highlighting\nthe limitations of LLMs as annotators for low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-resource languages face significant challenges due to the lack of\nsufficient linguistic data, resources, and tools for tasks such as supervised\nlearning, annotation, and classification. This shortage hinders the development\nof accurate models and datasets, making it difficult to perform critical NLP\ntasks like sentiment analysis or hate speech detection. To bridge this gap,\nLarge Language Models (LLMs) present an opportunity for potential annotators,\ncapable of generating datasets and resources for these underrepresented\nlanguages. In this paper, we focus on Marathi, a low-resource language, and\nevaluate the performance of both closed-source and open-source LLMs as\nannotators. We assess models such as GPT-4o and Gemini 1.0 Pro, Gemma 2 (2B and\n9B), and Llama 3.1 (8B) on classification tasks including sentiment analysis,\nnews classification, and hate speech detection. Our findings reveal that while\nLLMs excel in annotation tasks for high-resource languages like English, they\nstill fall short when applied to Marathi. Even advanced closed models like\nGemini and GPT underperform in comparison to BERT-based baselines, highlighting\nthe limitations of LLMs as annotators for low-resource languages."
                },
                "authors": [
                    {
                        "name": "Suramya Jadhav"
                    },
                    {
                        "name": "Abhay Shanbhag"
                    },
                    {
                        "name": "Amogh Thakurdesai"
                    },
                    {
                        "name": "Ridhima Sinare"
                    },
                    {
                        "name": "Raviraj Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Raviraj Joshi"
                },
                "author": "Raviraj Joshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17636v1",
                "updated": "2024-11-26T17:53:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    53,
                    44,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:53:44Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    53,
                    44,
                    1,
                    331,
                    0
                ],
                "title": "MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics\n  Manipulation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable planning abilities\nacross various domains, including robotics manipulation and navigation. While\nrecent efforts in robotics have leveraged LLMs both for high-level and\nlow-level planning, these approaches often face significant challenges, such as\nhallucinations in long-horizon tasks and limited adaptability due to the\ngeneration of plans in a single pass without real-time feedback. To address\nthese limitations, we propose a novel multi-agent LLM framework, Multi-Agent\nLarge Language Model for Manipulation (MALMM) that distributes high-level\nplanning and low-level control code generation across specialized LLM agents,\nsupervised by an additional agent that dynamically manages transitions. By\nincorporating observations from the environment after each step, our framework\neffectively handles intermediate failures and enables adaptive re-planning.\nUnlike existing methods, our approach does not rely on pre-trained skill\npolicies or in-context learning examples and generalizes to a variety of new\ntasks. We evaluate our approach on nine RLBench tasks, including long-horizon\ntasks, and demonstrate its ability to solve robotics manipulation in a\nzero-shot setting, thereby overcoming key limitations of existing LLM-based\nmanipulation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable planning abilities\nacross various domains, including robotics manipulation and navigation. While\nrecent efforts in robotics have leveraged LLMs both for high-level and\nlow-level planning, these approaches often face significant challenges, such as\nhallucinations in long-horizon tasks and limited adaptability due to the\ngeneration of plans in a single pass without real-time feedback. To address\nthese limitations, we propose a novel multi-agent LLM framework, Multi-Agent\nLarge Language Model for Manipulation (MALMM) that distributes high-level\nplanning and low-level control code generation across specialized LLM agents,\nsupervised by an additional agent that dynamically manages transitions. By\nincorporating observations from the environment after each step, our framework\neffectively handles intermediate failures and enables adaptive re-planning.\nUnlike existing methods, our approach does not rely on pre-trained skill\npolicies or in-context learning examples and generalizes to a variety of new\ntasks. We evaluate our approach on nine RLBench tasks, including long-horizon\ntasks, and demonstrate its ability to solve robotics manipulation in a\nzero-shot setting, thereby overcoming key limitations of existing LLM-based\nmanipulation methods."
                },
                "authors": [
                    {
                        "name": "Harsh Singh"
                    },
                    {
                        "name": "Rocktim Jyoti Das"
                    },
                    {
                        "name": "Mingfei Han"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Ivan Laptev"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Laptev"
                },
                "author": "Ivan Laptev",
                "arxiv_comment": "48 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14403v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14403v4",
                "updated": "2024-11-26T17:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    47,
                    5,
                    1,
                    331,
                    0
                ],
                "published": "2024-10-18T12:06:39Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    6,
                    39,
                    4,
                    292,
                    0
                ],
                "title": "A novel understanding of the role of plasma-molecular kinetics on\n  divertor power exhaust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel understanding of the role of plasma-molecular kinetics on\n  divertor power exhaust"
                },
                "summary": "During detachment, a buffer of neutral atoms and molecules builds up between\nthe target and the ionising plasma. Collisions between the plasma and the\nmolecules play an important role in the detachment process. Studies of\nplasma-molecular kinetics indicate that the gas temperature is increased during\ndetachment for a wide range of conditions on the MAST-U and TCV tokamaks. This\nis related to an increased $\\mathrm{D}_2$ lifetime during detachment, leading\nto more plasma-molecule collisions that raise the molecular temperature. Such\ncollisions subsequently result in significant power and momentum losses to the\ndivertor plasma during detachment. Using a simplified inference, these losses\nare estimated using the rotational temperature, neutral pressure and ionisation\nfront position. Significant power losses (about $10\\%$ of $P_{SOL}$) and\ndominant momentum losses (majority of the upstream pressure) from\nplasma-molecule collisions are inferred experimentally in long-legged, strongly\nbaffled, detached divertors (MAST-U Super-X divertor), consistent with\nSOLPS-ITER simulations. The vibrational distribution obtained is compared with\nan Eirene-like collisional-radiative model setup, indicating some qualitative\nagreements and disagreements, potentially highlighting model gaps.\n  These interpretations highlight the importance of plasma-molecular\ncollisions, leading to power and momentum losses during detachment. Our\nanalysis and reduced modelling of these processes provide further insights into\ndetachment control observations, the workings of long-legged divertors and\ndivertor power balance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "During detachment, a buffer of neutral atoms and molecules builds up between\nthe target and the ionising plasma. Collisions between the plasma and the\nmolecules play an important role in the detachment process. Studies of\nplasma-molecular kinetics indicate that the gas temperature is increased during\ndetachment for a wide range of conditions on the MAST-U and TCV tokamaks. This\nis related to an increased $\\mathrm{D}_2$ lifetime during detachment, leading\nto more plasma-molecule collisions that raise the molecular temperature. Such\ncollisions subsequently result in significant power and momentum losses to the\ndivertor plasma during detachment. Using a simplified inference, these losses\nare estimated using the rotational temperature, neutral pressure and ionisation\nfront position. Significant power losses (about $10\\%$ of $P_{SOL}$) and\ndominant momentum losses (majority of the upstream pressure) from\nplasma-molecule collisions are inferred experimentally in long-legged, strongly\nbaffled, detached divertors (MAST-U Super-X divertor), consistent with\nSOLPS-ITER simulations. The vibrational distribution obtained is compared with\nan Eirene-like collisional-radiative model setup, indicating some qualitative\nagreements and disagreements, potentially highlighting model gaps.\n  These interpretations highlight the importance of plasma-molecular\ncollisions, leading to power and momentum losses during detachment. Our\nanalysis and reduced modelling of these processes provide further insights into\ndetachment control observations, the workings of long-legged divertors and\ndivertor power balance."
                },
                "authors": [
                    {
                        "name": "N. Osborne"
                    },
                    {
                        "name": "K. Verhaegh"
                    },
                    {
                        "name": "D. Moulton"
                    },
                    {
                        "name": "H. Reimerdes"
                    },
                    {
                        "name": "P. Ryan"
                    },
                    {
                        "name": "N. Lonigro"
                    },
                    {
                        "name": "S. Mijin"
                    },
                    {
                        "name": "R. Osawa"
                    },
                    {
                        "name": "K. Murray"
                    },
                    {
                        "name": "S. Kobussen"
                    },
                    {
                        "name": "Y. Damizia"
                    },
                    {
                        "name": "A. Perek"
                    },
                    {
                        "name": "C. Theiler"
                    },
                    {
                        "name": "R. Ducker"
                    },
                    {
                        "name": "D. Mykytchuk"
                    }
                ],
                "author_detail": {
                    "name": "D. Mykytchuk"
                },
                "author": "D. Mykytchuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14403v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14403v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17625v1",
                "updated": "2024-11-26T17:37:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    37,
                    12,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:37:12Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    37,
                    12,
                    1,
                    331,
                    0
                ],
                "title": "Data-driven development of cycle prediction models for lithium metal\n  batteries using multi modal mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven development of cycle prediction models for lithium metal\n  batteries using multi modal mining"
                },
                "summary": "Recent advances in data-driven research have shown great potential in\nunderstanding the intricate relationships between materials and their\nperformances. Herein, we introduce a novel multi modal data-driven approach\nemploying an Automatic Battery data Collector (ABC) that integrates a large\nlanguage model (LLM) with an automatic graph mining tool, Material Graph\nDigitizer (MatGD). This platform enables state-of-the-art accurate extraction\nof battery material data and cyclability performance metrics from diverse\ntextual and graphical data sources. From the database derived through the ABC\nplatform, we developed machine learning models that can accurately predict the\ncapacity and stability of lithium metal batteries, which is the first-ever\nmodel developed to achieve such predictions. Our models were also\nexperimentally validated, confirming practical applicability and reliability of\nour data-driven approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in data-driven research have shown great potential in\nunderstanding the intricate relationships between materials and their\nperformances. Herein, we introduce a novel multi modal data-driven approach\nemploying an Automatic Battery data Collector (ABC) that integrates a large\nlanguage model (LLM) with an automatic graph mining tool, Material Graph\nDigitizer (MatGD). This platform enables state-of-the-art accurate extraction\nof battery material data and cyclability performance metrics from diverse\ntextual and graphical data sources. From the database derived through the ABC\nplatform, we developed machine learning models that can accurately predict the\ncapacity and stability of lithium metal batteries, which is the first-ever\nmodel developed to achieve such predictions. Our models were also\nexperimentally validated, confirming practical applicability and reliability of\nour data-driven approach."
                },
                "authors": [
                    {
                        "name": "Jaewoong Lee"
                    },
                    {
                        "name": "Junhee Woo"
                    },
                    {
                        "name": "Sejin Kim"
                    },
                    {
                        "name": "Cinthya Paulina"
                    },
                    {
                        "name": "Hyunmin Park"
                    },
                    {
                        "name": "Hee-Tak Kim"
                    },
                    {
                        "name": "Steve Park"
                    },
                    {
                        "name": "Jihan Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jihan Kim"
                },
                "author": "Jihan Kim",
                "arxiv_comment": "30 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17618v1",
                "updated": "2024-11-26T17:32:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    32,
                    22,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:32:22Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    32,
                    22,
                    1,
                    331,
                    0
                ],
                "title": "Valid Bayesian Inference based on Variance Weighted Projection for\n  High-Dimensional Logistic Regression with Binary Covariates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Valid Bayesian Inference based on Variance Weighted Projection for\n  High-Dimensional Logistic Regression with Binary Covariates"
                },
                "summary": "We address the challenge of conducting inference for a categorical treatment\neffect related to a binary outcome variable while taking into account\nhigh-dimensional baseline covariates. The conventional technique used to\nestablish orthogonality for the treatment effect from nuisance variables in\ncontinuous cases is inapplicable in the context of binary treatment. To\novercome this obstacle, an orthogonal score tailored specifically to this\nscenario is formulated which is based on a variance-weighted projection.\nAdditionally, a novel Bayesian framework is proposed to facilitate valid\ninference for the desired low-dimensional parameter within the complex\nframework of high-dimensional logistic regression. We provide uniform\nconvergence results, affirming the validity of credible intervals derived from\nthe posterior distribution. The effectiveness of the proposed method is\ndemonstrated through comprehensive simulation studies and real data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of conducting inference for a categorical treatment\neffect related to a binary outcome variable while taking into account\nhigh-dimensional baseline covariates. The conventional technique used to\nestablish orthogonality for the treatment effect from nuisance variables in\ncontinuous cases is inapplicable in the context of binary treatment. To\novercome this obstacle, an orthogonal score tailored specifically to this\nscenario is formulated which is based on a variance-weighted projection.\nAdditionally, a novel Bayesian framework is proposed to facilitate valid\ninference for the desired low-dimensional parameter within the complex\nframework of high-dimensional logistic regression. We provide uniform\nconvergence results, affirming the validity of credible intervals derived from\nthe posterior distribution. The effectiveness of the proposed method is\ndemonstrated through comprehensive simulation studies and real data analysis."
                },
                "authors": [
                    {
                        "name": "Abhishek Ojha"
                    },
                    {
                        "name": "Naveen N. Narisetty"
                    }
                ],
                "author_detail": {
                    "name": "Naveen N. Narisetty"
                },
                "author": "Naveen N. Narisetty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v1",
                "updated": "2024-11-26T17:28:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Cheng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Yu"
                },
                "author": "Cheng Yu",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17607v1",
                "updated": "2024-11-26T17:19:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    19,
                    9,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:19:09Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    19,
                    9,
                    1,
                    331,
                    0
                ],
                "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data"
                },
                "summary": "Speech language models (SpeechLMs) accept speech input and produce speech\noutput, allowing for more natural human-computer interaction compared to\ntext-based large language models (LLMs). Traditional approaches for developing\nSpeechLMs are constrained by the limited availability of unsupervised speech\ndata and parallel speech-text data, which are significantly less abundant than\ntext pre-training data, thereby limiting their scalability as LLMs. We propose\na novel approach to scaling speech-text pre-training by leveraging large-scale\nsynthetic interleaved data derived from text corpora, eliminating the need for\nparallel speech-text datasets. Our method efficiently constructs speech-text\ninterleaved data by sampling text spans from existing text corpora and\nsynthesizing corresponding speech spans using a text-to-token model, bypassing\nthe need to generate actual speech. We also employ a supervised speech\ntokenizer derived from an automatic speech recognition (ASR) model by\nincorporating a vector-quantized bottleneck into the encoder. This supervised\ntraining approach results in discrete speech tokens with strong semantic\npreservation even at lower sampling rates (e.g. 12.5Hz), while still\nmaintaining speech reconstruction quality. Starting from a pre-trained language\nmodel and scaling our pre-training to 1 trillion tokens (with 600B synthetic\ninterleaved speech-text data), we achieve state-of-the-art performance in\nspeech language modeling and spoken question answering, improving performance\non spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We\nfurther demonstrate that by fine-tuning the pre-trained model with speech\ndialogue data, we can develop an end-to-end spoken chatbot that achieves\ncompetitive performance comparable to existing baselines in both conversational\nabilities and speech quality, even operating exclusively in the speech domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech language models (SpeechLMs) accept speech input and produce speech\noutput, allowing for more natural human-computer interaction compared to\ntext-based large language models (LLMs). Traditional approaches for developing\nSpeechLMs are constrained by the limited availability of unsupervised speech\ndata and parallel speech-text data, which are significantly less abundant than\ntext pre-training data, thereby limiting their scalability as LLMs. We propose\na novel approach to scaling speech-text pre-training by leveraging large-scale\nsynthetic interleaved data derived from text corpora, eliminating the need for\nparallel speech-text datasets. Our method efficiently constructs speech-text\ninterleaved data by sampling text spans from existing text corpora and\nsynthesizing corresponding speech spans using a text-to-token model, bypassing\nthe need to generate actual speech. We also employ a supervised speech\ntokenizer derived from an automatic speech recognition (ASR) model by\nincorporating a vector-quantized bottleneck into the encoder. This supervised\ntraining approach results in discrete speech tokens with strong semantic\npreservation even at lower sampling rates (e.g. 12.5Hz), while still\nmaintaining speech reconstruction quality. Starting from a pre-trained language\nmodel and scaling our pre-training to 1 trillion tokens (with 600B synthetic\ninterleaved speech-text data), we achieve state-of-the-art performance in\nspeech language modeling and spoken question answering, improving performance\non spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We\nfurther demonstrate that by fine-tuning the pre-trained model with speech\ndialogue data, we can develop an end-to-end spoken chatbot that achieves\ncompetitive performance comparable to existing baselines in both conversational\nabilities and speech quality, even operating exclusively in the speech domain."
                },
                "authors": [
                    {
                        "name": "Aohan Zeng"
                    },
                    {
                        "name": "Zhengxiao Du"
                    },
                    {
                        "name": "Mingdao Liu"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Shengmin Jiang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17605v1",
                "updated": "2024-11-26T17:17:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    17,
                    41,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:17:41Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    17,
                    41,
                    1,
                    331,
                    0
                ],
                "title": "Distractor-free Generalizable 3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distractor-free Generalizable 3D Gaussian Splatting"
                },
                "summary": "We present DGGS, a novel framework addressing the previously unexplored\nchallenge of Distractor-free Generalizable 3D Gaussian Splatting (3DGS). It\naccomplishes two key objectives: fortifying generalizable 3DGS against\ndistractor-laden data during both training and inference phases, while\nsuccessfully extending cross-scene adaptation capabilities to conventional\ndistractor-free approaches. To achieve these objectives, DGGS introduces a\nscene-agnostic reference-based mask prediction and refinement methodology\nduring training phase, coupled with a training view selection strategy,\neffectively improving distractor prediction accuracy and training stability.\nMoreover, to address distractor-induced voids and artifacts during inference\nstage, we propose a two-stage inference framework for better reference\nselection based on the predicted distractor masks, complemented by a distractor\npruning module to eliminate residual distractor effects. Extensive\ngeneralization experiments demonstrate DGGS's advantages under distractor-laden\nconditions. Additionally, experimental results show that our scene-agnostic\nmask inference achieves accuracy comparable to scene-specific trained methods.\nHomepage is \\url{https://github.com/bbbbby-99/DGGS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DGGS, a novel framework addressing the previously unexplored\nchallenge of Distractor-free Generalizable 3D Gaussian Splatting (3DGS). It\naccomplishes two key objectives: fortifying generalizable 3DGS against\ndistractor-laden data during both training and inference phases, while\nsuccessfully extending cross-scene adaptation capabilities to conventional\ndistractor-free approaches. To achieve these objectives, DGGS introduces a\nscene-agnostic reference-based mask prediction and refinement methodology\nduring training phase, coupled with a training view selection strategy,\neffectively improving distractor prediction accuracy and training stability.\nMoreover, to address distractor-induced voids and artifacts during inference\nstage, we propose a two-stage inference framework for better reference\nselection based on the predicted distractor masks, complemented by a distractor\npruning module to eliminate residual distractor effects. Extensive\ngeneralization experiments demonstrate DGGS's advantages under distractor-laden\nconditions. Additionally, experimental results show that our scene-agnostic\nmask inference achieves accuracy comparable to scene-specific trained methods.\nHomepage is \\url{https://github.com/bbbbby-99/DGGS}."
                },
                "authors": [
                    {
                        "name": "Yanqi Bao"
                    },
                    {
                        "name": "Jing Liao"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17600v1",
                "updated": "2024-11-26T17:06:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    6,
                    58,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:06:58Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    6,
                    58,
                    1,
                    331,
                    0
                ],
                "title": "Making History Readable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making History Readable"
                },
                "summary": "The Virginia Tech University Libraries (VTUL) Digital Library Platform (DLP)\nhosts digital collections that offer our users access to a wide variety of\ndocuments of historical and cultural importance. These collections are not only\nof academic importance but also provide our users with a glance at local\nhistorical events. Our DLP contains collections comprising digital objects\nfeaturing complex layouts, faded imagery, and hard-to-read handwritten text,\nwhich makes providing online access to these materials challenging. To address\nthese issues, we integrate AI into our DLP workflow and convert the text in the\ndigital objects into a machine-readable format. To enhance the user experience\nwith our historical collections, we use custom AI agents for handwriting\nrecognition, text extraction, and large language models (LLMs) for\nsummarization. This poster highlights three collections focusing on handwritten\nletters, newspapers, and digitized topographic maps. We discuss the challenges\nwith each collection and detail our approaches to address them. Our proposed\nmethods aim to enhance the user experience by making the contents in these\ncollections easier to search and navigate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Virginia Tech University Libraries (VTUL) Digital Library Platform (DLP)\nhosts digital collections that offer our users access to a wide variety of\ndocuments of historical and cultural importance. These collections are not only\nof academic importance but also provide our users with a glance at local\nhistorical events. Our DLP contains collections comprising digital objects\nfeaturing complex layouts, faded imagery, and hard-to-read handwritten text,\nwhich makes providing online access to these materials challenging. To address\nthese issues, we integrate AI into our DLP workflow and convert the text in the\ndigital objects into a machine-readable format. To enhance the user experience\nwith our historical collections, we use custom AI agents for handwriting\nrecognition, text extraction, and large language models (LLMs) for\nsummarization. This poster highlights three collections focusing on handwritten\nletters, newspapers, and digitized topographic maps. We discuss the challenges\nwith each collection and detail our approaches to address them. Our proposed\nmethods aim to enhance the user experience by making the contents in these\ncollections easier to search and navigate."
                },
                "authors": [
                    {
                        "name": "Bipasha Banerjee"
                    },
                    {
                        "name": "Jennifer Goyne"
                    },
                    {
                        "name": "William A. Ingram"
                    }
                ],
                "author_detail": {
                    "name": "William A. Ingram"
                },
                "author": "William A. Ingram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17598v1",
                "updated": "2024-11-26T17:06:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    6,
                    30,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:06:30Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    6,
                    30,
                    1,
                    331,
                    0
                ],
                "title": "Agentic AI for Improving Precision in Identifying Contributions to\n  Sustainable Development Goals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI for Improving Precision in Identifying Contributions to\n  Sustainable Development Goals"
                },
                "summary": "As research institutions increasingly commit to supporting the United\nNations' Sustainable Development Goals (SDGs), there is a pressing need to\naccurately assess their research output against these goals. Current\napproaches, primarily reliant on keyword-based Boolean search queries, conflate\nincidental keyword matches with genuine contributions, reducing retrieval\nprecision and complicating benchmarking efforts. This study investigates the\napplication of autoregressive Large Language Models (LLMs) as evaluation agents\nto identify relevant scholarly contributions to SDG targets in scholarly\npublications. Using a dataset of academic abstracts retrieved via SDG-specific\nkeyword queries, we demonstrate that small, locally-hosted LLMs can\ndifferentiate semantically relevant contributions to SDG targets from documents\nretrieved due to incidental keyword matches, addressing the limitations of\ntraditional methods. By leveraging the contextual understanding of LLMs, this\napproach provides a scalable framework for improving SDG-related research\nmetrics and informing institutional reporting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As research institutions increasingly commit to supporting the United\nNations' Sustainable Development Goals (SDGs), there is a pressing need to\naccurately assess their research output against these goals. Current\napproaches, primarily reliant on keyword-based Boolean search queries, conflate\nincidental keyword matches with genuine contributions, reducing retrieval\nprecision and complicating benchmarking efforts. This study investigates the\napplication of autoregressive Large Language Models (LLMs) as evaluation agents\nto identify relevant scholarly contributions to SDG targets in scholarly\npublications. Using a dataset of academic abstracts retrieved via SDG-specific\nkeyword queries, we demonstrate that small, locally-hosted LLMs can\ndifferentiate semantically relevant contributions to SDG targets from documents\nretrieved due to incidental keyword matches, addressing the limitations of\ntraditional methods. By leveraging the contextual understanding of LLMs, this\napproach provides a scalable framework for improving SDG-related research\nmetrics and informing institutional reporting."
                },
                "authors": [
                    {
                        "name": "William A. Ingram"
                    },
                    {
                        "name": "Bipasha Banerjee"
                    },
                    {
                        "name": "Edward A. Fox"
                    }
                ],
                "author_detail": {
                    "name": "Edward A. Fox"
                },
                "author": "Edward A. Fox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17597v1",
                "updated": "2024-11-26T17:06:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    6,
                    1,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:06:01Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    6,
                    1,
                    1,
                    331,
                    0
                ],
                "title": "Belief patterns with information processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Belief patterns with information processing"
                },
                "summary": "This paper presents a model of costly information acquisition where\ndecision-makers can choose whether to elaborate information superficially or\nprecisely. The former action is costless, while the latter entails a processing\ncost. Within this framework, decision-makers' beliefs may polarize even after\nthey have access to the same evidence. From the perspective of a Bayesian\nobserver who neglects information processing constraints, the decision-makers'\noptimal behavior and belief updating may appear consistent with biases such as\ndisconfirmation, underreaction to information, and confirmation bias. However,\nthese phenomena emerge naturally within the model and are fully compatible with\nstandard Bayesian inference and rational decision-making when accounting for\nthe costs of information acquisition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a model of costly information acquisition where\ndecision-makers can choose whether to elaborate information superficially or\nprecisely. The former action is costless, while the latter entails a processing\ncost. Within this framework, decision-makers' beliefs may polarize even after\nthey have access to the same evidence. From the perspective of a Bayesian\nobserver who neglects information processing constraints, the decision-makers'\noptimal behavior and belief updating may appear consistent with biases such as\ndisconfirmation, underreaction to information, and confirmation bias. However,\nthese phenomena emerge naturally within the model and are fully compatible with\nstandard Bayesian inference and rational decision-making when accounting for\nthe costs of information acquisition."
                },
                "authors": [
                    {
                        "name": "Federico Vaccari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Vaccari"
                },
                "author": "Federico Vaccari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17595v1",
                "updated": "2024-11-26T17:05:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    5,
                    27,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:05:27Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    5,
                    27,
                    1,
                    331,
                    0
                ],
                "title": "Can artificial intelligence predict clinical trial outcomes?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can artificial intelligence predict clinical trial outcomes?"
                },
                "summary": "The increasing complexity and cost of clinical trials, particularly in the\ncontext of oncology and advanced therapies, pose significant challenges for\ndrug development. This study evaluates the predictive capabilities of large\nlanguage models (LLMs) such as GPT-3.5, GPT-4, and HINT in determining clinical\ntrial outcomes. By leveraging a curated dataset of trials from\nClinicalTrials.gov, we compare the models' performance using metrics including\nbalanced accuracy, specificity, recall, and Matthews Correlation Coefficient\n(MCC). Results indicate that GPT-4o demonstrates robust performance in early\ntrial phases, achieving high recall but facing limitations in specificity.\nConversely, the HINT model excels in recognizing negative outcomes,\nparticularly in later trial phases, offering a balanced approach across diverse\nendpoints. Oncology trials, characterized by high complexity, remain\nchallenging for all models. Additionally, trial duration and disease categories\ninfluence predictive performance, with longer durations and complex diseases\nsuch as neoplasms reducing accuracy. This study highlights the complementary\nstrengths of LLMs and HINT, providing insights into optimizing predictive tools\nfor clinical trial design and risk management. Future advancements in LLMs are\nessential to address current gaps in handling negative outcomes and complex\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity and cost of clinical trials, particularly in the\ncontext of oncology and advanced therapies, pose significant challenges for\ndrug development. This study evaluates the predictive capabilities of large\nlanguage models (LLMs) such as GPT-3.5, GPT-4, and HINT in determining clinical\ntrial outcomes. By leveraging a curated dataset of trials from\nClinicalTrials.gov, we compare the models' performance using metrics including\nbalanced accuracy, specificity, recall, and Matthews Correlation Coefficient\n(MCC). Results indicate that GPT-4o demonstrates robust performance in early\ntrial phases, achieving high recall but facing limitations in specificity.\nConversely, the HINT model excels in recognizing negative outcomes,\nparticularly in later trial phases, offering a balanced approach across diverse\nendpoints. Oncology trials, characterized by high complexity, remain\nchallenging for all models. Additionally, trial duration and disease categories\ninfluence predictive performance, with longer durations and complex diseases\nsuch as neoplasms reducing accuracy. This study highlights the complementary\nstrengths of LLMs and HINT, providing insights into optimizing predictive tools\nfor clinical trial design and risk management. Future advancements in LLMs are\nessential to address current gaps in handling negative outcomes and complex\ndomains."
                },
                "authors": [
                    {
                        "name": "Shuyi Jin"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Hongru Ding"
                    },
                    {
                        "name": "Meijie Wang"
                    },
                    {
                        "name": "Lun Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lun Yu"
                },
                "author": "Lun Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17585v1",
                "updated": "2024-11-26T16:51:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    51,
                    52,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:51:52Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    51,
                    52,
                    1,
                    331,
                    0
                ],
                "title": "Multi-Objective Reinforcement Learning for Automated Resilient Cyber\n  Defence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Objective Reinforcement Learning for Automated Resilient Cyber\n  Defence"
                },
                "summary": "Cyber-attacks pose a security threat to military command and control\nnetworks, Intelligence, Surveillance, and Reconnaissance (ISR) systems, and\ncivilian critical national infrastructure. The use of artificial intelligence\nand autonomous agents in these attacks increases the scale, range, and\ncomplexity of this threat and the subsequent disruption they cause. Autonomous\nCyber Defence (ACD) agents aim to mitigate this threat by responding at machine\nspeed and at the scale required to address the problem. Sequential\ndecision-making algorithms such as Deep Reinforcement Learning (RL) provide a\npromising route to create ACD agents. These algorithms focus on a single\nobjective such as minimizing the intrusion of red agents on the network, by\nusing a handcrafted weighted sum of rewards. This approach removes the ability\nto adapt the model during inference, and fails to address the many competing\nobjectives present when operating and protecting these networks. Conflicting\nobjectives, such as restoring a machine from a back-up image, must be carefully\nbalanced with the cost of associated down-time, or the disruption to network\ntraffic or services that might result. Instead of pursing a Single-Objective RL\n(SORL) approach, here we present a simple example of a multi-objective network\ndefence game that requires consideration of both defending the network against\nred-agents and maintaining critical functionality of green-agents. Two\nMulti-Objective Reinforcement Learning (MORL) algorithms, namely\nMulti-Objective Proximal Policy Optimization (MOPPO), and Pareto-Conditioned\nNetworks (PCN), are used to create two trained ACD agents whose performance is\ncompared on our Multi-Objective Cyber Defence game. The benefits and\nlimitations of MORL ACD agents in comparison to SORL ACD agents are discussed\nbased on the investigations of this game.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber-attacks pose a security threat to military command and control\nnetworks, Intelligence, Surveillance, and Reconnaissance (ISR) systems, and\ncivilian critical national infrastructure. The use of artificial intelligence\nand autonomous agents in these attacks increases the scale, range, and\ncomplexity of this threat and the subsequent disruption they cause. Autonomous\nCyber Defence (ACD) agents aim to mitigate this threat by responding at machine\nspeed and at the scale required to address the problem. Sequential\ndecision-making algorithms such as Deep Reinforcement Learning (RL) provide a\npromising route to create ACD agents. These algorithms focus on a single\nobjective such as minimizing the intrusion of red agents on the network, by\nusing a handcrafted weighted sum of rewards. This approach removes the ability\nto adapt the model during inference, and fails to address the many competing\nobjectives present when operating and protecting these networks. Conflicting\nobjectives, such as restoring a machine from a back-up image, must be carefully\nbalanced with the cost of associated down-time, or the disruption to network\ntraffic or services that might result. Instead of pursing a Single-Objective RL\n(SORL) approach, here we present a simple example of a multi-objective network\ndefence game that requires consideration of both defending the network against\nred-agents and maintaining critical functionality of green-agents. Two\nMulti-Objective Reinforcement Learning (MORL) algorithms, namely\nMulti-Objective Proximal Policy Optimization (MOPPO), and Pareto-Conditioned\nNetworks (PCN), are used to create two trained ACD agents whose performance is\ncompared on our Multi-Objective Cyber Defence game. The benefits and\nlimitations of MORL ACD agents in comparison to SORL ACD agents are discussed\nbased on the investigations of this game."
                },
                "authors": [
                    {
                        "name": "Ross O'Driscoll"
                    },
                    {
                        "name": "Claudia Hagen"
                    },
                    {
                        "name": "Joe Bater"
                    },
                    {
                        "name": "James M. Adams"
                    }
                ],
                "author_detail": {
                    "name": "James M. Adams"
                },
                "author": "James M. Adams",
                "arxiv_comment": "9 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15808v2",
                "updated": "2024-11-26T16:51:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    51,
                    34,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-24T12:30:12Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    12,
                    30,
                    12,
                    6,
                    329,
                    0
                ],
                "title": "LRSAA: Large-scale Remote Sensing Image Target Recognition and Automatic\n  Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LRSAA: Large-scale Remote Sensing Image Target Recognition and Automatic\n  Annotation"
                },
                "summary": "This paper presents a method for object recognition and automatic labeling in\nlarge-area remote sensing images called LRSAA. The method integrates YOLOv11\nand MobileNetV3-SSD object detection algorithms through ensemble learning to\nenhance model performance. Furthermore, it employs Poisson disk sampling\nsegmentation techniques and the EIOU metric to optimize the training and\ninference processes of segmented images, followed by the integration of\nresults. This approach not only reduces the demand for computational resources\nbut also achieves a good balance between accuracy and speed. The source code\nfor this project has been made publicly available on\nhttps://github.com/anaerovane/LRSAA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method for object recognition and automatic labeling in\nlarge-area remote sensing images called LRSAA. The method integrates YOLOv11\nand MobileNetV3-SSD object detection algorithms through ensemble learning to\nenhance model performance. Furthermore, it employs Poisson disk sampling\nsegmentation techniques and the EIOU metric to optimize the training and\ninference processes of segmented images, followed by the integration of\nresults. This approach not only reduces the demand for computational resources\nbut also achieves a good balance between accuracy and speed. The source code\nfor this project has been made publicly available on\nhttps://github.com/anaerovane/LRSAA."
                },
                "authors": [
                    {
                        "name": "Wuzheng Dong"
                    },
                    {
                        "name": "Yujuan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yujuan Zhu"
                },
                "author": "Yujuan Zhu",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2411.07802",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16310v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16310v2",
                "updated": "2024-11-26T16:45:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    45,
                    22,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-25T11:57:48Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    57,
                    48,
                    0,
                    330,
                    0
                ],
                "title": "Functionality understanding and segmentation in 3D scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functionality understanding and segmentation in 3D scenes"
                },
                "summary": "Understanding functionalities in 3D scenes involves interpreting natural\nlanguage descriptions to locate functional interactive objects, such as handles\nand buttons, in a 3D environment. Functionality understanding is highly\nchallenging, as it requires both world knowledge to interpret language and\nspatial perception to identify fine-grained objects. For example, given a task\nlike 'turn on the ceiling light', an embodied AI agent must infer that it needs\nto locate the light switch, even though the switch is not explicitly mentioned\nin the task description. To date, no dedicated methods have been developed for\nthis problem. In this paper, we introduce Fun3DU, the first approach designed\nfor functionality understanding in 3D scenes. Fun3DU uses a language model to\nparse the task description through Chain-of-Thought reasoning in order to\nidentify the object of interest. The identified object is segmented across\nmultiple views of the captured scene by using a vision and language model. The\nsegmentation results from each view are lifted in 3D and aggregated into the\npoint cloud using geometric information. Fun3DU is training-free, relying\nentirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most\nrecent and only dataset to benchmark this task, which comprises over 3000 task\ndescriptions on 230 scenes. Our method significantly outperforms\nstate-of-the-art open-vocabulary 3D segmentation approaches. Project page:\nhttps://jcorsetti.github.io/fun3du",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding functionalities in 3D scenes involves interpreting natural\nlanguage descriptions to locate functional interactive objects, such as handles\nand buttons, in a 3D environment. Functionality understanding is highly\nchallenging, as it requires both world knowledge to interpret language and\nspatial perception to identify fine-grained objects. For example, given a task\nlike 'turn on the ceiling light', an embodied AI agent must infer that it needs\nto locate the light switch, even though the switch is not explicitly mentioned\nin the task description. To date, no dedicated methods have been developed for\nthis problem. In this paper, we introduce Fun3DU, the first approach designed\nfor functionality understanding in 3D scenes. Fun3DU uses a language model to\nparse the task description through Chain-of-Thought reasoning in order to\nidentify the object of interest. The identified object is segmented across\nmultiple views of the captured scene by using a vision and language model. The\nsegmentation results from each view are lifted in 3D and aggregated into the\npoint cloud using geometric information. Fun3DU is training-free, relying\nentirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most\nrecent and only dataset to benchmark this task, which comprises over 3000 task\ndescriptions on 230 scenes. Our method significantly outperforms\nstate-of-the-art open-vocabulary 3D segmentation approaches. Project page:\nhttps://jcorsetti.github.io/fun3du"
                },
                "authors": [
                    {
                        "name": "Jaime Corsetti"
                    },
                    {
                        "name": "Francesco Giuliari"
                    },
                    {
                        "name": "Alice Fasoli"
                    },
                    {
                        "name": "Davide Boscaini"
                    },
                    {
                        "name": "Fabio Poiesi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Poiesi"
                },
                "author": "Fabio Poiesi",
                "arxiv_comment": "Technical report. 20 pages, 12 figures, 7 tables. Updated website\n  link",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16310v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16310v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13191v3",
                "updated": "2024-11-26T16:45:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    45,
                    19,
                    1,
                    331,
                    0
                ],
                "published": "2024-04-19T21:39:15Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    21,
                    39,
                    15,
                    4,
                    110,
                    0
                ],
                "title": "Action Contextualization: Adaptive Task Planning and Action Tuning using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action Contextualization: Adaptive Task Planning and Action Tuning using\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) present a promising frontier in robotic task\nplanning by leveraging extensive human knowledge. Nevertheless, the current\nliterature often overlooks the critical aspects of robots' adaptability and\nerror correction. This work aims to overcome this limitation by enabling robots\nto modify their motions and select the most suitable task plans based on the\ncontext. We introduce a novel framework to achieve action contextualization,\naimed at tailoring robot actions to the context of specific tasks, thereby\nenhancing adaptability through applying LLM-derived contextual insights. Our\nframework integrates motion metrics that evaluate robot performances for each\nmotion to resolve redundancy in planning. Moreover, it supports online feedback\nbetween the robot and the LLM, enabling immediate modifications to the task\nplans and corrections of errors. An overall success rate of 81.25% has been\nachieved through extensive experimental validation. Finally, when integrated\nwith dynamical system (DS)-based robot controllers, the robotic arm-hand system\ndemonstrates its proficiency in autonomously executing LLM-generated motion\nplans for sequential table-clearing tasks, rectifying errors without human\nintervention, and showcasing robustness against external disturbances. Our\nproposed framework also features the potential to be integrated with modular\ncontrol approaches, significantly enhancing robots' adaptability and autonomy\nin performing sequential tasks in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present a promising frontier in robotic task\nplanning by leveraging extensive human knowledge. Nevertheless, the current\nliterature often overlooks the critical aspects of robots' adaptability and\nerror correction. This work aims to overcome this limitation by enabling robots\nto modify their motions and select the most suitable task plans based on the\ncontext. We introduce a novel framework to achieve action contextualization,\naimed at tailoring robot actions to the context of specific tasks, thereby\nenhancing adaptability through applying LLM-derived contextual insights. Our\nframework integrates motion metrics that evaluate robot performances for each\nmotion to resolve redundancy in planning. Moreover, it supports online feedback\nbetween the robot and the LLM, enabling immediate modifications to the task\nplans and corrections of errors. An overall success rate of 81.25% has been\nachieved through extensive experimental validation. Finally, when integrated\nwith dynamical system (DS)-based robot controllers, the robotic arm-hand system\ndemonstrates its proficiency in autonomously executing LLM-generated motion\nplans for sequential table-clearing tasks, rectifying errors without human\nintervention, and showcasing robustness against external disturbances. Our\nproposed framework also features the potential to be integrated with modular\ncontrol approaches, significantly enhancing robots' adaptability and autonomy\nin performing sequential tasks in the real world."
                },
                "authors": [
                    {
                        "name": "Sthithpragya Gupta"
                    },
                    {
                        "name": "Kunpeng Yao"
                    },
                    {
                        "name": "Loïc Niederhauser"
                    },
                    {
                        "name": "Aude Billard"
                    }
                ],
                "author_detail": {
                    "name": "Aude Billard"
                },
                "author": "Aude Billard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16638v2",
                "updated": "2024-11-26T16:38:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    38,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-25T18:15:15Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    15,
                    15,
                    0,
                    330,
                    0
                ],
                "title": "Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation"
                },
                "summary": "Modern LLMs can now produce highly readable abstractive summaries, to the\npoint where traditional automated metrics for evaluating summary quality, such\nas ROUGE, have become saturated. However, LLMs still sometimes introduce\nunwanted content into summaries, i.e., information inconsistent with or\nunsupported by their source. Measuring the occurrence of these often subtle\n``hallucinations'' automatically has proved to be challenging. This in turn has\nmotivated development of a variety of metrics intended to measure the factual\nconsistency of generated summaries against their source. But are these\napproaches measuring what they purport to do? In this work, we stress-test\nautomatic factuality metrics. Specifically, we investigate whether and to what\ndegree superficial attributes of summary texts suffice to predict\n``factuality'', finding that a (supervised) model using only such shallow\nfeatures is reasonably competitive with SOTA factuality scoring methods. We\nthen evaluate how factuality metrics respond to factual corrections in\ninconsistent summaries and find that only a few show meaningful improvements.\nIn contrast, some metrics are more sensitive to benign, non-factual edits.\nMotivated by these insights, we show that one can ``game'' (most) automatic\nfactuality metrics, i.e., reliably inflate ``factuality'' scores by appending\ninnocuous sentences to generated summaries.Taken together, our results raise\nquestions about the degree to which we should rely on existing automated\nfactuality metrics and what exactly we want ``factuality metrics'' to measure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLMs can now produce highly readable abstractive summaries, to the\npoint where traditional automated metrics for evaluating summary quality, such\nas ROUGE, have become saturated. However, LLMs still sometimes introduce\nunwanted content into summaries, i.e., information inconsistent with or\nunsupported by their source. Measuring the occurrence of these often subtle\n``hallucinations'' automatically has proved to be challenging. This in turn has\nmotivated development of a variety of metrics intended to measure the factual\nconsistency of generated summaries against their source. But are these\napproaches measuring what they purport to do? In this work, we stress-test\nautomatic factuality metrics. Specifically, we investigate whether and to what\ndegree superficial attributes of summary texts suffice to predict\n``factuality'', finding that a (supervised) model using only such shallow\nfeatures is reasonably competitive with SOTA factuality scoring methods. We\nthen evaluate how factuality metrics respond to factual corrections in\ninconsistent summaries and find that only a few show meaningful improvements.\nIn contrast, some metrics are more sensitive to benign, non-factual edits.\nMotivated by these insights, we show that one can ``game'' (most) automatic\nfactuality metrics, i.e., reliably inflate ``factuality'' scores by appending\ninnocuous sentences to generated summaries.Taken together, our results raise\nquestions about the degree to which we should rely on existing automated\nfactuality metrics and what exactly we want ``factuality metrics'' to measure."
                },
                "authors": [
                    {
                        "name": "Sanjana Ramprasad"
                    },
                    {
                        "name": "Byron C. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Byron C. Wallace"
                },
                "author": "Byron C. Wallace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17569v1",
                "updated": "2024-11-26T16:31:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    31,
                    18,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    31,
                    18,
                    1,
                    331,
                    0
                ],
                "title": "RTL-Breaker: Assessing the Security of LLMs against Backdoor Attacks on\n  HDL Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTL-Breaker: Assessing the Security of LLMs against Backdoor Attacks on\n  HDL Code Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable potential with code\ngeneration/completion tasks for hardware design. In fact, LLM-based hardware\ndescription language (HDL) code generation has enabled the industry to realize\ncomplex designs more quickly, reducing the time and effort required in the\ndevelopment cycle. However, the increased reliance on such automation\nintroduces critical security risks. Notably, given that LLMs have to be trained\non vast datasets of codes that are typically sourced from publicly available\nrepositories (often without thorough validation), LLMs are susceptible to\nso-called data poisoning or backdoor attacks. Here, attackers inject malicious\ncode for the training data, which can be carried over into the HDL code\ngenerated by LLMs. This threat vector can compromise the security and integrity\nof entire hardware systems. In this work, we propose RTL-Breaker, a novel\nbackdoor attack framework on LLM-based HDL code generation. RTL-Breaker\nprovides an in-depth analysis for essential aspects of this novel problem: 1)\nvarious trigger mechanisms versus their effectiveness for inserting malicious\nmodifications, and 2) side-effects by backdoor attacks on code generation in\ngeneral, i.e., impact on code quality. RTL-Breaker emphasizes the urgent need\nfor more robust measures to safeguard against such attacks. Toward that end, we\nopen-source our framework and all data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable potential with code\ngeneration/completion tasks for hardware design. In fact, LLM-based hardware\ndescription language (HDL) code generation has enabled the industry to realize\ncomplex designs more quickly, reducing the time and effort required in the\ndevelopment cycle. However, the increased reliance on such automation\nintroduces critical security risks. Notably, given that LLMs have to be trained\non vast datasets of codes that are typically sourced from publicly available\nrepositories (often without thorough validation), LLMs are susceptible to\nso-called data poisoning or backdoor attacks. Here, attackers inject malicious\ncode for the training data, which can be carried over into the HDL code\ngenerated by LLMs. This threat vector can compromise the security and integrity\nof entire hardware systems. In this work, we propose RTL-Breaker, a novel\nbackdoor attack framework on LLM-based HDL code generation. RTL-Breaker\nprovides an in-depth analysis for essential aspects of this novel problem: 1)\nvarious trigger mechanisms versus their effectiveness for inserting malicious\nmodifications, and 2) side-effects by backdoor attacks on code generation in\ngeneral, i.e., impact on code quality. RTL-Breaker emphasizes the urgent need\nfor more robust measures to safeguard against such attacks. Toward that end, we\nopen-source our framework and all data."
                },
                "authors": [
                    {
                        "name": "Lakshmi Likhitha Mankali"
                    },
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Manaar Alam"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Michail Maniatakos"
                    },
                    {
                        "name": "Ozgur Sinanoglu"
                    },
                    {
                        "name": "Johann Knechtel"
                    }
                ],
                "author_detail": {
                    "name": "Johann Knechtel"
                },
                "author": "Johann Knechtel",
                "arxiv_comment": "Accepted at 2025 Design, Automation & Test in Europe (DATE)\n  Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17562v1",
                "updated": "2024-11-26T16:24:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    24,
                    59,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:24:59Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    24,
                    59,
                    1,
                    331,
                    0
                ],
                "title": "Chemically Self-Consistent Modeling of the Globular Cluster NGC 2808 and\n  its Effects on the Inferred Helium Abundance of Multiple Stellar Populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemically Self-Consistent Modeling of the Globular Cluster NGC 2808 and\n  its Effects on the Inferred Helium Abundance of Multiple Stellar Populations"
                },
                "summary": "The helium abundances in the multiple populations that are now known to\ncomprise all closely studied Milky Way globular clusters are often inferred by\nfitting isochrones generated from stellar evolutionary models to globular\ncluster photometry. It is therefore important to build stellar models that are\nchemically self-consistent in terms of their structure, atmosphere, and\nopacity. In this work we present the first chemically self-consistent stellar\nmodels of the Milky Way globular cluster NGC 2808 using MARCS model\natmospheres, OPLIB high-temperature radiative opacities, and AESOPUS\nlow-temperature radiative opacities. These stellar models were fit to the NGC\n2808 photometry using Fidanka , a new software tool that was developed to\noptimally fit cluster photometry to isochrones and for population synthesis.\nFidanka can determine, in a relatively unbiased way, the ideal number of\ndistinct populations that exist within a dataset and then fit isochrones to\neach population. We achieve this outcome through a combination of Bayesian\nGaussian Mixture Modeling and a novel number density estimation algorithm.\nUsing Fidanka and F275W-F814W photometry from the Hubble UV Globular Cluster\nSurvey we find that the helium abundance of the second generation of stars in\nNGC 2808 is higher than the first generation by $15\\pm3\\%$. This is in\nagreement with previous studies of NGC 2808. This work, along with previous\nwork by Dotter et al. (2015) focused on NGC 6752, demonstrates that chemically\nself-consistent models of globular clusters do not significantly alter inferred\nhelium abundances, and are therefore unlikely to be worth the significant\nadditional time investment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The helium abundances in the multiple populations that are now known to\ncomprise all closely studied Milky Way globular clusters are often inferred by\nfitting isochrones generated from stellar evolutionary models to globular\ncluster photometry. It is therefore important to build stellar models that are\nchemically self-consistent in terms of their structure, atmosphere, and\nopacity. In this work we present the first chemically self-consistent stellar\nmodels of the Milky Way globular cluster NGC 2808 using MARCS model\natmospheres, OPLIB high-temperature radiative opacities, and AESOPUS\nlow-temperature radiative opacities. These stellar models were fit to the NGC\n2808 photometry using Fidanka , a new software tool that was developed to\noptimally fit cluster photometry to isochrones and for population synthesis.\nFidanka can determine, in a relatively unbiased way, the ideal number of\ndistinct populations that exist within a dataset and then fit isochrones to\neach population. We achieve this outcome through a combination of Bayesian\nGaussian Mixture Modeling and a novel number density estimation algorithm.\nUsing Fidanka and F275W-F814W photometry from the Hubble UV Globular Cluster\nSurvey we find that the helium abundance of the second generation of stars in\nNGC 2808 is higher than the first generation by $15\\pm3\\%$. This is in\nagreement with previous studies of NGC 2808. This work, along with previous\nwork by Dotter et al. (2015) focused on NGC 6752, demonstrates that chemically\nself-consistent models of globular clusters do not significantly alter inferred\nhelium abundances, and are therefore unlikely to be worth the significant\nadditional time investment."
                },
                "authors": [
                    {
                        "name": "Emily M. Boudreaux"
                    },
                    {
                        "name": "Brian C. Chaboyer"
                    },
                    {
                        "name": "Amanda Ash"
                    },
                    {
                        "name": "Renata Edaes Hoh"
                    },
                    {
                        "name": "Gregory Feiden"
                    }
                ],
                "author_detail": {
                    "name": "Gregory Feiden"
                },
                "author": "Gregory Feiden",
                "arxiv_comment": "13 Pages, 9 Figures, 4 Tables. Accepted For publication in ApJ\n  (November 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17558v1",
                "updated": "2024-11-26T16:21:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    3,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:21:03Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    3,
                    1,
                    331,
                    0
                ],
                "title": "Natural Language Understanding and Inference with MLLM in Visual\n  Question Answering: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Understanding and Inference with MLLM in Visual\n  Question Answering: A Survey"
                },
                "summary": "Visual Question Answering (VQA) is a challenge task that combines natural\nlanguage processing and computer vision techniques and gradually becomes a\nbenchmark test task in multimodal large language models (MLLMs). The goal of\nour survey is to provide an overview of the development of VQA and a detailed\ndescription of the latest models with high timeliness. This survey gives an\nup-to-date synthesis of natural language understanding of images and text, as\nwell as the knowledge reasoning module based on image-question information on\nthe core VQA tasks. In addition, we elaborate on recent advances in extracting\nand fusing modal information with vision-language pretraining models and\nmultimodal large language models in VQA. We also exhaustively review the\nprogress of knowledge reasoning in VQA by detailing the extraction of internal\nknowledge and the introduction of external knowledge. Finally, we present the\ndatasets of VQA and different evaluation metrics and discuss possible\ndirections for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Question Answering (VQA) is a challenge task that combines natural\nlanguage processing and computer vision techniques and gradually becomes a\nbenchmark test task in multimodal large language models (MLLMs). The goal of\nour survey is to provide an overview of the development of VQA and a detailed\ndescription of the latest models with high timeliness. This survey gives an\nup-to-date synthesis of natural language understanding of images and text, as\nwell as the knowledge reasoning module based on image-question information on\nthe core VQA tasks. In addition, we elaborate on recent advances in extracting\nand fusing modal information with vision-language pretraining models and\nmultimodal large language models in VQA. We also exhaustively review the\nprogress of knowledge reasoning in VQA by detailing the extraction of internal\nknowledge and the introduction of external knowledge. Finally, we present the\ndatasets of VQA and different evaluation metrics and discuss possible\ndirections for future work."
                },
                "authors": [
                    {
                        "name": "Jiayi Kuang"
                    },
                    {
                        "name": "Jingyou Xie"
                    },
                    {
                        "name": "Haohao Luo"
                    },
                    {
                        "name": "Ronghao Li"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Xianfeng Cheng"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Xika Lin"
                    },
                    {
                        "name": "Ying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shen"
                },
                "author": "Ying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17554v1",
                "updated": "2024-11-26T16:15:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    15,
                    49,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:15:49Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    15,
                    49,
                    1,
                    331,
                    0
                ],
                "title": "Navigating Spatial Inequities in Freight Truck Crash Severity via\n  Counterfactual Inference in Los Angeles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating Spatial Inequities in Freight Truck Crash Severity via\n  Counterfactual Inference in Los Angeles"
                },
                "summary": "Freight truck-related crashes pose significant challenges, leading to\nsubstantial economic losses, injuries, and fatalities, with pronounced spatial\ndisparities across different regions. This study adopts a transport geography\nperspective to examine spatial justice concerns by employing deep\ncounterfactual inference models to analyze how socioeconomic disparities, road\ninfrastructure, and environmental conditions influence the geographical\ndistribution and severity of freight truck crashes. By integrating road network\ndatasets, socioeconomic attributes, and crash records from the Los Angeles\nmetropolitan area, this research provides a nuanced spatial analysis of how\ndifferent communities are disproportionately impacted. The results reveal\nsignificant spatial disparities in crash severity across areas with varying\npopulation densities, income levels, and minority populations, highlighting the\npivotal role of infrastructural and environmental improvements in mitigating\nthese disparities. The findings offer insights into targeted, location-specific\npolicy interventions, suggesting enhancements in road infrastructure, lighting,\nand traffic control systems, particularly in low-income and\nminority-concentrated areas. This research contributes to the literature on\ntransport geography and spatial equity by providing data-driven insights into\neffective measures for reducing spatial injustices associated with freight\ntruck-related crashes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Freight truck-related crashes pose significant challenges, leading to\nsubstantial economic losses, injuries, and fatalities, with pronounced spatial\ndisparities across different regions. This study adopts a transport geography\nperspective to examine spatial justice concerns by employing deep\ncounterfactual inference models to analyze how socioeconomic disparities, road\ninfrastructure, and environmental conditions influence the geographical\ndistribution and severity of freight truck crashes. By integrating road network\ndatasets, socioeconomic attributes, and crash records from the Los Angeles\nmetropolitan area, this research provides a nuanced spatial analysis of how\ndifferent communities are disproportionately impacted. The results reveal\nsignificant spatial disparities in crash severity across areas with varying\npopulation densities, income levels, and minority populations, highlighting the\npivotal role of infrastructural and environmental improvements in mitigating\nthese disparities. The findings offer insights into targeted, location-specific\npolicy interventions, suggesting enhancements in road infrastructure, lighting,\nand traffic control systems, particularly in low-income and\nminority-concentrated areas. This research contributes to the literature on\ntransport geography and spatial equity by providing data-driven insights into\neffective measures for reducing spatial injustices associated with freight\ntruck-related crashes."
                },
                "authors": [
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Hao Yin"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Siqin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Siqin Wang"
                },
                "author": "Siqin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17542v1",
                "updated": "2024-11-26T16:03:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    3,
                    58,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:03:58Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    3,
                    58,
                    1,
                    331,
                    0
                ],
                "title": "Causal Inference in Finance: An Expertise-Driven Model for Instrument\n  Variables Identification and Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Inference in Finance: An Expertise-Driven Model for Instrument\n  Variables Identification and Interpretation"
                },
                "summary": "Instrumental Variable (IV) provides a source of treatment randomization that\nis conditionally independent of the outcomes, responding to the challenges of\ncounterfactual and confounding biases. In finance, IV construction typically\nrelies on pre-designed synthetic IVs, with effectiveness measured by specific\nalgorithms. This classic paradigm cannot be generalized to address broader\nissues that require more and specific IVs. Therefore, we propose an\nexpertise-driven model (ETE-FinCa) to optimize the source of expertise,\ninstantiate IVs by the expertise concept, and interpret the cause-effect\nrelationship by integrating concept with real economic data. The results show\nthat the feature selection based on causal knowledge graphs improves the\nclassification performance than others, with up to a 11.7% increase in accuracy\nand a 23.0% increase in F1-score. Furthermore, the high-quality IVs we defined\ncan identify causal relationships between the treatment and outcome variables\nin the Two-Stage Least Squares Regression model with statistical significance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instrumental Variable (IV) provides a source of treatment randomization that\nis conditionally independent of the outcomes, responding to the challenges of\ncounterfactual and confounding biases. In finance, IV construction typically\nrelies on pre-designed synthetic IVs, with effectiveness measured by specific\nalgorithms. This classic paradigm cannot be generalized to address broader\nissues that require more and specific IVs. Therefore, we propose an\nexpertise-driven model (ETE-FinCa) to optimize the source of expertise,\ninstantiate IVs by the expertise concept, and interpret the cause-effect\nrelationship by integrating concept with real economic data. The results show\nthat the feature selection based on causal knowledge graphs improves the\nclassification performance than others, with up to a 11.7% increase in accuracy\nand a 23.0% increase in F1-score. Furthermore, the high-quality IVs we defined\ncan identify causal relationships between the treatment and outcome variables\nin the Two-Stage Least Squares Regression model with statistical significance."
                },
                "authors": [
                    {
                        "name": "Ying Chen"
                    },
                    {
                        "name": "Ziwei Xu"
                    },
                    {
                        "name": "Kotaro Inoue"
                    },
                    {
                        "name": "Ryutaro Ichise"
                    }
                ],
                "author_detail": {
                    "name": "Ryutaro Ichise"
                },
                "author": "Ryutaro Ichise",
                "arxiv_comment": "23rd International Conference on Machine Learning and Applications\n  (ICMLA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17538v1",
                "updated": "2024-11-26T15:53:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    53,
                    28,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:53:28Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    53,
                    28,
                    1,
                    331,
                    0
                ],
                "title": "Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code\n  Search"
                },
                "summary": "Low isotropy in an embedding space impairs performance on tasks involving\nsemantic inference. Our study investigates the impact of isotropy on semantic\ncode search performance and explores post-processing techniques to mitigate\nthis issue. We analyze various code language models, examine isotropy in their\nembedding spaces, and its influence on search effectiveness. We propose a\nmodified ZCA whitening technique to control isotropy levels in embeddings. Our\nresults demonstrate that Soft-ZCA whitening improves the performance of\npre-trained code language models and can complement contrastive fine-tuning.\nThe code for our experiments is available at\nhttps://github.com/drndr/code\\_isotropy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low isotropy in an embedding space impairs performance on tasks involving\nsemantic inference. Our study investigates the impact of isotropy on semantic\ncode search performance and explores post-processing techniques to mitigate\nthis issue. We analyze various code language models, examine isotropy in their\nembedding spaces, and its influence on search effectiveness. We propose a\nmodified ZCA whitening technique to control isotropy levels in embeddings. Our\nresults demonstrate that Soft-ZCA whitening improves the performance of\npre-trained code language models and can complement contrastive fine-tuning.\nThe code for our experiments is available at\nhttps://github.com/drndr/code\\_isotropy"
                },
                "authors": [
                    {
                        "name": "Andor Diera"
                    },
                    {
                        "name": "Lukas Galke"
                    },
                    {
                        "name": "Ansgar Scherp"
                    }
                ],
                "author_detail": {
                    "name": "Ansgar Scherp"
                },
                "author": "Ansgar Scherp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17537v1",
                "updated": "2024-11-26T15:53:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    53,
                    13,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:53:13Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    53,
                    13,
                    1,
                    331,
                    0
                ],
                "title": "Towards Maximum Likelihood Training for Transducer-based Streaming\n  Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Maximum Likelihood Training for Transducer-based Streaming\n  Speech Recognition"
                },
                "summary": "Transducer neural networks have emerged as the mainstream approach for\nstreaming automatic speech recognition (ASR), offering state-of-the-art\nperformance in balancing accuracy and latency. In the conventional framework,\nstreaming transducer models are trained to maximize the likelihood function\nbased on non-streaming recursion rules. However, this approach leads to a\nmismatch between training and inference, resulting in the issue of deformed\nlikelihood and consequently suboptimal ASR accuracy. We introduce a\nmathematical quantification of the gap between the actual likelihood and the\ndeformed likelihood, namely forward variable causal compensation (FoCC). We\nalso present its estimator, FoCCE, as a solution to estimate the exact\nlikelihood. Through experiments on the LibriSpeech dataset, we show that FoCCE\ntraining improves the accuracy of the streaming transducers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transducer neural networks have emerged as the mainstream approach for\nstreaming automatic speech recognition (ASR), offering state-of-the-art\nperformance in balancing accuracy and latency. In the conventional framework,\nstreaming transducer models are trained to maximize the likelihood function\nbased on non-streaming recursion rules. However, this approach leads to a\nmismatch between training and inference, resulting in the issue of deformed\nlikelihood and consequently suboptimal ASR accuracy. We introduce a\nmathematical quantification of the gap between the actual likelihood and the\ndeformed likelihood, namely forward variable causal compensation (FoCC). We\nalso present its estimator, FoCCE, as a solution to estimate the exact\nlikelihood. Through experiments on the LibriSpeech dataset, we show that FoCCE\ntraining improves the accuracy of the streaming transducers."
                },
                "authors": [
                    {
                        "name": "Hyeonseung Lee"
                    },
                    {
                        "name": "Ji Won Yoon"
                    },
                    {
                        "name": "Sungsoo Kim"
                    },
                    {
                        "name": "Nam Soo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Nam Soo Kim"
                },
                "author": "Nam Soo Kim",
                "arxiv_doi": "10.1109/LSP.2024.3491019",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LSP.2024.3491019",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.17537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figure, 1 table",
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17533v1",
                "updated": "2024-11-26T15:48:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    48,
                    35,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:48:35Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    48,
                    35,
                    1,
                    331,
                    0
                ],
                "title": "Simplifying Causal Mediation Analysis for Time-to-Event Outcomes using\n  Pseudo-Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplifying Causal Mediation Analysis for Time-to-Event Outcomes using\n  Pseudo-Values"
                },
                "summary": "Mediation analysis for survival outcomes is challenging. Most existing\nmethods quantify the treatment effect using the hazard ratio (HR) and attempt\nto decompose the HR into the direct effect of treatment plus an indirect, or\nmediated, effect. However, the HR is not expressible as an expectation, which\ncomplicates this decomposition, both in terms of estimation and interpretation.\nHere, we present an alternative approach which leverages pseudo-values to\nsimplify estimation and inference. Pseudo-values take censoring into account\nduring their construction, and once derived, can be modeled in the same way as\nany continuous outcome. Thus, pseudo-values enable mediation analysis for a\nsurvival outcome to fit seamlessly into standard mediation software (e.g.\nCMAverse in R). Pseudo-values are easy to calculate via a\nleave-one-observation-out procedure (i.e. jackknifing) and the calculation can\nbe accelerated when the influence function of the estimator is known. Mediation\nanalysis for causal effects defined by survival probabilities, restricted mean\nsurvival time, and cumulative incidence functions - in the presence of\ncompeting risks - can all be performed within this framework. Extensive\nsimulation studies demonstrate that the method is unbiased across 324\nscenarios/estimands and controls the type-I error at the nominal level under\nthe null of no mediation. We illustrate the approach using data from the\nPARADIGMS clinical trial for the treatment of pediatric multiple sclerosis\nusing fingolimod. In particular, we evaluate whether an imaging biomarker lies\non the causal path between treatment and time-to-relapse, which aids in\njustifying this biomarker as a surrogate outcome. Our approach greatly\nsimplifies mediation analysis for survival data and provides a decomposition of\nthe total effect that is both intuitive and interpretable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mediation analysis for survival outcomes is challenging. Most existing\nmethods quantify the treatment effect using the hazard ratio (HR) and attempt\nto decompose the HR into the direct effect of treatment plus an indirect, or\nmediated, effect. However, the HR is not expressible as an expectation, which\ncomplicates this decomposition, both in terms of estimation and interpretation.\nHere, we present an alternative approach which leverages pseudo-values to\nsimplify estimation and inference. Pseudo-values take censoring into account\nduring their construction, and once derived, can be modeled in the same way as\nany continuous outcome. Thus, pseudo-values enable mediation analysis for a\nsurvival outcome to fit seamlessly into standard mediation software (e.g.\nCMAverse in R). Pseudo-values are easy to calculate via a\nleave-one-observation-out procedure (i.e. jackknifing) and the calculation can\nbe accelerated when the influence function of the estimator is known. Mediation\nanalysis for causal effects defined by survival probabilities, restricted mean\nsurvival time, and cumulative incidence functions - in the presence of\ncompeting risks - can all be performed within this framework. Extensive\nsimulation studies demonstrate that the method is unbiased across 324\nscenarios/estimands and controls the type-I error at the nominal level under\nthe null of no mediation. We illustrate the approach using data from the\nPARADIGMS clinical trial for the treatment of pediatric multiple sclerosis\nusing fingolimod. In particular, we evaluate whether an imaging biomarker lies\non the causal path between treatment and time-to-relapse, which aids in\njustifying this biomarker as a surrogate outcome. Our approach greatly\nsimplifies mediation analysis for survival data and provides a decomposition of\nthe total effect that is both intuitive and interpretable."
                },
                "authors": [
                    {
                        "name": "Alex Ocampo"
                    },
                    {
                        "name": "Enrico Giudice"
                    },
                    {
                        "name": "Dieter A. Häring"
                    },
                    {
                        "name": "Baldur Magnusson"
                    },
                    {
                        "name": "Theis Lange"
                    },
                    {
                        "name": "Zachary R. McCaw"
                    }
                ],
                "author_detail": {
                    "name": "Zachary R. McCaw"
                },
                "author": "Zachary R. McCaw",
                "arxiv_comment": "Mediation, Pseudo-Values, Time-to-event, Survival Analysis,\n  Restricted Mean Survival Time, Competing Risks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10351v2",
                "updated": "2024-11-26T15:44:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    44,
                    21,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-15T16:55:57Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    55,
                    57,
                    4,
                    320,
                    0
                ],
                "title": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code"
                },
                "summary": "Large language models (LLMs) have significantly advanced the field of\nautomated code generation. However, a notable research gap exists in the\nevaluation of social biases that may be present in the code produced by LLMs.\nTo solve this issue, we propose a novel fairness framework, i.e., Solar, to\nassess and mitigate the social biases of LLM-generated code. Specifically,\nSolar can automatically generate test cases for quantitatively uncovering\nsocial biases of the auto-generated code by LLMs. To quantify the severity of\nsocial biases in generated code, we develop a dataset that covers a diverse set\nof social problems. We applied Solar and the crafted dataset to four\nstate-of-the-art LLMs for code generation. Our evaluation reveals severe bias\nin the LLM-generated code from all the subject LLMs. Furthermore, we explore\nseveral strategies for bias mitigation, including Chain-of-Thought (CoT)\nprompting, combining positive role-playing with CoT prompting and iterative\nprompting. Our experiments show that iterative prompting can effectively reduce\nsocial bias in LLM-generated code by up to 90%. Solar is highly extensible to\nevaluate new social problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced the field of\nautomated code generation. However, a notable research gap exists in the\nevaluation of social biases that may be present in the code produced by LLMs.\nTo solve this issue, we propose a novel fairness framework, i.e., Solar, to\nassess and mitigate the social biases of LLM-generated code. Specifically,\nSolar can automatically generate test cases for quantitatively uncovering\nsocial biases of the auto-generated code by LLMs. To quantify the severity of\nsocial biases in generated code, we develop a dataset that covers a diverse set\nof social problems. We applied Solar and the crafted dataset to four\nstate-of-the-art LLMs for code generation. Our evaluation reveals severe bias\nin the LLM-generated code from all the subject LLMs. Furthermore, we explore\nseveral strategies for bias mitigation, including Chain-of-Thought (CoT)\nprompting, combining positive role-playing with CoT prompting and iterative\nprompting. Our experiments show that iterative prompting can effectively reduce\nsocial bias in LLM-generated code by up to 90%. Solar is highly extensible to\nevaluate new social problems."
                },
                "authors": [
                    {
                        "name": "Lin Ling"
                    },
                    {
                        "name": "Fazle Rabbi"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Jinqiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiu Yang"
                },
                "author": "Jinqiu Yang",
                "arxiv_comment": "9pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03905v2",
                "updated": "2024-11-26T15:37:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    37,
                    57,
                    1,
                    331,
                    0
                ],
                "published": "2024-05-06T23:41:02Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    23,
                    41,
                    2,
                    0,
                    127,
                    0
                ],
                "title": "DeltaKWS: A 65nm 36nJ/Decision Bio-inspired Temporal-Sparsity-Aware\n  Digital Keyword Spotting IC with 0.6V Near-Threshold SRAM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeltaKWS: A 65nm 36nJ/Decision Bio-inspired Temporal-Sparsity-Aware\n  Digital Keyword Spotting IC with 0.6V Near-Threshold SRAM"
                },
                "summary": "This paper introduces DeltaKWS, to the best of our knowledge, the first\n$\\Delta$RNN-enabled fine-grained temporal sparsity-aware KWS IC for\nvoice-controlled devices. The 65 nm prototype chip features a number of\ntechniques to enhance performance, area, and power efficiencies, specifically:\n1) a bio-inspired delta-gated recurrent neural network ($\\Delta$RNN) classifier\nleveraging temporal similarities between neighboring feature vectors extracted\nfrom input frames and network hidden states, eliminating unnecessary operations\nand memory accesses; 2) an IIR BPF-based FEx that leverages mixed-precision\nquantization, low-cost computing structure and channel selection; 3) a 24 kB\n0.6 V near-$V_\\text{TH}$ weight SRAM that achieves 6.6X lower read power than\nthe foundry-provided SRAM. From chip measurement results, we show that the\nDeltaKWS achieves an 11/12-class GSCD accuracy of 90.5%/89.5% respectively and\nenergy consumption of 36 nJ/decision in 65 nm CMOS process. At 87% temporal\nsparsity, computing latency and energy/inference are reduced by 2.4X/3.4X,\nrespectively. The IIR BPF-based FEx, $\\Delta$RNN accelerator, and 24 kB\nnear-$V_\\text{TH}$ SRAM blocks occupy 0.084 mm$^{2}$, 0.319 mm$^{2}$, and 0.381\nmm$^{2}$ respectively (0.78 mm$^{2}$ in total).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces DeltaKWS, to the best of our knowledge, the first\n$\\Delta$RNN-enabled fine-grained temporal sparsity-aware KWS IC for\nvoice-controlled devices. The 65 nm prototype chip features a number of\ntechniques to enhance performance, area, and power efficiencies, specifically:\n1) a bio-inspired delta-gated recurrent neural network ($\\Delta$RNN) classifier\nleveraging temporal similarities between neighboring feature vectors extracted\nfrom input frames and network hidden states, eliminating unnecessary operations\nand memory accesses; 2) an IIR BPF-based FEx that leverages mixed-precision\nquantization, low-cost computing structure and channel selection; 3) a 24 kB\n0.6 V near-$V_\\text{TH}$ weight SRAM that achieves 6.6X lower read power than\nthe foundry-provided SRAM. From chip measurement results, we show that the\nDeltaKWS achieves an 11/12-class GSCD accuracy of 90.5%/89.5% respectively and\nenergy consumption of 36 nJ/decision in 65 nm CMOS process. At 87% temporal\nsparsity, computing latency and energy/inference are reduced by 2.4X/3.4X,\nrespectively. The IIR BPF-based FEx, $\\Delta$RNN accelerator, and 24 kB\nnear-$V_\\text{TH}$ SRAM blocks occupy 0.084 mm$^{2}$, 0.319 mm$^{2}$, and 0.381\nmm$^{2}$ respectively (0.78 mm$^{2}$ in total)."
                },
                "authors": [
                    {
                        "name": "Qinyu Chen"
                    },
                    {
                        "name": "Kwantae Kim"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Taekwang Jang"
                    },
                    {
                        "name": "Tobi Delbruck"
                    },
                    {
                        "name": "Shih-Chii Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Chii Liu"
                },
                "author": "Shih-Chii Liu",
                "arxiv_doi": "10.1109/TCASAI.2024.3507694",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TCASAI.2024.3507694",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.03905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted for publication in the IEEE Transactions\n  on Circuits and Systems for Artificial Intelligence (TCASAI)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20288v4",
                "updated": "2024-11-26T15:35:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    35,
                    49,
                    1,
                    331,
                    0
                ],
                "published": "2024-09-30T13:44:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    44,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have made significant progress in natural\nlanguage processing tasks and demonstrate considerable potential in the legal\ndomain. However, legal applications demand high standards of accuracy,\nreliability, and fairness. Applying existing LLMs to legal systems without\ncareful evaluation of their potential and limitations could pose significant\nrisks in legal practice. To this end, we introduce a standardized comprehensive\nChinese legal benchmark LexEval. This benchmark is notable in the following\nthree aspects: (1) Ability Modeling: We propose a new taxonomy of legal\ncognitive abilities to organize different tasks. (2) Scale: To our knowledge,\nLexEval is currently the largest Chinese legal evaluation dataset, comprising\n23 tasks and 14,150 questions. (3) Data: we utilize formatted existing\ndatasets, exam datasets and newly annotated datasets by legal experts to\ncomprehensively evaluate the various capabilities of LLMs. LexEval not only\nfocuses on the ability of LLMs to apply fundamental legal knowledge but also\ndedicates efforts to examining the ethical issues involved in their\napplication. We evaluated 38 open-source and commercial LLMs and obtained some\ninteresting findings. The experiments and findings offer valuable insights into\nthe challenges and potential solutions for developing Chinese legal systems and\nLLM evaluation pipelines. The LexEval dataset and leaderboard are publicly\navailable at \\url{https://github.com/CSHaitao/LexEval} and will be continuously\nupdated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant progress in natural\nlanguage processing tasks and demonstrate considerable potential in the legal\ndomain. However, legal applications demand high standards of accuracy,\nreliability, and fairness. Applying existing LLMs to legal systems without\ncareful evaluation of their potential and limitations could pose significant\nrisks in legal practice. To this end, we introduce a standardized comprehensive\nChinese legal benchmark LexEval. This benchmark is notable in the following\nthree aspects: (1) Ability Modeling: We propose a new taxonomy of legal\ncognitive abilities to organize different tasks. (2) Scale: To our knowledge,\nLexEval is currently the largest Chinese legal evaluation dataset, comprising\n23 tasks and 14,150 questions. (3) Data: we utilize formatted existing\ndatasets, exam datasets and newly annotated datasets by legal experts to\ncomprehensively evaluate the various capabilities of LLMs. LexEval not only\nfocuses on the ability of LLMs to apply fundamental legal knowledge but also\ndedicates efforts to examining the ethical issues involved in their\napplication. We evaluated 38 open-source and commercial LLMs and obtained some\ninteresting findings. The experiments and findings offer valuable insights into\nthe challenges and potential solutions for developing Chinese legal systems and\nLLM evaluation pipelines. The LexEval dataset and leaderboard are publicly\navailable at \\url{https://github.com/CSHaitao/LexEval} and will be continuously\nupdated."
                },
                "authors": [
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "You Chen"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Ruizhe Zhang"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "arxiv_comment": "NeurIPs 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17525v1",
                "updated": "2024-11-26T15:35:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    35,
                    44,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:35:44Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    35,
                    44,
                    1,
                    331,
                    0
                ],
                "title": "Pushing the Limits of Large Language Model Quantization via the\n  Linearity Theorem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of Large Language Model Quantization via the\n  Linearity Theorem"
                },
                "summary": "Quantizing large language models has become a standard way to reduce their\nmemory and computational costs. Typically, existing methods focus on breaking\ndown the problem into individual layer-wise sub-problems, and minimizing\nper-layer error, measured via various metrics. Yet, this approach currently\nlacks theoretical justification and the metrics employed may be sub-optimal. In\nthis paper, we present a \"linearity theorem\" establishing a direct relationship\nbetween the layer-wise $\\ell_2$ reconstruction error and the model perplexity\nincrease due to quantization. This insight enables two novel applications: (1)\na simple data-free LLM quantization method using Hadamard rotations and\nMSE-optimal grids, dubbed HIGGS, which outperforms all prior data-free\napproaches such as the extremely popular NF4 quantized format, and (2) an\noptimal solution to the problem of finding non-uniform per-layer quantization\nlevels which match a given compression constraint in the medium-bitwidth\nregime, obtained by reduction to dynamic programming. On the practical side, we\ndemonstrate improved accuracy-compression trade-offs on Llama-3.1 and\n3.2-family models, as well as on Qwen-family models. Further, we show that our\nmethod can be efficiently supported in terms of GPU kernels at various batch\nsizes, advancing both data-free and non-uniform quantization for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing large language models has become a standard way to reduce their\nmemory and computational costs. Typically, existing methods focus on breaking\ndown the problem into individual layer-wise sub-problems, and minimizing\nper-layer error, measured via various metrics. Yet, this approach currently\nlacks theoretical justification and the metrics employed may be sub-optimal. In\nthis paper, we present a \"linearity theorem\" establishing a direct relationship\nbetween the layer-wise $\\ell_2$ reconstruction error and the model perplexity\nincrease due to quantization. This insight enables two novel applications: (1)\na simple data-free LLM quantization method using Hadamard rotations and\nMSE-optimal grids, dubbed HIGGS, which outperforms all prior data-free\napproaches such as the extremely popular NF4 quantized format, and (2) an\noptimal solution to the problem of finding non-uniform per-layer quantization\nlevels which match a given compression constraint in the medium-bitwidth\nregime, obtained by reduction to dynamic programming. On the practical side, we\ndemonstrate improved accuracy-compression trade-offs on Llama-3.1 and\n3.2-family models, as well as on Qwen-family models. Further, we show that our\nmethod can be efficiently supported in terms of GPU kernels at various batch\nsizes, advancing both data-free and non-uniform quantization for LLMs."
                },
                "authors": [
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Ivan Ilin"
                    },
                    {
                        "name": "Han Guo"
                    },
                    {
                        "name": "Peter Richtárik"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09937v2",
                "updated": "2024-11-26T15:31:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    31,
                    44,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-15T04:22:21Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    22,
                    21,
                    4,
                    320,
                    0
                ],
                "title": "Refined and Segmented Price Sentiment Indices from Survey Comments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refined and Segmented Price Sentiment Indices from Survey Comments"
                },
                "summary": "We aim to enhance a price sentiment index and to more precisely understand\nprice trends from the perspective of not only consumers but also businesses. We\nextract comments related to prices from the Economy Watchers Survey conducted\nby the Cabinet Office of Japan and classify price trends using a large language\nmodel (LLM). We classify whether the survey sample reflects the perspective of\nconsumers or businesses, and whether the comments pertain to goods or services\nby utilizing information on the fields of comments and the industries of\nrespondents included in the Economy Watchers Survey. From these classified\nprice-related comments, we construct price sentiment indices not only for a\ngeneral purpose but also for more specific objectives by combining perspectives\non consumers and prices, as well as goods and services. It becomes possible to\nachieve a more accurate classification of price directions by employing a LLM\nfor classification. Furthermore, integrating the outputs of multiple LLMs\nsuggests the potential for the better performance of the classification. The\nuse of more accurately classified comments allows for the construction of an\nindex with a higher correlation to existing indices than previous studies. We\ndemonstrate that the correlation of the price index for consumers, which has a\nlarger sample size, is further enhanced by selecting comments for aggregation\nbased on the industry of the survey respondents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We aim to enhance a price sentiment index and to more precisely understand\nprice trends from the perspective of not only consumers but also businesses. We\nextract comments related to prices from the Economy Watchers Survey conducted\nby the Cabinet Office of Japan and classify price trends using a large language\nmodel (LLM). We classify whether the survey sample reflects the perspective of\nconsumers or businesses, and whether the comments pertain to goods or services\nby utilizing information on the fields of comments and the industries of\nrespondents included in the Economy Watchers Survey. From these classified\nprice-related comments, we construct price sentiment indices not only for a\ngeneral purpose but also for more specific objectives by combining perspectives\non consumers and prices, as well as goods and services. It becomes possible to\nachieve a more accurate classification of price directions by employing a LLM\nfor classification. Furthermore, integrating the outputs of multiple LLMs\nsuggests the potential for the better performance of the classification. The\nuse of more accurately classified comments allows for the construction of an\nindex with a higher correlation to existing indices than previous studies. We\ndemonstrate that the correlation of the price index for consumers, which has a\nlarger sample size, is further enhanced by selecting comments for aggregation\nbased on the industry of the survey respondents."
                },
                "authors": [
                    {
                        "name": "Masahiro Suzuki"
                    },
                    {
                        "name": "Hiroki Sakaji"
                    }
                ],
                "author_detail": {
                    "name": "Hiroki Sakaji"
                },
                "author": "Hiroki Sakaji",
                "arxiv_comment": "Accepted to IEEE BigData 2024. 9 pages, 11 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17515v1",
                "updated": "2024-11-26T15:26:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    26,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:26:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    26,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "SuperMat: Physically Consistent PBR Material Estimation at Interactive\n  Rates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperMat: Physically Consistent PBR Material Estimation at Interactive\n  Rates"
                },
                "summary": "Decomposing physically-based materials from images into their constituent\nproperties remains challenging, particularly when maintaining both\ncomputational efficiency and physical consistency. While recent diffusion-based\napproaches have shown promise, they face substantial computational overhead due\nto multiple denoising steps and separate models for different material\nproperties. We present SuperMat, a single-step framework that achieves\nhigh-quality material decomposition with one-step inference. This enables\nend-to-end training with perceptual and re-render losses while decomposing\nalbedo, metallic, and roughness maps at millisecond-scale speeds. We further\nextend our framework to 3D objects through a UV refinement network, enabling\nconsistent material estimation across viewpoints while maintaining efficiency.\nExperiments demonstrate that SuperMat achieves state-of-the-art PBR material\ndecomposition quality while reducing inference time from seconds to\nmilliseconds per image, and completes PBR material estimation for 3D objects in\napproximately 3 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposing physically-based materials from images into their constituent\nproperties remains challenging, particularly when maintaining both\ncomputational efficiency and physical consistency. While recent diffusion-based\napproaches have shown promise, they face substantial computational overhead due\nto multiple denoising steps and separate models for different material\nproperties. We present SuperMat, a single-step framework that achieves\nhigh-quality material decomposition with one-step inference. This enables\nend-to-end training with perceptual and re-render losses while decomposing\nalbedo, metallic, and roughness maps at millisecond-scale speeds. We further\nextend our framework to 3D objects through a UV refinement network, enabling\nconsistent material estimation across viewpoints while maintaining efficiency.\nExperiments demonstrate that SuperMat achieves state-of-the-art PBR material\ndecomposition quality while reducing inference time from seconds to\nmilliseconds per image, and completes PBR material estimation for 3D objects in\napproximately 3 seconds."
                },
                "authors": [
                    {
                        "name": "Yijia Hong"
                    },
                    {
                        "name": "Yuan-Chen Guo"
                    },
                    {
                        "name": "Ran Yi"
                    },
                    {
                        "name": "Yulong Chen"
                    },
                    {
                        "name": "Yan-Pei Cao"
                    },
                    {
                        "name": "Lizhuang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lizhuang Ma"
                },
                "author": "Lizhuang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01372v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01372v2",
                "updated": "2024-11-26T15:14:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    14,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-05-02T15:14:16Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    15,
                    14,
                    16,
                    3,
                    123,
                    0
                ],
                "title": "Statistical algorithms for low-frequency diffusion data: A PDE approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical algorithms for low-frequency diffusion data: A PDE approach"
                },
                "summary": "We consider the problem of making nonparametric inference in a class of\nmulti-dimensional diffusions in divergence form, from low-frequency data.\nStatistical analysis in this setting is notoriously challenging due to the\nintractability of the likelihood and its gradient, and computational methods\nhave thus far largely resorted to expensive simulation-based techniques. In\nthis article, we propose a new computational approach which is motivated by PDE\ntheory and is built around the characterisation of the transition densities as\nsolutions of the associated heat (Fokker-Planck) equation. Employing optimal\nregularity results from the theory of parabolic PDEs, we prove a novel\ncharacterisation for the gradient of the likelihood. Using these developments,\nfor the nonlinear inverse problem of recovering the diffusivity, we then show\nthat the numerical evaluation of the likelihood and its gradient can be reduced\nto standard elliptic eigenvalue problems, solvable by powerful finite element\nmethods. This enables the efficient implementation of a large class of popular\nstatistical algorithms, including (i) preconditioned Crank-Nicolson and\nLangevin-type methods for posterior sampling, and (ii) gradient-based descent\noptimisation schemes to compute maximum likelihood and maximum-a-posteriori\nestimates. We showcase the effectiveness of these methods via extensive\nsimulation studies in a nonparametric Bayesian model with Gaussian process\npriors, in which both the proposed optimisation and sampling schemes provide\ngood numerical recovery. The reproducible code is available online at\nhttps://github.com/MattGiord/LF-Diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of making nonparametric inference in a class of\nmulti-dimensional diffusions in divergence form, from low-frequency data.\nStatistical analysis in this setting is notoriously challenging due to the\nintractability of the likelihood and its gradient, and computational methods\nhave thus far largely resorted to expensive simulation-based techniques. In\nthis article, we propose a new computational approach which is motivated by PDE\ntheory and is built around the characterisation of the transition densities as\nsolutions of the associated heat (Fokker-Planck) equation. Employing optimal\nregularity results from the theory of parabolic PDEs, we prove a novel\ncharacterisation for the gradient of the likelihood. Using these developments,\nfor the nonlinear inverse problem of recovering the diffusivity, we then show\nthat the numerical evaluation of the likelihood and its gradient can be reduced\nto standard elliptic eigenvalue problems, solvable by powerful finite element\nmethods. This enables the efficient implementation of a large class of popular\nstatistical algorithms, including (i) preconditioned Crank-Nicolson and\nLangevin-type methods for posterior sampling, and (ii) gradient-based descent\noptimisation schemes to compute maximum likelihood and maximum-a-posteriori\nestimates. We showcase the effectiveness of these methods via extensive\nsimulation studies in a nonparametric Bayesian model with Gaussian process\npriors, in which both the proposed optimisation and sampling schemes provide\ngood numerical recovery. The reproducible code is available online at\nhttps://github.com/MattGiord/LF-Diffusion."
                },
                "authors": [
                    {
                        "name": "Matteo Giordano"
                    },
                    {
                        "name": "Sven Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sven Wang"
                },
                "author": "Sven Wang",
                "arxiv_comment": "50 pages, 9 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01372v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01372v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 62M15, secondary 62F15, 62G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17501v1",
                "updated": "2024-11-26T15:13:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    13,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:13:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    13,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Inference Scaling $\\scriptsize\\mathtt{F}$Laws: The Limits of LLM\n  Resampling with Imperfect Verifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Scaling $\\scriptsize\\mathtt{F}$Laws: The Limits of LLM\n  Resampling with Imperfect Verifiers"
                },
                "summary": "Recent research has generated hope that inference scaling could allow weaker\nlanguage models to match or exceed the accuracy of stronger models, such as by\nrepeatedly sampling solutions to a coding problem until it passes unit tests.\nThe central thesis of this paper is that there is no free lunch for inference\nscaling: indefinite accuracy improvement through resampling can only be\nrealized if the \"verifier\" (in this case, a set of unit tests) is perfect. When\nthe verifier is imperfect, as it almost always is in domains such as reasoning\nor coding (for example, unit tests have imperfect coverage), there is a nonzero\nprobability of false positives: incorrect solutions that pass the verifier.\nResampling cannot decrease this probability, so it imposes an upper bound to\nthe accuracy of resampling-based inference scaling even with an infinite\ncompute budget. We find that there is a very strong correlation between the\nmodel's single-sample accuracy (i.e. accuracy without unit tests) and its false\npositive rate on coding benchmarks HumanEval and MBPP, whose unit tests have\nlimited coverage. Therefore, no amount of inference scaling of weaker models\ncan enable them to match the single-sample accuracy of a sufficiently strong\nmodel (Fig. 1a). When we consider that false positives have a negative utility\ncompared to abstaining from producing a solution, it bends the inference\nscaling curve further downward. Empirically, we find that the optimal number of\nsamples can be less than 10 under realistic assumptions (Fig. 1b). Finally, we\nshow that beyond accuracy, false positives may have other undesirable\nqualities, such as poor adherence to coding style conventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has generated hope that inference scaling could allow weaker\nlanguage models to match or exceed the accuracy of stronger models, such as by\nrepeatedly sampling solutions to a coding problem until it passes unit tests.\nThe central thesis of this paper is that there is no free lunch for inference\nscaling: indefinite accuracy improvement through resampling can only be\nrealized if the \"verifier\" (in this case, a set of unit tests) is perfect. When\nthe verifier is imperfect, as it almost always is in domains such as reasoning\nor coding (for example, unit tests have imperfect coverage), there is a nonzero\nprobability of false positives: incorrect solutions that pass the verifier.\nResampling cannot decrease this probability, so it imposes an upper bound to\nthe accuracy of resampling-based inference scaling even with an infinite\ncompute budget. We find that there is a very strong correlation between the\nmodel's single-sample accuracy (i.e. accuracy without unit tests) and its false\npositive rate on coding benchmarks HumanEval and MBPP, whose unit tests have\nlimited coverage. Therefore, no amount of inference scaling of weaker models\ncan enable them to match the single-sample accuracy of a sufficiently strong\nmodel (Fig. 1a). When we consider that false positives have a negative utility\ncompared to abstaining from producing a solution, it bends the inference\nscaling curve further downward. Empirically, we find that the optimal number of\nsamples can be less than 10 under realistic assumptions (Fig. 1b). Finally, we\nshow that beyond accuracy, false positives may have other undesirable\nqualities, such as poor adherence to coding style conventions."
                },
                "authors": [
                    {
                        "name": "Benedikt Stroebl"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Arvind Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Narayanan"
                },
                "author": "Arvind Narayanan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16316v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16316v2",
                "updated": "2024-11-26T14:31:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    31,
                    8,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-25T12:09:43Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    9,
                    43,
                    0,
                    330,
                    0
                ],
                "title": "Monocular Lane Detection Based on Deep Learning: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monocular Lane Detection Based on Deep Learning: A Survey"
                },
                "summary": "Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on deep learning have demonstrated superior performance and\nemerged as a key research direction in autonomous driving perception. The core\ndesign of these algorithmic frameworks can be summarized as follows: (1) Task\nparadigm, focusing on lane instance-level discrimination; (2) Lane modeling,\nrepresenting lanes as a set of learnable parameters in the neural network; (3)\nGlobal context supplementation, enhancing the detection of obscure lanes; (4)\nPerspective effect elimination, providing 3D lanes usable for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. For a\nrelatively fair comparison, in addition to comparing the performance of\nmainstream methods on different benchmarks, their inference speed is also\ninvestigated under a unified setting. Moreover, we present some extended works\non lane detection, including multi-task perception, video lane detection,\nonline high-definition map construction, and lane topology reasoning, to offer\nreaders a comprehensive roadmap for the evolution of lane detection. Finally,\nwe point out some potential future research directions in this field. We\nexhaustively collect the papers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on deep learning have demonstrated superior performance and\nemerged as a key research direction in autonomous driving perception. The core\ndesign of these algorithmic frameworks can be summarized as follows: (1) Task\nparadigm, focusing on lane instance-level discrimination; (2) Lane modeling,\nrepresenting lanes as a set of learnable parameters in the neural network; (3)\nGlobal context supplementation, enhancing the detection of obscure lanes; (4)\nPerspective effect elimination, providing 3D lanes usable for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. For a\nrelatively fair comparison, in addition to comparing the performance of\nmainstream methods on different benchmarks, their inference speed is also\ninvestigated under a unified setting. Moreover, we present some extended works\non lane detection, including multi-task perception, video lane detection,\nonline high-definition map construction, and lane topology reasoning, to offer\nreaders a comprehensive roadmap for the evolution of lane detection. Finally,\nwe point out some potential future research directions in this field. We\nexhaustively collect the papers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Haiyun Guo"
                    },
                    {
                        "name": "Kuan Zhu"
                    },
                    {
                        "name": "Bingke Zhu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Jianwu Fang"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16316v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16316v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19100v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19100v3",
                "updated": "2024-11-26T14:29:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    29,
                    59,
                    1,
                    331,
                    0
                ],
                "published": "2024-05-29T14:06:09Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    14,
                    6,
                    9,
                    2,
                    150,
                    0
                ],
                "title": "Enhancing Zero-Shot Facial Expression Recognition by LLM Knowledge\n  Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Zero-Shot Facial Expression Recognition by LLM Knowledge\n  Transfer"
                },
                "summary": "Current facial expression recognition (FER) models are often designed in a\nsupervised learning manner and thus are constrained by the lack of large-scale\nfacial expression images with high-quality annotations. Consequently, these\nmodels often fail to generalize well, performing poorly on unseen images in\ninference. Vision-language-based zero-shot models demonstrate a promising\npotential for addressing such challenges. However, these models lack\ntask-specific knowledge and therefore are not optimized for the nuances of\nrecognizing facial expressions. To bridge this gap, this work proposes a novel\nmethod, Exp-CLIP, to enhance zero-shot FER by transferring the task knowledge\nfrom large language models (LLMs). Specifically, based on the pre-trained\nvision-language encoders, we incorporate a projection head designed to map the\ninitial joint vision-language space into a space that captures representations\nof facial actions. To train this projection head for subsequent zero-shot\npredictions, we propose to align the projected visual representations with\ntask-specific semantic meanings derived from the LLM encoder, and the text\ninstruction-based strategy is employed to customize the LLM knowledge. Given\nunlabelled facial data and efficient training of the projection head, Exp-CLIP\nachieves superior zero-shot results to the CLIP models and several other large\nvision-language models (LVLMs) on seven in-the-wild FER datasets. The code and\npre-trained models are available at https://github.com/zengqunzhao/Exp-CLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current facial expression recognition (FER) models are often designed in a\nsupervised learning manner and thus are constrained by the lack of large-scale\nfacial expression images with high-quality annotations. Consequently, these\nmodels often fail to generalize well, performing poorly on unseen images in\ninference. Vision-language-based zero-shot models demonstrate a promising\npotential for addressing such challenges. However, these models lack\ntask-specific knowledge and therefore are not optimized for the nuances of\nrecognizing facial expressions. To bridge this gap, this work proposes a novel\nmethod, Exp-CLIP, to enhance zero-shot FER by transferring the task knowledge\nfrom large language models (LLMs). Specifically, based on the pre-trained\nvision-language encoders, we incorporate a projection head designed to map the\ninitial joint vision-language space into a space that captures representations\nof facial actions. To train this projection head for subsequent zero-shot\npredictions, we propose to align the projected visual representations with\ntask-specific semantic meanings derived from the LLM encoder, and the text\ninstruction-based strategy is employed to customize the LLM knowledge. Given\nunlabelled facial data and efficient training of the projection head, Exp-CLIP\nachieves superior zero-shot results to the CLIP models and several other large\nvision-language models (LVLMs) on seven in-the-wild FER datasets. The code and\npre-trained models are available at https://github.com/zengqunzhao/Exp-CLIP."
                },
                "authors": [
                    {
                        "name": "Zengqun Zhao"
                    },
                    {
                        "name": "Yu Cao"
                    },
                    {
                        "name": "Shaogang Gong"
                    },
                    {
                        "name": "Ioannis Patras"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Patras"
                },
                "author": "Ioannis Patras",
                "arxiv_comment": "Accepted at WACV 2025 (Camera-Ready Version)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19100v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19100v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17461v1",
                "updated": "2024-11-26T14:28:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    28,
                    25,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T14:28:25Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    28,
                    25,
                    1,
                    331,
                    0
                ],
                "title": "SoK: Decentralized AI (DeAI)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Decentralized AI (DeAI)"
                },
                "summary": "The centralization of Artificial Intelligence (AI) poses significant\nchallenges, including single points of failure, inherent biases, data privacy\nconcerns, and scalability issues. These problems are especially prevalent in\nclosed-source large language models (LLMs), where user data is collected and\nused without transparency. To mitigate these issues, blockchain-based\ndecentralized AI (DeAI) has emerged as a promising solution. DeAI combines the\nstrengths of both blockchain and AI technologies to enhance the transparency,\nsecurity, decentralization, and trustworthiness of AI systems. However, a\ncomprehensive understanding of state-of-the-art DeAI development, particularly\nfor active industry solutions, is still lacking. In this work, we present a\nSystematization of Knowledge (SoK) for blockchain-based DeAI solutions. We\npropose a taxonomy to classify existing DeAI protocols based on the model\nlifecycle. Based on this taxonomy, we provide a structured way to clarify the\nlandscape of DeAI protocols and identify their similarities and differences. We\nanalyze the functionalities of blockchain in DeAI, investigating how blockchain\nfeatures contribute to enhancing the security, transparency, and\ntrustworthiness of AI processes, while also ensuring fair incentives for AI\ndata and model contributors. In addition, we identify key insights and research\ngaps in developing DeAI protocols, highlighting several critical avenues for\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The centralization of Artificial Intelligence (AI) poses significant\nchallenges, including single points of failure, inherent biases, data privacy\nconcerns, and scalability issues. These problems are especially prevalent in\nclosed-source large language models (LLMs), where user data is collected and\nused without transparency. To mitigate these issues, blockchain-based\ndecentralized AI (DeAI) has emerged as a promising solution. DeAI combines the\nstrengths of both blockchain and AI technologies to enhance the transparency,\nsecurity, decentralization, and trustworthiness of AI systems. However, a\ncomprehensive understanding of state-of-the-art DeAI development, particularly\nfor active industry solutions, is still lacking. In this work, we present a\nSystematization of Knowledge (SoK) for blockchain-based DeAI solutions. We\npropose a taxonomy to classify existing DeAI protocols based on the model\nlifecycle. Based on this taxonomy, we provide a structured way to clarify the\nlandscape of DeAI protocols and identify their similarities and differences. We\nanalyze the functionalities of blockchain in DeAI, investigating how blockchain\nfeatures contribute to enhancing the security, transparency, and\ntrustworthiness of AI processes, while also ensuring fair incentives for AI\ndata and model contributors. In addition, we identify key insights and research\ngaps in developing DeAI protocols, highlighting several critical avenues for\nfuture research."
                },
                "authors": [
                    {
                        "name": "Zhipeng Wang"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Elizabeth Lui"
                    },
                    {
                        "name": "Vatsal Shah"
                    },
                    {
                        "name": "Xihan Xiong"
                    },
                    {
                        "name": "Jiahao Sun"
                    },
                    {
                        "name": "Davide Crapis"
                    },
                    {
                        "name": "William Knottenbelt"
                    }
                ],
                "author_detail": {
                    "name": "William Knottenbelt"
                },
                "author": "William Knottenbelt",
                "arxiv_comment": "This is a Systematization of Knowledge (SoK) for the rapidly evolving\n  field of Decentralized AI (DeAI). We welcome valuable comments, suggestions,\n  and collaboration to further refine and enhance this work. We hope our\n  contribution will help accelerate the advancement of DeAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v1",
                "updated": "2024-11-26T14:23:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15376v2",
                "updated": "2024-11-26T14:20:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    20,
                    19,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-22T23:34:36Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    23,
                    34,
                    36,
                    4,
                    327,
                    0
                ],
                "title": "Kaon Distribution Functions from Empirical Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kaon Distribution Functions from Empirical Information"
                },
                "summary": "Using available information from Drell-Yan data on pion and kaon structure\nfunctions, an approach is described which enables the development of pointwise\nprofiles for all pion and kaon parton distribution functions (DFs) without\nreference to theories of hadron structure. The key steps are construction of\nstructure-function-constrained probability-weighted ensembles of valence DF\nreplicas and use of an evolution scheme for parton DFs that is all-orders\nexact. The DFs obtained express qualitatively sound features of light-meson\nstructure, e.g., the effects of Higgs boson couplings into QCD and the size of\nheavy-quark momentum fractions in light hadrons. In order to improve the\nresults, additional and more precise data on the $u$-quark-in-kaon, $u^K$, to\n$u$-quark-in-pion, $u^\\pi$, DF ratio would be necessary. Of greater value would\nbe extraction of $u^K$ alone, thereby avoiding inference from the ratio:\ncurrently, the data-based form of $u^K$ is materially influenced by results for\n$u^\\pi$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using available information from Drell-Yan data on pion and kaon structure\nfunctions, an approach is described which enables the development of pointwise\nprofiles for all pion and kaon parton distribution functions (DFs) without\nreference to theories of hadron structure. The key steps are construction of\nstructure-function-constrained probability-weighted ensembles of valence DF\nreplicas and use of an evolution scheme for parton DFs that is all-orders\nexact. The DFs obtained express qualitatively sound features of light-meson\nstructure, e.g., the effects of Higgs boson couplings into QCD and the size of\nheavy-quark momentum fractions in light hadrons. In order to improve the\nresults, additional and more precise data on the $u$-quark-in-kaon, $u^K$, to\n$u$-quark-in-pion, $u^\\pi$, DF ratio would be necessary. Of greater value would\nbe extraction of $u^K$ alone, thereby avoiding inference from the ratio:\ncurrently, the data-based form of $u^K$ is materially influenced by results for\n$u^\\pi$."
                },
                "authors": [
                    {
                        "name": "Zhen-Ni Xu"
                    },
                    {
                        "name": "Daniele Binosi"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Khépani Raya"
                    },
                    {
                        "name": "Craig D. Roberts"
                    },
                    {
                        "name": "José Rodríguez-Quintero"
                    }
                ],
                "author_detail": {
                    "name": "José Rodríguez-Quintero"
                },
                "author": "José Rodríguez-Quintero",
                "arxiv_comment": "8 pages, 5 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-lat",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13549v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13549v3",
                "updated": "2024-11-26T14:15:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    15,
                    57,
                    1,
                    331,
                    0
                ],
                "published": "2023-06-23T15:21:52Z",
                "published_parsed": [
                    2023,
                    6,
                    23,
                    15,
                    21,
                    52,
                    4,
                    174,
                    0
                ],
                "title": "A Survey on Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Multimodal Large Language Models"
                },
                "summary": "Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has\nbeen a new rising research hotspot, which uses powerful Large Language Models\n(LLMs) as a brain to perform multimodal tasks. The surprising emergent\ncapabilities of MLLM, such as writing stories based on images and Optical\nCharacter Recognition (OCR)-free math reasoning, are rare in traditional\nmultimodal methods, suggesting a potential path to artificial general\nintelligence. To this end, both academia and industry have endeavored to\ndevelop MLLMs that can compete with or even outperform GPT-4V, pushing the\nlimit of research at a surprising speed. In this paper, we aim to trace and\nsummarize the recent progress of MLLMs. First of all, we present the basic\nformulation of MLLM and delineate its related concepts, including architecture,\ntraining strategy and data, as well as evaluation. Then, we introduce research\ntopics about how MLLMs can be extended to support more granularity, modalities,\nlanguages, and scenarios. We continue with multimodal hallucination and\nextended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT),\nand LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss\nexisting challenges and point out promising research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has\nbeen a new rising research hotspot, which uses powerful Large Language Models\n(LLMs) as a brain to perform multimodal tasks. The surprising emergent\ncapabilities of MLLM, such as writing stories based on images and Optical\nCharacter Recognition (OCR)-free math reasoning, are rare in traditional\nmultimodal methods, suggesting a potential path to artificial general\nintelligence. To this end, both academia and industry have endeavored to\ndevelop MLLMs that can compete with or even outperform GPT-4V, pushing the\nlimit of research at a surprising speed. In this paper, we aim to trace and\nsummarize the recent progress of MLLMs. First of all, we present the basic\nformulation of MLLM and delineate its related concepts, including architecture,\ntraining strategy and data, as well as evaluation. Then, we introduce research\ntopics about how MLLMs can be extended to support more granularity, modalities,\nlanguages, and scenarios. We continue with multimodal hallucination and\nextended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT),\nand LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss\nexisting challenges and point out promising research directions."
                },
                "authors": [
                    {
                        "name": "Shukang Yin"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_doi": "10.1093/nsr/nwae403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/nsr/nwae403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2306.13549v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13549v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 pages, 3 figures, 9 tables, accepted for publication in National\n  Science Review. Project\n  page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17453v1",
                "updated": "2024-11-26T14:12:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    12,
                    9,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T14:12:09Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    12,
                    9,
                    1,
                    331,
                    0
                ],
                "title": "PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient\n  Fine-Tuning"
                },
                "summary": "Fine-tuning is an essential process to improve the performance of Large\nLanguage Models (LLMs) in specific domains, with Parameter-Efficient\nFine-Tuning (PEFT) gaining popularity due to its capacity to reduce\ncomputational demands through the integration of low-rank adapters. These\nlightweight adapters, such as LoRA, can be shared and utilized on open-source\nplatforms. However, adversaries could exploit this mechanism to inject\nbackdoors into these adapters, resulting in malicious behaviors like incorrect\nor harmful outputs, which pose serious security risks to the community.\nUnfortunately, few of the current efforts concentrate on analyzing the backdoor\npatterns or detecting the backdoors in the adapters.\n  To fill this gap, we first construct (and will release) PADBench, a\ncomprehensive benchmark that contains 13,300 benign and backdoored adapters\nfine-tuned with various datasets, attack strategies, PEFT methods, and LLMs.\nMoreover, we propose PEFTGuard, the first backdoor detection framework against\nPEFT-based adapters. Extensive evaluation upon PADBench shows that PEFTGuard\noutperforms existing detection methods, achieving nearly perfect detection\naccuracy (100%) in most cases. Notably, PEFTGuard exhibits zero-shot\ntransferability on three aspects, including different attacks, PEFT methods,\nand adapter ranks. In addition, we consider various adaptive attacks to\ndemonstrate the high robustness of PEFTGuard. We further explore several\npossible backdoor mitigation defenses, finding fine-mixing to be the most\neffective method. We envision our benchmark and method can shed light on future\nLLM backdoor detection research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning is an essential process to improve the performance of Large\nLanguage Models (LLMs) in specific domains, with Parameter-Efficient\nFine-Tuning (PEFT) gaining popularity due to its capacity to reduce\ncomputational demands through the integration of low-rank adapters. These\nlightweight adapters, such as LoRA, can be shared and utilized on open-source\nplatforms. However, adversaries could exploit this mechanism to inject\nbackdoors into these adapters, resulting in malicious behaviors like incorrect\nor harmful outputs, which pose serious security risks to the community.\nUnfortunately, few of the current efforts concentrate on analyzing the backdoor\npatterns or detecting the backdoors in the adapters.\n  To fill this gap, we first construct (and will release) PADBench, a\ncomprehensive benchmark that contains 13,300 benign and backdoored adapters\nfine-tuned with various datasets, attack strategies, PEFT methods, and LLMs.\nMoreover, we propose PEFTGuard, the first backdoor detection framework against\nPEFT-based adapters. Extensive evaluation upon PADBench shows that PEFTGuard\noutperforms existing detection methods, achieving nearly perfect detection\naccuracy (100%) in most cases. Notably, PEFTGuard exhibits zero-shot\ntransferability on three aspects, including different attacks, PEFT methods,\nand adapter ranks. In addition, we consider various adaptive attacks to\ndemonstrate the high robustness of PEFTGuard. We further explore several\npossible backdoor mitigation defenses, finding fine-mixing to be the most\neffective method. We envision our benchmark and method can shed light on future\nLLM backdoor detection research."
                },
                "authors": [
                    {
                        "name": "Zhen Sun"
                    },
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Yule Liu"
                    },
                    {
                        "name": "Chenhao Lin"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Rongmao Chen"
                    },
                    {
                        "name": "Xingshuo Han"
                    },
                    {
                        "name": "Xinyi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xinyi Huang"
                },
                "author": "Xinyi Huang",
                "arxiv_comment": "20 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17451v1",
                "updated": "2024-11-26T14:08:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    8,
                    34,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T14:08:34Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    8,
                    34,
                    1,
                    331,
                    0
                ],
                "title": "VLRewardBench: A Challenging Benchmark for Vision-Language Generative\n  Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLRewardBench: A Challenging Benchmark for Vision-Language Generative\n  Reward Models"
                },
                "summary": "Vision-language generative reward models (VL-GenRMs) play a crucial role in\naligning and evaluating multimodal AI systems, yet their own evaluation remains\nunder-explored. Current assessment methods primarily rely on AI-annotated\npreference labels from traditional VL tasks, which can introduce biases and\noften fail to effectively challenge state-of-the-art models. To address these\nlimitations, we introduce VL-RewardBench, a comprehensive benchmark spanning\ngeneral multimodal queries, visual hallucination detection, and complex\nreasoning tasks. Through our AI-assisted annotation pipeline combining sample\nselection with human verification, we curate 1,250 high-quality examples\nspecifically designed to probe model limitations. Comprehensive evaluation\nacross 16 leading large vision-language models, demonstrates VL-RewardBench's\neffectiveness as a challenging testbed, where even GPT-4o achieves only 65.4%\naccuracy, and state-of-the-art open-source models such as Qwen2-VL-72B,\nstruggle to surpass random-guessing. Importantly, performance on VL-RewardBench\nstrongly correlates (Pearson's r > 0.9) with MMMU-Pro accuracy using Best-of-N\nsampling with VL-GenRMs. Analysis experiments uncover three critical insights\nfor improving VL-GenRMs: (i) models predominantly fail at basic visual\nperception tasks rather than reasoning tasks; (ii) inference-time scaling\nbenefits vary dramatically by model capacity; and (iii) training VL-GenRMs to\nlearn to judge substantially boosts judgment capability (+14.7% accuracy for a\n7B VL-GenRM). We believe VL-RewardBench along with the experimental insights\nwill become a valuable resource for advancing VL-GenRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language generative reward models (VL-GenRMs) play a crucial role in\naligning and evaluating multimodal AI systems, yet their own evaluation remains\nunder-explored. Current assessment methods primarily rely on AI-annotated\npreference labels from traditional VL tasks, which can introduce biases and\noften fail to effectively challenge state-of-the-art models. To address these\nlimitations, we introduce VL-RewardBench, a comprehensive benchmark spanning\ngeneral multimodal queries, visual hallucination detection, and complex\nreasoning tasks. Through our AI-assisted annotation pipeline combining sample\nselection with human verification, we curate 1,250 high-quality examples\nspecifically designed to probe model limitations. Comprehensive evaluation\nacross 16 leading large vision-language models, demonstrates VL-RewardBench's\neffectiveness as a challenging testbed, where even GPT-4o achieves only 65.4%\naccuracy, and state-of-the-art open-source models such as Qwen2-VL-72B,\nstruggle to surpass random-guessing. Importantly, performance on VL-RewardBench\nstrongly correlates (Pearson's r > 0.9) with MMMU-Pro accuracy using Best-of-N\nsampling with VL-GenRMs. Analysis experiments uncover three critical insights\nfor improving VL-GenRMs: (i) models predominantly fail at basic visual\nperception tasks rather than reasoning tasks; (ii) inference-time scaling\nbenefits vary dramatically by model capacity; and (iii) training VL-GenRMs to\nlearn to judge substantially boosts judgment capability (+14.7% accuracy for a\n7B VL-GenRM). We believe VL-RewardBench along with the experimental insights\nwill become a valuable resource for advancing VL-GenRMs."
                },
                "authors": [
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Yuancheng Wei"
                    },
                    {
                        "name": "Zhihui Xie"
                    },
                    {
                        "name": "Xuqing Yang"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Chenxin An"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Sujian Li"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "arxiv_comment": "Project page: https://vl-rewardbench.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00073v2",
                "updated": "2024-11-26T13:55:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    55,
                    29,
                    1,
                    331,
                    0
                ],
                "published": "2024-10-31T16:22:26Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    22,
                    26,
                    3,
                    305,
                    0
                ],
                "title": "RSL-SQL: Robust Schema Linking in Text-to-SQL Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RSL-SQL: Robust Schema Linking in Text-to-SQL Generation"
                },
                "summary": "Text-to-SQL generation aims to translate natural language questions into SQL\nstatements. In Text-to-SQL based on large language models, schema linking is a\nwidely adopted strategy to streamline the input for LLMs by selecting only\nrelevant schema elements, therefore reducing noise and computational overhead.\nHowever, schema linking faces risks that require caution, including the\npotential omission of necessary elements and disruption of database structural\nintegrity. To address these challenges, we propose a novel framework called\nRSL-SQL that combines bidirectional schema linking, contextual information\naugmentation, binary selection strategy, and multi-turn self-correction. We\nimprove the recall of pattern linking using forward and backward pruning\nmethods, achieving a strict recall of 94% while reducing the number of input\ncolumns by 83%. Furthermore, it hedges the risk by voting between a full mode\nand a simplified mode enhanced with contextual information. Experiments on the\nBIRD and Spider benchmarks demonstrate that our approach achieves SOTA\nexecution accuracy among open-source solutions, with 67.2% on BIRD and 87.9% on\nSpider using GPT-4o. Furthermore, our approach outperforms a series of GPT-4\nbased Text-to-SQL systems when adopting DeepSeek (much cheaper) with same\nintact prompts. Extensive analysis and ablation studies confirm the\neffectiveness of each component in our framework. The codes are available at\nhttps://github.com/Laqcce-cao/RSL-SQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL generation aims to translate natural language questions into SQL\nstatements. In Text-to-SQL based on large language models, schema linking is a\nwidely adopted strategy to streamline the input for LLMs by selecting only\nrelevant schema elements, therefore reducing noise and computational overhead.\nHowever, schema linking faces risks that require caution, including the\npotential omission of necessary elements and disruption of database structural\nintegrity. To address these challenges, we propose a novel framework called\nRSL-SQL that combines bidirectional schema linking, contextual information\naugmentation, binary selection strategy, and multi-turn self-correction. We\nimprove the recall of pattern linking using forward and backward pruning\nmethods, achieving a strict recall of 94% while reducing the number of input\ncolumns by 83%. Furthermore, it hedges the risk by voting between a full mode\nand a simplified mode enhanced with contextual information. Experiments on the\nBIRD and Spider benchmarks demonstrate that our approach achieves SOTA\nexecution accuracy among open-source solutions, with 67.2% on BIRD and 87.9% on\nSpider using GPT-4o. Furthermore, our approach outperforms a series of GPT-4\nbased Text-to-SQL systems when adopting DeepSeek (much cheaper) with same\nintact prompts. Extensive analysis and ablation studies confirm the\neffectiveness of each component in our framework. The codes are available at\nhttps://github.com/Laqcce-cao/RSL-SQL."
                },
                "authors": [
                    {
                        "name": "Zhenbiao Cao"
                    },
                    {
                        "name": "Yuanlei Zheng"
                    },
                    {
                        "name": "Zhihao Fan"
                    },
                    {
                        "name": "Xiaojin Zhang"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17437v1",
                "updated": "2024-11-26T13:51:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    51,
                    48,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T13:51:48Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    51,
                    48,
                    1,
                    331,
                    0
                ],
                "title": "\"Stupid robot, I want to speak to a human!\" User Frustration Detection\n  in Task-Oriented Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Stupid robot, I want to speak to a human!\" User Frustration Detection\n  in Task-Oriented Dialog Systems"
                },
                "summary": "Detecting user frustration in modern-day task-oriented dialog (TOD) systems\nis imperative for maintaining overall user satisfaction, engagement, and\nretention. However, most recent research is focused on sentiment and emotion\ndetection in academic settings, thus failing to fully encapsulate implications\nof real-world user data. To mitigate this gap, in this work, we focus on user\nfrustration in a deployed TOD system, assessing the feasibility of\nout-of-the-box solutions for user frustration detection. Specifically, we\ncompare the performance of our deployed keyword-based approach, open-source\napproaches to sentiment analysis, dialog breakdown detection methods, and\nemerging in-context learning LLM-based detection. Our analysis highlights the\nlimitations of open-source methods for real-world frustration detection, while\ndemonstrating the superior performance of the LLM-based approach, achieving a\n16\\% relative improvement in F1 score on an internal benchmark. Finally, we\nanalyze advantages and limitations of our methods and provide an insight into\nuser frustration detection task for industry practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting user frustration in modern-day task-oriented dialog (TOD) systems\nis imperative for maintaining overall user satisfaction, engagement, and\nretention. However, most recent research is focused on sentiment and emotion\ndetection in academic settings, thus failing to fully encapsulate implications\nof real-world user data. To mitigate this gap, in this work, we focus on user\nfrustration in a deployed TOD system, assessing the feasibility of\nout-of-the-box solutions for user frustration detection. Specifically, we\ncompare the performance of our deployed keyword-based approach, open-source\napproaches to sentiment analysis, dialog breakdown detection methods, and\nemerging in-context learning LLM-based detection. Our analysis highlights the\nlimitations of open-source methods for real-world frustration detection, while\ndemonstrating the superior performance of the LLM-based approach, achieving a\n16\\% relative improvement in F1 score on an internal benchmark. Finally, we\nanalyze advantages and limitations of our methods and provide an insight into\nuser frustration detection task for industry practitioners."
                },
                "authors": [
                    {
                        "name": "Mireia Hernandez Caralt"
                    },
                    {
                        "name": "Ivan Sekulić"
                    },
                    {
                        "name": "Filip Carević"
                    },
                    {
                        "name": "Nghia Khau"
                    },
                    {
                        "name": "Diana Nicoleta Popa"
                    },
                    {
                        "name": "Bruna Guedes"
                    },
                    {
                        "name": "Victor Guimarães"
                    },
                    {
                        "name": "Zeyu Yang"
                    },
                    {
                        "name": "Andre Manso"
                    },
                    {
                        "name": "Meghana Reddy"
                    },
                    {
                        "name": "Paolo Rosso"
                    },
                    {
                        "name": "Roland Mathis"
                    }
                ],
                "author_detail": {
                    "name": "Roland Mathis"
                },
                "author": "Roland Mathis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17431v1",
                "updated": "2024-11-26T13:39:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    39,
                    52,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T13:39:52Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    39,
                    52,
                    1,
                    331,
                    0
                ],
                "title": "Noise Adaptor: Enhancing Low-Latency Spiking Neural Networks through\n  Noise-Injected Low-Bit ANN Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise Adaptor: Enhancing Low-Latency Spiking Neural Networks through\n  Noise-Injected Low-Bit ANN Conversion"
                },
                "summary": "We present Noise Adaptor, a novel method for constructing competitive\nlow-latency spiking neural networks (SNNs) by converting noise-injected,\nlow-bit artificial neural networks (ANNs). This approach builds on existing\nANN-to-SNN conversion techniques but offers several key improvements: (1) By\ninjecting noise during quantized ANN training, Noise Adaptor better accounts\nfor the dynamic differences between ANNs and SNNs, significantly enhancing SNN\naccuracy. (2) Unlike previous methods, Noise Adaptor does not require the\napplication of run-time noise correction techniques in SNNs, thereby avoiding\nmodifications to the spiking neuron model and control flow during inference.\n(3) Our method extends the capability of handling deeper architectures,\nachieving successful conversions of activation-quantized ResNet-101 and\nResNet-152 to SNNs. We demonstrate the effectiveness of our method on CIFAR-10\nand ImageNet, achieving competitive performance. The code will be made\navailable as open-source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Noise Adaptor, a novel method for constructing competitive\nlow-latency spiking neural networks (SNNs) by converting noise-injected,\nlow-bit artificial neural networks (ANNs). This approach builds on existing\nANN-to-SNN conversion techniques but offers several key improvements: (1) By\ninjecting noise during quantized ANN training, Noise Adaptor better accounts\nfor the dynamic differences between ANNs and SNNs, significantly enhancing SNN\naccuracy. (2) Unlike previous methods, Noise Adaptor does not require the\napplication of run-time noise correction techniques in SNNs, thereby avoiding\nmodifications to the spiking neuron model and control flow during inference.\n(3) Our method extends the capability of handling deeper architectures,\nachieving successful conversions of activation-quantized ResNet-101 and\nResNet-152 to SNNs. We demonstrate the effectiveness of our method on CIFAR-10\nand ImageNet, achieving competitive performance. The code will be made\navailable as open-source."
                },
                "authors": [
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Bipin. Rajendran"
                    }
                ],
                "author_detail": {
                    "name": "Bipin. Rajendran"
                },
                "author": "Bipin. Rajendran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10511v2",
                "updated": "2024-11-26T13:38:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    38,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-08-20T03:20:13Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    3,
                    20,
                    13,
                    1,
                    233,
                    0
                ],
                "title": "Single-cell Curriculum Learning-based Deep Graph Embedding Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-cell Curriculum Learning-based Deep Graph Embedding Clustering"
                },
                "summary": "The swift advancement of single-cell RNA sequencing (scRNA-seq) technologies\nenables the investigation of cellular-level tissue heterogeneity. Cell\nannotation significantly contributes to the extensive downstream analysis of\nscRNA-seq data. However, The analysis of scRNA-seq for biological inference\npresents challenges owing to its intricate and indeterminate data distribution,\ncharacterized by a substantial volume and a high frequency of dropout events.\nFurthermore, the quality of training samples varies greatly, and the\nperformance of the popular scRNA-seq data clustering solution GNN could be\nharmed by two types of low-quality training nodes: 1) nodes on the boundary; 2)\nnodes that contribute little additional information to the graph. To address\nthese problems, we propose a single-cell curriculum learning-based deep graph\nembedding clustering (scCLG). We first propose a Chebyshev graph convolutional\nautoencoder with multi-decoder (ChebAE) that combines three optimization\nobjectives corresponding to three decoders, including topology reconstruction\nloss of cell graphs, zero-inflated negative binomial (ZINB) loss, and\nclustering loss, to learn cell-cell topology representation. Meanwhile, we\nemploy a selective training strategy to train GNN based on the features and\nentropy of nodes and prune the difficult nodes based on the difficulty scores\nto keep the high-quality graph. Empirical results on a variety of gene\nexpression datasets show that our model outperforms state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The swift advancement of single-cell RNA sequencing (scRNA-seq) technologies\nenables the investigation of cellular-level tissue heterogeneity. Cell\nannotation significantly contributes to the extensive downstream analysis of\nscRNA-seq data. However, The analysis of scRNA-seq for biological inference\npresents challenges owing to its intricate and indeterminate data distribution,\ncharacterized by a substantial volume and a high frequency of dropout events.\nFurthermore, the quality of training samples varies greatly, and the\nperformance of the popular scRNA-seq data clustering solution GNN could be\nharmed by two types of low-quality training nodes: 1) nodes on the boundary; 2)\nnodes that contribute little additional information to the graph. To address\nthese problems, we propose a single-cell curriculum learning-based deep graph\nembedding clustering (scCLG). We first propose a Chebyshev graph convolutional\nautoencoder with multi-decoder (ChebAE) that combines three optimization\nobjectives corresponding to three decoders, including topology reconstruction\nloss of cell graphs, zero-inflated negative binomial (ZINB) loss, and\nclustering loss, to learn cell-cell topology representation. Meanwhile, we\nemploy a selective training strategy to train GNN based on the features and\nentropy of nodes and prune the difficult nodes based on the difficulty scores\nto keep the high-quality graph. Empirical results on a variety of gene\nexpression datasets show that our model outperforms state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Xinpeng Ling"
                    },
                    {
                        "name": "Zhiyu Sun"
                    },
                    {
                        "name": "Kuncan Wang"
                    },
                    {
                        "name": "Zhili Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhili Chen"
                },
                "author": "Zhili Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12736v3",
                "updated": "2024-11-26T13:35:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    35,
                    5,
                    1,
                    331,
                    0
                ],
                "published": "2024-04-19T09:29:53Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    9,
                    29,
                    53,
                    4,
                    110,
                    0
                ],
                "title": "Large Language Model Supply Chain: A Research Agenda",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Supply Chain: A Research Agenda"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized\nartificial intelligence, introducing unprecedented capabilities in natural\nlanguage processing and multimodal content generation. However, the increasing\ncomplexity and scale of these models have given rise to a multifaceted supply\nchain that presents unique challenges across infrastructure, foundation models,\nand downstream applications. This paper provides the first comprehensive\nresearch agenda of the LLM supply chain, offering a structured approach to\nidentify critical challenges and opportunities through the dual lenses of\nsoftware engineering (SE) and security & privacy (S\\&P). We begin by\nestablishing a clear definition of the LLM supply chain, encompassing its\ncomponents and dependencies. We then analyze each layer of the supply chain,\npresenting a vision for robust and secure LLM development, reviewing the\ncurrent state of practices and technologies, and identifying key challenges and\nresearch opportunities. This work aims to bridge the existing research gap in\nsystematically understanding the multifaceted issues within the LLM supply\nchain, offering valuable insights to guide future efforts in this rapidly\nevolving domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized\nartificial intelligence, introducing unprecedented capabilities in natural\nlanguage processing and multimodal content generation. However, the increasing\ncomplexity and scale of these models have given rise to a multifaceted supply\nchain that presents unique challenges across infrastructure, foundation models,\nand downstream applications. This paper provides the first comprehensive\nresearch agenda of the LLM supply chain, offering a structured approach to\nidentify critical challenges and opportunities through the dual lenses of\nsoftware engineering (SE) and security & privacy (S\\&P). We begin by\nestablishing a clear definition of the LLM supply chain, encompassing its\ncomponents and dependencies. We then analyze each layer of the supply chain,\npresenting a vision for robust and secure LLM development, reviewing the\ncurrent state of practices and technologies, and identifying key challenges and\nresearch opportunities. This work aims to bridge the existing research gap in\nsystematically understanding the multifaceted issues within the LLM supply\nchain, offering valuable insights to guide future efforts in this rapidly\nevolving domain."
                },
                "authors": [
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM) Special Issue: 2030 Software Engineering Roadmap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10779v2",
                "updated": "2024-11-26T13:32:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    32,
                    22,
                    1,
                    331,
                    0
                ],
                "published": "2024-02-16T16:02:33Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    16,
                    2,
                    33,
                    4,
                    47,
                    0
                ],
                "title": "A Condensed Transition Graph Framework for Zero-shot Link Prediction\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Condensed Transition Graph Framework for Zero-shot Link Prediction\n  with Large Language Models"
                },
                "summary": "Zero-shot link prediction (ZSLP) on knowledge graphs aims at automatically\nidentifying relations between given entities. Existing methods primarily employ\nauxiliary information to predict tail entity given head entity and its\nrelation, yet face challenges due to the occasional unavailability of such\ndetailed information and the inherent simplicity of predicting tail entities\nbased on semantic similarities. Even though Large Language Models (LLMs) offer\na promising solution to predict unobserved relations between the head and tail\nentity in a zero-shot manner, their performance is still restricted due to the\ninability to leverage all the (exponentially many) paths' information between\ntwo entities, which are critical in collectively indicating their relation\ntypes. To address this, in this work, we introduce a Condensed Transition Graph\nFramework for Zero-Shot Link Prediction (CTLP), which encodes all the paths'\ninformation in linear time complexity to predict unseen relations between\nentities, attaining both efficiency and information preservation. Specifically,\nwe design a condensed transition graph encoder with theoretical guarantees on\nits coverage, expressiveness, and efficiency. It is learned by a transition\ngraph contrastive learning strategy. Subsequently, we design a soft instruction\ntuning to learn and map the all-path embedding to the input of LLMs.\nExperimental results show that our proposed CTLP method achieves\nstate-of-the-art performance on three standard ZSLP datasets",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot link prediction (ZSLP) on knowledge graphs aims at automatically\nidentifying relations between given entities. Existing methods primarily employ\nauxiliary information to predict tail entity given head entity and its\nrelation, yet face challenges due to the occasional unavailability of such\ndetailed information and the inherent simplicity of predicting tail entities\nbased on semantic similarities. Even though Large Language Models (LLMs) offer\na promising solution to predict unobserved relations between the head and tail\nentity in a zero-shot manner, their performance is still restricted due to the\ninability to leverage all the (exponentially many) paths' information between\ntwo entities, which are critical in collectively indicating their relation\ntypes. To address this, in this work, we introduce a Condensed Transition Graph\nFramework for Zero-Shot Link Prediction (CTLP), which encodes all the paths'\ninformation in linear time complexity to predict unseen relations between\nentities, attaining both efficiency and information preservation. Specifically,\nwe design a condensed transition graph encoder with theoretical guarantees on\nits coverage, expressiveness, and efficiency. It is learned by a transition\ngraph contrastive learning strategy. Subsequently, we design a soft instruction\ntuning to learn and map the all-path embedding to the input of LLMs.\nExperimental results show that our proposed CTLP method achieves\nstate-of-the-art performance on three standard ZSLP datasets"
                },
                "authors": [
                    {
                        "name": "Mingchen Li"
                    },
                    {
                        "name": "Chen Ling"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Liang Zhao"
                },
                "author": "Liang Zhao",
                "arxiv_comment": "Published as a conference paper at ICDM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11581v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11581v4",
                "updated": "2024-11-26T13:22:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    22,
                    19,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-18T13:57:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    57,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "OASIS: Open Agent Social Interaction Simulations with One Million Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS: Open Agent Social Interaction Simulations with One Million Agents"
                },
                "summary": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments."
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Zaibin Zhang"
                    },
                    {
                        "name": "Zirui Zheng"
                    },
                    {
                        "name": "Yuxian Jiang"
                    },
                    {
                        "name": "Ziyue Gan"
                    },
                    {
                        "name": "Zhiyu Wang"
                    },
                    {
                        "name": "Zijian Ling"
                    },
                    {
                        "name": "Jinsong Chen"
                    },
                    {
                        "name": "Martz Ma"
                    },
                    {
                        "name": "Bowen Dong"
                    },
                    {
                        "name": "Prateek Gupta"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Guohao Li"
                    },
                    {
                        "name": "Xu Jia"
                    },
                    {
                        "name": "Lijun Wang"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Chaochao Lu"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11581v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11581v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13990v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13990v3",
                "updated": "2024-11-26T13:21:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    21,
                    44,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-21T10:00:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    0,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Repository-level Code Translation Benchmark Targeting Rust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level Code Translation Benchmark Targeting Rust"
                },
                "summary": "Recent advances in large language models (LLMs) have shown significant\ncapabilities in code translation, often evaluated using benchmarks like\nCodeTransOcean. However, these evaluations typically focus on simple,\nfunction-level translations without considering dependencies, which does not\nreflect the complexities of real-world software development. Further, their\neffectiveness in translating to newer, lower-resource languages like Rust in\nrealistic scenarios is still under-explored. To address this gap, we introduce\nfirst repository-level code translation benchmark comprising 375 tasks\ntargeting Rust, complete with relevant dependencies. Using this benchmark, we\nstudy four state-of-the-art LLMs, analyzing their erroneous outputs to\nunderstand their performance in more complex translation scenarios. Our\nfindings reveal that LLMs exhibit substantially worse performance (41.5%-56.2%\nPass@1 drop of GPT-4) on repository-level translations compared to simpler\ntasks, highlighting limitations in existing evaluation methods. The model that\nperformed the best is Claude-3.5, demonstrating the strongest translation\ncapabilities in both basic functionality accuracy and several relevant\nadditional abilities. Additionally, we discover that LLMs struggle with\nidentifying language differences in complex tasks, and that increased\ndependencies correlate with greater translation difficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown significant\ncapabilities in code translation, often evaluated using benchmarks like\nCodeTransOcean. However, these evaluations typically focus on simple,\nfunction-level translations without considering dependencies, which does not\nreflect the complexities of real-world software development. Further, their\neffectiveness in translating to newer, lower-resource languages like Rust in\nrealistic scenarios is still under-explored. To address this gap, we introduce\nfirst repository-level code translation benchmark comprising 375 tasks\ntargeting Rust, complete with relevant dependencies. Using this benchmark, we\nstudy four state-of-the-art LLMs, analyzing their erroneous outputs to\nunderstand their performance in more complex translation scenarios. Our\nfindings reveal that LLMs exhibit substantially worse performance (41.5%-56.2%\nPass@1 drop of GPT-4) on repository-level translations compared to simpler\ntasks, highlighting limitations in existing evaluation methods. The model that\nperformed the best is Claude-3.5, demonstrating the strongest translation\ncapabilities in both basic functionality accuracy and several relevant\nadditional abilities. Additionally, we discover that LLMs struggle with\nidentifying language differences in complex tasks, and that increased\ndependencies correlate with greater translation difficulty."
                },
                "authors": [
                    {
                        "name": "Guangsheng Ou"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13990v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13990v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17406v1",
                "updated": "2024-11-26T13:09:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    9,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T13:09:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    9,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "CoA: Chain-of-Action for Generative Semantic Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoA: Chain-of-Action for Generative Semantic Labels"
                },
                "summary": "Recent advances in vision-language models (VLM) have demonstrated remarkable\ncapability in image classification. These VLMs leverage a predefined set of\ncategories to construct text prompts for zero-shot reasoning. However, in more\nopen-ended domains like autonomous driving, using a predefined set of labels\nbecomes impractical, as the semantic label space is unknown and constantly\nevolving. Additionally, fixed embedding text prompts often tend to predict a\nsingle label (while in reality, multiple labels commonly exist per image). In\nthis paper, we introduce CoA, an innovative Chain-of-Action (CoA) method that\ngenerates labels aligned with all contextually relevant features of an image.\nCoA is designed based on the observation that enriched and valuable contextual\ninformation improves generative performance during inference. Traditional\nvision-language models tend to output singular and redundant responses.\nTherefore, we employ a tailored CoA to alleviate this problem. We first break\ndown the generative labeling task into detailed actions and construct an CoA\nleading to the final generative objective. Each action extracts and merges key\ninformation from the previous action and passes the enriched information as\ncontext to the next action, ultimately improving the VLM in generating\ncomprehensive and accurate semantic labels. We assess the effectiveness of CoA\nthrough comprehensive evaluations on widely-used benchmark datasets and the\nresults demonstrate significant improvements across key performance metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language models (VLM) have demonstrated remarkable\ncapability in image classification. These VLMs leverage a predefined set of\ncategories to construct text prompts for zero-shot reasoning. However, in more\nopen-ended domains like autonomous driving, using a predefined set of labels\nbecomes impractical, as the semantic label space is unknown and constantly\nevolving. Additionally, fixed embedding text prompts often tend to predict a\nsingle label (while in reality, multiple labels commonly exist per image). In\nthis paper, we introduce CoA, an innovative Chain-of-Action (CoA) method that\ngenerates labels aligned with all contextually relevant features of an image.\nCoA is designed based on the observation that enriched and valuable contextual\ninformation improves generative performance during inference. Traditional\nvision-language models tend to output singular and redundant responses.\nTherefore, we employ a tailored CoA to alleviate this problem. We first break\ndown the generative labeling task into detailed actions and construct an CoA\nleading to the final generative objective. Each action extracts and merges key\ninformation from the previous action and passes the enriched information as\ncontext to the next action, ultimately improving the VLM in generating\ncomprehensive and accurate semantic labels. We assess the effectiveness of CoA\nthrough comprehensive evaluations on widely-used benchmark datasets and the\nresults demonstrate significant improvements across key performance metrics."
                },
                "authors": [
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Zhongnian Li"
                    },
                    {
                        "name": "Peng Ying"
                    },
                    {
                        "name": "Xinzheng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinzheng Xu"
                },
                "author": "Xinzheng Xu",
                "arxiv_comment": "15 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17404v1",
                "updated": "2024-11-26T13:05:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    5,
                    53,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T13:05:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    5,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving"
                },
                "summary": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source operations research datasets lack detailed annotations of\nthe modeling process, such as variable definitions, focusing solely on\nobjective values, which hinders reinforcement learning applications. To address\nthis, we release the StructuredOR dataset, annotated with comprehensive labels\nthat capture the complete mathematical modeling process. We further propose\nBPP-Search, a algorithm that integrates reinforcement learning into a\ntree-of-thought structure using Beam search, a Process reward model, and a\npairwise Preference algorithm. This approach enables efficient exploration of\ntree structures, avoiding exhaustive search while improving accuracy. Extensive\nexperiments on StructuredOR, NL4OPT, and MAMO-ComplexLP datasets show that\nBPP-Search significantly outperforms state-of-the-art methods, including\nChain-of-Thought, Self-Consistency, and Tree-of-Thought. In tree-based\nreasoning, BPP-Search also surpasses Process Reward Model combined with Greedy\nor Beam Search, demonstrating superior accuracy and efficiency, and enabling\nfaster retrieval of correct solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source operations research datasets lack detailed annotations of\nthe modeling process, such as variable definitions, focusing solely on\nobjective values, which hinders reinforcement learning applications. To address\nthis, we release the StructuredOR dataset, annotated with comprehensive labels\nthat capture the complete mathematical modeling process. We further propose\nBPP-Search, a algorithm that integrates reinforcement learning into a\ntree-of-thought structure using Beam search, a Process reward model, and a\npairwise Preference algorithm. This approach enables efficient exploration of\ntree structures, avoiding exhaustive search while improving accuracy. Extensive\nexperiments on StructuredOR, NL4OPT, and MAMO-ComplexLP datasets show that\nBPP-Search significantly outperforms state-of-the-art methods, including\nChain-of-Thought, Self-Consistency, and Tree-of-Thought. In tree-based\nreasoning, BPP-Search also surpasses Process Reward Model combined with Greedy\nor Beam Search, demonstrating superior accuracy and efficiency, and enabling\nfaster retrieval of correct solutions."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Wing-Yin Yu"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Zehua Liu"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Hailei Gong"
                    },
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Ruifeng She"
                    },
                    {
                        "name": "Fangzhou Zhu"
                    },
                    {
                        "name": "Tao Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhong"
                },
                "author": "Tao Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17401v1",
                "updated": "2024-11-26T13:03:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    3,
                    49,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T13:03:49Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    3,
                    49,
                    1,
                    331,
                    0
                ],
                "title": "One Mind, Many Tongues: A Deep Dive into Language-Agnostic Knowledge\n  Neurons in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Mind, Many Tongues: A Deep Dive into Language-Agnostic Knowledge\n  Neurons in Large Language Models"
                },
                "summary": "Large language models (LLMs) have learned vast amounts of factual knowledge\nthrough self-supervised pre-training on large-scale corpora. Meanwhile, LLMs\nhave also demonstrated excellent multilingual capabilities, which can express\nthe learned knowledge in multiple languages. However, the knowledge storage\nmechanism in LLMs still remains mysterious. Some researchers attempt to\ndemystify the factual knowledge in LLMs from the perspective of knowledge\nneurons, and subsequently discover language-agnostic knowledge neurons that\nstore factual knowledge in a form that transcends language barriers. However,\nthe preliminary finding suffers from two limitations: 1) High Uncertainty in\nLocalization Results. Existing study only uses a prompt-based probe to localize\nknowledge neurons for each fact, while LLMs cannot provide consistent answers\nfor semantically equivalent queries. Thus, it leads to inaccurate localization\nresults with high uncertainty. 2) Lack of Analysis in More Languages. The study\nonly analyzes language-agnostic knowledge neurons on English and Chinese data,\nwithout exploring more language families and languages. Naturally, it limits\nthe generalizability of the findings. To address aforementioned problems, we\nfirst construct a new benchmark called Rephrased Multilingual LAMA (RML-LAMA),\nwhich contains high-quality cloze-style multilingual parallel queries for each\nfact. Then, we propose a novel method named Multilingual Integrated Gradients\nwith Uncertainty Estimation (MATRICE), which quantifies the uncertainty across\nqueries and languages during knowledge localization. Extensive experiments show\nthat our method can accurately localize language-agnostic knowledge neurons. We\nalso further investigate the role of language-agnostic knowledge neurons in\ncross-lingual knowledge editing, knowledge enhancement and new knowledge\ninjection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have learned vast amounts of factual knowledge\nthrough self-supervised pre-training on large-scale corpora. Meanwhile, LLMs\nhave also demonstrated excellent multilingual capabilities, which can express\nthe learned knowledge in multiple languages. However, the knowledge storage\nmechanism in LLMs still remains mysterious. Some researchers attempt to\ndemystify the factual knowledge in LLMs from the perspective of knowledge\nneurons, and subsequently discover language-agnostic knowledge neurons that\nstore factual knowledge in a form that transcends language barriers. However,\nthe preliminary finding suffers from two limitations: 1) High Uncertainty in\nLocalization Results. Existing study only uses a prompt-based probe to localize\nknowledge neurons for each fact, while LLMs cannot provide consistent answers\nfor semantically equivalent queries. Thus, it leads to inaccurate localization\nresults with high uncertainty. 2) Lack of Analysis in More Languages. The study\nonly analyzes language-agnostic knowledge neurons on English and Chinese data,\nwithout exploring more language families and languages. Naturally, it limits\nthe generalizability of the findings. To address aforementioned problems, we\nfirst construct a new benchmark called Rephrased Multilingual LAMA (RML-LAMA),\nwhich contains high-quality cloze-style multilingual parallel queries for each\nfact. Then, we propose a novel method named Multilingual Integrated Gradients\nwith Uncertainty Estimation (MATRICE), which quantifies the uncertainty across\nqueries and languages during knowledge localization. Extensive experiments show\nthat our method can accurately localize language-agnostic knowledge neurons. We\nalso further investigate the role of language-agnostic knowledge neurons in\ncross-lingual knowledge editing, knowledge enhancement and new knowledge\ninjection."
                },
                "authors": [
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Yuheng Chen"
                    },
                    {
                        "name": "Zhuoran Jin"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17400v1",
                "updated": "2024-11-26T13:00:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    0,
                    39,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T13:00:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    0,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "A Generalized Unified Skew-Normal Process with Neural Bayes Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generalized Unified Skew-Normal Process with Neural Bayes Inference"
                },
                "summary": "In recent decades, statisticians have been increasingly encountering spatial\ndata that exhibit non-Gaussian behaviors such as asymmetry and\nheavy-tailedness. As a result, the assumptions of symmetry and fixed tail\nweight in Gaussian processes have become restrictive and may fail to capture\nthe intrinsic properties of the data. To address the limitations of the\nGaussian models, a variety of skewed models has been proposed, of which the\npopularity has grown rapidly. These skewed models introduce parameters that\ngovern skewness and tail weight. Among various proposals in the literature,\nunified skewed distributions, such as the Unified Skew-Normal (SUN), have\nreceived considerable attention. In this work, we revisit a more concise and\nintepretable re-parameterization of the SUN distribution and apply the\ndistribution to random fields by constructing a generalized unified skew-normal\n(GSUN) spatial process. We demonstrate { that the GSUN is a valid spatial\nprocess by showing its vanishing correlation in large distances} and provide\nthe corresponding spatial interpolation method. In addition, we develop an\ninference mechanism for the GSUN process using the concept of neural Bayes\nestimators with deep graphical attention networks (GATs) and encoder\ntransformer. We show the superiority of our proposed estimator over the\nconventional CNN-based architectures regarding stability and accuracy by means\nof a simulation study and application to Pb-contaminated soil data.\nFurthermore, we show that the GSUN process is different from the conventional\nGaussian processes and Tukey g-and-h processes, through the probability\nintegral transform (PIT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent decades, statisticians have been increasingly encountering spatial\ndata that exhibit non-Gaussian behaviors such as asymmetry and\nheavy-tailedness. As a result, the assumptions of symmetry and fixed tail\nweight in Gaussian processes have become restrictive and may fail to capture\nthe intrinsic properties of the data. To address the limitations of the\nGaussian models, a variety of skewed models has been proposed, of which the\npopularity has grown rapidly. These skewed models introduce parameters that\ngovern skewness and tail weight. Among various proposals in the literature,\nunified skewed distributions, such as the Unified Skew-Normal (SUN), have\nreceived considerable attention. In this work, we revisit a more concise and\nintepretable re-parameterization of the SUN distribution and apply the\ndistribution to random fields by constructing a generalized unified skew-normal\n(GSUN) spatial process. We demonstrate { that the GSUN is a valid spatial\nprocess by showing its vanishing correlation in large distances} and provide\nthe corresponding spatial interpolation method. In addition, we develop an\ninference mechanism for the GSUN process using the concept of neural Bayes\nestimators with deep graphical attention networks (GATs) and encoder\ntransformer. We show the superiority of our proposed estimator over the\nconventional CNN-based architectures regarding stability and accuracy by means\nof a simulation study and application to Pb-contaminated soil data.\nFurthermore, we show that the GSUN process is different from the conventional\nGaussian processes and Tukey g-and-h processes, through the probability\nintegral transform (PIT)."
                },
                "authors": [
                    {
                        "name": "Kesen Wang"
                    },
                    {
                        "name": "Marc G. Genton"
                    }
                ],
                "author_detail": {
                    "name": "Marc G. Genton"
                },
                "author": "Marc G. Genton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17395v1",
                "updated": "2024-11-26T12:55:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    55,
                    51,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T12:55:51Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    55,
                    51,
                    1,
                    331,
                    0
                ],
                "title": "Asymptotics for estimating a diverging number of parameters -- with and\n  without sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotics for estimating a diverging number of parameters -- with and\n  without sparsity"
                },
                "summary": "We consider high-dimensional estimation problems where the number of\nparameters diverges with the sample size. General conditions are established\nfor consistency, uniqueness, and asymptotic normality in both unpenalized and\npenalized estimation settings. The conditions are weak and accommodate a broad\nclass of estimation problems, including ones with non-convex and group\nstructured penalties. The wide applicability of the results is illustrated\nthrough diverse examples, including generalized linear models, multi-sample\ninference, and stepwise estimation procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider high-dimensional estimation problems where the number of\nparameters diverges with the sample size. General conditions are established\nfor consistency, uniqueness, and asymptotic normality in both unpenalized and\npenalized estimation settings. The conditions are weak and accommodate a broad\nclass of estimation problems, including ones with non-convex and group\nstructured penalties. The wide applicability of the results is illustrated\nthrough diverse examples, including generalized linear models, multi-sample\ninference, and stepwise estimation procedures."
                },
                "authors": [
                    {
                        "name": "Jana Gauss"
                    },
                    {
                        "name": "Thomas Nagler"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Nagler"
                },
                "author": "Thomas Nagler",
                "arxiv_comment": "47 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17388v1",
                "updated": "2024-11-26T12:46:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    46,
                    57,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T12:46:57Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    46,
                    57,
                    1,
                    331,
                    0
                ],
                "title": "Can LLMs be Good Graph Judger for Knowledge Graph Construction?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs be Good Graph Judger for Knowledge Graph Construction?"
                },
                "summary": "In real-world scenarios, most of the data obtained from information retrieval\n(IR) system is unstructured. Converting natural language sentences into\nstructured Knowledge Graphs (KGs) remains a critical challenge. The quality of\nconstructed KGs may also impact the performance of some KG-dependent domains\nlike GraphRAG systems and recommendation systems. Recently, Large Language\nModels (LLMs) have demonstrated impressive capabilities in addressing a wide\nrange of natural language processing tasks. However, there are still challenges\nwhen utilizing LLMs to address the task of generating structured KGs. And we\nhave identified three limitations with respect to existing KG construction\nmethods. (1)There is a large amount of information and excessive noise in\nreal-world documents, which could result in extracting messy information.\n(2)Native LLMs struggle to effectively extract accuracy knowledge from some\ndomain-specific documents. (3)Hallucinations phenomenon cannot be overlooked\nwhen utilizing LLMs directly as an unsupervised method for constructing KGs.\n  In this paper, we propose GraphJudger, a knowledge graph construction\nframework to address the aforementioned challenges. We introduce three\ninnovative modules in our method, which are entity-centric iterative text\ndenoising, knowledge aware instruction tuning and graph judgement,\nrespectively. We seek to utilize the capacity of LLMs to function as a graph\njudger, a capability superior to their role only as a predictor for KG\nconstruction problems. Experiments conducted on two general text-graph pair\ndatasets and one domain-specific text-graph pair dataset show superior\nperformances compared to baseline methods. The code of our proposed method is\navailable at https://github.com/hhy-huang/GraphJudger.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world scenarios, most of the data obtained from information retrieval\n(IR) system is unstructured. Converting natural language sentences into\nstructured Knowledge Graphs (KGs) remains a critical challenge. The quality of\nconstructed KGs may also impact the performance of some KG-dependent domains\nlike GraphRAG systems and recommendation systems. Recently, Large Language\nModels (LLMs) have demonstrated impressive capabilities in addressing a wide\nrange of natural language processing tasks. However, there are still challenges\nwhen utilizing LLMs to address the task of generating structured KGs. And we\nhave identified three limitations with respect to existing KG construction\nmethods. (1)There is a large amount of information and excessive noise in\nreal-world documents, which could result in extracting messy information.\n(2)Native LLMs struggle to effectively extract accuracy knowledge from some\ndomain-specific documents. (3)Hallucinations phenomenon cannot be overlooked\nwhen utilizing LLMs directly as an unsupervised method for constructing KGs.\n  In this paper, we propose GraphJudger, a knowledge graph construction\nframework to address the aforementioned challenges. We introduce three\ninnovative modules in our method, which are entity-centric iterative text\ndenoising, knowledge aware instruction tuning and graph judgement,\nrespectively. We seek to utilize the capacity of LLMs to function as a graph\njudger, a capability superior to their role only as a predictor for KG\nconstruction problems. Experiments conducted on two general text-graph pair\ndatasets and one domain-specific text-graph pair dataset show superior\nperformances compared to baseline methods. The code of our proposed method is\navailable at https://github.com/hhy-huang/GraphJudger."
                },
                "authors": [
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05062v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05062v2",
                "updated": "2024-11-26T12:39:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    39,
                    36,
                    1,
                    331,
                    0
                ],
                "published": "2024-10-07T14:21:17Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    21,
                    17,
                    0,
                    281,
                    0
                ],
                "title": "Large Language Model Based Multi-Objective Optimization for Integrated\n  Sensing and Communications in UAV Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Based Multi-Objective Optimization for Integrated\n  Sensing and Communications in UAV Networks"
                },
                "summary": "This letter investigates an unmanned aerial vehicle (UAV) network with\nintegrated sensing and communication (ISAC) systems, where multiple UAVs\nsimultaneously sense the locations of ground users and provide communication\nservices with radars. To find the trade-off between communication and sensing\n(C\\&S) in the system, we formulate a multi-objective optimization problem (MOP)\nto maximize the total network utility and the localization Cram\\'er-Rao bounds\n(CRB) of ground users, which jointly optimizes the deployment and power control\nof UAVs. Inspired by the huge potential of large language models (LLM) for\nprediction and inference, we propose an LLM-enabled decomposition-based\nmulti-objective evolutionary algorithm (LEDMA) for solving the highly\nnon-convex MOP. We first adopt a decomposition-based scheme to decompose the\nMOP into a series of optimization sub-problems. We second integrate LLMs as\nblack-box search operators with MOP-specifically designed prompt engineering\ninto the framework of MOEA to solve optimization sub-problems simultaneously.\nNumerical results demonstrate that the proposed LEDMA can find the clear\ntrade-off between C\\&S and outperforms baseline MOEAs in terms of obtained\nPareto fronts and convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This letter investigates an unmanned aerial vehicle (UAV) network with\nintegrated sensing and communication (ISAC) systems, where multiple UAVs\nsimultaneously sense the locations of ground users and provide communication\nservices with radars. To find the trade-off between communication and sensing\n(C\\&S) in the system, we formulate a multi-objective optimization problem (MOP)\nto maximize the total network utility and the localization Cram\\'er-Rao bounds\n(CRB) of ground users, which jointly optimizes the deployment and power control\nof UAVs. Inspired by the huge potential of large language models (LLM) for\nprediction and inference, we propose an LLM-enabled decomposition-based\nmulti-objective evolutionary algorithm (LEDMA) for solving the highly\nnon-convex MOP. We first adopt a decomposition-based scheme to decompose the\nMOP into a series of optimization sub-problems. We second integrate LLMs as\nblack-box search operators with MOP-specifically designed prompt engineering\ninto the framework of MOEA to solve optimization sub-problems simultaneously.\nNumerical results demonstrate that the proposed LEDMA can find the clear\ntrade-off between C\\&S and outperforms baseline MOEAs in terms of obtained\nPareto fronts and convergence."
                },
                "authors": [
                    {
                        "name": "Haoyun Li"
                    },
                    {
                        "name": "Ming Xiao"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Dong In Kim"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05062v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13256v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13256v3",
                "updated": "2024-11-26T12:37:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    37,
                    39,
                    1,
                    331,
                    0
                ],
                "published": "2024-01-24T06:50:20Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    6,
                    50,
                    20,
                    2,
                    24,
                    0
                ],
                "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for\n  Personalized Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for\n  Personalized Dialogue Systems"
                },
                "summary": "Large Language Models (LLMs) has shown exceptional capabilities in many\nnatual language understanding and generation tasks. However, the\npersonalization issue still remains a much-coveted property, especially when it\ncomes to the multiple sources involved in the dialogue system. To better plan\nand incorporate the use of multiple sources in generating personalized\nresponse, we firstly decompose it into three sub-tasks: Knowledge Source\nSelection, Knowledge Retrieval, and Response Generation. We then propose a\nnovel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)\nSpecifically, we unify these three sub-tasks with different formulations into\nthe same sequence-to-sequence paradigm during the training, to adaptively\nretrieve evidences and evaluate the relevance on-demand using special tokens,\ncalled acting tokens and evaluation tokens. Enabling language models to\ngenerate acting tokens facilitates interaction with various knowledge sources,\nallowing them to adapt their behavior to diverse task requirements. Meanwhile,\nevaluation tokens gauge the relevance score between the dialogue context and\nthe retrieved evidence. In addition, we carefully design a self-refinement\nmechanism to iteratively refine the generated response considering 1) the\nconsistency scores between the generated response and retrieved evidence; and\n2) the relevance scores. Experiments on two personalized datasets (DuLeMon and\nKBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge\nsource selection and response generation task with itself as a retriever in a\nunified manner. Extensive analyses and discussions are provided for shedding\nsome new perspectives for personalized dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) has shown exceptional capabilities in many\nnatual language understanding and generation tasks. However, the\npersonalization issue still remains a much-coveted property, especially when it\ncomes to the multiple sources involved in the dialogue system. To better plan\nand incorporate the use of multiple sources in generating personalized\nresponse, we firstly decompose it into three sub-tasks: Knowledge Source\nSelection, Knowledge Retrieval, and Response Generation. We then propose a\nnovel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)\nSpecifically, we unify these three sub-tasks with different formulations into\nthe same sequence-to-sequence paradigm during the training, to adaptively\nretrieve evidences and evaluate the relevance on-demand using special tokens,\ncalled acting tokens and evaluation tokens. Enabling language models to\ngenerate acting tokens facilitates interaction with various knowledge sources,\nallowing them to adapt their behavior to diverse task requirements. Meanwhile,\nevaluation tokens gauge the relevance score between the dialogue context and\nthe retrieved evidence. In addition, we carefully design a self-refinement\nmechanism to iteratively refine the generated response considering 1) the\nconsistency scores between the generated response and retrieved evidence; and\n2) the relevance scores. Experiments on two personalized datasets (DuLeMon and\nKBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge\nsource selection and response generation task with itself as a retriever in a\nunified manner. Extensive analyses and discussions are provided for shedding\nsome new perspectives for personalized dialogue systems."
                },
                "authors": [
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Wenyu Huang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zezhong Wang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Fei Mi"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.13256v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13256v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17375v1",
                "updated": "2024-11-26T12:34:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    34,
                    52,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T12:34:52Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    34,
                    52,
                    1,
                    331,
                    0
                ],
                "title": "The Extractive-Abstractive Spectrum: Uncovering Verifiability Trade-offs\n  in LLM Generations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Extractive-Abstractive Spectrum: Uncovering Verifiability Trade-offs\n  in LLM Generations"
                },
                "summary": "Across all fields of academic study, experts cite their sources when sharing\ninformation. While large language models (LLMs) excel at synthesizing\ninformation, they do not provide reliable citation to sources, making it\ndifficult to trace and verify the origins of the information they present. In\ncontrast, search engines make sources readily accessible to users and place the\nburden of synthesizing information on the user. Through a survey, we find that\nusers prefer search engines over LLMs for high-stakes queries, where concerns\nregarding information provenance outweigh the perceived utility of LLM\nresponses. To examine the interplay between verifiability and utility of\ninformation-sharing tools, we introduce the extractive-abstractive spectrum, in\nwhich search engines and LLMs are extreme endpoints encapsulating multiple\nunexplored intermediate operating points. Search engines are extractive because\nthey respond to queries with snippets of sources with links (citations) to the\noriginal webpages. LLMs are abstractive because they address queries with\nanswers that synthesize and logically transform relevant information from\ntraining and in-context sources without reliable citation. We define five\noperating points that span the extractive-abstractive spectrum and conduct\nhuman evaluations on seven systems across four diverse query distributions that\nreflect real-world QA settings: web search, language simplification, multi-step\nreasoning, and medical advice. As outputs become more abstractive, we find that\nperceived utility improves by as much as 200%, while the proportion of properly\ncited sentences decreases by as much as 50% and users take up to 3 times as\nlong to verify cited information. Our findings recommend distinct operating\npoints for domain-specific LLM systems and our failure analysis informs\napproaches to high-utility LLM systems that empower users to verify\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across all fields of academic study, experts cite their sources when sharing\ninformation. While large language models (LLMs) excel at synthesizing\ninformation, they do not provide reliable citation to sources, making it\ndifficult to trace and verify the origins of the information they present. In\ncontrast, search engines make sources readily accessible to users and place the\nburden of synthesizing information on the user. Through a survey, we find that\nusers prefer search engines over LLMs for high-stakes queries, where concerns\nregarding information provenance outweigh the perceived utility of LLM\nresponses. To examine the interplay between verifiability and utility of\ninformation-sharing tools, we introduce the extractive-abstractive spectrum, in\nwhich search engines and LLMs are extreme endpoints encapsulating multiple\nunexplored intermediate operating points. Search engines are extractive because\nthey respond to queries with snippets of sources with links (citations) to the\noriginal webpages. LLMs are abstractive because they address queries with\nanswers that synthesize and logically transform relevant information from\ntraining and in-context sources without reliable citation. We define five\noperating points that span the extractive-abstractive spectrum and conduct\nhuman evaluations on seven systems across four diverse query distributions that\nreflect real-world QA settings: web search, language simplification, multi-step\nreasoning, and medical advice. As outputs become more abstractive, we find that\nperceived utility improves by as much as 200%, while the proportion of properly\ncited sentences decreases by as much as 50% and users take up to 3 times as\nlong to verify cited information. Our findings recommend distinct operating\npoints for domain-specific LLM systems and our failure analysis informs\napproaches to high-utility LLM systems that empower users to verify\ninformation."
                },
                "authors": [
                    {
                        "name": "Theodora Worledge"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    },
                    {
                        "name": "Carlos Guestrin"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Guestrin"
                },
                "author": "Carlos Guestrin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15193v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15193v5",
                "updated": "2024-11-26T12:13:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    13,
                    21,
                    1,
                    331,
                    0
                ],
                "published": "2024-06-21T14:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    14,
                    35,
                    16,
                    4,
                    173,
                    0
                ],
                "title": "Inference Time Alignment with Reward-Guided Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Time Alignment with Reward-Guided Tree Search"
                },
                "summary": "Inference-time computation methods enhance the performance of Large Language\nModels (LLMs) by leveraging additional computational resources to achieve\nsuperior results. Common techniques, such as Best-of-N sampling, Majority\nVoting, and variants of tree-search algorithms have proven to be effective in\nboosting the performance of LLMs. These approaches strategically trade\nincreased computational resources for improved model responses. In this work,\nwe proposed DARWIN, an inference-time alignment method that leverages the\nguidance of a reward model to achieve alignment through a reward-guided tree\nsearch. Empirical evidences indicates that our method outperforms other\ninference-time alignment methods such as Best-of-N and ARGS on two widely\naccepted alignment benchmarks AlpacaEval 2 and MT-Bench. Furthermore, we show\nthat our inference-time approach achieves performance comparable to\npreference-tuned models on both benchmarks, highlighting the effectiveness of\ntrading inference-time compute for enhanced performance during inference. We\nhave released our codes at https://github.com/declare-lab/darwin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time computation methods enhance the performance of Large Language\nModels (LLMs) by leveraging additional computational resources to achieve\nsuperior results. Common techniques, such as Best-of-N sampling, Majority\nVoting, and variants of tree-search algorithms have proven to be effective in\nboosting the performance of LLMs. These approaches strategically trade\nincreased computational resources for improved model responses. In this work,\nwe proposed DARWIN, an inference-time alignment method that leverages the\nguidance of a reward model to achieve alignment through a reward-guided tree\nsearch. Empirical evidences indicates that our method outperforms other\ninference-time alignment methods such as Best-of-N and ARGS on two widely\naccepted alignment benchmarks AlpacaEval 2 and MT-Bench. Furthermore, we show\nthat our inference-time approach achieves performance comparable to\npreference-tuned models on both benchmarks, highlighting the effectiveness of\ntrading inference-time compute for enhanced performance during inference. We\nhave released our codes at https://github.com/declare-lab/darwin."
                },
                "authors": [
                    {
                        "name": "Chia-Yu Hung"
                    },
                    {
                        "name": "Navonil Majumder"
                    },
                    {
                        "name": "Ambuj Mehrish"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15193v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15193v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05595v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05595v3",
                "updated": "2024-11-26T12:12:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    12,
                    50,
                    1,
                    331,
                    0
                ],
                "published": "2024-04-08T15:14:20Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    15,
                    14,
                    20,
                    0,
                    99,
                    0
                ],
                "title": "UniFL: Improve Latent Diffusion Model via Unified Feedback Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniFL: Improve Latent Diffusion Model via Unified Feedback Learning"
                },
                "summary": "Latent diffusion models (LDM) have revolutionized text-to-image generation,\nleading to the proliferation of various advanced models and diverse downstream\napplications. However, despite these significant advancements, current\ndiffusion models still suffer from several limitations, including inferior\nvisual quality, inadequate aesthetic appeal, and inefficient inference, without\na comprehensive solution in sight. To address these challenges, we present\nUniFL, a unified framework that leverages feedback learning to enhance\ndiffusion models comprehensively. UniFL stands out as a universal, effective,\nand generalizable solution applicable to various diffusion models, such as\nSD1.5 and SDXL. Notably, UniFL consists of three key components: perceptual\nfeedback learning, which enhances visual quality; decoupled feedback learning,\nwhich improves aesthetic appeal; and adversarial feedback learning, which\naccelerates inference. In-depth experiments and extensive user studies validate\nthe superior performance of our method in enhancing generation quality and\ninference acceleration. For instance, UniFL surpasses ImageReward by 17% user\npreference in terms of generation quality and outperforms LCM and SDXL Turbo by\n57% and 20% general preference with 4-step inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent diffusion models (LDM) have revolutionized text-to-image generation,\nleading to the proliferation of various advanced models and diverse downstream\napplications. However, despite these significant advancements, current\ndiffusion models still suffer from several limitations, including inferior\nvisual quality, inadequate aesthetic appeal, and inefficient inference, without\na comprehensive solution in sight. To address these challenges, we present\nUniFL, a unified framework that leverages feedback learning to enhance\ndiffusion models comprehensively. UniFL stands out as a universal, effective,\nand generalizable solution applicable to various diffusion models, such as\nSD1.5 and SDXL. Notably, UniFL consists of three key components: perceptual\nfeedback learning, which enhances visual quality; decoupled feedback learning,\nwhich improves aesthetic appeal; and adversarial feedback learning, which\naccelerates inference. In-depth experiments and extensive user studies validate\nthe superior performance of our method in enhancing generation quality and\ninference acceleration. For instance, UniFL surpasses ImageReward by 17% user\npreference in terms of generation quality and outperforms LCM and SDXL Turbo by\n57% and 20% general preference with 4-step inference."
                },
                "authors": [
                    {
                        "name": "Jiacheng Zhang"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Huafeng Kuang"
                    },
                    {
                        "name": "Pan Xie"
                    },
                    {
                        "name": "Jiashi Li"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Weilin Huang"
                    },
                    {
                        "name": "Shilei Wen"
                    },
                    {
                        "name": "Lean Fu"
                    },
                    {
                        "name": "Guanbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Guanbin Li"
                },
                "author": "Guanbin Li",
                "arxiv_comment": "Accepted by Neurips2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05595v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05595v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00629v2",
                "updated": "2024-11-26T11:59:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    11,
                    59,
                    17,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-31T09:50:39Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    9,
                    50,
                    39,
                    6,
                    91,
                    0
                ],
                "title": "Against The Achilles' Heel: A Survey on Red Teaming for Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Against The Achilles' Heel: A Survey on Red Teaming for Generative\n  Models"
                },
                "summary": "Generative models are rapidly gaining popularity and being integrated into\neveryday applications, raising concerns over their safe use as various\nvulnerabilities are exposed. In light of this, the field of red teaming is\nundergoing fast-paced growth, highlighting the need for a comprehensive survey\ncovering the entire pipeline and addressing emerging topics. Our extensive\nsurvey, which examines over 120 papers, introduces a taxonomy of fine-grained\nattack strategies grounded in the inherent capabilities of language models.\nAdditionally, we have developed the \"searcher\" framework to unify various\nautomatic red teaming approaches. Moreover, our survey covers novel areas\nincluding multimodal attacks and defenses, risks around LLM-based agents,\noverkill of harmless queries, and the balance between harmlessness and\nhelpfulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models are rapidly gaining popularity and being integrated into\neveryday applications, raising concerns over their safe use as various\nvulnerabilities are exposed. In light of this, the field of red teaming is\nundergoing fast-paced growth, highlighting the need for a comprehensive survey\ncovering the entire pipeline and addressing emerging topics. Our extensive\nsurvey, which examines over 120 papers, introduces a taxonomy of fine-grained\nattack strategies grounded in the inherent capabilities of language models.\nAdditionally, we have developed the \"searcher\" framework to unify various\nautomatic red teaming approaches. Moreover, our survey covers novel areas\nincluding multimodal attacks and defenses, risks around LLM-based agents,\noverkill of harmless queries, and the balance between harmlessness and\nhelpfulness."
                },
                "authors": [
                    {
                        "name": "Lizhi Lin"
                    },
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Zenan Zhai"
                    },
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Junjie Gao"
                    },
                    {
                        "name": "Yixuan Zhang"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Haonan Li"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Li"
                },
                "author": "Haonan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.11914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.11914v3",
                "updated": "2024-11-26T11:39:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    11,
                    39,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2023-08-23T04:59:21Z",
                "published_parsed": [
                    2023,
                    8,
                    23,
                    4,
                    59,
                    21,
                    2,
                    235,
                    0
                ],
                "title": "Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge\n  Reasoning via Promoting Causal Consistency in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge\n  Reasoning via Promoting Causal Consistency in LLMs"
                },
                "summary": "Despite the progress of foundation models, knowledge-based reasoning remains\na persistent challenge due to their limited capacity for knowledge recall and\ninference. Existing methods primarily focus on encouraging these models to plan\nand solve problems or extensively sample reasoning chains independently.\nHowever, these methods often overlook conceptual errors and inferential\nfallacies, inevitably leading to a series of notorious issues such as\nmisleading conclusions, cognitive biases, and reduced decision quality. While\nexplicit modeling of causality is argued to hold promise in addressing these\nissues, contemporary research efforts have thus far fallen short in achieving\ncausality-based foundation models. Drawing inspiration from the orchestration\nof diverse specialized agents collaborating to tackle intricate tasks, we\npropose a framework named Causal-Consistency Chain-of-Thought (CaCo-CoT) that\nharnesses multi-agent collaboration to bolster the faithfulness and causality\nof foundation models, involving a set of reasoners and evaluators. These agents\ncollaboratively work within a reasoning-and-consensus paradigm to improve\nfaithfulness. The reasoners are tasked with generating reasoning chains for\nknowledge-intensive problems by mimicking human causal reasoning. Meanwhile,\nthe evaluator scrutinizes the causal consistency of a reasoner's reasoning\nchain from a non-causal and a counterfactual perspective. Our framework\ndemonstrates significant superiority over state-of-the-art methods through\nextensive and comprehensive evaluations across text-based and multi-modal\nknowledge reasoning tasks (e.g., science question answering and commonsense\nreasoning).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the progress of foundation models, knowledge-based reasoning remains\na persistent challenge due to their limited capacity for knowledge recall and\ninference. Existing methods primarily focus on encouraging these models to plan\nand solve problems or extensively sample reasoning chains independently.\nHowever, these methods often overlook conceptual errors and inferential\nfallacies, inevitably leading to a series of notorious issues such as\nmisleading conclusions, cognitive biases, and reduced decision quality. While\nexplicit modeling of causality is argued to hold promise in addressing these\nissues, contemporary research efforts have thus far fallen short in achieving\ncausality-based foundation models. Drawing inspiration from the orchestration\nof diverse specialized agents collaborating to tackle intricate tasks, we\npropose a framework named Causal-Consistency Chain-of-Thought (CaCo-CoT) that\nharnesses multi-agent collaboration to bolster the faithfulness and causality\nof foundation models, involving a set of reasoners and evaluators. These agents\ncollaboratively work within a reasoning-and-consensus paradigm to improve\nfaithfulness. The reasoners are tasked with generating reasoning chains for\nknowledge-intensive problems by mimicking human causal reasoning. Meanwhile,\nthe evaluator scrutinizes the causal consistency of a reasoner's reasoning\nchain from a non-causal and a counterfactual perspective. Our framework\ndemonstrates significant superiority over state-of-the-art methods through\nextensive and comprehensive evaluations across text-based and multi-modal\nknowledge reasoning tasks (e.g., science question answering and commonsense\nreasoning)."
                },
                "authors": [
                    {
                        "name": "Ziyi Tang"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Weixing Chen"
                    },
                    {
                        "name": "Keze Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liang Lin"
                },
                "author": "Liang Lin",
                "arxiv_comment": "8 pages, 3 figures. 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.11914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.11914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17338v1",
                "updated": "2024-11-26T11:32:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    11,
                    32,
                    43,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T11:32:43Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    11,
                    32,
                    43,
                    1,
                    331,
                    0
                ],
                "title": "Different Bias Under Different Criteria: Assessing Bias in LLMs with a\n  Fact-Based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Different Bias Under Different Criteria: Assessing Bias in LLMs with a\n  Fact-Based Approach"
                },
                "summary": "Large language models (LLMs) often reflect real-world biases, leading to\nefforts to mitigate these effects and make the models unbiased. Achieving this\ngoal requires defining clear criteria for an unbiased state, with any deviation\nfrom these criteria considered biased. Some studies define an unbiased state as\nequal treatment across diverse demographic groups, aiming for balanced outputs\nfrom LLMs. However, differing perspectives on equality and the importance of\npluralism make it challenging to establish a universal standard. Alternatively,\nother approaches propose using fact-based criteria for more consistent and\nobjective evaluations, though these methods have not yet been fully applied to\nLLM bias assessments. Thus, there is a need for a metric with objective\ncriteria that offers a distinct perspective from equality-based approaches.\nMotivated by this need, we introduce a novel metric to assess bias using\nfact-based criteria and real-world statistics. In this paper, we conducted a\nhuman survey demonstrating that humans tend to perceive LLM outputs more\npositively when they align closely with real-world demographic distributions.\nEvaluating various LLMs with our proposed metric reveals that model bias varies\ndepending on the criteria used, highlighting the need for multi-perspective\nassessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often reflect real-world biases, leading to\nefforts to mitigate these effects and make the models unbiased. Achieving this\ngoal requires defining clear criteria for an unbiased state, with any deviation\nfrom these criteria considered biased. Some studies define an unbiased state as\nequal treatment across diverse demographic groups, aiming for balanced outputs\nfrom LLMs. However, differing perspectives on equality and the importance of\npluralism make it challenging to establish a universal standard. Alternatively,\nother approaches propose using fact-based criteria for more consistent and\nobjective evaluations, though these methods have not yet been fully applied to\nLLM bias assessments. Thus, there is a need for a metric with objective\ncriteria that offers a distinct perspective from equality-based approaches.\nMotivated by this need, we introduce a novel metric to assess bias using\nfact-based criteria and real-world statistics. In this paper, we conducted a\nhuman survey demonstrating that humans tend to perceive LLM outputs more\npositively when they align closely with real-world demographic distributions.\nEvaluating various LLMs with our proposed metric reveals that model bias varies\ndepending on the criteria used, highlighting the need for multi-perspective\nassessment."
                },
                "authors": [
                    {
                        "name": "Changgeon Ko"
                    },
                    {
                        "name": "Jisu Shin"
                    },
                    {
                        "name": "Hoyun Song"
                    },
                    {
                        "name": "Jeongyeon Seo"
                    },
                    {
                        "name": "Jong C. Park"
                    }
                ],
                "author_detail": {
                    "name": "Jong C. Park"
                },
                "author": "Jong C. Park",
                "arxiv_comment": "Accepted in NeurIPS 2024 Workshop on Socially Responsible Language\n  Modelling Research (SoLaR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17337v1",
                "updated": "2024-11-26T11:31:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    11,
                    31,
                    47,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T11:31:47Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    11,
                    31,
                    47,
                    1,
                    331,
                    0
                ],
                "title": "sbi reloaded: a toolkit for simulation-based inference workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sbi reloaded: a toolkit for simulation-based inference workflows"
                },
                "summary": "Scientists and engineers use simulators to model empirically observed\nphenomena. However, tuning the parameters of a simulator to ensure its outputs\nmatch observed data presents a significant challenge. Simulation-based\ninference (SBI) addresses this by enabling Bayesian inference for simulators,\nidentifying parameters that match observed data and align with prior knowledge.\nUnlike traditional Bayesian inference, SBI only needs access to simulations\nfrom the model and does not require evaluations of the likelihood-function. In\naddition, SBI algorithms do not require gradients through the simulator, allow\nfor massive parallelization of simulations, and can perform inference for\ndifferent observations without further simulations or training, thereby\namortizing inference. Over the past years, we have developed, maintained, and\nextended $\\texttt{sbi}$, a PyTorch-based package that implements Bayesian SBI\nalgorithms based on neural networks. The $\\texttt{sbi}$ toolkit implements a\nwide range of inference methods, neural network architectures, sampling\nmethods, and diagnostic tools. In addition, it provides well-tested default\nsettings but also offers flexibility to fully customize every step of the\nsimulation-based inference workflow. Taken together, the $\\texttt{sbi}$ toolkit\nenables scientists and engineers to apply state-of-the-art SBI methods to\nblack-box simulators, opening up new possibilities for aligning simulations\nwith empirically observed data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientists and engineers use simulators to model empirically observed\nphenomena. However, tuning the parameters of a simulator to ensure its outputs\nmatch observed data presents a significant challenge. Simulation-based\ninference (SBI) addresses this by enabling Bayesian inference for simulators,\nidentifying parameters that match observed data and align with prior knowledge.\nUnlike traditional Bayesian inference, SBI only needs access to simulations\nfrom the model and does not require evaluations of the likelihood-function. In\naddition, SBI algorithms do not require gradients through the simulator, allow\nfor massive parallelization of simulations, and can perform inference for\ndifferent observations without further simulations or training, thereby\namortizing inference. Over the past years, we have developed, maintained, and\nextended $\\texttt{sbi}$, a PyTorch-based package that implements Bayesian SBI\nalgorithms based on neural networks. The $\\texttt{sbi}$ toolkit implements a\nwide range of inference methods, neural network architectures, sampling\nmethods, and diagnostic tools. In addition, it provides well-tested default\nsettings but also offers flexibility to fully customize every step of the\nsimulation-based inference workflow. Taken together, the $\\texttt{sbi}$ toolkit\nenables scientists and engineers to apply state-of-the-art SBI methods to\nblack-box simulators, opening up new possibilities for aligning simulations\nwith empirically observed data."
                },
                "authors": [
                    {
                        "name": "Jan Boelts"
                    },
                    {
                        "name": "Michael Deistler"
                    },
                    {
                        "name": "Manuel Gloeckler"
                    },
                    {
                        "name": "Álvaro Tejero-Cantero"
                    },
                    {
                        "name": "Jan-Matthis Lueckmann"
                    },
                    {
                        "name": "Guy Moss"
                    },
                    {
                        "name": "Peter Steinbach"
                    },
                    {
                        "name": "Thomas Moreau"
                    },
                    {
                        "name": "Fabio Muratore"
                    },
                    {
                        "name": "Julia Linhart"
                    },
                    {
                        "name": "Conor Durkan"
                    },
                    {
                        "name": "Julius Vetter"
                    },
                    {
                        "name": "Benjamin Kurt Miller"
                    },
                    {
                        "name": "Maternus Herold"
                    },
                    {
                        "name": "Abolfazl Ziaeemehr"
                    },
                    {
                        "name": "Matthijs Pals"
                    },
                    {
                        "name": "Theo Gruner"
                    },
                    {
                        "name": "Sebastian Bischoff"
                    },
                    {
                        "name": "Nastya Krouglova"
                    },
                    {
                        "name": "Richard Gao"
                    },
                    {
                        "name": "Janne K. Lappalainen"
                    },
                    {
                        "name": "Bálint Mucsányi"
                    },
                    {
                        "name": "Felix Pei"
                    },
                    {
                        "name": "Auguste Schulz"
                    },
                    {
                        "name": "Zinovia Stefanidi"
                    },
                    {
                        "name": "Pedro Rodrigues"
                    },
                    {
                        "name": "Cornelius Schröder"
                    },
                    {
                        "name": "Faried Abu Zaid"
                    },
                    {
                        "name": "Jonas Beck"
                    },
                    {
                        "name": "Jaivardhan Kapoor"
                    },
                    {
                        "name": "David S. Greenberg"
                    },
                    {
                        "name": "Pedro J. Gonçalves"
                    },
                    {
                        "name": "Jakob H. Macke"
                    }
                ],
                "author_detail": {
                    "name": "Jakob H. Macke"
                },
                "author": "Jakob H. Macke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17312v1",
                "updated": "2024-11-26T10:56:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    56,
                    59,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T10:56:59Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    56,
                    59,
                    1,
                    331,
                    0
                ],
                "title": "A Shallow Slope for the Stellar Mass--Angular Momentum Relation of\n  Star-Forming Galaxies at $1.5 < z < 2.5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Shallow Slope for the Stellar Mass--Angular Momentum Relation of\n  Star-Forming Galaxies at $1.5 < z < 2.5$"
                },
                "summary": "We present measurements of the specific angular momentum $j_\\star$ of 41\nstar-forming galaxies at $1.5<z<2.5$. These measurements are based on radial\nprofiles inferred from near-IR \\textit{HST} photometry, along with\nmulti-resolution emission-line kinematic modelling using integral field\nspectroscopy (IFS) data from KMOS, SINFONI, and OSIRIS. We identified 24 disks\n(disk fraction of $58.6\\pm 7.7\\%$) and used them to parametrize the $j_\\star$\n\\textit{vs} stellar mass $M_\\star$ relation (Fall relation) as $j_\\star\\propto\nM_\\star^{\\beta}$. We measure a power-law slope $\\beta=0.25\\pm0.15$, which\ndeviates by approximately $3\\sigma$ from the commonly adopted local value\n$\\beta = 0.67$, indicating a statistically significant difference. We find that\ntwo key systematic effects could drive the steep slopes in previous\nhigh-redshift studies: first, including irregular (non-disk) systems due to\nlimitations in spatial resolution and second, using the commonly used\napproximation $\\tilde{j}_\\star\\approx k_n v_s r_\\mathrm{eff}$, which depends on\nglobal unresolved quantities. In our sample, both effects lead to steeper\nslopes of $\\beta=0.48\\pm0.21$ and $\\beta=0.61\\pm0.21$, respectively. To\nunderstand the shallow slope, we discuss observational effects and systematic\nuncertainties and analyze the retention of $j_\\star$ relative to the angular\nmomentum of the halo $j_h$ (angular momentum retention factor $f_j\n=j_\\star/j_h$). For the $M_\\star$ range covered by the sample $9.5 <\\log_{10}\n(M_\\star/M_\\odot) < 11.5$ (halo mass $11.5 < \\log_{10} (M_h/M_\\odot) < 14$), we\nfind large $f_j$ values ($>1$ in some cases) in low-mass haloes that decrease\nwith increasing mass, suggesting a significant role of efficient angular\nmomentum transport in these gas-rich systems, aided by the removal of\nlow-$j_\\star$ gas via feedback-driven outflows in low-mass galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present measurements of the specific angular momentum $j_\\star$ of 41\nstar-forming galaxies at $1.5<z<2.5$. These measurements are based on radial\nprofiles inferred from near-IR \\textit{HST} photometry, along with\nmulti-resolution emission-line kinematic modelling using integral field\nspectroscopy (IFS) data from KMOS, SINFONI, and OSIRIS. We identified 24 disks\n(disk fraction of $58.6\\pm 7.7\\%$) and used them to parametrize the $j_\\star$\n\\textit{vs} stellar mass $M_\\star$ relation (Fall relation) as $j_\\star\\propto\nM_\\star^{\\beta}$. We measure a power-law slope $\\beta=0.25\\pm0.15$, which\ndeviates by approximately $3\\sigma$ from the commonly adopted local value\n$\\beta = 0.67$, indicating a statistically significant difference. We find that\ntwo key systematic effects could drive the steep slopes in previous\nhigh-redshift studies: first, including irregular (non-disk) systems due to\nlimitations in spatial resolution and second, using the commonly used\napproximation $\\tilde{j}_\\star\\approx k_n v_s r_\\mathrm{eff}$, which depends on\nglobal unresolved quantities. In our sample, both effects lead to steeper\nslopes of $\\beta=0.48\\pm0.21$ and $\\beta=0.61\\pm0.21$, respectively. To\nunderstand the shallow slope, we discuss observational effects and systematic\nuncertainties and analyze the retention of $j_\\star$ relative to the angular\nmomentum of the halo $j_h$ (angular momentum retention factor $f_j\n=j_\\star/j_h$). For the $M_\\star$ range covered by the sample $9.5 <\\log_{10}\n(M_\\star/M_\\odot) < 11.5$ (halo mass $11.5 < \\log_{10} (M_h/M_\\odot) < 14$), we\nfind large $f_j$ values ($>1$ in some cases) in low-mass haloes that decrease\nwith increasing mass, suggesting a significant role of efficient angular\nmomentum transport in these gas-rich systems, aided by the removal of\nlow-$j_\\star$ gas via feedback-driven outflows in low-mass galaxies."
                },
                "authors": [
                    {
                        "name": "Juan M. Espejo Salcedo"
                    },
                    {
                        "name": "Karl Glazebrook"
                    },
                    {
                        "name": "Deanne B. Fisher"
                    },
                    {
                        "name": "Sarah M. Sweet"
                    },
                    {
                        "name": "Danail Obreschkow"
                    },
                    {
                        "name": "N. M. Förster Schreiber"
                    }
                ],
                "author_detail": {
                    "name": "N. M. Förster Schreiber"
                },
                "author": "N. M. Förster Schreiber",
                "arxiv_comment": "29 pages, 18 figures, 24 appendix figures. Accepted for publication\n  in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17309v1",
                "updated": "2024-11-26T10:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    54,
                    19,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T10:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    54,
                    19,
                    1,
                    331,
                    0
                ],
                "title": "PIM-AI: A Novel Architecture for High-Efficiency LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM-AI: A Novel Architecture for High-Efficiency LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have become essential in a variety of\napplications due to their advanced language understanding and generation\ncapabilities. However, their computational and memory requirements pose\nsignificant challenges to traditional hardware architectures.\nProcessing-in-Memory (PIM), which integrates computational units directly into\nmemory chips, offers several advantages for LLM inference, including reduced\ndata transfer bottlenecks and improved power efficiency.\n  This paper introduces PIM-AI, a novel DDR5/LPDDR5 PIM architecture designed\nfor LLM inference without modifying the memory controller or DDR/LPDDR memory\nPHY. We have developed a simulator to evaluate the performance of PIM-AI in\nvarious scenarios and demonstrate its significant advantages over conventional\narchitectures. In cloud-based scenarios, PIM-AI reduces the 3-year TCO per\nqueries-per-second by up to 6.94x compared to state-of-the-art GPUs, depending\non the LLM model used. In mobile scenarios, PIM-AI achieves a 10- to 20-fold\nreduction in energy per token compared to state-of-the-art mobile SoCs,\nresulting in 25 to 45~\\% more queries per second and 6.9x to 13.4x less energy\nper query, extending battery life and enabling more inferences per charge.\n  These results highlight PIM-AI's potential to revolutionize LLM deployments,\nmaking them more efficient, scalable, and sustainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become essential in a variety of\napplications due to their advanced language understanding and generation\ncapabilities. However, their computational and memory requirements pose\nsignificant challenges to traditional hardware architectures.\nProcessing-in-Memory (PIM), which integrates computational units directly into\nmemory chips, offers several advantages for LLM inference, including reduced\ndata transfer bottlenecks and improved power efficiency.\n  This paper introduces PIM-AI, a novel DDR5/LPDDR5 PIM architecture designed\nfor LLM inference without modifying the memory controller or DDR/LPDDR memory\nPHY. We have developed a simulator to evaluate the performance of PIM-AI in\nvarious scenarios and demonstrate its significant advantages over conventional\narchitectures. In cloud-based scenarios, PIM-AI reduces the 3-year TCO per\nqueries-per-second by up to 6.94x compared to state-of-the-art GPUs, depending\non the LLM model used. In mobile scenarios, PIM-AI achieves a 10- to 20-fold\nreduction in energy per token compared to state-of-the-art mobile SoCs,\nresulting in 25 to 45~\\% more queries per second and 6.9x to 13.4x less energy\nper query, extending battery life and enabling more inferences per charge.\n  These results highlight PIM-AI's potential to revolutionize LLM deployments,\nmaking them more efficient, scalable, and sustainable."
                },
                "authors": [
                    {
                        "name": "Cristobal Ortega"
                    },
                    {
                        "name": "Yann Falevoz"
                    },
                    {
                        "name": "Renaud Ayrignac"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Ayrignac"
                },
                "author": "Renaud Ayrignac",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17304v1",
                "updated": "2024-11-26T10:52:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    52,
                    8,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T10:52:08Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    52,
                    8,
                    1,
                    331,
                    0
                ],
                "title": "Meaningless is better: hashing bias-inducing words in LLM prompts\n  improves performance in logical reasoning and statistical learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meaningless is better: hashing bias-inducing words in LLM prompts\n  improves performance in logical reasoning and statistical learning"
                },
                "summary": "This paper introduces a novel method, referred to as \"hashing\", which\ninvolves masking potentially bias-inducing words in large language models\n(LLMs) with hash-like meaningless identifiers to reduce cognitive biases and\nreliance on external knowledge. The method was tested across three sets of\nexperiments involving a total of 490 prompts. Statistical analysis using\nchi-square tests showed significant improvements in all tested scenarios, which\ncovered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first\nexperiment, hashing decreased the fallacy rate in a modified version of the\n\"Linda\" problem aimed at evaluating susceptibility to cognitive biases. In the\nsecond experiment, it improved LLM results on the frequent itemset extraction\ntask. In the third experiment, we found hashing is also effective when the\nLinda problem is presented in a tabular format rather than text, indicating\nthat the technique works across various input representations. Overall, the\nmethod was shown to improve bias reduction and incorporation of external\nknowledge. Despite bias reduction, hallucination rates were inconsistently\nreduced across types of LLM models. These findings suggest that masking\nbias-inducing terms can improve LLM performance, although its effectiveness is\nmodel- and task-dependent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel method, referred to as \"hashing\", which\ninvolves masking potentially bias-inducing words in large language models\n(LLMs) with hash-like meaningless identifiers to reduce cognitive biases and\nreliance on external knowledge. The method was tested across three sets of\nexperiments involving a total of 490 prompts. Statistical analysis using\nchi-square tests showed significant improvements in all tested scenarios, which\ncovered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first\nexperiment, hashing decreased the fallacy rate in a modified version of the\n\"Linda\" problem aimed at evaluating susceptibility to cognitive biases. In the\nsecond experiment, it improved LLM results on the frequent itemset extraction\ntask. In the third experiment, we found hashing is also effective when the\nLinda problem is presented in a tabular format rather than text, indicating\nthat the technique works across various input representations. Overall, the\nmethod was shown to improve bias reduction and incorporation of external\nknowledge. Despite bias reduction, hallucination rates were inconsistently\nreduced across types of LLM models. These findings suggest that masking\nbias-inducing terms can improve LLM performance, although its effectiveness is\nmodel- and task-dependent."
                },
                "authors": [
                    {
                        "name": "Milena Chadimová"
                    },
                    {
                        "name": "Eduard Jurášek"
                    },
                    {
                        "name": "Tomáš Kliegr"
                    }
                ],
                "author_detail": {
                    "name": "Tomáš Kliegr"
                },
                "author": "Tomáš Kliegr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17301v1",
                "updated": "2024-11-26T10:48:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    48,
                    55,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T10:48:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    48,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "ER2Score: LLM-based Explainable and Customizable Metric for Assessing\n  Radiology Reports with Reward-Control Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ER2Score: LLM-based Explainable and Customizable Metric for Assessing\n  Radiology Reports with Reward-Control Loss"
                },
                "summary": "Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ER2Score, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ER2Score. Our experiments demonstrate ER2Score's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ER2Score, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ER2Score. Our experiments demonstrate ER2Score's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems."
                },
                "authors": [
                    {
                        "name": "Yunyi Liu"
                    },
                    {
                        "name": "Yingshu Li"
                    },
                    {
                        "name": "Zhanyu Wang"
                    },
                    {
                        "name": "Xinyu Liang"
                    },
                    {
                        "name": "Lingqiao Liu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Luping Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Luping Zhou"
                },
                "author": "Luping Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04403v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04403v2",
                "updated": "2024-11-26T10:24:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    24,
                    18,
                    1,
                    331,
                    0
                ],
                "published": "2024-07-05T10:34:06Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    10,
                    34,
                    6,
                    4,
                    187,
                    0
                ],
                "title": "Constraining the high-density behavior of nuclear symmetry energy with\n  direct Urca processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the high-density behavior of nuclear symmetry energy with\n  direct Urca processes"
                },
                "summary": "The density dependence of the symmetry energy in relativistic mean-field\nmodels with density dependent couplings is discussed in terms of the possible\nopening of nucleonic direct Urca processes inside neutron stars, which induce a\nvery rapid cooling of the star. The modification of the parametrization of the\nisospin channel of two models, DD2 and DDMEX, keeping the same isoscalar\nproperties is considered and the implications are discussed. Within the models\ndiscussed it is not possible the onset of nucleonic direct Urca processes in\nstars with a mass below $\\sim1.6\\,M_\\odot$ if chiral effective field theory\nconstraints for neutron matter are imposed. A Bayesian inference calculation\nconfirms the low probability that nucleonic direct Urca processes occur inside\nstars with masses below 1.8$M_\\odot$, considering the isoscalar channel of the\nequation of state described by DD2 or DDMEX and the same symmetry energy at\nsaturation. The lowest masses allowing direct Urca processes are associated\nwith a slope of the symmetry energy above $60$ MeV and most likely a positive\nsymmetry energy incompressibility. It is shown that the parametrization of the\nisospin channel proposed destroys the correlation between symmetry energy slope\nand incompressibility previously identified in several works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The density dependence of the symmetry energy in relativistic mean-field\nmodels with density dependent couplings is discussed in terms of the possible\nopening of nucleonic direct Urca processes inside neutron stars, which induce a\nvery rapid cooling of the star. The modification of the parametrization of the\nisospin channel of two models, DD2 and DDMEX, keeping the same isoscalar\nproperties is considered and the implications are discussed. Within the models\ndiscussed it is not possible the onset of nucleonic direct Urca processes in\nstars with a mass below $\\sim1.6\\,M_\\odot$ if chiral effective field theory\nconstraints for neutron matter are imposed. A Bayesian inference calculation\nconfirms the low probability that nucleonic direct Urca processes occur inside\nstars with masses below 1.8$M_\\odot$, considering the isoscalar channel of the\nequation of state described by DD2 or DDMEX and the same symmetry energy at\nsaturation. The lowest masses allowing direct Urca processes are associated\nwith a slope of the symmetry energy above $60$ MeV and most likely a positive\nsymmetry energy incompressibility. It is shown that the parametrization of the\nisospin channel proposed destroys the correlation between symmetry energy slope\nand incompressibility previously identified in several works."
                },
                "authors": [
                    {
                        "name": "Olfa Boukari"
                    },
                    {
                        "name": "Tuhin Malik"
                    },
                    {
                        "name": "Aziz Rabhi"
                    },
                    {
                        "name": "Constança Providência"
                    }
                ],
                "author_detail": {
                    "name": "Constança Providência"
                },
                "author": "Constança Providência",
                "arxiv_comment": "18 pages, 7 figures, 6 tables, accepted for publication in Phys. Rev.\n  C",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04403v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04403v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17284v1",
                "updated": "2024-11-26T10:13:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    13,
                    39,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T10:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    13,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling"
                },
                "summary": "Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes using LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. We compare LLM-elicited and uninformative priors,\nevaluate whether LLMs truthfully generate parameter distributions, and propose\na model selection strategy for in-context learning and prior elicitation. Our\nfindings show that LLM-elicited prior parameter distributions significantly\nreduce predictive error compared to uninformative priors in low-data settings.\nApplied to clinical problems, this translates to fewer required biological\nsamples, lowering cost and resources. Prior elicitation also consistently\noutperforms and proves more reliable than in-context learning at a lower cost,\nmaking it a preferred alternative in our setting. We demonstrate the utility of\nthis method across various use cases, including clinical applications. For\ninfection prediction, using LLM-elicited priors reduced the number of required\nlabels to achieve the same accuracy as an uninformative prior by 55%, at 200\ndays earlier in the study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes using LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. We compare LLM-elicited and uninformative priors,\nevaluate whether LLMs truthfully generate parameter distributions, and propose\na model selection strategy for in-context learning and prior elicitation. Our\nfindings show that LLM-elicited prior parameter distributions significantly\nreduce predictive error compared to uninformative priors in low-data settings.\nApplied to clinical problems, this translates to fewer required biological\nsamples, lowering cost and resources. Prior elicitation also consistently\noutperforms and proves more reliable than in-context learning at a lower cost,\nmaking it a preferred alternative in our setting. We demonstrate the utility of\nthis method across various use cases, including clinical applications. For\ninfection prediction, using LLM-elicited priors reduced the number of required\nlabels to achieve the same accuracy as an uninformative prior by 55%, at 200\ndays earlier in the study."
                },
                "authors": [
                    {
                        "name": "Alexander Capstick"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Payam Barnaghi"
                    }
                ],
                "author_detail": {
                    "name": "Payam Barnaghi"
                },
                "author": "Payam Barnaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2109.03641v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2109.03641v3",
                "updated": "2024-11-26T10:06:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    6,
                    44,
                    1,
                    331,
                    0
                ],
                "published": "2021-09-08T13:32:34Z",
                "published_parsed": [
                    2021,
                    9,
                    8,
                    13,
                    32,
                    34,
                    2,
                    251,
                    0
                ],
                "title": "Confidence surfaces for the mean of locally stationary functional time\n  series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence surfaces for the mean of locally stationary functional time\n  series"
                },
                "summary": "The problem of constructing a simultaneous confidence surface for the\n2-dimensional mean function of a non-stationary functional time series is\nchallenging as these bands can not be built on classical limit theory for the\nmaximum absolute deviation between an estimate and the time-dependent\nregression function. In this paper, we propose a new bootstrap methodology to\nconstruct such a region. Our approach is based on a Gaussian approximation for\nthe maximum norm of sparse high-dimensional vectors approximating the maximum\nabsolute deviation which is suitable for nonparametric inference of\nhigh-dimensional time series. The elimination of the zero entries produces\n(besides the time dependence) additional dependencies such that the \"classical\"\nmultiplier bootstrap is not applicable. To solve this issue we develop a novel\nmultiplier bootstrap, where blocks of the coordinates of the vectors are\nmultiplied with random variables, which mimic the specific structure between\nthe vectors appearing in the Gaussian approximation. We prove the validity of\nour approach by asymptotic theory, demonstrate good finite sample properties by\nmeans of a simulation study and illustrate its applicability by analyzing a\ndata example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of constructing a simultaneous confidence surface for the\n2-dimensional mean function of a non-stationary functional time series is\nchallenging as these bands can not be built on classical limit theory for the\nmaximum absolute deviation between an estimate and the time-dependent\nregression function. In this paper, we propose a new bootstrap methodology to\nconstruct such a region. Our approach is based on a Gaussian approximation for\nthe maximum norm of sparse high-dimensional vectors approximating the maximum\nabsolute deviation which is suitable for nonparametric inference of\nhigh-dimensional time series. The elimination of the zero entries produces\n(besides the time dependence) additional dependencies such that the \"classical\"\nmultiplier bootstrap is not applicable. To solve this issue we develop a novel\nmultiplier bootstrap, where blocks of the coordinates of the vectors are\nmultiplied with random variables, which mimic the specific structure between\nthe vectors appearing in the Gaussian approximation. We prove the validity of\nour approach by asymptotic theory, demonstrate good finite sample properties by\nmeans of a simulation study and illustrate its applicability by analyzing a\ndata example."
                },
                "authors": [
                    {
                        "name": "Holger Dette"
                    },
                    {
                        "name": "Weichi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Weichi Wu"
                },
                "author": "Weichi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2109.03641v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2109.03641v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17274v1",
                "updated": "2024-11-26T09:51:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    51,
                    55,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T09:51:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    51,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics"
                },
                "summary": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 11,632\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 11,632\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul."
                },
                "authors": [
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Yan Naing Tun"
                    },
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Tan Bui"
                    },
                    {
                        "name": "Ivana Clairine Irsan"
                    },
                    {
                        "name": "Yiran Cheng"
                    },
                    {
                        "name": "Xiang Lan"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17272v1",
                "updated": "2024-11-26T09:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    47,
                    5,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T09:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    47,
                    5,
                    1,
                    331,
                    0
                ],
                "title": "Reservoir computing with all-optical non-fading memory in a self-pulsing\n  microresonator network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reservoir computing with all-optical non-fading memory in a self-pulsing\n  microresonator network"
                },
                "summary": "Photonic neuromorphic computing may offer promising applications for a broad\nrange of photonic sensors, including optical fiber sensors, to enhance their\nfunctionality while avoiding loss of information, energy consumption, and\nlatency due to optical-electrical conversion. However, time-dependent sensor\nsignals usually exhibit much slower timescales than photonic processors, which\nalso generally lack energy-efficient long-term memory. To address this, we\nexperimentally demonstrate a first implementation of physical reservoir\ncomputing with non-fading memory for multi-timescale signal processing. This is\nbased on a fully passive network of 64 coupled silicon microring resonators.\nOur compact photonic reservoir is capable of hosting energy-efficient nonlinear\ndynamics and multistability. It can process and retain input signal information\nfor an extended duration, at least tens of microseconds. Our reservoir\ncomputing system can learn to infer the timing of a single input pulse and the\nspike rate of an input spike train, even after a relatively long period\nfollowing the end of the input excitation. We demonstrate this operation at two\ndifferent timescales, with approximately a factor of 5 difference. This work\npresents a novel approach to extending the memory of photonic reservoir\ncomputing and its timescale of application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photonic neuromorphic computing may offer promising applications for a broad\nrange of photonic sensors, including optical fiber sensors, to enhance their\nfunctionality while avoiding loss of information, energy consumption, and\nlatency due to optical-electrical conversion. However, time-dependent sensor\nsignals usually exhibit much slower timescales than photonic processors, which\nalso generally lack energy-efficient long-term memory. To address this, we\nexperimentally demonstrate a first implementation of physical reservoir\ncomputing with non-fading memory for multi-timescale signal processing. This is\nbased on a fully passive network of 64 coupled silicon microring resonators.\nOur compact photonic reservoir is capable of hosting energy-efficient nonlinear\ndynamics and multistability. It can process and retain input signal information\nfor an extended duration, at least tens of microseconds. Our reservoir\ncomputing system can learn to infer the timing of a single input pulse and the\nspike rate of an input spike train, even after a relatively long period\nfollowing the end of the input excitation. We demonstrate this operation at two\ndifferent timescales, with approximately a factor of 5 difference. This work\npresents a novel approach to extending the memory of photonic reservoir\ncomputing and its timescale of application."
                },
                "authors": [
                    {
                        "name": "Alessio Lugnan"
                    },
                    {
                        "name": "Stefano Biasi"
                    },
                    {
                        "name": "Alessandro Foradori"
                    },
                    {
                        "name": "Peter Bienstman"
                    },
                    {
                        "name": "Lorenzo Pavesi"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Pavesi"
                },
                "author": "Lorenzo Pavesi",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13476v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13476v2",
                "updated": "2024-11-26T09:46:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    46,
                    25,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-20T17:22:31Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    22,
                    31,
                    2,
                    325,
                    0
                ],
                "title": "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context\n  Training"
                },
                "summary": "Extending context window sizes allows large language models (LLMs) to process\nlonger sequences and handle more complex tasks. Rotary Positional Embedding\n(RoPE) has become the de facto standard due to its relative positional encoding\nproperties that benefit long-context training. However, we observe that using\nRoPE with BFloat16 format results in numerical issues, causing it to deviate\nfrom its intended relative positional encoding, especially in long-context\nscenarios. This issue arises from BFloat16's limited precision and accumulates\nas context length increases, with the first token contributing significantly to\nthis problem. To address this, we develop AnchorAttention, a plug-and-play\nattention method that alleviates numerical issues caused by BFloat16, improves\nlong-context capabilities, and speeds up training. AnchorAttention reduces\nunnecessary attention computations, maintains semantic coherence, and boosts\ncomputational efficiency by treating the first token as a shared anchor with a\nconsistent position ID, making it visible to all documents within the training\ncontext. Experiments on three types of LLMs demonstrate that AnchorAttention\nsignificantly improves long-context performance and reduces training time by\nover 50\\% compared to standard full attention mechanisms, while preserving the\noriginal LLM's capabilities on general tasks. Our code is available at\nhttps://github.com/haonan3/AnchorContext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending context window sizes allows large language models (LLMs) to process\nlonger sequences and handle more complex tasks. Rotary Positional Embedding\n(RoPE) has become the de facto standard due to its relative positional encoding\nproperties that benefit long-context training. However, we observe that using\nRoPE with BFloat16 format results in numerical issues, causing it to deviate\nfrom its intended relative positional encoding, especially in long-context\nscenarios. This issue arises from BFloat16's limited precision and accumulates\nas context length increases, with the first token contributing significantly to\nthis problem. To address this, we develop AnchorAttention, a plug-and-play\nattention method that alleviates numerical issues caused by BFloat16, improves\nlong-context capabilities, and speeds up training. AnchorAttention reduces\nunnecessary attention computations, maintains semantic coherence, and boosts\ncomputational efficiency by treating the first token as a shared anchor with a\nconsistent position ID, making it visible to all documents within the training\ncontext. Experiments on three types of LLMs demonstrate that AnchorAttention\nsignificantly improves long-context performance and reduces training time by\nover 50\\% compared to standard full attention mechanisms, while preserving the\noriginal LLM's capabilities on general tasks. Our code is available at\nhttps://github.com/haonan3/AnchorContext."
                },
                "authors": [
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Tianyu Pang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Pang"
                },
                "author": "Tianyu Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13476v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17269v1",
                "updated": "2024-11-26T09:45:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    45,
                    40,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T09:45:40Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    45,
                    40,
                    1,
                    331,
                    0
                ],
                "title": "CASBI -- Chemical Abundance Simulation-Based Inference for Galactic\n  Archeology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASBI -- Chemical Abundance Simulation-Based Inference for Galactic\n  Archeology"
                },
                "summary": "Galaxies evolve hierarchically through merging with lower-mass systems and\nthe remnants of destroyed galaxies are a key indicator of the past assembly\nhistory of our Galaxy. However, accurately measuring the properties of the\naccreted galaxies and hence unraveling the Milky Way's (MW) formation history\nis a challenging task. Here we introduce CASBI (Chemical Abundance Simulation\nBased Inference), a novel inference pipeline for Galactic Archeology based on\nSimulation-based Inference methods. CASBI leverages on the fact that there is a\nwell defined mass-metallicity relation for galaxies and performs inference of\nkey galaxy properties based on multi-dimensional chemical abundances of stars\nin the stellar halo. Hence, we recast the problem of unraveling the merger\nhistory of the MW into a SBI problem to recover the properties of the building\nblocks (e.g. total stellar mass and infall time) using the multi-dimensional\nchemical abundances of stars in the stellar halo as observable. With CASBI we\nare able to recover the full posterior probability of properties of building\nblocks of Milky Way like galaxies. We highlight CASBI's potential by inferring\nposteriors for the stellar masses of completely phase mixed dwarf galaxies\nsolely from the 2d-distributions of stellar abundance in the iron vs. oxygen\nplane and find accurate and precise inference results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxies evolve hierarchically through merging with lower-mass systems and\nthe remnants of destroyed galaxies are a key indicator of the past assembly\nhistory of our Galaxy. However, accurately measuring the properties of the\naccreted galaxies and hence unraveling the Milky Way's (MW) formation history\nis a challenging task. Here we introduce CASBI (Chemical Abundance Simulation\nBased Inference), a novel inference pipeline for Galactic Archeology based on\nSimulation-based Inference methods. CASBI leverages on the fact that there is a\nwell defined mass-metallicity relation for galaxies and performs inference of\nkey galaxy properties based on multi-dimensional chemical abundances of stars\nin the stellar halo. Hence, we recast the problem of unraveling the merger\nhistory of the MW into a SBI problem to recover the properties of the building\nblocks (e.g. total stellar mass and infall time) using the multi-dimensional\nchemical abundances of stars in the stellar halo as observable. With CASBI we\nare able to recover the full posterior probability of properties of building\nblocks of Milky Way like galaxies. We highlight CASBI's potential by inferring\nposteriors for the stellar masses of completely phase mixed dwarf galaxies\nsolely from the 2d-distributions of stellar abundance in the iron vs. oxygen\nplane and find accurate and precise inference results."
                },
                "authors": [
                    {
                        "name": "Giuseppe Viterbo"
                    },
                    {
                        "name": "Tobias Buck"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Buck"
                },
                "author": "Tobias Buck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17261v1",
                "updated": "2024-11-26T09:37:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    37,
                    59,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T09:37:59Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    37,
                    59,
                    1,
                    331,
                    0
                ],
                "title": "HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility\n  Evaluator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility\n  Evaluator"
                },
                "summary": "AIGC images are prevalent across various fields, yet they frequently suffer\nfrom quality issues like artifacts and unnatural textures. Specialized models\naim to predict defect region heatmaps but face two primary challenges: (1) lack\nof explainability, failing to provide reasons and analyses for subtle defects,\nand (2) inability to leverage common sense and logical reasoning, leading to\npoor generalization. Multimodal large language models (MLLMs) promise better\ncomprehension and reasoning but face their own challenges: (1) difficulty in\nfine-grained defect localization due to the limitations in capturing tiny\ndetails; and (2) constraints in providing pixel-wise outputs necessary for\nprecise heatmap generation. To address these challenges, we propose HEIE: a\nnovel MLLM-Based Hierarchical Explainable image Implausibility Evaluator. We\nintroduce the CoT-Driven Explainable Trinity Evaluator, which integrates\nheatmaps, scores, and explanation outputs, using CoT to decompose complex tasks\ninto subtasks of increasing difficulty and enhance interpretability. Our\nAdaptive Hierarchical Implausibility Mapper synergizes low-level image features\nwith high-level mapper tokens from LLMs, enabling precise local-to-global\nhierarchical heatmap predictions through an uncertainty-based adaptive token\napproach. Moreover, we propose a new dataset: Expl-AIGI-Eval, designed to\nfacilitate interpretable implausibility evaluation of AIGC images. Our method\ndemonstrates state-of-the-art performance through extensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIGC images are prevalent across various fields, yet they frequently suffer\nfrom quality issues like artifacts and unnatural textures. Specialized models\naim to predict defect region heatmaps but face two primary challenges: (1) lack\nof explainability, failing to provide reasons and analyses for subtle defects,\nand (2) inability to leverage common sense and logical reasoning, leading to\npoor generalization. Multimodal large language models (MLLMs) promise better\ncomprehension and reasoning but face their own challenges: (1) difficulty in\nfine-grained defect localization due to the limitations in capturing tiny\ndetails; and (2) constraints in providing pixel-wise outputs necessary for\nprecise heatmap generation. To address these challenges, we propose HEIE: a\nnovel MLLM-Based Hierarchical Explainable image Implausibility Evaluator. We\nintroduce the CoT-Driven Explainable Trinity Evaluator, which integrates\nheatmaps, scores, and explanation outputs, using CoT to decompose complex tasks\ninto subtasks of increasing difficulty and enhance interpretability. Our\nAdaptive Hierarchical Implausibility Mapper synergizes low-level image features\nwith high-level mapper tokens from LLMs, enabling precise local-to-global\nhierarchical heatmap predictions through an uncertainty-based adaptive token\napproach. Moreover, we propose a new dataset: Expl-AIGI-Eval, designed to\nfacilitate interpretable implausibility evaluation of AIGC images. Our method\ndemonstrates state-of-the-art performance through extensive experiments."
                },
                "authors": [
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Ru Zhen"
                    },
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haoxiang Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17255v1",
                "updated": "2024-11-26T09:31:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    31,
                    28,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T09:31:28Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    31,
                    28,
                    1,
                    331,
                    0
                ],
                "title": "APT: Architectural Planning and Text-to-Blueprint Construction Using\n  Large Language Models for Open-World Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APT: Architectural Planning and Text-to-Blueprint Construction Using\n  Large Language Models for Open-World Agents"
                },
                "summary": "We present APT, an advanced Large Language Model (LLM)-driven framework that\nenables autonomous agents to construct complex and creative structures within\nthe Minecraft environment. Unlike previous approaches that primarily\nconcentrate on skill-based open-world tasks or rely on image-based diffusion\nmodels for generating voxel-based structures, our method leverages the\nintrinsic spatial reasoning capabilities of LLMs. By employing chain-of-thought\ndecomposition along with multimodal inputs, the framework generates detailed\narchitectural layouts and blueprints that the agent can execute under zero-shot\nor few-shot learning scenarios. Our agent incorporates both memory and\nreflection modules to facilitate lifelong learning, adaptive refinement, and\nerror correction throughout the building process. To rigorously evaluate the\nagent's performance in this emerging research area, we introduce a\ncomprehensive benchmark consisting of diverse construction tasks designed to\ntest creativity, spatial reasoning, adherence to in-game rules, and the\neffective integration of multimodal instructions. Experimental results using\nvarious GPT-based LLM backends and agent configurations demonstrate the agent's\ncapacity to accurately interpret extensive instructions involving numerous\nitems, their positions, and orientations. The agent successfully produces\ncomplex structures complete with internal functionalities such as\nRedstone-powered systems. A/B testing indicates that the inclusion of a memory\nmodule leads to a significant increase in performance, emphasizing its role in\nenabling continuous learning and the reuse of accumulated experience.\nAdditionally, the agent's unexpected emergence of scaffolding behavior\nhighlights the potential of future LLM-driven agents to utilize subroutine\nplanning and leverage the emergence ability of LLMs to autonomously develop\nhuman-like problem-solving techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present APT, an advanced Large Language Model (LLM)-driven framework that\nenables autonomous agents to construct complex and creative structures within\nthe Minecraft environment. Unlike previous approaches that primarily\nconcentrate on skill-based open-world tasks or rely on image-based diffusion\nmodels for generating voxel-based structures, our method leverages the\nintrinsic spatial reasoning capabilities of LLMs. By employing chain-of-thought\ndecomposition along with multimodal inputs, the framework generates detailed\narchitectural layouts and blueprints that the agent can execute under zero-shot\nor few-shot learning scenarios. Our agent incorporates both memory and\nreflection modules to facilitate lifelong learning, adaptive refinement, and\nerror correction throughout the building process. To rigorously evaluate the\nagent's performance in this emerging research area, we introduce a\ncomprehensive benchmark consisting of diverse construction tasks designed to\ntest creativity, spatial reasoning, adherence to in-game rules, and the\neffective integration of multimodal instructions. Experimental results using\nvarious GPT-based LLM backends and agent configurations demonstrate the agent's\ncapacity to accurately interpret extensive instructions involving numerous\nitems, their positions, and orientations. The agent successfully produces\ncomplex structures complete with internal functionalities such as\nRedstone-powered systems. A/B testing indicates that the inclusion of a memory\nmodule leads to a significant increase in performance, emphasizing its role in\nenabling continuous learning and the reuse of accumulated experience.\nAdditionally, the agent's unexpected emergence of scaffolding behavior\nhighlights the potential of future LLM-driven agents to utilize subroutine\nplanning and leverage the emergence ability of LLMs to autonomously develop\nhuman-like problem-solving techniques."
                },
                "authors": [
                    {
                        "name": "Jun Yu Chen"
                    },
                    {
                        "name": "Tao Gao"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gao"
                },
                "author": "Tao Gao",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11211v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11211v4",
                "updated": "2024-11-26T09:28:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    28,
                    35,
                    1,
                    331,
                    0
                ],
                "published": "2024-07-15T19:53:02Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    19,
                    53,
                    2,
                    0,
                    197,
                    0
                ],
                "title": "Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer\n  from Text to Image via CLIP Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer\n  from Text to Image via CLIP Inversion"
                },
                "summary": "We introduce NOVIC, an innovative real-time uNconstrained Open Vocabulary\nImage Classifier that uses an autoregressive transformer to generatively output\nclassification labels as language. Leveraging the extensive knowledge of CLIP\nmodels, NOVIC harnesses the embedding space to enable zero-shot transfer from\npure text to images. Traditional CLIP models, despite their ability for open\nvocabulary classification, require an exhaustive prompt of potential class\nlabels, restricting their application to images of known content or context. To\naddress this, we propose an \"object decoder\" model that is trained on a\nlarge-scale 92M-target dataset of templated object noun sets and LLM-generated\ncaptions to always output the object noun in question. This effectively inverts\nthe CLIP text encoder and allows textual object labels from essentially the\nentire English language to be generated directly from image-derived embedding\nvectors, without requiring any a priori knowledge of the potential content of\nan image, and without any label biases. The trained decoders are tested on a\nmix of manually and web-curated datasets, as well as standard image\nclassification benchmarks, and achieve fine-grained prompt-free prediction\nscores of up to 87.5%, a strong result considering the model must work for any\nconceivable image and without any contextual clues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NOVIC, an innovative real-time uNconstrained Open Vocabulary\nImage Classifier that uses an autoregressive transformer to generatively output\nclassification labels as language. Leveraging the extensive knowledge of CLIP\nmodels, NOVIC harnesses the embedding space to enable zero-shot transfer from\npure text to images. Traditional CLIP models, despite their ability for open\nvocabulary classification, require an exhaustive prompt of potential class\nlabels, restricting their application to images of known content or context. To\naddress this, we propose an \"object decoder\" model that is trained on a\nlarge-scale 92M-target dataset of templated object noun sets and LLM-generated\ncaptions to always output the object noun in question. This effectively inverts\nthe CLIP text encoder and allows textual object labels from essentially the\nentire English language to be generated directly from image-derived embedding\nvectors, without requiring any a priori knowledge of the potential content of\nan image, and without any label biases. The trained decoders are tested on a\nmix of manually and web-curated datasets, as well as standard image\nclassification benchmarks, and achieve fine-grained prompt-free prediction\nscores of up to 87.5%, a strong result considering the model must work for any\nconceivable image and without any contextual clues."
                },
                "authors": [
                    {
                        "name": "Philipp Allgeuer"
                    },
                    {
                        "name": "Kyra Ahrens"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "Published at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11211v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11211v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15560v2",
                "updated": "2024-11-26T09:25:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    25,
                    22,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-23T13:34:50Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    13,
                    34,
                    50,
                    5,
                    328,
                    0
                ],
                "title": "Do LLMs Agree on the Creativity Evaluation of Alternative Uses?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Agree on the Creativity Evaluation of Alternative Uses?"
                },
                "summary": "This paper investigates whether large language models (LLMs) show agreement\nin assessing creativity in responses to the Alternative Uses Test (AUT). While\nLLMs are increasingly used to evaluate creative content, previous studies have\nprimarily focused on a single model assessing responses generated by the same\nmodel or humans. This paper explores whether LLMs can impartially and\naccurately evaluate creativity in outputs generated by both themselves and\nother models. Using an oracle benchmark set of AUT responses, categorized by\ncreativity level (common, creative, and highly creative), we experiment with\nfour state-of-the-art LLMs evaluating these outputs. We test both scoring and\nranking methods and employ two evaluation settings (comprehensive and\nsegmented) to examine if LLMs agree on the creativity evaluation of alternative\nuses. Results reveal high inter-model agreement, with Spearman correlations\naveraging above 0.7 across models and reaching over 0.77 with respect to the\noracle, indicating a high level of agreement and validating the reliability of\nLLMs in creativity assessment of alternative uses. Notably, models do not\nfavour their own responses, instead they provide similar creativity assessment\nscores or rankings for alternative uses generated by other models. These\nfindings suggest that LLMs exhibit impartiality and high alignment in\ncreativity evaluation, offering promising implications for their use in\nautomated creativity assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates whether large language models (LLMs) show agreement\nin assessing creativity in responses to the Alternative Uses Test (AUT). While\nLLMs are increasingly used to evaluate creative content, previous studies have\nprimarily focused on a single model assessing responses generated by the same\nmodel or humans. This paper explores whether LLMs can impartially and\naccurately evaluate creativity in outputs generated by both themselves and\nother models. Using an oracle benchmark set of AUT responses, categorized by\ncreativity level (common, creative, and highly creative), we experiment with\nfour state-of-the-art LLMs evaluating these outputs. We test both scoring and\nranking methods and employ two evaluation settings (comprehensive and\nsegmented) to examine if LLMs agree on the creativity evaluation of alternative\nuses. Results reveal high inter-model agreement, with Spearman correlations\naveraging above 0.7 across models and reaching over 0.77 with respect to the\noracle, indicating a high level of agreement and validating the reliability of\nLLMs in creativity assessment of alternative uses. Notably, models do not\nfavour their own responses, instead they provide similar creativity assessment\nscores or rankings for alternative uses generated by other models. These\nfindings suggest that LLMs exhibit impartiality and high alignment in\ncreativity evaluation, offering promising implications for their use in\nautomated creativity assessment."
                },
                "authors": [
                    {
                        "name": "Abdullah Al Rabeyah"
                    },
                    {
                        "name": "Fabrício Góes"
                    },
                    {
                        "name": "Marco Volpe"
                    },
                    {
                        "name": "Talles Medeiros"
                    }
                ],
                "author_detail": {
                    "name": "Talles Medeiros"
                },
                "author": "Talles Medeiros",
                "arxiv_comment": "19 pages, 7 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13577v2",
                "updated": "2024-11-26T09:20:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    20,
                    48,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-15T04:16:45Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    16,
                    45,
                    4,
                    320,
                    0
                ],
                "title": "WavChat: A Survey of Spoken Dialogue Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WavChat: A Survey of Spoken Dialogue Models"
                },
                "summary": "Recent advancements in spoken dialogue models, exemplified by systems like\nGPT-4o, have captured significant attention in the speech domain. Compared to\ntraditional three-tier cascaded spoken dialogue models that comprise speech\nrecognition (ASR), large language models (LLMs), and text-to-speech (TTS),\nmodern spoken dialogue models exhibit greater intelligence. These advanced\nspoken dialogue models not only comprehend audio, music, and other\nspeech-related features, but also capture stylistic and timbral characteristics\nin speech. Moreover, they generate high-quality, multi-turn speech responses\nwith low latency, enabling real-time interaction through simultaneous listening\nand speaking capability. Despite the progress in spoken dialogue systems, there\nis a lack of comprehensive surveys that systematically organize and analyze\nthese systems and the underlying technologies. To address this, we have first\ncompiled existing spoken dialogue systems in the chronological order and\ncategorized them into the cascaded and end-to-end paradigms. We then provide an\nin-depth overview of the core technologies in spoken dialogue models, covering\naspects such as speech representation, training paradigm, streaming, duplex,\nand interaction capabilities. Each section discusses the limitations of these\ntechnologies and outlines considerations for future research. Additionally, we\npresent a thorough review of relevant datasets, evaluation metrics, and\nbenchmarks from the perspectives of training and evaluating spoken dialogue\nsystems. We hope this survey will contribute to advancing both academic\nresearch and industrial applications in the field of spoken dialogue systems.\nThe related material is available at https://github.com/jishengpeng/WavChat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in spoken dialogue models, exemplified by systems like\nGPT-4o, have captured significant attention in the speech domain. Compared to\ntraditional three-tier cascaded spoken dialogue models that comprise speech\nrecognition (ASR), large language models (LLMs), and text-to-speech (TTS),\nmodern spoken dialogue models exhibit greater intelligence. These advanced\nspoken dialogue models not only comprehend audio, music, and other\nspeech-related features, but also capture stylistic and timbral characteristics\nin speech. Moreover, they generate high-quality, multi-turn speech responses\nwith low latency, enabling real-time interaction through simultaneous listening\nand speaking capability. Despite the progress in spoken dialogue systems, there\nis a lack of comprehensive surveys that systematically organize and analyze\nthese systems and the underlying technologies. To address this, we have first\ncompiled existing spoken dialogue systems in the chronological order and\ncategorized them into the cascaded and end-to-end paradigms. We then provide an\nin-depth overview of the core technologies in spoken dialogue models, covering\naspects such as speech representation, training paradigm, streaming, duplex,\nand interaction capabilities. Each section discusses the limitations of these\ntechnologies and outlines considerations for future research. Additionally, we\npresent a thorough review of relevant datasets, evaluation metrics, and\nbenchmarks from the perspectives of training and evaluating spoken dialogue\nsystems. We hope this survey will contribute to advancing both academic\nresearch and industrial applications in the field of spoken dialogue systems.\nThe related material is available at https://github.com/jishengpeng/WavChat."
                },
                "authors": [
                    {
                        "name": "Shengpeng Ji"
                    },
                    {
                        "name": "Yifu Chen"
                    },
                    {
                        "name": "Minghui Fang"
                    },
                    {
                        "name": "Jialong Zuo"
                    },
                    {
                        "name": "Jingyu Lu"
                    },
                    {
                        "name": "Hanting Wang"
                    },
                    {
                        "name": "Ziyue Jiang"
                    },
                    {
                        "name": "Long Zhou"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Xize Cheng"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Zehan Wang"
                    },
                    {
                        "name": "Qian Yang"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yidi Jiang"
                    },
                    {
                        "name": "Jingzhen He"
                    },
                    {
                        "name": "Yunfei Chu"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Zhou Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Zhao"
                },
                "author": "Zhou Zhao",
                "arxiv_comment": "60 papes, working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17244v1",
                "updated": "2024-11-26T09:18:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    18,
                    58,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T09:18:58Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    18,
                    58,
                    1,
                    331,
                    0
                ],
                "title": "The apparent and cosmic rates of short gamma-ray bursts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The apparent and cosmic rates of short gamma-ray bursts"
                },
                "summary": "The short gamma-ray burst (sGRB), GRB~170817A, is often considered a rare\nevent. However, its inferred event rate, $\\mathcal{O}(100s)\\ \\text{Gpc}^{-3}\\\n\\text{yr}^{-1}$, exceeds cosmic sGRB rate estimates from high-redshift samples\nby an order of magnitude. This discrepancy can be explained by geometric\neffects related to the structure of the relativistic jet. We first illustrate\nhow adopting a detector flux threshold point estimate rather than an efficiency\nfunction, can lead to a large variation in rate estimates. Simulating the\nFermi-GBM sGRB detection efficiency, we then show that for a given a universal\nstructured jet profile, one can model a geometric bias with redshift. Assuming\ndifferent jet profiles, we show a geometrically scaled rate of GRB~170817A is\nconsistent with the cosmic beaming uncorrected rate estimates of short\n$\\gamma$-ray bursts (sGRBs) and that geometry can boost observational rates\nwithin $\\mathcal{O}(100s)$\\,Mpc. We find an apparent GRB~170817A rate of\n$303_{-300}^{+1580}$ $\\mathrm{Gpc}^{-3}\\, \\mathrm{yr}^{-1} $ which when\ncorrected for geometry yields $6.15_{-6.06}^{+31.2}$ $\\mathrm{Gpc}^{-3}\\,\n\\mathrm{yr}^{-1} $ and $3.34_{-3.29}^{+16.7}$ $\\mathrm{Gpc}^{-3}\\,\n\\mathrm{yr}^{-1} $ for two different jet profiles, consistent with pre-2017\nestimates of the isotropic sGRB rate. Our study shows how jet structure can\nimpact rate estimations and could allow one to test structured jet profiles. We\nfinally show that modelling the maximum structured jet viewing angle with\nredshift can transform a cosmic beaming uncorrected rate to a representative\nestimate of the binary neutron star merger rate. We suggest this framework can\nbe used to demonstrate parity with merger rates or to yield estimates of the\nsuccessful jet fraction of sGRBs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The short gamma-ray burst (sGRB), GRB~170817A, is often considered a rare\nevent. However, its inferred event rate, $\\mathcal{O}(100s)\\ \\text{Gpc}^{-3}\\\n\\text{yr}^{-1}$, exceeds cosmic sGRB rate estimates from high-redshift samples\nby an order of magnitude. This discrepancy can be explained by geometric\neffects related to the structure of the relativistic jet. We first illustrate\nhow adopting a detector flux threshold point estimate rather than an efficiency\nfunction, can lead to a large variation in rate estimates. Simulating the\nFermi-GBM sGRB detection efficiency, we then show that for a given a universal\nstructured jet profile, one can model a geometric bias with redshift. Assuming\ndifferent jet profiles, we show a geometrically scaled rate of GRB~170817A is\nconsistent with the cosmic beaming uncorrected rate estimates of short\n$\\gamma$-ray bursts (sGRBs) and that geometry can boost observational rates\nwithin $\\mathcal{O}(100s)$\\,Mpc. We find an apparent GRB~170817A rate of\n$303_{-300}^{+1580}$ $\\mathrm{Gpc}^{-3}\\, \\mathrm{yr}^{-1} $ which when\ncorrected for geometry yields $6.15_{-6.06}^{+31.2}$ $\\mathrm{Gpc}^{-3}\\,\n\\mathrm{yr}^{-1} $ and $3.34_{-3.29}^{+16.7}$ $\\mathrm{Gpc}^{-3}\\,\n\\mathrm{yr}^{-1} $ for two different jet profiles, consistent with pre-2017\nestimates of the isotropic sGRB rate. Our study shows how jet structure can\nimpact rate estimations and could allow one to test structured jet profiles. We\nfinally show that modelling the maximum structured jet viewing angle with\nredshift can transform a cosmic beaming uncorrected rate to a representative\nestimate of the binary neutron star merger rate. We suggest this framework can\nbe used to demonstrate parity with merger rates or to yield estimates of the\nsuccessful jet fraction of sGRBs."
                },
                "authors": [
                    {
                        "name": "E. J. Howell"
                    },
                    {
                        "name": "E. Burns"
                    },
                    {
                        "name": "A. Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "A. Goldstein"
                },
                "author": "A. Goldstein",
                "arxiv_comment": "Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17236v1",
                "updated": "2024-11-26T08:57:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    8,
                    57,
                    41,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T08:57:41Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    8,
                    57,
                    41,
                    1,
                    331,
                    0
                ],
                "title": "From Graph Diffusion to Graph Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Graph Diffusion to Graph Classification"
                },
                "summary": "Generative models such as diffusion models have achieved remarkable success\nin state-of-the-art image and text tasks. Recently, score-based diffusion\nmodels have extended their success beyond image generation, showing competitive\nperformance with discriminative methods in image {\\em classification}\ntasks~\\cite{zimmermann2021score}. However, their application to classification\nin the {\\em graph} domain, which presents unique challenges such as complex\ntopologies, remains underexplored. We show how graph diffusion models can be\napplied for graph classification. We find that to achieve competitive\nclassification accuracy, score-based graph diffusion models should be trained\nwith a novel training objective that is tailored to graph classification. In\nexperiments with a sampling-based inference method, our discriminative training\nobjective achieves state-of-the-art graph classification accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models such as diffusion models have achieved remarkable success\nin state-of-the-art image and text tasks. Recently, score-based diffusion\nmodels have extended their success beyond image generation, showing competitive\nperformance with discriminative methods in image {\\em classification}\ntasks~\\cite{zimmermann2021score}. However, their application to classification\nin the {\\em graph} domain, which presents unique challenges such as complex\ntopologies, remains underexplored. We show how graph diffusion models can be\napplied for graph classification. We find that to achieve competitive\nclassification accuracy, score-based graph diffusion models should be trained\nwith a novel training objective that is tailored to graph classification. In\nexperiments with a sampling-based inference method, our discriminative training\nobjective achieves state-of-the-art graph classification accuracy."
                },
                "authors": [
                    {
                        "name": "Jia Jun Cheng Xian"
                    },
                    {
                        "name": "Sadegh Mahdavi"
                    },
                    {
                        "name": "Renjie Liao"
                    },
                    {
                        "name": "Oliver Schulte"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Schulte"
                },
                "author": "Oliver Schulte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.04997v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04997v3",
                "updated": "2024-11-26T18:59:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    59,
                    28,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-07T18:59:16Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    16,
                    3,
                    312,
                    0
                ],
                "title": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation"
                },
                "summary": "CLIP is a foundational multimodal model that aligns image and text features\ninto a shared space using contrastive learning on large-scale image-text pairs.\nIts strength lies in leveraging natural language as a rich supervisory signal.\nWith the rapid progress of large language models (LLMs), we explore their\npotential to further enhance CLIP's multimodal representation learning. This\nwork introduces a fine-tuning approach that integrates LLMs with the pretrained\nCLIP visual encoder, leveraging LLMs' advanced text understanding and\nopen-world knowledge to improve CLIP's ability to process long and complex\ncaptions. To address the challenge of LLMs' autoregressive nature, we propose a\ncaption-to-caption contrastive learning framework to enhance the discriminative\npower of their outputs. Our method achieves substantial performance gains on\nvarious downstream tasks, demonstrating the effectiveness of combining LLMs\nwith CLIP for enhanced multimodal learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP is a foundational multimodal model that aligns image and text features\ninto a shared space using contrastive learning on large-scale image-text pairs.\nIts strength lies in leveraging natural language as a rich supervisory signal.\nWith the rapid progress of large language models (LLMs), we explore their\npotential to further enhance CLIP's multimodal representation learning. This\nwork introduces a fine-tuning approach that integrates LLMs with the pretrained\nCLIP visual encoder, leveraging LLMs' advanced text understanding and\nopen-world knowledge to improve CLIP's ability to process long and complex\ncaptions. To address the challenge of LLMs' autoregressive nature, we propose a\ncaption-to-caption contrastive learning framework to enhance the discriminative\npower of their outputs. Our method achieves substantial performance gains on\nvarious downstream tasks, demonstrating the effectiveness of combining LLMs\nwith CLIP for enhanced multimodal learning."
                },
                "authors": [
                    {
                        "name": "Weiquan Huang"
                    },
                    {
                        "name": "Aoqi Wu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Liang Hu"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Xiyang Dai"
                    },
                    {
                        "name": "Dongdong Chen"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04997v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04997v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17693v1",
                "updated": "2024-11-26T18:58:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    58,
                    20,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:58:20Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    58,
                    20,
                    1,
                    331,
                    0
                ],
                "title": "Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats"
                },
                "summary": "As large language models (LLMs) become increasingly capable, it is prudent to\nassess whether safety measures remain effective even if LLMs intentionally try\nto bypass them. Previous work introduced control evaluations, an adversarial\nframework for testing deployment strategies of untrusted models (i.e., models\nwhich might be trying to bypass safety measures). While prior work treats a\nsingle failure as unacceptable, we perform control evaluations in a\n\"distributed threat setting\" -- a setting where no single action is\ncatastrophic and no single action provides overwhelming evidence of\nmisalignment. We approach this problem with a two-level deployment framework\nthat uses an adaptive macro-protocol to choose between micro-protocols.\nMicro-protocols operate on a single task, using a less capable, but extensively\ntested (trusted) model to harness and monitor the untrusted model. Meanwhile,\nthe macro-protocol maintains an adaptive credence on the untrusted model's\nalignment based on its past actions, using it to pick between safer and riskier\nmicro-protocols. We evaluate our method in a code generation testbed where a\nred team attempts to generate subtly backdoored code with an LLM whose\ndeployment is safeguarded by a blue team. We plot Pareto frontiers of safety (#\nof non-backdoored solutions) and usefulness (# of correct solutions). At a\ngiven level of usefulness, our adaptive deployment strategy reduces the number\nof backdoors by 80% compared to non-adaptive baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly capable, it is prudent to\nassess whether safety measures remain effective even if LLMs intentionally try\nto bypass them. Previous work introduced control evaluations, an adversarial\nframework for testing deployment strategies of untrusted models (i.e., models\nwhich might be trying to bypass safety measures). While prior work treats a\nsingle failure as unacceptable, we perform control evaluations in a\n\"distributed threat setting\" -- a setting where no single action is\ncatastrophic and no single action provides overwhelming evidence of\nmisalignment. We approach this problem with a two-level deployment framework\nthat uses an adaptive macro-protocol to choose between micro-protocols.\nMicro-protocols operate on a single task, using a less capable, but extensively\ntested (trusted) model to harness and monitor the untrusted model. Meanwhile,\nthe macro-protocol maintains an adaptive credence on the untrusted model's\nalignment based on its past actions, using it to pick between safer and riskier\nmicro-protocols. We evaluate our method in a code generation testbed where a\nred team attempts to generate subtly backdoored code with an LLM whose\ndeployment is safeguarded by a blue team. We plot Pareto frontiers of safety (#\nof non-backdoored solutions) and usefulness (# of correct solutions). At a\ngiven level of usefulness, our adaptive deployment strategy reduces the number\nof backdoors by 80% compared to non-adaptive baselines."
                },
                "authors": [
                    {
                        "name": "Jiaxin Wen"
                    },
                    {
                        "name": "Vivek Hebbar"
                    },
                    {
                        "name": "Caleb Larson"
                    },
                    {
                        "name": "Aryan Bhatt"
                    },
                    {
                        "name": "Ansh Radhakrishnan"
                    },
                    {
                        "name": "Mrinank Sharma"
                    },
                    {
                        "name": "Henry Sleight"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "He He"
                    },
                    {
                        "name": "Ethan Perez"
                    },
                    {
                        "name": "Buck Shlegeris"
                    },
                    {
                        "name": "Akbir Khan"
                    }
                ],
                "author_detail": {
                    "name": "Akbir Khan"
                },
                "author": "Akbir Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17691v1",
                "updated": "2024-11-26T18:57:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    57,
                    58,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:57:58Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    57,
                    58,
                    1,
                    331,
                    0
                ],
                "title": "Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for\n  Quantized LLMs with 100T Training Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for\n  Quantized LLMs with 100T Training Tokens"
                },
                "summary": "We reveal that low-bit quantization favors undertrained large language models\n(LLMs) by observing that models with larger sizes or fewer training tokens\nexperience less quantization-induced degradation (QiD) when applying low-bit\nquantization, whereas smaller models with extensive training tokens suffer\nsignificant QiD. To gain deeper insights into this trend, we study over 1500\nquantized LLM checkpoints of various sizes and at different training levels\n(undertrained or fully trained) in a controlled setting, deriving scaling laws\nfor understanding the relationship between QiD and factors such as the number\nof training tokens, model size and bit width.\n  With the derived scaling laws, we propose a novel perspective that we can use\nQiD to measure an LLM's training levels and determine the number of training\ntokens required for fully training LLMs of various sizes. Moreover, we use the\nscaling laws to predict the quantization performance of different-sized LLMs\ntrained with 100 trillion tokens. Our projection shows that the low-bit\nquantization performance of future models, which are expected to be trained\nwith over 100 trillion tokens, may NOT be desirable. This poses a potential\nchallenge for low-bit quantization in the future and highlights the need for\nawareness of a model's training level when evaluating low-bit quantization\nresearch. To facilitate future research on this problem, we release all the\n1500+ quantized checkpoints used in this work at\nhttps://huggingface.co/Xu-Ouyang.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We reveal that low-bit quantization favors undertrained large language models\n(LLMs) by observing that models with larger sizes or fewer training tokens\nexperience less quantization-induced degradation (QiD) when applying low-bit\nquantization, whereas smaller models with extensive training tokens suffer\nsignificant QiD. To gain deeper insights into this trend, we study over 1500\nquantized LLM checkpoints of various sizes and at different training levels\n(undertrained or fully trained) in a controlled setting, deriving scaling laws\nfor understanding the relationship between QiD and factors such as the number\nof training tokens, model size and bit width.\n  With the derived scaling laws, we propose a novel perspective that we can use\nQiD to measure an LLM's training levels and determine the number of training\ntokens required for fully training LLMs of various sizes. Moreover, we use the\nscaling laws to predict the quantization performance of different-sized LLMs\ntrained with 100 trillion tokens. Our projection shows that the low-bit\nquantization performance of future models, which are expected to be trained\nwith over 100 trillion tokens, may NOT be desirable. This poses a potential\nchallenge for low-bit quantization in the future and highlights the need for\nawareness of a model's training level when evaluating low-bit quantization\nresearch. To facilitate future research on this problem, we release all the\n1500+ quantized checkpoints used in this work at\nhttps://huggingface.co/Xu-Ouyang."
                },
                "authors": [
                    {
                        "name": "Xu Ouyang"
                    },
                    {
                        "name": "Tao Ge"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "Work in progress; Please note that Figure 1's gray areas may not be\n  displayed properly using Chrome (maybe due to bugs in Chrome)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02611v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02611v3",
                "updated": "2024-11-26T18:51:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    51,
                    57,
                    1,
                    331,
                    0
                ],
                "published": "2024-06-03T07:56:58Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    7,
                    56,
                    58,
                    0,
                    155,
                    0
                ],
                "title": "LOLA: LLM-Assisted Online Learning Algorithm for Content Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LOLA: LLM-Assisted Online Learning Algorithm for Content Experiments"
                },
                "summary": "Modern media firms require automated and efficient methods to identify\ncontent that is most engaging and appealing to users. Leveraging a large-scale\ndataset from Upworthy (a news publisher), which includes 17,681 headline A/B\ntests, we first investigate the ability of three pure-LLM approaches to\nidentify the catchiest headline: prompt-based methods, embedding-based methods,\nand fine-tuned open-source LLMs. Prompt-based approaches perform poorly, while\nboth OpenAI-embedding-based models and the fine-tuned Llama-3-8B achieve\nmarginally higher accuracy than random predictions. In sum, none of the\npure-LLM-based methods can predict the best-performing headline with high\naccuracy. We then introduce the LLM-Assisted Online Learning Algorithm (LOLA),\na novel framework that integrates Large Language Models (LLMs) with adaptive\nexperimentation to optimize content delivery. LOLA combines the best pure-LLM\napproach with the Upper Confidence Bound algorithm to allocate traffic and\nmaximize clicks adaptively. Our numerical experiments on Upworthy data show\nthat LOLA outperforms the standard A/B test method (the current status quo at\nUpworthy), pure bandit algorithms, and pure-LLM approaches, particularly in\nscenarios with limited experimental traffic. Our approach is scalable and\napplicable to content experiments across various settings where firms seek to\noptimize user engagement, including digital advertising and social media\nrecommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern media firms require automated and efficient methods to identify\ncontent that is most engaging and appealing to users. Leveraging a large-scale\ndataset from Upworthy (a news publisher), which includes 17,681 headline A/B\ntests, we first investigate the ability of three pure-LLM approaches to\nidentify the catchiest headline: prompt-based methods, embedding-based methods,\nand fine-tuned open-source LLMs. Prompt-based approaches perform poorly, while\nboth OpenAI-embedding-based models and the fine-tuned Llama-3-8B achieve\nmarginally higher accuracy than random predictions. In sum, none of the\npure-LLM-based methods can predict the best-performing headline with high\naccuracy. We then introduce the LLM-Assisted Online Learning Algorithm (LOLA),\na novel framework that integrates Large Language Models (LLMs) with adaptive\nexperimentation to optimize content delivery. LOLA combines the best pure-LLM\napproach with the Upper Confidence Bound algorithm to allocate traffic and\nmaximize clicks adaptively. Our numerical experiments on Upworthy data show\nthat LOLA outperforms the standard A/B test method (the current status quo at\nUpworthy), pure bandit algorithms, and pure-LLM approaches, particularly in\nscenarios with limited experimental traffic. Our approach is scalable and\napplicable to content experiments across various settings where firms seek to\noptimize user engagement, including digital advertising and social media\nrecommendations."
                },
                "authors": [
                    {
                        "name": "Zikun Ye"
                    },
                    {
                        "name": "Hema Yoganarasimhan"
                    },
                    {
                        "name": "Yufeng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yufeng Zheng"
                },
                "author": "Yufeng Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02611v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02611v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17679v1",
                "updated": "2024-11-26T18:44:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    44,
                    39,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:44:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    44,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "Enhancing Character-Level Understanding in LLMs through Token Internal\n  Structure Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Character-Level Understanding in LLMs through Token Internal\n  Structure Learning"
                },
                "summary": "Tokenization techniques such as Byte-Pair Encoding (BPE) and Byte-Level BPE\n(BBPE) have significantly improved the computational efficiency and vocabulary\nrepresentation stability of large language models (LLMs) by segmenting text\ninto tokens. However, this segmentation often obscures the internal character\nstructures and sequences within tokens, preventing models from fully learning\nthese intricate details during training. Consequently, LLMs struggle to\ncomprehend the character compositions and positional relationships within\ntokens, especially when fine-tuned on downstream tasks with limited data. In\nthis paper, we introduce Token Internal Position Awareness (TIPA), a novel\napproach that enhances LLMs' understanding of internal token structures by\ntraining them on reverse character prediction tasks using the tokenizer's own\nvocabulary. This method enables models to effectively learn and generalize\ncharacter positions and internal structures. Experimental results demonstrate\nthat LLMs trained with TIPA outperform baseline models in predicting character\npositions at the token level. Furthermore, when applied to the downstream task\nof Chinese Spelling Correction (CSC), TIPA not only accelerates model\nconvergence but also significantly improves task performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization techniques such as Byte-Pair Encoding (BPE) and Byte-Level BPE\n(BBPE) have significantly improved the computational efficiency and vocabulary\nrepresentation stability of large language models (LLMs) by segmenting text\ninto tokens. However, this segmentation often obscures the internal character\nstructures and sequences within tokens, preventing models from fully learning\nthese intricate details during training. Consequently, LLMs struggle to\ncomprehend the character compositions and positional relationships within\ntokens, especially when fine-tuned on downstream tasks with limited data. In\nthis paper, we introduce Token Internal Position Awareness (TIPA), a novel\napproach that enhances LLMs' understanding of internal token structures by\ntraining them on reverse character prediction tasks using the tokenizer's own\nvocabulary. This method enables models to effectively learn and generalize\ncharacter positions and internal structures. Experimental results demonstrate\nthat LLMs trained with TIPA outperform baseline models in predicting character\npositions at the token level. Furthermore, when applied to the downstream task\nof Chinese Spelling Correction (CSC), TIPA not only accelerates model\nconvergence but also significantly improves task performance."
                },
                "authors": [
                    {
                        "name": "Zhu Xu"
                    },
                    {
                        "name": "Zhiqiang Zhao"
                    },
                    {
                        "name": "Zihan Zhang"
                    },
                    {
                        "name": "Yuchi Liu"
                    },
                    {
                        "name": "Quanwei Shen"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Yu Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Kuang"
                },
                "author": "Yu Kuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17674v1",
                "updated": "2024-11-26T18:35:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    35,
                    24,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:35:24Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    35,
                    24,
                    1,
                    331,
                    0
                ],
                "title": "Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with\n  Receptive-Field-Aware Attention Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with\n  Receptive-Field-Aware Attention Weighting"
                },
                "summary": "Understanding the emotions in a dialogue usually requires external knowledge\nto accurately understand the contents. As the LLMs become more and more\npowerful, we do not want to settle on the limited ability of the pre-trained\nlanguage model. However, the LLMs either can only process text modality or are\ntoo expensive to process the multimedia information. We aim to utilize both the\npower of LLMs and the supplementary features from the multimedia modalities. In\nthis paper, we present a framework, Lantern, that can improve the performance\nof a certain vanilla model by prompting large language models with\nreceptive-field-aware attention weighting. This framework trained a multi-task\nvanilla model to produce probabilities of emotion classes and dimension scores.\nThese predictions are fed into the LLMs as references to adjust the predicted\nprobabilities of each emotion class with its external knowledge and contextual\nunderstanding. We slice the dialogue into different receptive fields, and each\nsample is included in exactly t receptive fields. Finally, the predictions of\nLLMs are merged with a receptive-field-aware attention-driven weighting module.\nIn the experiments, vanilla models CORECT and SDT are deployed in Lantern with\nGPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way\nsettings demonstrated that the Lantern can significantly improve the\nperformance of current vanilla models by up to 1.23% and 1.80%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the emotions in a dialogue usually requires external knowledge\nto accurately understand the contents. As the LLMs become more and more\npowerful, we do not want to settle on the limited ability of the pre-trained\nlanguage model. However, the LLMs either can only process text modality or are\ntoo expensive to process the multimedia information. We aim to utilize both the\npower of LLMs and the supplementary features from the multimedia modalities. In\nthis paper, we present a framework, Lantern, that can improve the performance\nof a certain vanilla model by prompting large language models with\nreceptive-field-aware attention weighting. This framework trained a multi-task\nvanilla model to produce probabilities of emotion classes and dimension scores.\nThese predictions are fed into the LLMs as references to adjust the predicted\nprobabilities of each emotion class with its external knowledge and contextual\nunderstanding. We slice the dialogue into different receptive fields, and each\nsample is included in exactly t receptive fields. Finally, the predictions of\nLLMs are merged with a receptive-field-aware attention-driven weighting module.\nIn the experiments, vanilla models CORECT and SDT are deployed in Lantern with\nGPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way\nsettings demonstrated that the Lantern can significantly improve the\nperformance of current vanilla models by up to 1.23% and 1.80%."
                },
                "authors": [
                    {
                        "name": "Liyun Zhang"
                    },
                    {
                        "name": "Dian Ding"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Yi-Chao Chen"
                    },
                    {
                        "name": "Guangtao Xue"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Xue"
                },
                "author": "Guangtao Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17673v1",
                "updated": "2024-11-26T18:32:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    32,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:32:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    32,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "SketchAgent: Language-Driven Sequential Sketch Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SketchAgent: Language-Driven Sequential Sketch Generation"
                },
                "summary": "Sketching serves as a versatile tool for externalizing ideas, enabling rapid\nexploration and visual communication that spans various disciplines. While\nartificial systems have driven substantial advances in content creation and\nhuman-computer interaction, capturing the dynamic and abstract nature of human\nsketching remains challenging. In this work, we introduce SketchAgent, a\nlanguage-driven, sequential sketch generation method that enables users to\ncreate, modify, and refine sketches through dynamic, conversational\ninteractions. Our approach requires no training or fine-tuning. Instead, we\nleverage the sequential nature and rich prior knowledge of off-the-shelf\nmultimodal large language models (LLMs). We present an intuitive sketching\nlanguage, introduced to the model through in-context examples, enabling it to\n\"draw\" using string-based actions. These are processed into vector graphics and\nthen rendered to create a sketch on a pixel canvas, which can be accessed again\nfor further tasks. By drawing stroke by stroke, our agent captures the\nevolving, dynamic qualities intrinsic to sketching. We demonstrate that\nSketchAgent can generate sketches from diverse prompts, engage in\ndialogue-driven drawing, and collaborate meaningfully with human users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sketching serves as a versatile tool for externalizing ideas, enabling rapid\nexploration and visual communication that spans various disciplines. While\nartificial systems have driven substantial advances in content creation and\nhuman-computer interaction, capturing the dynamic and abstract nature of human\nsketching remains challenging. In this work, we introduce SketchAgent, a\nlanguage-driven, sequential sketch generation method that enables users to\ncreate, modify, and refine sketches through dynamic, conversational\ninteractions. Our approach requires no training or fine-tuning. Instead, we\nleverage the sequential nature and rich prior knowledge of off-the-shelf\nmultimodal large language models (LLMs). We present an intuitive sketching\nlanguage, introduced to the model through in-context examples, enabling it to\n\"draw\" using string-based actions. These are processed into vector graphics and\nthen rendered to create a sketch on a pixel canvas, which can be accessed again\nfor further tasks. By drawing stroke by stroke, our agent captures the\nevolving, dynamic qualities intrinsic to sketching. We demonstrate that\nSketchAgent can generate sketches from diverse prompts, engage in\ndialogue-driven drawing, and collaborate meaningfully with human users."
                },
                "authors": [
                    {
                        "name": "Yael Vinker"
                    },
                    {
                        "name": "Tamar Rott Shaham"
                    },
                    {
                        "name": "Kristine Zheng"
                    },
                    {
                        "name": "Alex Zhao"
                    },
                    {
                        "name": "Judith E Fan"
                    },
                    {
                        "name": "Antonio Torralba"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Torralba"
                },
                "author": "Antonio Torralba",
                "arxiv_comment": "project page: https://sketch-agent.csail.mit.edu/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17672v1",
                "updated": "2024-11-26T18:31:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    31,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:31:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    31,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Synthetic Data Generation with LLM for Improved Depression Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Data Generation with LLM for Improved Depression Prediction"
                },
                "summary": "Automatic detection of depression is a rapidly growing field of research at\nthe intersection of psychology and machine learning. However, with its\nexponential interest comes a growing concern for data privacy and scarcity due\nto the sensitivity of such a topic. In this paper, we propose a pipeline for\nLarge Language Models (LLMs) to generate synthetic data to improve the\nperformance of depression prediction models. Starting from unstructured,\nnaturalistic text data from recorded transcripts of clinical interviews, we\nutilize an open-source LLM to generate synthetic data through chain-of-thought\nprompting. This pipeline involves two key steps: the first step is the\ngeneration of the synopsis and sentiment analysis based on the original\ntranscript and depression score, while the second is the generation of the\nsynthetic synopsis/sentiment analysis based on the summaries generated in the\nfirst step and a new depression score. Not only was the synthetic data\nsatisfactory in terms of fidelity and privacy-preserving metrics, it also\nbalanced the distribution of severity in the training dataset, thereby\nsignificantly enhancing the model's capability in predicting the intensity of\nthe patient's depression. By leveraging LLMs to generate synthetic data that\ncan be augmented to limited and imbalanced real-world datasets, we demonstrate\na novel approach to addressing data scarcity and privacy concerns commonly\nfaced in automatic depression detection, all while maintaining the statistical\nintegrity of the original dataset. This approach offers a robust framework for\nfuture mental health research and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic detection of depression is a rapidly growing field of research at\nthe intersection of psychology and machine learning. However, with its\nexponential interest comes a growing concern for data privacy and scarcity due\nto the sensitivity of such a topic. In this paper, we propose a pipeline for\nLarge Language Models (LLMs) to generate synthetic data to improve the\nperformance of depression prediction models. Starting from unstructured,\nnaturalistic text data from recorded transcripts of clinical interviews, we\nutilize an open-source LLM to generate synthetic data through chain-of-thought\nprompting. This pipeline involves two key steps: the first step is the\ngeneration of the synopsis and sentiment analysis based on the original\ntranscript and depression score, while the second is the generation of the\nsynthetic synopsis/sentiment analysis based on the summaries generated in the\nfirst step and a new depression score. Not only was the synthetic data\nsatisfactory in terms of fidelity and privacy-preserving metrics, it also\nbalanced the distribution of severity in the training dataset, thereby\nsignificantly enhancing the model's capability in predicting the intensity of\nthe patient's depression. By leveraging LLMs to generate synthetic data that\ncan be augmented to limited and imbalanced real-world datasets, we demonstrate\na novel approach to addressing data scarcity and privacy concerns commonly\nfaced in automatic depression detection, all while maintaining the statistical\nintegrity of the original dataset. This approach offers a robust framework for\nfuture mental health research and applications."
                },
                "authors": [
                    {
                        "name": "Andrea Kang"
                    },
                    {
                        "name": "Jun Yu Chen"
                    },
                    {
                        "name": "Zoe Lee-Youngzie"
                    },
                    {
                        "name": "Shuhao Fu"
                    }
                ],
                "author_detail": {
                    "name": "Shuhao Fu"
                },
                "author": "Shuhao Fu",
                "arxiv_comment": "6 pages excluding references and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17651v1",
                "updated": "2024-11-26T18:16:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    16,
                    56,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:16:56Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    16,
                    56,
                    1,
                    331,
                    0
                ],
                "title": "Toward High-Performance LLM Serving: A Simulation-Based Approach for\n  Identifying Optimal Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward High-Performance LLM Serving: A Simulation-Based Approach for\n  Identifying Optimal Parallelism"
                },
                "summary": "Serving Large Language Models (LLMs) efficiently has become crucial. LLMs are\noften served with multiple devices using techniques like data, pipeline, and\ntensor parallelisms. Each parallelism presents trade-offs between computation,\nmemory, and communication overhead, making it challenging to determine the\noptimal parallel execution plan. Moreover, input workloads also impact\nparallelism strategies. Tasks with long prompts like article summarization are\ncompute-intensive, while tasks with long generation lengths like code\ngeneration are often memory-intensive; these differing characteristics result\nin distinct optimal execution plans. Since searching for the optimal plan via\nactual deployment is prohibitively expensive, we propose APEX, an LLM serving\nsystem simulator that efficiently identifies an optimal parallel execution\nplan. APEX captures the complex characteristics of iteration-level batching, a\ntechnique widely used in SOTA LLM serving systems. APEX leverages the\nrepetitive structure of LLMs to reduce design space, maintaining a similar\nsimulation overhead, even when scaling to trillion scale models. APEX supports\na wide range of LLMs, device clusters, etc., and it can be easily extended\nthrough its high-level templates. We run APEX simulations using a CPU and\nevaluate the identified optimal plans using 8 H100 GPUs, encompassing a wide\nrange of LLMs and input workloads. We show that APEX can find optimal execution\nplans that are up to 4.42x faster than heuristic plans in terms of end-to-end\nserving latency. APEX also reports a set of metrics used in LLM serving\nsystems, such as time per output token and time to first token. Furthermore,\nAPEX can identify an optimal parallel execution plan within 15 minutes using a\nCPU. This is 71x faster and 1234x more cost-effective than actual deployment on\na GPU cluster using cloud services. APEX will be open-sourced upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) efficiently has become crucial. LLMs are\noften served with multiple devices using techniques like data, pipeline, and\ntensor parallelisms. Each parallelism presents trade-offs between computation,\nmemory, and communication overhead, making it challenging to determine the\noptimal parallel execution plan. Moreover, input workloads also impact\nparallelism strategies. Tasks with long prompts like article summarization are\ncompute-intensive, while tasks with long generation lengths like code\ngeneration are often memory-intensive; these differing characteristics result\nin distinct optimal execution plans. Since searching for the optimal plan via\nactual deployment is prohibitively expensive, we propose APEX, an LLM serving\nsystem simulator that efficiently identifies an optimal parallel execution\nplan. APEX captures the complex characteristics of iteration-level batching, a\ntechnique widely used in SOTA LLM serving systems. APEX leverages the\nrepetitive structure of LLMs to reduce design space, maintaining a similar\nsimulation overhead, even when scaling to trillion scale models. APEX supports\na wide range of LLMs, device clusters, etc., and it can be easily extended\nthrough its high-level templates. We run APEX simulations using a CPU and\nevaluate the identified optimal plans using 8 H100 GPUs, encompassing a wide\nrange of LLMs and input workloads. We show that APEX can find optimal execution\nplans that are up to 4.42x faster than heuristic plans in terms of end-to-end\nserving latency. APEX also reports a set of metrics used in LLM serving\nsystems, such as time per output token and time to first token. Furthermore,\nAPEX can identify an optimal parallel execution plan within 15 minutes using a\nCPU. This is 71x faster and 1234x more cost-effective than actual deployment on\na GPU cluster using cloud services. APEX will be open-sourced upon acceptance."
                },
                "authors": [
                    {
                        "name": "Yi-Chien Lin"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Ronald Pineda"
                    },
                    {
                        "name": "Fanny Nina Paravecino"
                    }
                ],
                "author_detail": {
                    "name": "Fanny Nina Paravecino"
                },
                "author": "Fanny Nina Paravecino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12240v2",
                "updated": "2024-11-26T18:14:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    14,
                    50,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-19T05:37:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    5,
                    37,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "Evaluating Tokenizer Performance of Large Language Models Across\n  Official Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Tokenizer Performance of Large Language Models Across\n  Official Indian Languages"
                },
                "summary": "Large Language Models (LLMs) based on transformer architectures have\nrevolutionized a variety of domains, with tokenization playing a pivotal role\nin their pre-processing and fine-tuning stages. In multilingual models,\nparticularly those tailored for Indic languages, effective tokenization is\ncrucial for optimizing performance. This paper presents a comprehensive\nevaluation of tokenizers used by 12 LLMs across all 22 official languages of\nIndia, with a focus on comparing the efficiency of their tokenization\nprocesses. We employed the Normalized Sequence Length (NSL) as a key metric in\nour analysis. Our findings reveal that the SUTRA tokenizer outperforms all\nother models, including several Indic-specific models, excelling in 14\nlanguages. Notable insights include the SUTRA tokenizer's superior handling of\nIndic languages, GPT-4o's advancement over its predecessor GPT-4 in processing\nIndian languages, and the limited performance of Project Indus in certain\nlanguages. This study underscores the critical importance of developing\ntargeted tokenization strategies for multilingual and Indic-centric models,\nlaying the groundwork for future improvements in tokenizer design to enhance\nlinguistic coverage and model efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on transformer architectures have\nrevolutionized a variety of domains, with tokenization playing a pivotal role\nin their pre-processing and fine-tuning stages. In multilingual models,\nparticularly those tailored for Indic languages, effective tokenization is\ncrucial for optimizing performance. This paper presents a comprehensive\nevaluation of tokenizers used by 12 LLMs across all 22 official languages of\nIndia, with a focus on comparing the efficiency of their tokenization\nprocesses. We employed the Normalized Sequence Length (NSL) as a key metric in\nour analysis. Our findings reveal that the SUTRA tokenizer outperforms all\nother models, including several Indic-specific models, excelling in 14\nlanguages. Notable insights include the SUTRA tokenizer's superior handling of\nIndic languages, GPT-4o's advancement over its predecessor GPT-4 in processing\nIndian languages, and the limited performance of Project Indus in certain\nlanguages. This study underscores the critical importance of developing\ntargeted tokenization strategies for multilingual and Indic-centric models,\nlaying the groundwork for future improvements in tokenizer design to enhance\nlinguistic coverage and model efficiency."
                },
                "authors": [
                    {
                        "name": "S. Tamang"
                    },
                    {
                        "name": "D. J. Bora"
                    }
                ],
                "author_detail": {
                    "name": "D. J. Bora"
                },
                "author": "D. J. Bora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17637v1",
                "updated": "2024-11-26T17:55:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    55,
                    37,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:55:37Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    55,
                    37,
                    1,
                    331,
                    0
                ],
                "title": "On Limitations of LLM as Annotator for Low Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Limitations of LLM as Annotator for Low Resource Languages"
                },
                "summary": "Low-resource languages face significant challenges due to the lack of\nsufficient linguistic data, resources, and tools for tasks such as supervised\nlearning, annotation, and classification. This shortage hinders the development\nof accurate models and datasets, making it difficult to perform critical NLP\ntasks like sentiment analysis or hate speech detection. To bridge this gap,\nLarge Language Models (LLMs) present an opportunity for potential annotators,\ncapable of generating datasets and resources for these underrepresented\nlanguages. In this paper, we focus on Marathi, a low-resource language, and\nevaluate the performance of both closed-source and open-source LLMs as\nannotators. We assess models such as GPT-4o and Gemini 1.0 Pro, Gemma 2 (2B and\n9B), and Llama 3.1 (8B) on classification tasks including sentiment analysis,\nnews classification, and hate speech detection. Our findings reveal that while\nLLMs excel in annotation tasks for high-resource languages like English, they\nstill fall short when applied to Marathi. Even advanced closed models like\nGemini and GPT underperform in comparison to BERT-based baselines, highlighting\nthe limitations of LLMs as annotators for low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-resource languages face significant challenges due to the lack of\nsufficient linguistic data, resources, and tools for tasks such as supervised\nlearning, annotation, and classification. This shortage hinders the development\nof accurate models and datasets, making it difficult to perform critical NLP\ntasks like sentiment analysis or hate speech detection. To bridge this gap,\nLarge Language Models (LLMs) present an opportunity for potential annotators,\ncapable of generating datasets and resources for these underrepresented\nlanguages. In this paper, we focus on Marathi, a low-resource language, and\nevaluate the performance of both closed-source and open-source LLMs as\nannotators. We assess models such as GPT-4o and Gemini 1.0 Pro, Gemma 2 (2B and\n9B), and Llama 3.1 (8B) on classification tasks including sentiment analysis,\nnews classification, and hate speech detection. Our findings reveal that while\nLLMs excel in annotation tasks for high-resource languages like English, they\nstill fall short when applied to Marathi. Even advanced closed models like\nGemini and GPT underperform in comparison to BERT-based baselines, highlighting\nthe limitations of LLMs as annotators for low-resource languages."
                },
                "authors": [
                    {
                        "name": "Suramya Jadhav"
                    },
                    {
                        "name": "Abhay Shanbhag"
                    },
                    {
                        "name": "Amogh Thakurdesai"
                    },
                    {
                        "name": "Ridhima Sinare"
                    },
                    {
                        "name": "Raviraj Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Raviraj Joshi"
                },
                "author": "Raviraj Joshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17636v1",
                "updated": "2024-11-26T17:53:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    53,
                    44,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:53:44Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    53,
                    44,
                    1,
                    331,
                    0
                ],
                "title": "MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics\n  Manipulation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable planning abilities\nacross various domains, including robotics manipulation and navigation. While\nrecent efforts in robotics have leveraged LLMs both for high-level and\nlow-level planning, these approaches often face significant challenges, such as\nhallucinations in long-horizon tasks and limited adaptability due to the\ngeneration of plans in a single pass without real-time feedback. To address\nthese limitations, we propose a novel multi-agent LLM framework, Multi-Agent\nLarge Language Model for Manipulation (MALMM) that distributes high-level\nplanning and low-level control code generation across specialized LLM agents,\nsupervised by an additional agent that dynamically manages transitions. By\nincorporating observations from the environment after each step, our framework\neffectively handles intermediate failures and enables adaptive re-planning.\nUnlike existing methods, our approach does not rely on pre-trained skill\npolicies or in-context learning examples and generalizes to a variety of new\ntasks. We evaluate our approach on nine RLBench tasks, including long-horizon\ntasks, and demonstrate its ability to solve robotics manipulation in a\nzero-shot setting, thereby overcoming key limitations of existing LLM-based\nmanipulation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable planning abilities\nacross various domains, including robotics manipulation and navigation. While\nrecent efforts in robotics have leveraged LLMs both for high-level and\nlow-level planning, these approaches often face significant challenges, such as\nhallucinations in long-horizon tasks and limited adaptability due to the\ngeneration of plans in a single pass without real-time feedback. To address\nthese limitations, we propose a novel multi-agent LLM framework, Multi-Agent\nLarge Language Model for Manipulation (MALMM) that distributes high-level\nplanning and low-level control code generation across specialized LLM agents,\nsupervised by an additional agent that dynamically manages transitions. By\nincorporating observations from the environment after each step, our framework\neffectively handles intermediate failures and enables adaptive re-planning.\nUnlike existing methods, our approach does not rely on pre-trained skill\npolicies or in-context learning examples and generalizes to a variety of new\ntasks. We evaluate our approach on nine RLBench tasks, including long-horizon\ntasks, and demonstrate its ability to solve robotics manipulation in a\nzero-shot setting, thereby overcoming key limitations of existing LLM-based\nmanipulation methods."
                },
                "authors": [
                    {
                        "name": "Harsh Singh"
                    },
                    {
                        "name": "Rocktim Jyoti Das"
                    },
                    {
                        "name": "Mingfei Han"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Ivan Laptev"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Laptev"
                },
                "author": "Ivan Laptev",
                "arxiv_comment": "48 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17626v1",
                "updated": "2024-11-26T17:37:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    37,
                    21,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:37:21Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    37,
                    21,
                    1,
                    331,
                    0
                ],
                "title": "Semi-analytical model for the calculation of solar radiation pressure\n  and its effects on a LEO satellite with predicting the change in position\n  vectors using machine learning techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-analytical model for the calculation of solar radiation pressure\n  and its effects on a LEO satellite with predicting the change in position\n  vectors using machine learning techniques"
                },
                "summary": "The rapid increase in the deployment of Low Earth Orbit (LEO) satellites,\ncatering to diverse applications such as communication, Earth observation,\nenvironmental monitoring, and scientific research, has significantly amplified\nthe complexity of trajectory management. The current work focuses on\ncalculating and analyzing perturbation effects on a satellite's anticipated\ntrajectory in LEO, considering Solar Radiation Pressure (SRP) as the main\nperturbing force. The acceleration due to SRP and it's effects on the satellite\nwas calculated using a custom-built Python module mainly based on the\nhypothesis of the cannonball model. The study demonstrates the effectiveness of\nthe proposed model through comprehensive simulations and comparisons with\nexisting analytical and numerical methods. Here, the primary Keplerian orbital\ncharacteristics were employed to analyze a simulated low-earth orbit LEO\nsatellite, initially visualizing the satellite's trajectory and ground tracks\nat a designated altitude. The study also focuses on a comparative analysis of\nground stations, primarily considering the main regions of the subcontinent,\nwith revisit time as the key parameter for comparison. In the end, we combine\nanalytical techniques with Machine Learning (ML) algorithms to predict changes\nin the position vectors of the satellite. Using ML techniques, the model can\nadaptively learn and refine predictions based on historical data and real-time\ninput, thus improving accuracy over time. In addition, the incorporation of\nanalytical methods allows for a deeper understanding of the underlying physics\ngoverning satellite motion, enabling more precise adjustments and corrections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid increase in the deployment of Low Earth Orbit (LEO) satellites,\ncatering to diverse applications such as communication, Earth observation,\nenvironmental monitoring, and scientific research, has significantly amplified\nthe complexity of trajectory management. The current work focuses on\ncalculating and analyzing perturbation effects on a satellite's anticipated\ntrajectory in LEO, considering Solar Radiation Pressure (SRP) as the main\nperturbing force. The acceleration due to SRP and it's effects on the satellite\nwas calculated using a custom-built Python module mainly based on the\nhypothesis of the cannonball model. The study demonstrates the effectiveness of\nthe proposed model through comprehensive simulations and comparisons with\nexisting analytical and numerical methods. Here, the primary Keplerian orbital\ncharacteristics were employed to analyze a simulated low-earth orbit LEO\nsatellite, initially visualizing the satellite's trajectory and ground tracks\nat a designated altitude. The study also focuses on a comparative analysis of\nground stations, primarily considering the main regions of the subcontinent,\nwith revisit time as the key parameter for comparison. In the end, we combine\nanalytical techniques with Machine Learning (ML) algorithms to predict changes\nin the position vectors of the satellite. Using ML techniques, the model can\nadaptively learn and refine predictions based on historical data and real-time\ninput, thus improving accuracy over time. In addition, the incorporation of\nanalytical methods allows for a deeper understanding of the underlying physics\ngoverning satellite motion, enabling more precise adjustments and corrections."
                },
                "authors": [
                    {
                        "name": "Pranava Seth"
                    },
                    {
                        "name": "Mamta Gulati"
                    }
                ],
                "author_detail": {
                    "name": "Mamta Gulati"
                },
                "author": "Mamta Gulati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "85-08, 70F15, 68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17625v1",
                "updated": "2024-11-26T17:37:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    37,
                    12,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:37:12Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    37,
                    12,
                    1,
                    331,
                    0
                ],
                "title": "Data-driven development of cycle prediction models for lithium metal\n  batteries using multi modal mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven development of cycle prediction models for lithium metal\n  batteries using multi modal mining"
                },
                "summary": "Recent advances in data-driven research have shown great potential in\nunderstanding the intricate relationships between materials and their\nperformances. Herein, we introduce a novel multi modal data-driven approach\nemploying an Automatic Battery data Collector (ABC) that integrates a large\nlanguage model (LLM) with an automatic graph mining tool, Material Graph\nDigitizer (MatGD). This platform enables state-of-the-art accurate extraction\nof battery material data and cyclability performance metrics from diverse\ntextual and graphical data sources. From the database derived through the ABC\nplatform, we developed machine learning models that can accurately predict the\ncapacity and stability of lithium metal batteries, which is the first-ever\nmodel developed to achieve such predictions. Our models were also\nexperimentally validated, confirming practical applicability and reliability of\nour data-driven approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in data-driven research have shown great potential in\nunderstanding the intricate relationships between materials and their\nperformances. Herein, we introduce a novel multi modal data-driven approach\nemploying an Automatic Battery data Collector (ABC) that integrates a large\nlanguage model (LLM) with an automatic graph mining tool, Material Graph\nDigitizer (MatGD). This platform enables state-of-the-art accurate extraction\nof battery material data and cyclability performance metrics from diverse\ntextual and graphical data sources. From the database derived through the ABC\nplatform, we developed machine learning models that can accurately predict the\ncapacity and stability of lithium metal batteries, which is the first-ever\nmodel developed to achieve such predictions. Our models were also\nexperimentally validated, confirming practical applicability and reliability of\nour data-driven approach."
                },
                "authors": [
                    {
                        "name": "Jaewoong Lee"
                    },
                    {
                        "name": "Junhee Woo"
                    },
                    {
                        "name": "Sejin Kim"
                    },
                    {
                        "name": "Cinthya Paulina"
                    },
                    {
                        "name": "Hyunmin Park"
                    },
                    {
                        "name": "Hee-Tak Kim"
                    },
                    {
                        "name": "Steve Park"
                    },
                    {
                        "name": "Jihan Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jihan Kim"
                },
                "author": "Jihan Kim",
                "arxiv_comment": "30 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v1",
                "updated": "2024-11-26T17:28:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Cheng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Yu"
                },
                "author": "Cheng Yu",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17607v1",
                "updated": "2024-11-26T17:19:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    19,
                    9,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:19:09Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    19,
                    9,
                    1,
                    331,
                    0
                ],
                "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data"
                },
                "summary": "Speech language models (SpeechLMs) accept speech input and produce speech\noutput, allowing for more natural human-computer interaction compared to\ntext-based large language models (LLMs). Traditional approaches for developing\nSpeechLMs are constrained by the limited availability of unsupervised speech\ndata and parallel speech-text data, which are significantly less abundant than\ntext pre-training data, thereby limiting their scalability as LLMs. We propose\na novel approach to scaling speech-text pre-training by leveraging large-scale\nsynthetic interleaved data derived from text corpora, eliminating the need for\nparallel speech-text datasets. Our method efficiently constructs speech-text\ninterleaved data by sampling text spans from existing text corpora and\nsynthesizing corresponding speech spans using a text-to-token model, bypassing\nthe need to generate actual speech. We also employ a supervised speech\ntokenizer derived from an automatic speech recognition (ASR) model by\nincorporating a vector-quantized bottleneck into the encoder. This supervised\ntraining approach results in discrete speech tokens with strong semantic\npreservation even at lower sampling rates (e.g. 12.5Hz), while still\nmaintaining speech reconstruction quality. Starting from a pre-trained language\nmodel and scaling our pre-training to 1 trillion tokens (with 600B synthetic\ninterleaved speech-text data), we achieve state-of-the-art performance in\nspeech language modeling and spoken question answering, improving performance\non spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We\nfurther demonstrate that by fine-tuning the pre-trained model with speech\ndialogue data, we can develop an end-to-end spoken chatbot that achieves\ncompetitive performance comparable to existing baselines in both conversational\nabilities and speech quality, even operating exclusively in the speech domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech language models (SpeechLMs) accept speech input and produce speech\noutput, allowing for more natural human-computer interaction compared to\ntext-based large language models (LLMs). Traditional approaches for developing\nSpeechLMs are constrained by the limited availability of unsupervised speech\ndata and parallel speech-text data, which are significantly less abundant than\ntext pre-training data, thereby limiting their scalability as LLMs. We propose\na novel approach to scaling speech-text pre-training by leveraging large-scale\nsynthetic interleaved data derived from text corpora, eliminating the need for\nparallel speech-text datasets. Our method efficiently constructs speech-text\ninterleaved data by sampling text spans from existing text corpora and\nsynthesizing corresponding speech spans using a text-to-token model, bypassing\nthe need to generate actual speech. We also employ a supervised speech\ntokenizer derived from an automatic speech recognition (ASR) model by\nincorporating a vector-quantized bottleneck into the encoder. This supervised\ntraining approach results in discrete speech tokens with strong semantic\npreservation even at lower sampling rates (e.g. 12.5Hz), while still\nmaintaining speech reconstruction quality. Starting from a pre-trained language\nmodel and scaling our pre-training to 1 trillion tokens (with 600B synthetic\ninterleaved speech-text data), we achieve state-of-the-art performance in\nspeech language modeling and spoken question answering, improving performance\non spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We\nfurther demonstrate that by fine-tuning the pre-trained model with speech\ndialogue data, we can develop an end-to-end spoken chatbot that achieves\ncompetitive performance comparable to existing baselines in both conversational\nabilities and speech quality, even operating exclusively in the speech domain."
                },
                "authors": [
                    {
                        "name": "Aohan Zeng"
                    },
                    {
                        "name": "Zhengxiao Du"
                    },
                    {
                        "name": "Mingdao Liu"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Shengmin Jiang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17600v1",
                "updated": "2024-11-26T17:06:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    6,
                    58,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:06:58Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    6,
                    58,
                    1,
                    331,
                    0
                ],
                "title": "Making History Readable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making History Readable"
                },
                "summary": "The Virginia Tech University Libraries (VTUL) Digital Library Platform (DLP)\nhosts digital collections that offer our users access to a wide variety of\ndocuments of historical and cultural importance. These collections are not only\nof academic importance but also provide our users with a glance at local\nhistorical events. Our DLP contains collections comprising digital objects\nfeaturing complex layouts, faded imagery, and hard-to-read handwritten text,\nwhich makes providing online access to these materials challenging. To address\nthese issues, we integrate AI into our DLP workflow and convert the text in the\ndigital objects into a machine-readable format. To enhance the user experience\nwith our historical collections, we use custom AI agents for handwriting\nrecognition, text extraction, and large language models (LLMs) for\nsummarization. This poster highlights three collections focusing on handwritten\nletters, newspapers, and digitized topographic maps. We discuss the challenges\nwith each collection and detail our approaches to address them. Our proposed\nmethods aim to enhance the user experience by making the contents in these\ncollections easier to search and navigate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Virginia Tech University Libraries (VTUL) Digital Library Platform (DLP)\nhosts digital collections that offer our users access to a wide variety of\ndocuments of historical and cultural importance. These collections are not only\nof academic importance but also provide our users with a glance at local\nhistorical events. Our DLP contains collections comprising digital objects\nfeaturing complex layouts, faded imagery, and hard-to-read handwritten text,\nwhich makes providing online access to these materials challenging. To address\nthese issues, we integrate AI into our DLP workflow and convert the text in the\ndigital objects into a machine-readable format. To enhance the user experience\nwith our historical collections, we use custom AI agents for handwriting\nrecognition, text extraction, and large language models (LLMs) for\nsummarization. This poster highlights three collections focusing on handwritten\nletters, newspapers, and digitized topographic maps. We discuss the challenges\nwith each collection and detail our approaches to address them. Our proposed\nmethods aim to enhance the user experience by making the contents in these\ncollections easier to search and navigate."
                },
                "authors": [
                    {
                        "name": "Bipasha Banerjee"
                    },
                    {
                        "name": "Jennifer Goyne"
                    },
                    {
                        "name": "William A. Ingram"
                    }
                ],
                "author_detail": {
                    "name": "William A. Ingram"
                },
                "author": "William A. Ingram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17598v1",
                "updated": "2024-11-26T17:06:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    6,
                    30,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:06:30Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    6,
                    30,
                    1,
                    331,
                    0
                ],
                "title": "Agentic AI for Improving Precision in Identifying Contributions to\n  Sustainable Development Goals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI for Improving Precision in Identifying Contributions to\n  Sustainable Development Goals"
                },
                "summary": "As research institutions increasingly commit to supporting the United\nNations' Sustainable Development Goals (SDGs), there is a pressing need to\naccurately assess their research output against these goals. Current\napproaches, primarily reliant on keyword-based Boolean search queries, conflate\nincidental keyword matches with genuine contributions, reducing retrieval\nprecision and complicating benchmarking efforts. This study investigates the\napplication of autoregressive Large Language Models (LLMs) as evaluation agents\nto identify relevant scholarly contributions to SDG targets in scholarly\npublications. Using a dataset of academic abstracts retrieved via SDG-specific\nkeyword queries, we demonstrate that small, locally-hosted LLMs can\ndifferentiate semantically relevant contributions to SDG targets from documents\nretrieved due to incidental keyword matches, addressing the limitations of\ntraditional methods. By leveraging the contextual understanding of LLMs, this\napproach provides a scalable framework for improving SDG-related research\nmetrics and informing institutional reporting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As research institutions increasingly commit to supporting the United\nNations' Sustainable Development Goals (SDGs), there is a pressing need to\naccurately assess their research output against these goals. Current\napproaches, primarily reliant on keyword-based Boolean search queries, conflate\nincidental keyword matches with genuine contributions, reducing retrieval\nprecision and complicating benchmarking efforts. This study investigates the\napplication of autoregressive Large Language Models (LLMs) as evaluation agents\nto identify relevant scholarly contributions to SDG targets in scholarly\npublications. Using a dataset of academic abstracts retrieved via SDG-specific\nkeyword queries, we demonstrate that small, locally-hosted LLMs can\ndifferentiate semantically relevant contributions to SDG targets from documents\nretrieved due to incidental keyword matches, addressing the limitations of\ntraditional methods. By leveraging the contextual understanding of LLMs, this\napproach provides a scalable framework for improving SDG-related research\nmetrics and informing institutional reporting."
                },
                "authors": [
                    {
                        "name": "William A. Ingram"
                    },
                    {
                        "name": "Bipasha Banerjee"
                    },
                    {
                        "name": "Edward A. Fox"
                    }
                ],
                "author_detail": {
                    "name": "Edward A. Fox"
                },
                "author": "Edward A. Fox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17595v1",
                "updated": "2024-11-26T17:05:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    5,
                    27,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:05:27Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    5,
                    27,
                    1,
                    331,
                    0
                ],
                "title": "Can artificial intelligence predict clinical trial outcomes?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can artificial intelligence predict clinical trial outcomes?"
                },
                "summary": "The increasing complexity and cost of clinical trials, particularly in the\ncontext of oncology and advanced therapies, pose significant challenges for\ndrug development. This study evaluates the predictive capabilities of large\nlanguage models (LLMs) such as GPT-3.5, GPT-4, and HINT in determining clinical\ntrial outcomes. By leveraging a curated dataset of trials from\nClinicalTrials.gov, we compare the models' performance using metrics including\nbalanced accuracy, specificity, recall, and Matthews Correlation Coefficient\n(MCC). Results indicate that GPT-4o demonstrates robust performance in early\ntrial phases, achieving high recall but facing limitations in specificity.\nConversely, the HINT model excels in recognizing negative outcomes,\nparticularly in later trial phases, offering a balanced approach across diverse\nendpoints. Oncology trials, characterized by high complexity, remain\nchallenging for all models. Additionally, trial duration and disease categories\ninfluence predictive performance, with longer durations and complex diseases\nsuch as neoplasms reducing accuracy. This study highlights the complementary\nstrengths of LLMs and HINT, providing insights into optimizing predictive tools\nfor clinical trial design and risk management. Future advancements in LLMs are\nessential to address current gaps in handling negative outcomes and complex\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity and cost of clinical trials, particularly in the\ncontext of oncology and advanced therapies, pose significant challenges for\ndrug development. This study evaluates the predictive capabilities of large\nlanguage models (LLMs) such as GPT-3.5, GPT-4, and HINT in determining clinical\ntrial outcomes. By leveraging a curated dataset of trials from\nClinicalTrials.gov, we compare the models' performance using metrics including\nbalanced accuracy, specificity, recall, and Matthews Correlation Coefficient\n(MCC). Results indicate that GPT-4o demonstrates robust performance in early\ntrial phases, achieving high recall but facing limitations in specificity.\nConversely, the HINT model excels in recognizing negative outcomes,\nparticularly in later trial phases, offering a balanced approach across diverse\nendpoints. Oncology trials, characterized by high complexity, remain\nchallenging for all models. Additionally, trial duration and disease categories\ninfluence predictive performance, with longer durations and complex diseases\nsuch as neoplasms reducing accuracy. This study highlights the complementary\nstrengths of LLMs and HINT, providing insights into optimizing predictive tools\nfor clinical trial design and risk management. Future advancements in LLMs are\nessential to address current gaps in handling negative outcomes and complex\ndomains."
                },
                "authors": [
                    {
                        "name": "Shuyi Jin"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Hongru Ding"
                    },
                    {
                        "name": "Meijie Wang"
                    },
                    {
                        "name": "Lun Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lun Yu"
                },
                "author": "Lun Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13191v3",
                "updated": "2024-11-26T16:45:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    45,
                    19,
                    1,
                    331,
                    0
                ],
                "published": "2024-04-19T21:39:15Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    21,
                    39,
                    15,
                    4,
                    110,
                    0
                ],
                "title": "Action Contextualization: Adaptive Task Planning and Action Tuning using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action Contextualization: Adaptive Task Planning and Action Tuning using\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) present a promising frontier in robotic task\nplanning by leveraging extensive human knowledge. Nevertheless, the current\nliterature often overlooks the critical aspects of robots' adaptability and\nerror correction. This work aims to overcome this limitation by enabling robots\nto modify their motions and select the most suitable task plans based on the\ncontext. We introduce a novel framework to achieve action contextualization,\naimed at tailoring robot actions to the context of specific tasks, thereby\nenhancing adaptability through applying LLM-derived contextual insights. Our\nframework integrates motion metrics that evaluate robot performances for each\nmotion to resolve redundancy in planning. Moreover, it supports online feedback\nbetween the robot and the LLM, enabling immediate modifications to the task\nplans and corrections of errors. An overall success rate of 81.25% has been\nachieved through extensive experimental validation. Finally, when integrated\nwith dynamical system (DS)-based robot controllers, the robotic arm-hand system\ndemonstrates its proficiency in autonomously executing LLM-generated motion\nplans for sequential table-clearing tasks, rectifying errors without human\nintervention, and showcasing robustness against external disturbances. Our\nproposed framework also features the potential to be integrated with modular\ncontrol approaches, significantly enhancing robots' adaptability and autonomy\nin performing sequential tasks in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present a promising frontier in robotic task\nplanning by leveraging extensive human knowledge. Nevertheless, the current\nliterature often overlooks the critical aspects of robots' adaptability and\nerror correction. This work aims to overcome this limitation by enabling robots\nto modify their motions and select the most suitable task plans based on the\ncontext. We introduce a novel framework to achieve action contextualization,\naimed at tailoring robot actions to the context of specific tasks, thereby\nenhancing adaptability through applying LLM-derived contextual insights. Our\nframework integrates motion metrics that evaluate robot performances for each\nmotion to resolve redundancy in planning. Moreover, it supports online feedback\nbetween the robot and the LLM, enabling immediate modifications to the task\nplans and corrections of errors. An overall success rate of 81.25% has been\nachieved through extensive experimental validation. Finally, when integrated\nwith dynamical system (DS)-based robot controllers, the robotic arm-hand system\ndemonstrates its proficiency in autonomously executing LLM-generated motion\nplans for sequential table-clearing tasks, rectifying errors without human\nintervention, and showcasing robustness against external disturbances. Our\nproposed framework also features the potential to be integrated with modular\ncontrol approaches, significantly enhancing robots' adaptability and autonomy\nin performing sequential tasks in the real world."
                },
                "authors": [
                    {
                        "name": "Sthithpragya Gupta"
                    },
                    {
                        "name": "Kunpeng Yao"
                    },
                    {
                        "name": "Loïc Niederhauser"
                    },
                    {
                        "name": "Aude Billard"
                    }
                ],
                "author_detail": {
                    "name": "Aude Billard"
                },
                "author": "Aude Billard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16638v2",
                "updated": "2024-11-26T16:38:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    38,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-25T18:15:15Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    15,
                    15,
                    0,
                    330,
                    0
                ],
                "title": "Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation"
                },
                "summary": "Modern LLMs can now produce highly readable abstractive summaries, to the\npoint where traditional automated metrics for evaluating summary quality, such\nas ROUGE, have become saturated. However, LLMs still sometimes introduce\nunwanted content into summaries, i.e., information inconsistent with or\nunsupported by their source. Measuring the occurrence of these often subtle\n``hallucinations'' automatically has proved to be challenging. This in turn has\nmotivated development of a variety of metrics intended to measure the factual\nconsistency of generated summaries against their source. But are these\napproaches measuring what they purport to do? In this work, we stress-test\nautomatic factuality metrics. Specifically, we investigate whether and to what\ndegree superficial attributes of summary texts suffice to predict\n``factuality'', finding that a (supervised) model using only such shallow\nfeatures is reasonably competitive with SOTA factuality scoring methods. We\nthen evaluate how factuality metrics respond to factual corrections in\ninconsistent summaries and find that only a few show meaningful improvements.\nIn contrast, some metrics are more sensitive to benign, non-factual edits.\nMotivated by these insights, we show that one can ``game'' (most) automatic\nfactuality metrics, i.e., reliably inflate ``factuality'' scores by appending\ninnocuous sentences to generated summaries.Taken together, our results raise\nquestions about the degree to which we should rely on existing automated\nfactuality metrics and what exactly we want ``factuality metrics'' to measure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLMs can now produce highly readable abstractive summaries, to the\npoint where traditional automated metrics for evaluating summary quality, such\nas ROUGE, have become saturated. However, LLMs still sometimes introduce\nunwanted content into summaries, i.e., information inconsistent with or\nunsupported by their source. Measuring the occurrence of these often subtle\n``hallucinations'' automatically has proved to be challenging. This in turn has\nmotivated development of a variety of metrics intended to measure the factual\nconsistency of generated summaries against their source. But are these\napproaches measuring what they purport to do? In this work, we stress-test\nautomatic factuality metrics. Specifically, we investigate whether and to what\ndegree superficial attributes of summary texts suffice to predict\n``factuality'', finding that a (supervised) model using only such shallow\nfeatures is reasonably competitive with SOTA factuality scoring methods. We\nthen evaluate how factuality metrics respond to factual corrections in\ninconsistent summaries and find that only a few show meaningful improvements.\nIn contrast, some metrics are more sensitive to benign, non-factual edits.\nMotivated by these insights, we show that one can ``game'' (most) automatic\nfactuality metrics, i.e., reliably inflate ``factuality'' scores by appending\ninnocuous sentences to generated summaries.Taken together, our results raise\nquestions about the degree to which we should rely on existing automated\nfactuality metrics and what exactly we want ``factuality metrics'' to measure."
                },
                "authors": [
                    {
                        "name": "Sanjana Ramprasad"
                    },
                    {
                        "name": "Byron C. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Byron C. Wallace"
                },
                "author": "Byron C. Wallace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17569v1",
                "updated": "2024-11-26T16:31:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    31,
                    18,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    31,
                    18,
                    1,
                    331,
                    0
                ],
                "title": "RTL-Breaker: Assessing the Security of LLMs against Backdoor Attacks on\n  HDL Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTL-Breaker: Assessing the Security of LLMs against Backdoor Attacks on\n  HDL Code Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable potential with code\ngeneration/completion tasks for hardware design. In fact, LLM-based hardware\ndescription language (HDL) code generation has enabled the industry to realize\ncomplex designs more quickly, reducing the time and effort required in the\ndevelopment cycle. However, the increased reliance on such automation\nintroduces critical security risks. Notably, given that LLMs have to be trained\non vast datasets of codes that are typically sourced from publicly available\nrepositories (often without thorough validation), LLMs are susceptible to\nso-called data poisoning or backdoor attacks. Here, attackers inject malicious\ncode for the training data, which can be carried over into the HDL code\ngenerated by LLMs. This threat vector can compromise the security and integrity\nof entire hardware systems. In this work, we propose RTL-Breaker, a novel\nbackdoor attack framework on LLM-based HDL code generation. RTL-Breaker\nprovides an in-depth analysis for essential aspects of this novel problem: 1)\nvarious trigger mechanisms versus their effectiveness for inserting malicious\nmodifications, and 2) side-effects by backdoor attacks on code generation in\ngeneral, i.e., impact on code quality. RTL-Breaker emphasizes the urgent need\nfor more robust measures to safeguard against such attacks. Toward that end, we\nopen-source our framework and all data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable potential with code\ngeneration/completion tasks for hardware design. In fact, LLM-based hardware\ndescription language (HDL) code generation has enabled the industry to realize\ncomplex designs more quickly, reducing the time and effort required in the\ndevelopment cycle. However, the increased reliance on such automation\nintroduces critical security risks. Notably, given that LLMs have to be trained\non vast datasets of codes that are typically sourced from publicly available\nrepositories (often without thorough validation), LLMs are susceptible to\nso-called data poisoning or backdoor attacks. Here, attackers inject malicious\ncode for the training data, which can be carried over into the HDL code\ngenerated by LLMs. This threat vector can compromise the security and integrity\nof entire hardware systems. In this work, we propose RTL-Breaker, a novel\nbackdoor attack framework on LLM-based HDL code generation. RTL-Breaker\nprovides an in-depth analysis for essential aspects of this novel problem: 1)\nvarious trigger mechanisms versus their effectiveness for inserting malicious\nmodifications, and 2) side-effects by backdoor attacks on code generation in\ngeneral, i.e., impact on code quality. RTL-Breaker emphasizes the urgent need\nfor more robust measures to safeguard against such attacks. Toward that end, we\nopen-source our framework and all data."
                },
                "authors": [
                    {
                        "name": "Lakshmi Likhitha Mankali"
                    },
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Manaar Alam"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Michail Maniatakos"
                    },
                    {
                        "name": "Ozgur Sinanoglu"
                    },
                    {
                        "name": "Johann Knechtel"
                    }
                ],
                "author_detail": {
                    "name": "Johann Knechtel"
                },
                "author": "Johann Knechtel",
                "arxiv_comment": "Accepted at 2025 Design, Automation & Test in Europe (DATE)\n  Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17543v1",
                "updated": "2024-11-26T16:04:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    4,
                    20,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:04:20Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    4,
                    20,
                    1,
                    331,
                    0
                ],
                "title": "Rapid Deployment of Domain-specific Hyperspectral Image Processors with\n  Application to Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Deployment of Domain-specific Hyperspectral Image Processors with\n  Application to Autonomous Driving"
                },
                "summary": "The article discusses the use of low cost System-On-Module (SOM) platforms\nfor the implementation of efficient hyperspectral imaging (HSI) processors for\napplication in autonomous driving. The work addresses the challenges of shaping\nand deploying multiple layer fully convolutional networks (FCN) for\nlow-latency, on-board image semantic segmentation using resource- and\npower-constrained processing devices. The paper describes in detail the steps\nfollowed to redesign and customize a successfully trained HSI segmentation\nlightweight FCN that was previously tested on a high-end heterogeneous\nmultiprocessing system-on-chip (MPSoC) to accommodate it to the constraints\nimposed by a low-cost SOM. This SOM features a lower-end but much cheaper MPSoC\nsuitable for the deployment of automatic driving systems (ADS). In particular\nthe article reports the data- and hardware-specific quantization techniques\nutilized to fit the FCN into a commercial fixed-point programmable AI\ncoprocessor IP, and proposes a full customized post-training quantization\nscheme to reduce computation and storage costs without compromising\nsegmentation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The article discusses the use of low cost System-On-Module (SOM) platforms\nfor the implementation of efficient hyperspectral imaging (HSI) processors for\napplication in autonomous driving. The work addresses the challenges of shaping\nand deploying multiple layer fully convolutional networks (FCN) for\nlow-latency, on-board image semantic segmentation using resource- and\npower-constrained processing devices. The paper describes in detail the steps\nfollowed to redesign and customize a successfully trained HSI segmentation\nlightweight FCN that was previously tested on a high-end heterogeneous\nmultiprocessing system-on-chip (MPSoC) to accommodate it to the constraints\nimposed by a low-cost SOM. This SOM features a lower-end but much cheaper MPSoC\nsuitable for the deployment of automatic driving systems (ADS). In particular\nthe article reports the data- and hardware-specific quantization techniques\nutilized to fit the FCN into a commercial fixed-point programmable AI\ncoprocessor IP, and proposes a full customized post-training quantization\nscheme to reduce computation and storage costs without compromising\nsegmentation accuracy."
                },
                "authors": [
                    {
                        "name": "Jon Gutiérrez-Zaballa"
                    },
                    {
                        "name": "Koldo Basterretxea"
                    },
                    {
                        "name": "Javier Echanobe"
                    },
                    {
                        "name": "Óscar Mata-Carballeira"
                    },
                    {
                        "name": "M. Victoria Martínez"
                    }
                ],
                "author_detail": {
                    "name": "M. Victoria Martínez"
                },
                "author": "M. Victoria Martínez",
                "arxiv_doi": "10.1109/ICECS58634.2023.10382745",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICECS58634.2023.10382745",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.17543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2023 30th IEEE International Conference on Electronics, Circuits\n  and Systems (ICECS)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10351v2",
                "updated": "2024-11-26T15:44:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    44,
                    21,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-15T16:55:57Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    55,
                    57,
                    4,
                    320,
                    0
                ],
                "title": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Unveiled: Investigating Social Bias in LLM-Generated Code"
                },
                "summary": "Large language models (LLMs) have significantly advanced the field of\nautomated code generation. However, a notable research gap exists in the\nevaluation of social biases that may be present in the code produced by LLMs.\nTo solve this issue, we propose a novel fairness framework, i.e., Solar, to\nassess and mitigate the social biases of LLM-generated code. Specifically,\nSolar can automatically generate test cases for quantitatively uncovering\nsocial biases of the auto-generated code by LLMs. To quantify the severity of\nsocial biases in generated code, we develop a dataset that covers a diverse set\nof social problems. We applied Solar and the crafted dataset to four\nstate-of-the-art LLMs for code generation. Our evaluation reveals severe bias\nin the LLM-generated code from all the subject LLMs. Furthermore, we explore\nseveral strategies for bias mitigation, including Chain-of-Thought (CoT)\nprompting, combining positive role-playing with CoT prompting and iterative\nprompting. Our experiments show that iterative prompting can effectively reduce\nsocial bias in LLM-generated code by up to 90%. Solar is highly extensible to\nevaluate new social problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced the field of\nautomated code generation. However, a notable research gap exists in the\nevaluation of social biases that may be present in the code produced by LLMs.\nTo solve this issue, we propose a novel fairness framework, i.e., Solar, to\nassess and mitigate the social biases of LLM-generated code. Specifically,\nSolar can automatically generate test cases for quantitatively uncovering\nsocial biases of the auto-generated code by LLMs. To quantify the severity of\nsocial biases in generated code, we develop a dataset that covers a diverse set\nof social problems. We applied Solar and the crafted dataset to four\nstate-of-the-art LLMs for code generation. Our evaluation reveals severe bias\nin the LLM-generated code from all the subject LLMs. Furthermore, we explore\nseveral strategies for bias mitigation, including Chain-of-Thought (CoT)\nprompting, combining positive role-playing with CoT prompting and iterative\nprompting. Our experiments show that iterative prompting can effectively reduce\nsocial bias in LLM-generated code by up to 90%. Solar is highly extensible to\nevaluate new social problems."
                },
                "authors": [
                    {
                        "name": "Lin Ling"
                    },
                    {
                        "name": "Fazle Rabbi"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Jinqiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiu Yang"
                },
                "author": "Jinqiu Yang",
                "arxiv_comment": "9pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20288v4",
                "updated": "2024-11-26T15:35:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    35,
                    49,
                    1,
                    331,
                    0
                ],
                "published": "2024-09-30T13:44:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    44,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have made significant progress in natural\nlanguage processing tasks and demonstrate considerable potential in the legal\ndomain. However, legal applications demand high standards of accuracy,\nreliability, and fairness. Applying existing LLMs to legal systems without\ncareful evaluation of their potential and limitations could pose significant\nrisks in legal practice. To this end, we introduce a standardized comprehensive\nChinese legal benchmark LexEval. This benchmark is notable in the following\nthree aspects: (1) Ability Modeling: We propose a new taxonomy of legal\ncognitive abilities to organize different tasks. (2) Scale: To our knowledge,\nLexEval is currently the largest Chinese legal evaluation dataset, comprising\n23 tasks and 14,150 questions. (3) Data: we utilize formatted existing\ndatasets, exam datasets and newly annotated datasets by legal experts to\ncomprehensively evaluate the various capabilities of LLMs. LexEval not only\nfocuses on the ability of LLMs to apply fundamental legal knowledge but also\ndedicates efforts to examining the ethical issues involved in their\napplication. We evaluated 38 open-source and commercial LLMs and obtained some\ninteresting findings. The experiments and findings offer valuable insights into\nthe challenges and potential solutions for developing Chinese legal systems and\nLLM evaluation pipelines. The LexEval dataset and leaderboard are publicly\navailable at \\url{https://github.com/CSHaitao/LexEval} and will be continuously\nupdated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant progress in natural\nlanguage processing tasks and demonstrate considerable potential in the legal\ndomain. However, legal applications demand high standards of accuracy,\nreliability, and fairness. Applying existing LLMs to legal systems without\ncareful evaluation of their potential and limitations could pose significant\nrisks in legal practice. To this end, we introduce a standardized comprehensive\nChinese legal benchmark LexEval. This benchmark is notable in the following\nthree aspects: (1) Ability Modeling: We propose a new taxonomy of legal\ncognitive abilities to organize different tasks. (2) Scale: To our knowledge,\nLexEval is currently the largest Chinese legal evaluation dataset, comprising\n23 tasks and 14,150 questions. (3) Data: we utilize formatted existing\ndatasets, exam datasets and newly annotated datasets by legal experts to\ncomprehensively evaluate the various capabilities of LLMs. LexEval not only\nfocuses on the ability of LLMs to apply fundamental legal knowledge but also\ndedicates efforts to examining the ethical issues involved in their\napplication. We evaluated 38 open-source and commercial LLMs and obtained some\ninteresting findings. The experiments and findings offer valuable insights into\nthe challenges and potential solutions for developing Chinese legal systems and\nLLM evaluation pipelines. The LexEval dataset and leaderboard are publicly\navailable at \\url{https://github.com/CSHaitao/LexEval} and will be continuously\nupdated."
                },
                "authors": [
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "You Chen"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Ruizhe Zhang"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "arxiv_comment": "NeurIPs 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17525v1",
                "updated": "2024-11-26T15:35:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    35,
                    44,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:35:44Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    35,
                    44,
                    1,
                    331,
                    0
                ],
                "title": "Pushing the Limits of Large Language Model Quantization via the\n  Linearity Theorem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of Large Language Model Quantization via the\n  Linearity Theorem"
                },
                "summary": "Quantizing large language models has become a standard way to reduce their\nmemory and computational costs. Typically, existing methods focus on breaking\ndown the problem into individual layer-wise sub-problems, and minimizing\nper-layer error, measured via various metrics. Yet, this approach currently\nlacks theoretical justification and the metrics employed may be sub-optimal. In\nthis paper, we present a \"linearity theorem\" establishing a direct relationship\nbetween the layer-wise $\\ell_2$ reconstruction error and the model perplexity\nincrease due to quantization. This insight enables two novel applications: (1)\na simple data-free LLM quantization method using Hadamard rotations and\nMSE-optimal grids, dubbed HIGGS, which outperforms all prior data-free\napproaches such as the extremely popular NF4 quantized format, and (2) an\noptimal solution to the problem of finding non-uniform per-layer quantization\nlevels which match a given compression constraint in the medium-bitwidth\nregime, obtained by reduction to dynamic programming. On the practical side, we\ndemonstrate improved accuracy-compression trade-offs on Llama-3.1 and\n3.2-family models, as well as on Qwen-family models. Further, we show that our\nmethod can be efficiently supported in terms of GPU kernels at various batch\nsizes, advancing both data-free and non-uniform quantization for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing large language models has become a standard way to reduce their\nmemory and computational costs. Typically, existing methods focus on breaking\ndown the problem into individual layer-wise sub-problems, and minimizing\nper-layer error, measured via various metrics. Yet, this approach currently\nlacks theoretical justification and the metrics employed may be sub-optimal. In\nthis paper, we present a \"linearity theorem\" establishing a direct relationship\nbetween the layer-wise $\\ell_2$ reconstruction error and the model perplexity\nincrease due to quantization. This insight enables two novel applications: (1)\na simple data-free LLM quantization method using Hadamard rotations and\nMSE-optimal grids, dubbed HIGGS, which outperforms all prior data-free\napproaches such as the extremely popular NF4 quantized format, and (2) an\noptimal solution to the problem of finding non-uniform per-layer quantization\nlevels which match a given compression constraint in the medium-bitwidth\nregime, obtained by reduction to dynamic programming. On the practical side, we\ndemonstrate improved accuracy-compression trade-offs on Llama-3.1 and\n3.2-family models, as well as on Qwen-family models. Further, we show that our\nmethod can be efficiently supported in terms of GPU kernels at various batch\nsizes, advancing both data-free and non-uniform quantization for LLMs."
                },
                "authors": [
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Ivan Ilin"
                    },
                    {
                        "name": "Han Guo"
                    },
                    {
                        "name": "Peter Richtárik"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09937v2",
                "updated": "2024-11-26T15:31:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    31,
                    44,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-15T04:22:21Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    22,
                    21,
                    4,
                    320,
                    0
                ],
                "title": "Refined and Segmented Price Sentiment Indices from Survey Comments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refined and Segmented Price Sentiment Indices from Survey Comments"
                },
                "summary": "We aim to enhance a price sentiment index and to more precisely understand\nprice trends from the perspective of not only consumers but also businesses. We\nextract comments related to prices from the Economy Watchers Survey conducted\nby the Cabinet Office of Japan and classify price trends using a large language\nmodel (LLM). We classify whether the survey sample reflects the perspective of\nconsumers or businesses, and whether the comments pertain to goods or services\nby utilizing information on the fields of comments and the industries of\nrespondents included in the Economy Watchers Survey. From these classified\nprice-related comments, we construct price sentiment indices not only for a\ngeneral purpose but also for more specific objectives by combining perspectives\non consumers and prices, as well as goods and services. It becomes possible to\nachieve a more accurate classification of price directions by employing a LLM\nfor classification. Furthermore, integrating the outputs of multiple LLMs\nsuggests the potential for the better performance of the classification. The\nuse of more accurately classified comments allows for the construction of an\nindex with a higher correlation to existing indices than previous studies. We\ndemonstrate that the correlation of the price index for consumers, which has a\nlarger sample size, is further enhanced by selecting comments for aggregation\nbased on the industry of the survey respondents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We aim to enhance a price sentiment index and to more precisely understand\nprice trends from the perspective of not only consumers but also businesses. We\nextract comments related to prices from the Economy Watchers Survey conducted\nby the Cabinet Office of Japan and classify price trends using a large language\nmodel (LLM). We classify whether the survey sample reflects the perspective of\nconsumers or businesses, and whether the comments pertain to goods or services\nby utilizing information on the fields of comments and the industries of\nrespondents included in the Economy Watchers Survey. From these classified\nprice-related comments, we construct price sentiment indices not only for a\ngeneral purpose but also for more specific objectives by combining perspectives\non consumers and prices, as well as goods and services. It becomes possible to\nachieve a more accurate classification of price directions by employing a LLM\nfor classification. Furthermore, integrating the outputs of multiple LLMs\nsuggests the potential for the better performance of the classification. The\nuse of more accurately classified comments allows for the construction of an\nindex with a higher correlation to existing indices than previous studies. We\ndemonstrate that the correlation of the price index for consumers, which has a\nlarger sample size, is further enhanced by selecting comments for aggregation\nbased on the industry of the survey respondents."
                },
                "authors": [
                    {
                        "name": "Masahiro Suzuki"
                    },
                    {
                        "name": "Hiroki Sakaji"
                    }
                ],
                "author_detail": {
                    "name": "Hiroki Sakaji"
                },
                "author": "Hiroki Sakaji",
                "arxiv_comment": "Accepted to IEEE BigData 2024. 9 pages, 11 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17502v1",
                "updated": "2024-11-26T15:13:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    13,
                    13,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:13:13Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    13,
                    13,
                    1,
                    331,
                    0
                ],
                "title": "Confidence-Aware Deep Learning for Load Plan Adjustments in the Parcel\n  Service Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence-Aware Deep Learning for Load Plan Adjustments in the Parcel\n  Service Industry"
                },
                "summary": "This study develops a deep learning-based approach to automate inbound load\nplan adjustments for a large transportation and logistics company. It addresses\na critical challenge for the efficient and resilient planning of E-commerce\noperations in presence of increasing uncertainties. The paper introduces an\ninnovative data-driven approach to inbound load planning. Leveraging extensive\nhistorical data, the paper presents a two-stage decision-making process using\ndeep learning and conformal prediction to provide scalable, accurate, and\nconfidence-aware solutions. The first stage of the prediction is dedicated to\ntactical load-planning, while the second stage is dedicated to the operational\nplanning, incorporating the latest available data to refine the decisions at\nthe finest granularity. Extensive experiments compare traditional machine\nlearning models and deep learning methods. They highlight the importance and\neffectiveness of the embedding layers for enhancing the performance of deep\nlearning models. Furthermore, the results emphasize the efficacy of conformal\nprediction to provide confidence-aware prediction sets. The findings suggest\nthat data-driven methods can substantially improve decision making in inbound\nload planning, offering planners a comprehensive, trustworthy, and real-time\nframework to make decisions. The initial deployment in the industry setting\nindicates a high accuracy of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study develops a deep learning-based approach to automate inbound load\nplan adjustments for a large transportation and logistics company. It addresses\na critical challenge for the efficient and resilient planning of E-commerce\noperations in presence of increasing uncertainties. The paper introduces an\ninnovative data-driven approach to inbound load planning. Leveraging extensive\nhistorical data, the paper presents a two-stage decision-making process using\ndeep learning and conformal prediction to provide scalable, accurate, and\nconfidence-aware solutions. The first stage of the prediction is dedicated to\ntactical load-planning, while the second stage is dedicated to the operational\nplanning, incorporating the latest available data to refine the decisions at\nthe finest granularity. Extensive experiments compare traditional machine\nlearning models and deep learning methods. They highlight the importance and\neffectiveness of the embedding layers for enhancing the performance of deep\nlearning models. Furthermore, the results emphasize the efficacy of conformal\nprediction to provide confidence-aware prediction sets. The findings suggest\nthat data-driven methods can substantially improve decision making in inbound\nload planning, offering planners a comprehensive, trustworthy, and real-time\nframework to make decisions. The initial deployment in the industry setting\nindicates a high accuracy of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Thomas Bruys"
                    },
                    {
                        "name": "Reza Zandehshahvar"
                    },
                    {
                        "name": "Amira Hijazi"
                    },
                    {
                        "name": "Pascal Van Hentenryck"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Van Hentenryck"
                },
                "author": "Pascal Van Hentenryck",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17501v1",
                "updated": "2024-11-26T15:13:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    13,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:13:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    13,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Inference Scaling $\\scriptsize\\mathtt{F}$Laws: The Limits of LLM\n  Resampling with Imperfect Verifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Scaling $\\scriptsize\\mathtt{F}$Laws: The Limits of LLM\n  Resampling with Imperfect Verifiers"
                },
                "summary": "Recent research has generated hope that inference scaling could allow weaker\nlanguage models to match or exceed the accuracy of stronger models, such as by\nrepeatedly sampling solutions to a coding problem until it passes unit tests.\nThe central thesis of this paper is that there is no free lunch for inference\nscaling: indefinite accuracy improvement through resampling can only be\nrealized if the \"verifier\" (in this case, a set of unit tests) is perfect. When\nthe verifier is imperfect, as it almost always is in domains such as reasoning\nor coding (for example, unit tests have imperfect coverage), there is a nonzero\nprobability of false positives: incorrect solutions that pass the verifier.\nResampling cannot decrease this probability, so it imposes an upper bound to\nthe accuracy of resampling-based inference scaling even with an infinite\ncompute budget. We find that there is a very strong correlation between the\nmodel's single-sample accuracy (i.e. accuracy without unit tests) and its false\npositive rate on coding benchmarks HumanEval and MBPP, whose unit tests have\nlimited coverage. Therefore, no amount of inference scaling of weaker models\ncan enable them to match the single-sample accuracy of a sufficiently strong\nmodel (Fig. 1a). When we consider that false positives have a negative utility\ncompared to abstaining from producing a solution, it bends the inference\nscaling curve further downward. Empirically, we find that the optimal number of\nsamples can be less than 10 under realistic assumptions (Fig. 1b). Finally, we\nshow that beyond accuracy, false positives may have other undesirable\nqualities, such as poor adherence to coding style conventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has generated hope that inference scaling could allow weaker\nlanguage models to match or exceed the accuracy of stronger models, such as by\nrepeatedly sampling solutions to a coding problem until it passes unit tests.\nThe central thesis of this paper is that there is no free lunch for inference\nscaling: indefinite accuracy improvement through resampling can only be\nrealized if the \"verifier\" (in this case, a set of unit tests) is perfect. When\nthe verifier is imperfect, as it almost always is in domains such as reasoning\nor coding (for example, unit tests have imperfect coverage), there is a nonzero\nprobability of false positives: incorrect solutions that pass the verifier.\nResampling cannot decrease this probability, so it imposes an upper bound to\nthe accuracy of resampling-based inference scaling even with an infinite\ncompute budget. We find that there is a very strong correlation between the\nmodel's single-sample accuracy (i.e. accuracy without unit tests) and its false\npositive rate on coding benchmarks HumanEval and MBPP, whose unit tests have\nlimited coverage. Therefore, no amount of inference scaling of weaker models\ncan enable them to match the single-sample accuracy of a sufficiently strong\nmodel (Fig. 1a). When we consider that false positives have a negative utility\ncompared to abstaining from producing a solution, it bends the inference\nscaling curve further downward. Empirically, we find that the optimal number of\nsamples can be less than 10 under realistic assumptions (Fig. 1b). Finally, we\nshow that beyond accuracy, false positives may have other undesirable\nqualities, such as poor adherence to coding style conventions."
                },
                "authors": [
                    {
                        "name": "Benedikt Stroebl"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Arvind Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Narayanan"
                },
                "author": "Arvind Narayanan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19100v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19100v3",
                "updated": "2024-11-26T14:29:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    29,
                    59,
                    1,
                    331,
                    0
                ],
                "published": "2024-05-29T14:06:09Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    14,
                    6,
                    9,
                    2,
                    150,
                    0
                ],
                "title": "Enhancing Zero-Shot Facial Expression Recognition by LLM Knowledge\n  Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Zero-Shot Facial Expression Recognition by LLM Knowledge\n  Transfer"
                },
                "summary": "Current facial expression recognition (FER) models are often designed in a\nsupervised learning manner and thus are constrained by the lack of large-scale\nfacial expression images with high-quality annotations. Consequently, these\nmodels often fail to generalize well, performing poorly on unseen images in\ninference. Vision-language-based zero-shot models demonstrate a promising\npotential for addressing such challenges. However, these models lack\ntask-specific knowledge and therefore are not optimized for the nuances of\nrecognizing facial expressions. To bridge this gap, this work proposes a novel\nmethod, Exp-CLIP, to enhance zero-shot FER by transferring the task knowledge\nfrom large language models (LLMs). Specifically, based on the pre-trained\nvision-language encoders, we incorporate a projection head designed to map the\ninitial joint vision-language space into a space that captures representations\nof facial actions. To train this projection head for subsequent zero-shot\npredictions, we propose to align the projected visual representations with\ntask-specific semantic meanings derived from the LLM encoder, and the text\ninstruction-based strategy is employed to customize the LLM knowledge. Given\nunlabelled facial data and efficient training of the projection head, Exp-CLIP\nachieves superior zero-shot results to the CLIP models and several other large\nvision-language models (LVLMs) on seven in-the-wild FER datasets. The code and\npre-trained models are available at https://github.com/zengqunzhao/Exp-CLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current facial expression recognition (FER) models are often designed in a\nsupervised learning manner and thus are constrained by the lack of large-scale\nfacial expression images with high-quality annotations. Consequently, these\nmodels often fail to generalize well, performing poorly on unseen images in\ninference. Vision-language-based zero-shot models demonstrate a promising\npotential for addressing such challenges. However, these models lack\ntask-specific knowledge and therefore are not optimized for the nuances of\nrecognizing facial expressions. To bridge this gap, this work proposes a novel\nmethod, Exp-CLIP, to enhance zero-shot FER by transferring the task knowledge\nfrom large language models (LLMs). Specifically, based on the pre-trained\nvision-language encoders, we incorporate a projection head designed to map the\ninitial joint vision-language space into a space that captures representations\nof facial actions. To train this projection head for subsequent zero-shot\npredictions, we propose to align the projected visual representations with\ntask-specific semantic meanings derived from the LLM encoder, and the text\ninstruction-based strategy is employed to customize the LLM knowledge. Given\nunlabelled facial data and efficient training of the projection head, Exp-CLIP\nachieves superior zero-shot results to the CLIP models and several other large\nvision-language models (LVLMs) on seven in-the-wild FER datasets. The code and\npre-trained models are available at https://github.com/zengqunzhao/Exp-CLIP."
                },
                "authors": [
                    {
                        "name": "Zengqun Zhao"
                    },
                    {
                        "name": "Yu Cao"
                    },
                    {
                        "name": "Shaogang Gong"
                    },
                    {
                        "name": "Ioannis Patras"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Patras"
                },
                "author": "Ioannis Patras",
                "arxiv_comment": "Accepted at WACV 2025 (Camera-Ready Version)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19100v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19100v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17461v1",
                "updated": "2024-11-26T14:28:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    28,
                    25,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T14:28:25Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    28,
                    25,
                    1,
                    331,
                    0
                ],
                "title": "SoK: Decentralized AI (DeAI)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Decentralized AI (DeAI)"
                },
                "summary": "The centralization of Artificial Intelligence (AI) poses significant\nchallenges, including single points of failure, inherent biases, data privacy\nconcerns, and scalability issues. These problems are especially prevalent in\nclosed-source large language models (LLMs), where user data is collected and\nused without transparency. To mitigate these issues, blockchain-based\ndecentralized AI (DeAI) has emerged as a promising solution. DeAI combines the\nstrengths of both blockchain and AI technologies to enhance the transparency,\nsecurity, decentralization, and trustworthiness of AI systems. However, a\ncomprehensive understanding of state-of-the-art DeAI development, particularly\nfor active industry solutions, is still lacking. In this work, we present a\nSystematization of Knowledge (SoK) for blockchain-based DeAI solutions. We\npropose a taxonomy to classify existing DeAI protocols based on the model\nlifecycle. Based on this taxonomy, we provide a structured way to clarify the\nlandscape of DeAI protocols and identify their similarities and differences. We\nanalyze the functionalities of blockchain in DeAI, investigating how blockchain\nfeatures contribute to enhancing the security, transparency, and\ntrustworthiness of AI processes, while also ensuring fair incentives for AI\ndata and model contributors. In addition, we identify key insights and research\ngaps in developing DeAI protocols, highlighting several critical avenues for\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The centralization of Artificial Intelligence (AI) poses significant\nchallenges, including single points of failure, inherent biases, data privacy\nconcerns, and scalability issues. These problems are especially prevalent in\nclosed-source large language models (LLMs), where user data is collected and\nused without transparency. To mitigate these issues, blockchain-based\ndecentralized AI (DeAI) has emerged as a promising solution. DeAI combines the\nstrengths of both blockchain and AI technologies to enhance the transparency,\nsecurity, decentralization, and trustworthiness of AI systems. However, a\ncomprehensive understanding of state-of-the-art DeAI development, particularly\nfor active industry solutions, is still lacking. In this work, we present a\nSystematization of Knowledge (SoK) for blockchain-based DeAI solutions. We\npropose a taxonomy to classify existing DeAI protocols based on the model\nlifecycle. Based on this taxonomy, we provide a structured way to clarify the\nlandscape of DeAI protocols and identify their similarities and differences. We\nanalyze the functionalities of blockchain in DeAI, investigating how blockchain\nfeatures contribute to enhancing the security, transparency, and\ntrustworthiness of AI processes, while also ensuring fair incentives for AI\ndata and model contributors. In addition, we identify key insights and research\ngaps in developing DeAI protocols, highlighting several critical avenues for\nfuture research."
                },
                "authors": [
                    {
                        "name": "Zhipeng Wang"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Elizabeth Lui"
                    },
                    {
                        "name": "Vatsal Shah"
                    },
                    {
                        "name": "Xihan Xiong"
                    },
                    {
                        "name": "Jiahao Sun"
                    },
                    {
                        "name": "Davide Crapis"
                    },
                    {
                        "name": "William Knottenbelt"
                    }
                ],
                "author_detail": {
                    "name": "William Knottenbelt"
                },
                "author": "William Knottenbelt",
                "arxiv_comment": "This is a Systematization of Knowledge (SoK) for the rapidly evolving\n  field of Decentralized AI (DeAI). We welcome valuable comments, suggestions,\n  and collaboration to further refine and enhance this work. We hope our\n  contribution will help accelerate the advancement of DeAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13549v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13549v3",
                "updated": "2024-11-26T14:15:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    15,
                    57,
                    1,
                    331,
                    0
                ],
                "published": "2023-06-23T15:21:52Z",
                "published_parsed": [
                    2023,
                    6,
                    23,
                    15,
                    21,
                    52,
                    4,
                    174,
                    0
                ],
                "title": "A Survey on Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Multimodal Large Language Models"
                },
                "summary": "Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has\nbeen a new rising research hotspot, which uses powerful Large Language Models\n(LLMs) as a brain to perform multimodal tasks. The surprising emergent\ncapabilities of MLLM, such as writing stories based on images and Optical\nCharacter Recognition (OCR)-free math reasoning, are rare in traditional\nmultimodal methods, suggesting a potential path to artificial general\nintelligence. To this end, both academia and industry have endeavored to\ndevelop MLLMs that can compete with or even outperform GPT-4V, pushing the\nlimit of research at a surprising speed. In this paper, we aim to trace and\nsummarize the recent progress of MLLMs. First of all, we present the basic\nformulation of MLLM and delineate its related concepts, including architecture,\ntraining strategy and data, as well as evaluation. Then, we introduce research\ntopics about how MLLMs can be extended to support more granularity, modalities,\nlanguages, and scenarios. We continue with multimodal hallucination and\nextended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT),\nand LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss\nexisting challenges and point out promising research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has\nbeen a new rising research hotspot, which uses powerful Large Language Models\n(LLMs) as a brain to perform multimodal tasks. The surprising emergent\ncapabilities of MLLM, such as writing stories based on images and Optical\nCharacter Recognition (OCR)-free math reasoning, are rare in traditional\nmultimodal methods, suggesting a potential path to artificial general\nintelligence. To this end, both academia and industry have endeavored to\ndevelop MLLMs that can compete with or even outperform GPT-4V, pushing the\nlimit of research at a surprising speed. In this paper, we aim to trace and\nsummarize the recent progress of MLLMs. First of all, we present the basic\nformulation of MLLM and delineate its related concepts, including architecture,\ntraining strategy and data, as well as evaluation. Then, we introduce research\ntopics about how MLLMs can be extended to support more granularity, modalities,\nlanguages, and scenarios. We continue with multimodal hallucination and\nextended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT),\nand LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss\nexisting challenges and point out promising research directions."
                },
                "authors": [
                    {
                        "name": "Shukang Yin"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_doi": "10.1093/nsr/nwae403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/nsr/nwae403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2306.13549v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13549v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 pages, 3 figures, 9 tables, accepted for publication in National\n  Science Review. Project\n  page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17453v1",
                "updated": "2024-11-26T14:12:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    12,
                    9,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T14:12:09Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    12,
                    9,
                    1,
                    331,
                    0
                ],
                "title": "PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient\n  Fine-Tuning"
                },
                "summary": "Fine-tuning is an essential process to improve the performance of Large\nLanguage Models (LLMs) in specific domains, with Parameter-Efficient\nFine-Tuning (PEFT) gaining popularity due to its capacity to reduce\ncomputational demands through the integration of low-rank adapters. These\nlightweight adapters, such as LoRA, can be shared and utilized on open-source\nplatforms. However, adversaries could exploit this mechanism to inject\nbackdoors into these adapters, resulting in malicious behaviors like incorrect\nor harmful outputs, which pose serious security risks to the community.\nUnfortunately, few of the current efforts concentrate on analyzing the backdoor\npatterns or detecting the backdoors in the adapters.\n  To fill this gap, we first construct (and will release) PADBench, a\ncomprehensive benchmark that contains 13,300 benign and backdoored adapters\nfine-tuned with various datasets, attack strategies, PEFT methods, and LLMs.\nMoreover, we propose PEFTGuard, the first backdoor detection framework against\nPEFT-based adapters. Extensive evaluation upon PADBench shows that PEFTGuard\noutperforms existing detection methods, achieving nearly perfect detection\naccuracy (100%) in most cases. Notably, PEFTGuard exhibits zero-shot\ntransferability on three aspects, including different attacks, PEFT methods,\nand adapter ranks. In addition, we consider various adaptive attacks to\ndemonstrate the high robustness of PEFTGuard. We further explore several\npossible backdoor mitigation defenses, finding fine-mixing to be the most\neffective method. We envision our benchmark and method can shed light on future\nLLM backdoor detection research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning is an essential process to improve the performance of Large\nLanguage Models (LLMs) in specific domains, with Parameter-Efficient\nFine-Tuning (PEFT) gaining popularity due to its capacity to reduce\ncomputational demands through the integration of low-rank adapters. These\nlightweight adapters, such as LoRA, can be shared and utilized on open-source\nplatforms. However, adversaries could exploit this mechanism to inject\nbackdoors into these adapters, resulting in malicious behaviors like incorrect\nor harmful outputs, which pose serious security risks to the community.\nUnfortunately, few of the current efforts concentrate on analyzing the backdoor\npatterns or detecting the backdoors in the adapters.\n  To fill this gap, we first construct (and will release) PADBench, a\ncomprehensive benchmark that contains 13,300 benign and backdoored adapters\nfine-tuned with various datasets, attack strategies, PEFT methods, and LLMs.\nMoreover, we propose PEFTGuard, the first backdoor detection framework against\nPEFT-based adapters. Extensive evaluation upon PADBench shows that PEFTGuard\noutperforms existing detection methods, achieving nearly perfect detection\naccuracy (100%) in most cases. Notably, PEFTGuard exhibits zero-shot\ntransferability on three aspects, including different attacks, PEFT methods,\nand adapter ranks. In addition, we consider various adaptive attacks to\ndemonstrate the high robustness of PEFTGuard. We further explore several\npossible backdoor mitigation defenses, finding fine-mixing to be the most\neffective method. We envision our benchmark and method can shed light on future\nLLM backdoor detection research."
                },
                "authors": [
                    {
                        "name": "Zhen Sun"
                    },
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Yule Liu"
                    },
                    {
                        "name": "Chenhao Lin"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Rongmao Chen"
                    },
                    {
                        "name": "Xingshuo Han"
                    },
                    {
                        "name": "Xinyi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xinyi Huang"
                },
                "author": "Xinyi Huang",
                "arxiv_comment": "20 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00073v2",
                "updated": "2024-11-26T13:55:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    55,
                    29,
                    1,
                    331,
                    0
                ],
                "published": "2024-10-31T16:22:26Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    22,
                    26,
                    3,
                    305,
                    0
                ],
                "title": "RSL-SQL: Robust Schema Linking in Text-to-SQL Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RSL-SQL: Robust Schema Linking in Text-to-SQL Generation"
                },
                "summary": "Text-to-SQL generation aims to translate natural language questions into SQL\nstatements. In Text-to-SQL based on large language models, schema linking is a\nwidely adopted strategy to streamline the input for LLMs by selecting only\nrelevant schema elements, therefore reducing noise and computational overhead.\nHowever, schema linking faces risks that require caution, including the\npotential omission of necessary elements and disruption of database structural\nintegrity. To address these challenges, we propose a novel framework called\nRSL-SQL that combines bidirectional schema linking, contextual information\naugmentation, binary selection strategy, and multi-turn self-correction. We\nimprove the recall of pattern linking using forward and backward pruning\nmethods, achieving a strict recall of 94% while reducing the number of input\ncolumns by 83%. Furthermore, it hedges the risk by voting between a full mode\nand a simplified mode enhanced with contextual information. Experiments on the\nBIRD and Spider benchmarks demonstrate that our approach achieves SOTA\nexecution accuracy among open-source solutions, with 67.2% on BIRD and 87.9% on\nSpider using GPT-4o. Furthermore, our approach outperforms a series of GPT-4\nbased Text-to-SQL systems when adopting DeepSeek (much cheaper) with same\nintact prompts. Extensive analysis and ablation studies confirm the\neffectiveness of each component in our framework. The codes are available at\nhttps://github.com/Laqcce-cao/RSL-SQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL generation aims to translate natural language questions into SQL\nstatements. In Text-to-SQL based on large language models, schema linking is a\nwidely adopted strategy to streamline the input for LLMs by selecting only\nrelevant schema elements, therefore reducing noise and computational overhead.\nHowever, schema linking faces risks that require caution, including the\npotential omission of necessary elements and disruption of database structural\nintegrity. To address these challenges, we propose a novel framework called\nRSL-SQL that combines bidirectional schema linking, contextual information\naugmentation, binary selection strategy, and multi-turn self-correction. We\nimprove the recall of pattern linking using forward and backward pruning\nmethods, achieving a strict recall of 94% while reducing the number of input\ncolumns by 83%. Furthermore, it hedges the risk by voting between a full mode\nand a simplified mode enhanced with contextual information. Experiments on the\nBIRD and Spider benchmarks demonstrate that our approach achieves SOTA\nexecution accuracy among open-source solutions, with 67.2% on BIRD and 87.9% on\nSpider using GPT-4o. Furthermore, our approach outperforms a series of GPT-4\nbased Text-to-SQL systems when adopting DeepSeek (much cheaper) with same\nintact prompts. Extensive analysis and ablation studies confirm the\neffectiveness of each component in our framework. The codes are available at\nhttps://github.com/Laqcce-cao/RSL-SQL."
                },
                "authors": [
                    {
                        "name": "Zhenbiao Cao"
                    },
                    {
                        "name": "Yuanlei Zheng"
                    },
                    {
                        "name": "Zhihao Fan"
                    },
                    {
                        "name": "Xiaojin Zhang"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17437v1",
                "updated": "2024-11-26T13:51:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    51,
                    48,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T13:51:48Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    51,
                    48,
                    1,
                    331,
                    0
                ],
                "title": "\"Stupid robot, I want to speak to a human!\" User Frustration Detection\n  in Task-Oriented Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Stupid robot, I want to speak to a human!\" User Frustration Detection\n  in Task-Oriented Dialog Systems"
                },
                "summary": "Detecting user frustration in modern-day task-oriented dialog (TOD) systems\nis imperative for maintaining overall user satisfaction, engagement, and\nretention. However, most recent research is focused on sentiment and emotion\ndetection in academic settings, thus failing to fully encapsulate implications\nof real-world user data. To mitigate this gap, in this work, we focus on user\nfrustration in a deployed TOD system, assessing the feasibility of\nout-of-the-box solutions for user frustration detection. Specifically, we\ncompare the performance of our deployed keyword-based approach, open-source\napproaches to sentiment analysis, dialog breakdown detection methods, and\nemerging in-context learning LLM-based detection. Our analysis highlights the\nlimitations of open-source methods for real-world frustration detection, while\ndemonstrating the superior performance of the LLM-based approach, achieving a\n16\\% relative improvement in F1 score on an internal benchmark. Finally, we\nanalyze advantages and limitations of our methods and provide an insight into\nuser frustration detection task for industry practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting user frustration in modern-day task-oriented dialog (TOD) systems\nis imperative for maintaining overall user satisfaction, engagement, and\nretention. However, most recent research is focused on sentiment and emotion\ndetection in academic settings, thus failing to fully encapsulate implications\nof real-world user data. To mitigate this gap, in this work, we focus on user\nfrustration in a deployed TOD system, assessing the feasibility of\nout-of-the-box solutions for user frustration detection. Specifically, we\ncompare the performance of our deployed keyword-based approach, open-source\napproaches to sentiment analysis, dialog breakdown detection methods, and\nemerging in-context learning LLM-based detection. Our analysis highlights the\nlimitations of open-source methods for real-world frustration detection, while\ndemonstrating the superior performance of the LLM-based approach, achieving a\n16\\% relative improvement in F1 score on an internal benchmark. Finally, we\nanalyze advantages and limitations of our methods and provide an insight into\nuser frustration detection task for industry practitioners."
                },
                "authors": [
                    {
                        "name": "Mireia Hernandez Caralt"
                    },
                    {
                        "name": "Ivan Sekulić"
                    },
                    {
                        "name": "Filip Carević"
                    },
                    {
                        "name": "Nghia Khau"
                    },
                    {
                        "name": "Diana Nicoleta Popa"
                    },
                    {
                        "name": "Bruna Guedes"
                    },
                    {
                        "name": "Victor Guimarães"
                    },
                    {
                        "name": "Zeyu Yang"
                    },
                    {
                        "name": "Andre Manso"
                    },
                    {
                        "name": "Meghana Reddy"
                    },
                    {
                        "name": "Paolo Rosso"
                    },
                    {
                        "name": "Roland Mathis"
                    }
                ],
                "author_detail": {
                    "name": "Roland Mathis"
                },
                "author": "Roland Mathis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12736v3",
                "updated": "2024-11-26T13:35:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    35,
                    5,
                    1,
                    331,
                    0
                ],
                "published": "2024-04-19T09:29:53Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    9,
                    29,
                    53,
                    4,
                    110,
                    0
                ],
                "title": "Large Language Model Supply Chain: A Research Agenda",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Supply Chain: A Research Agenda"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized\nartificial intelligence, introducing unprecedented capabilities in natural\nlanguage processing and multimodal content generation. However, the increasing\ncomplexity and scale of these models have given rise to a multifaceted supply\nchain that presents unique challenges across infrastructure, foundation models,\nand downstream applications. This paper provides the first comprehensive\nresearch agenda of the LLM supply chain, offering a structured approach to\nidentify critical challenges and opportunities through the dual lenses of\nsoftware engineering (SE) and security & privacy (S\\&P). We begin by\nestablishing a clear definition of the LLM supply chain, encompassing its\ncomponents and dependencies. We then analyze each layer of the supply chain,\npresenting a vision for robust and secure LLM development, reviewing the\ncurrent state of practices and technologies, and identifying key challenges and\nresearch opportunities. This work aims to bridge the existing research gap in\nsystematically understanding the multifaceted issues within the LLM supply\nchain, offering valuable insights to guide future efforts in this rapidly\nevolving domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized\nartificial intelligence, introducing unprecedented capabilities in natural\nlanguage processing and multimodal content generation. However, the increasing\ncomplexity and scale of these models have given rise to a multifaceted supply\nchain that presents unique challenges across infrastructure, foundation models,\nand downstream applications. This paper provides the first comprehensive\nresearch agenda of the LLM supply chain, offering a structured approach to\nidentify critical challenges and opportunities through the dual lenses of\nsoftware engineering (SE) and security & privacy (S\\&P). We begin by\nestablishing a clear definition of the LLM supply chain, encompassing its\ncomponents and dependencies. We then analyze each layer of the supply chain,\npresenting a vision for robust and secure LLM development, reviewing the\ncurrent state of practices and technologies, and identifying key challenges and\nresearch opportunities. This work aims to bridge the existing research gap in\nsystematically understanding the multifaceted issues within the LLM supply\nchain, offering valuable insights to guide future efforts in this rapidly\nevolving domain."
                },
                "authors": [
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM) Special Issue: 2030 Software Engineering Roadmap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10779v2",
                "updated": "2024-11-26T13:32:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    32,
                    22,
                    1,
                    331,
                    0
                ],
                "published": "2024-02-16T16:02:33Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    16,
                    2,
                    33,
                    4,
                    47,
                    0
                ],
                "title": "A Condensed Transition Graph Framework for Zero-shot Link Prediction\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Condensed Transition Graph Framework for Zero-shot Link Prediction\n  with Large Language Models"
                },
                "summary": "Zero-shot link prediction (ZSLP) on knowledge graphs aims at automatically\nidentifying relations between given entities. Existing methods primarily employ\nauxiliary information to predict tail entity given head entity and its\nrelation, yet face challenges due to the occasional unavailability of such\ndetailed information and the inherent simplicity of predicting tail entities\nbased on semantic similarities. Even though Large Language Models (LLMs) offer\na promising solution to predict unobserved relations between the head and tail\nentity in a zero-shot manner, their performance is still restricted due to the\ninability to leverage all the (exponentially many) paths' information between\ntwo entities, which are critical in collectively indicating their relation\ntypes. To address this, in this work, we introduce a Condensed Transition Graph\nFramework for Zero-Shot Link Prediction (CTLP), which encodes all the paths'\ninformation in linear time complexity to predict unseen relations between\nentities, attaining both efficiency and information preservation. Specifically,\nwe design a condensed transition graph encoder with theoretical guarantees on\nits coverage, expressiveness, and efficiency. It is learned by a transition\ngraph contrastive learning strategy. Subsequently, we design a soft instruction\ntuning to learn and map the all-path embedding to the input of LLMs.\nExperimental results show that our proposed CTLP method achieves\nstate-of-the-art performance on three standard ZSLP datasets",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot link prediction (ZSLP) on knowledge graphs aims at automatically\nidentifying relations between given entities. Existing methods primarily employ\nauxiliary information to predict tail entity given head entity and its\nrelation, yet face challenges due to the occasional unavailability of such\ndetailed information and the inherent simplicity of predicting tail entities\nbased on semantic similarities. Even though Large Language Models (LLMs) offer\na promising solution to predict unobserved relations between the head and tail\nentity in a zero-shot manner, their performance is still restricted due to the\ninability to leverage all the (exponentially many) paths' information between\ntwo entities, which are critical in collectively indicating their relation\ntypes. To address this, in this work, we introduce a Condensed Transition Graph\nFramework for Zero-Shot Link Prediction (CTLP), which encodes all the paths'\ninformation in linear time complexity to predict unseen relations between\nentities, attaining both efficiency and information preservation. Specifically,\nwe design a condensed transition graph encoder with theoretical guarantees on\nits coverage, expressiveness, and efficiency. It is learned by a transition\ngraph contrastive learning strategy. Subsequently, we design a soft instruction\ntuning to learn and map the all-path embedding to the input of LLMs.\nExperimental results show that our proposed CTLP method achieves\nstate-of-the-art performance on three standard ZSLP datasets"
                },
                "authors": [
                    {
                        "name": "Mingchen Li"
                    },
                    {
                        "name": "Chen Ling"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Liang Zhao"
                },
                "author": "Liang Zhao",
                "arxiv_comment": "Published as a conference paper at ICDM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11581v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11581v4",
                "updated": "2024-11-26T13:22:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    22,
                    19,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-18T13:57:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    57,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "OASIS: Open Agent Social Interaction Simulations with One Million Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS: Open Agent Social Interaction Simulations with One Million Agents"
                },
                "summary": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments."
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Zaibin Zhang"
                    },
                    {
                        "name": "Zirui Zheng"
                    },
                    {
                        "name": "Yuxian Jiang"
                    },
                    {
                        "name": "Ziyue Gan"
                    },
                    {
                        "name": "Zhiyu Wang"
                    },
                    {
                        "name": "Zijian Ling"
                    },
                    {
                        "name": "Jinsong Chen"
                    },
                    {
                        "name": "Martz Ma"
                    },
                    {
                        "name": "Bowen Dong"
                    },
                    {
                        "name": "Prateek Gupta"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Guohao Li"
                    },
                    {
                        "name": "Xu Jia"
                    },
                    {
                        "name": "Lijun Wang"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Chaochao Lu"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11581v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11581v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13990v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13990v3",
                "updated": "2024-11-26T13:21:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    21,
                    44,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-21T10:00:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    10,
                    0,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Repository-level Code Translation Benchmark Targeting Rust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level Code Translation Benchmark Targeting Rust"
                },
                "summary": "Recent advances in large language models (LLMs) have shown significant\ncapabilities in code translation, often evaluated using benchmarks like\nCodeTransOcean. However, these evaluations typically focus on simple,\nfunction-level translations without considering dependencies, which does not\nreflect the complexities of real-world software development. Further, their\neffectiveness in translating to newer, lower-resource languages like Rust in\nrealistic scenarios is still under-explored. To address this gap, we introduce\nfirst repository-level code translation benchmark comprising 375 tasks\ntargeting Rust, complete with relevant dependencies. Using this benchmark, we\nstudy four state-of-the-art LLMs, analyzing their erroneous outputs to\nunderstand their performance in more complex translation scenarios. Our\nfindings reveal that LLMs exhibit substantially worse performance (41.5%-56.2%\nPass@1 drop of GPT-4) on repository-level translations compared to simpler\ntasks, highlighting limitations in existing evaluation methods. The model that\nperformed the best is Claude-3.5, demonstrating the strongest translation\ncapabilities in both basic functionality accuracy and several relevant\nadditional abilities. Additionally, we discover that LLMs struggle with\nidentifying language differences in complex tasks, and that increased\ndependencies correlate with greater translation difficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown significant\ncapabilities in code translation, often evaluated using benchmarks like\nCodeTransOcean. However, these evaluations typically focus on simple,\nfunction-level translations without considering dependencies, which does not\nreflect the complexities of real-world software development. Further, their\neffectiveness in translating to newer, lower-resource languages like Rust in\nrealistic scenarios is still under-explored. To address this gap, we introduce\nfirst repository-level code translation benchmark comprising 375 tasks\ntargeting Rust, complete with relevant dependencies. Using this benchmark, we\nstudy four state-of-the-art LLMs, analyzing their erroneous outputs to\nunderstand their performance in more complex translation scenarios. Our\nfindings reveal that LLMs exhibit substantially worse performance (41.5%-56.2%\nPass@1 drop of GPT-4) on repository-level translations compared to simpler\ntasks, highlighting limitations in existing evaluation methods. The model that\nperformed the best is Claude-3.5, demonstrating the strongest translation\ncapabilities in both basic functionality accuracy and several relevant\nadditional abilities. Additionally, we discover that LLMs struggle with\nidentifying language differences in complex tasks, and that increased\ndependencies correlate with greater translation difficulty."
                },
                "authors": [
                    {
                        "name": "Guangsheng Ou"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13990v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13990v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17404v1",
                "updated": "2024-11-26T13:05:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    5,
                    53,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T13:05:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    5,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving"
                },
                "summary": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source operations research datasets lack detailed annotations of\nthe modeling process, such as variable definitions, focusing solely on\nobjective values, which hinders reinforcement learning applications. To address\nthis, we release the StructuredOR dataset, annotated with comprehensive labels\nthat capture the complete mathematical modeling process. We further propose\nBPP-Search, a algorithm that integrates reinforcement learning into a\ntree-of-thought structure using Beam search, a Process reward model, and a\npairwise Preference algorithm. This approach enables efficient exploration of\ntree structures, avoiding exhaustive search while improving accuracy. Extensive\nexperiments on StructuredOR, NL4OPT, and MAMO-ComplexLP datasets show that\nBPP-Search significantly outperforms state-of-the-art methods, including\nChain-of-Thought, Self-Consistency, and Tree-of-Thought. In tree-based\nreasoning, BPP-Search also surpasses Process Reward Model combined with Greedy\nor Beam Search, demonstrating superior accuracy and efficiency, and enabling\nfaster retrieval of correct solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source operations research datasets lack detailed annotations of\nthe modeling process, such as variable definitions, focusing solely on\nobjective values, which hinders reinforcement learning applications. To address\nthis, we release the StructuredOR dataset, annotated with comprehensive labels\nthat capture the complete mathematical modeling process. We further propose\nBPP-Search, a algorithm that integrates reinforcement learning into a\ntree-of-thought structure using Beam search, a Process reward model, and a\npairwise Preference algorithm. This approach enables efficient exploration of\ntree structures, avoiding exhaustive search while improving accuracy. Extensive\nexperiments on StructuredOR, NL4OPT, and MAMO-ComplexLP datasets show that\nBPP-Search significantly outperforms state-of-the-art methods, including\nChain-of-Thought, Self-Consistency, and Tree-of-Thought. In tree-based\nreasoning, BPP-Search also surpasses Process Reward Model combined with Greedy\nor Beam Search, demonstrating superior accuracy and efficiency, and enabling\nfaster retrieval of correct solutions."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Wing-Yin Yu"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Zehua Liu"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Hailei Gong"
                    },
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Ruifeng She"
                    },
                    {
                        "name": "Fangzhou Zhu"
                    },
                    {
                        "name": "Tao Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhong"
                },
                "author": "Tao Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17401v1",
                "updated": "2024-11-26T13:03:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    3,
                    49,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T13:03:49Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    3,
                    49,
                    1,
                    331,
                    0
                ],
                "title": "One Mind, Many Tongues: A Deep Dive into Language-Agnostic Knowledge\n  Neurons in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Mind, Many Tongues: A Deep Dive into Language-Agnostic Knowledge\n  Neurons in Large Language Models"
                },
                "summary": "Large language models (LLMs) have learned vast amounts of factual knowledge\nthrough self-supervised pre-training on large-scale corpora. Meanwhile, LLMs\nhave also demonstrated excellent multilingual capabilities, which can express\nthe learned knowledge in multiple languages. However, the knowledge storage\nmechanism in LLMs still remains mysterious. Some researchers attempt to\ndemystify the factual knowledge in LLMs from the perspective of knowledge\nneurons, and subsequently discover language-agnostic knowledge neurons that\nstore factual knowledge in a form that transcends language barriers. However,\nthe preliminary finding suffers from two limitations: 1) High Uncertainty in\nLocalization Results. Existing study only uses a prompt-based probe to localize\nknowledge neurons for each fact, while LLMs cannot provide consistent answers\nfor semantically equivalent queries. Thus, it leads to inaccurate localization\nresults with high uncertainty. 2) Lack of Analysis in More Languages. The study\nonly analyzes language-agnostic knowledge neurons on English and Chinese data,\nwithout exploring more language families and languages. Naturally, it limits\nthe generalizability of the findings. To address aforementioned problems, we\nfirst construct a new benchmark called Rephrased Multilingual LAMA (RML-LAMA),\nwhich contains high-quality cloze-style multilingual parallel queries for each\nfact. Then, we propose a novel method named Multilingual Integrated Gradients\nwith Uncertainty Estimation (MATRICE), which quantifies the uncertainty across\nqueries and languages during knowledge localization. Extensive experiments show\nthat our method can accurately localize language-agnostic knowledge neurons. We\nalso further investigate the role of language-agnostic knowledge neurons in\ncross-lingual knowledge editing, knowledge enhancement and new knowledge\ninjection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have learned vast amounts of factual knowledge\nthrough self-supervised pre-training on large-scale corpora. Meanwhile, LLMs\nhave also demonstrated excellent multilingual capabilities, which can express\nthe learned knowledge in multiple languages. However, the knowledge storage\nmechanism in LLMs still remains mysterious. Some researchers attempt to\ndemystify the factual knowledge in LLMs from the perspective of knowledge\nneurons, and subsequently discover language-agnostic knowledge neurons that\nstore factual knowledge in a form that transcends language barriers. However,\nthe preliminary finding suffers from two limitations: 1) High Uncertainty in\nLocalization Results. Existing study only uses a prompt-based probe to localize\nknowledge neurons for each fact, while LLMs cannot provide consistent answers\nfor semantically equivalent queries. Thus, it leads to inaccurate localization\nresults with high uncertainty. 2) Lack of Analysis in More Languages. The study\nonly analyzes language-agnostic knowledge neurons on English and Chinese data,\nwithout exploring more language families and languages. Naturally, it limits\nthe generalizability of the findings. To address aforementioned problems, we\nfirst construct a new benchmark called Rephrased Multilingual LAMA (RML-LAMA),\nwhich contains high-quality cloze-style multilingual parallel queries for each\nfact. Then, we propose a novel method named Multilingual Integrated Gradients\nwith Uncertainty Estimation (MATRICE), which quantifies the uncertainty across\nqueries and languages during knowledge localization. Extensive experiments show\nthat our method can accurately localize language-agnostic knowledge neurons. We\nalso further investigate the role of language-agnostic knowledge neurons in\ncross-lingual knowledge editing, knowledge enhancement and new knowledge\ninjection."
                },
                "authors": [
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Yuheng Chen"
                    },
                    {
                        "name": "Zhuoran Jin"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17388v1",
                "updated": "2024-11-26T12:46:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    46,
                    57,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T12:46:57Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    46,
                    57,
                    1,
                    331,
                    0
                ],
                "title": "Can LLMs be Good Graph Judger for Knowledge Graph Construction?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs be Good Graph Judger for Knowledge Graph Construction?"
                },
                "summary": "In real-world scenarios, most of the data obtained from information retrieval\n(IR) system is unstructured. Converting natural language sentences into\nstructured Knowledge Graphs (KGs) remains a critical challenge. The quality of\nconstructed KGs may also impact the performance of some KG-dependent domains\nlike GraphRAG systems and recommendation systems. Recently, Large Language\nModels (LLMs) have demonstrated impressive capabilities in addressing a wide\nrange of natural language processing tasks. However, there are still challenges\nwhen utilizing LLMs to address the task of generating structured KGs. And we\nhave identified three limitations with respect to existing KG construction\nmethods. (1)There is a large amount of information and excessive noise in\nreal-world documents, which could result in extracting messy information.\n(2)Native LLMs struggle to effectively extract accuracy knowledge from some\ndomain-specific documents. (3)Hallucinations phenomenon cannot be overlooked\nwhen utilizing LLMs directly as an unsupervised method for constructing KGs.\n  In this paper, we propose GraphJudger, a knowledge graph construction\nframework to address the aforementioned challenges. We introduce three\ninnovative modules in our method, which are entity-centric iterative text\ndenoising, knowledge aware instruction tuning and graph judgement,\nrespectively. We seek to utilize the capacity of LLMs to function as a graph\njudger, a capability superior to their role only as a predictor for KG\nconstruction problems. Experiments conducted on two general text-graph pair\ndatasets and one domain-specific text-graph pair dataset show superior\nperformances compared to baseline methods. The code of our proposed method is\navailable at https://github.com/hhy-huang/GraphJudger.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world scenarios, most of the data obtained from information retrieval\n(IR) system is unstructured. Converting natural language sentences into\nstructured Knowledge Graphs (KGs) remains a critical challenge. The quality of\nconstructed KGs may also impact the performance of some KG-dependent domains\nlike GraphRAG systems and recommendation systems. Recently, Large Language\nModels (LLMs) have demonstrated impressive capabilities in addressing a wide\nrange of natural language processing tasks. However, there are still challenges\nwhen utilizing LLMs to address the task of generating structured KGs. And we\nhave identified three limitations with respect to existing KG construction\nmethods. (1)There is a large amount of information and excessive noise in\nreal-world documents, which could result in extracting messy information.\n(2)Native LLMs struggle to effectively extract accuracy knowledge from some\ndomain-specific documents. (3)Hallucinations phenomenon cannot be overlooked\nwhen utilizing LLMs directly as an unsupervised method for constructing KGs.\n  In this paper, we propose GraphJudger, a knowledge graph construction\nframework to address the aforementioned challenges. We introduce three\ninnovative modules in our method, which are entity-centric iterative text\ndenoising, knowledge aware instruction tuning and graph judgement,\nrespectively. We seek to utilize the capacity of LLMs to function as a graph\njudger, a capability superior to their role only as a predictor for KG\nconstruction problems. Experiments conducted on two general text-graph pair\ndatasets and one domain-specific text-graph pair dataset show superior\nperformances compared to baseline methods. The code of our proposed method is\navailable at https://github.com/hhy-huang/GraphJudger."
                },
                "authors": [
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17384v1",
                "updated": "2024-11-26T12:43:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    43,
                    19,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T12:43:19Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    43,
                    19,
                    1,
                    331,
                    0
                ],
                "title": "Assessing Electricity Network Capacity Requirements for Industrial\n  Decarbonisation in Great Britain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Electricity Network Capacity Requirements for Industrial\n  Decarbonisation in Great Britain"
                },
                "summary": "Decarbonising the industrial sector is vital to reach net zero targets. The\ndeployment of industrial decarbonisation technologies is expected to increase\nindustrial electricity demand in many countries and this may require upgrades\nto the existing electricity network or new network investment. While the\ninfrastructure requirements to support the introduction of new fuels and\ntechnologies in industry, such as hydrogen and carbon capture, utilisation and\nstorage are often discussed, the need for investment to increase the capacity\nof the electricity network to meet increasing industrial electricity demands is\noften overlooked in the literature. This paper addresses this gap by\nquantifying the requirements for additional electricity network capacity to\nsupport the decarbonisation of industrial sectors across Great Britain (GB).\nThe Net Zero Industrial Pathways model is used to predict the future\nelectricity demand from industrial sites to 2050 which is then compared\nspatially to the available headroom across the distribution network in GB. The\nresults show that network headroom is sufficient to meet extra capacity demands\nfrom industrial sites over the period to 2030 in nearly all GB regions and\nnetwork scenarios. However, as electricity demand rises due to increased\nelectrification across all sectors and industrial decarbonisation accelerates\ntowards 2050, the network will need significant new capacity (71 GW + by 2050)\nparticularly in the central, south, and north-west regions of England, and\nWales. Without solving these network constraints, around 65% of industrial\nsites that are large point sources of emissions would be constrained in terms\nof electric capacity by 2040. These sites are responsible for 69% of industrial\npoint source emissions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decarbonising the industrial sector is vital to reach net zero targets. The\ndeployment of industrial decarbonisation technologies is expected to increase\nindustrial electricity demand in many countries and this may require upgrades\nto the existing electricity network or new network investment. While the\ninfrastructure requirements to support the introduction of new fuels and\ntechnologies in industry, such as hydrogen and carbon capture, utilisation and\nstorage are often discussed, the need for investment to increase the capacity\nof the electricity network to meet increasing industrial electricity demands is\noften overlooked in the literature. This paper addresses this gap by\nquantifying the requirements for additional electricity network capacity to\nsupport the decarbonisation of industrial sectors across Great Britain (GB).\nThe Net Zero Industrial Pathways model is used to predict the future\nelectricity demand from industrial sites to 2050 which is then compared\nspatially to the available headroom across the distribution network in GB. The\nresults show that network headroom is sufficient to meet extra capacity demands\nfrom industrial sites over the period to 2030 in nearly all GB regions and\nnetwork scenarios. However, as electricity demand rises due to increased\nelectrification across all sectors and industrial decarbonisation accelerates\ntowards 2050, the network will need significant new capacity (71 GW + by 2050)\nparticularly in the central, south, and north-west regions of England, and\nWales. Without solving these network constraints, around 65% of industrial\nsites that are large point sources of emissions would be constrained in terms\nof electric capacity by 2040. These sites are responsible for 69% of industrial\npoint source emissions."
                },
                "authors": [
                    {
                        "name": "Ahmed Gailani"
                    },
                    {
                        "name": "Peter Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Peter Taylor"
                },
                "author": "Peter Taylor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05062v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05062v2",
                "updated": "2024-11-26T12:39:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    39,
                    36,
                    1,
                    331,
                    0
                ],
                "published": "2024-10-07T14:21:17Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    21,
                    17,
                    0,
                    281,
                    0
                ],
                "title": "Large Language Model Based Multi-Objective Optimization for Integrated\n  Sensing and Communications in UAV Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Based Multi-Objective Optimization for Integrated\n  Sensing and Communications in UAV Networks"
                },
                "summary": "This letter investigates an unmanned aerial vehicle (UAV) network with\nintegrated sensing and communication (ISAC) systems, where multiple UAVs\nsimultaneously sense the locations of ground users and provide communication\nservices with radars. To find the trade-off between communication and sensing\n(C\\&S) in the system, we formulate a multi-objective optimization problem (MOP)\nto maximize the total network utility and the localization Cram\\'er-Rao bounds\n(CRB) of ground users, which jointly optimizes the deployment and power control\nof UAVs. Inspired by the huge potential of large language models (LLM) for\nprediction and inference, we propose an LLM-enabled decomposition-based\nmulti-objective evolutionary algorithm (LEDMA) for solving the highly\nnon-convex MOP. We first adopt a decomposition-based scheme to decompose the\nMOP into a series of optimization sub-problems. We second integrate LLMs as\nblack-box search operators with MOP-specifically designed prompt engineering\ninto the framework of MOEA to solve optimization sub-problems simultaneously.\nNumerical results demonstrate that the proposed LEDMA can find the clear\ntrade-off between C\\&S and outperforms baseline MOEAs in terms of obtained\nPareto fronts and convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This letter investigates an unmanned aerial vehicle (UAV) network with\nintegrated sensing and communication (ISAC) systems, where multiple UAVs\nsimultaneously sense the locations of ground users and provide communication\nservices with radars. To find the trade-off between communication and sensing\n(C\\&S) in the system, we formulate a multi-objective optimization problem (MOP)\nto maximize the total network utility and the localization Cram\\'er-Rao bounds\n(CRB) of ground users, which jointly optimizes the deployment and power control\nof UAVs. Inspired by the huge potential of large language models (LLM) for\nprediction and inference, we propose an LLM-enabled decomposition-based\nmulti-objective evolutionary algorithm (LEDMA) for solving the highly\nnon-convex MOP. We first adopt a decomposition-based scheme to decompose the\nMOP into a series of optimization sub-problems. We second integrate LLMs as\nblack-box search operators with MOP-specifically designed prompt engineering\ninto the framework of MOEA to solve optimization sub-problems simultaneously.\nNumerical results demonstrate that the proposed LEDMA can find the clear\ntrade-off between C\\&S and outperforms baseline MOEAs in terms of obtained\nPareto fronts and convergence."
                },
                "authors": [
                    {
                        "name": "Haoyun Li"
                    },
                    {
                        "name": "Ming Xiao"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Dong In Kim"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05062v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13256v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13256v3",
                "updated": "2024-11-26T12:37:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    37,
                    39,
                    1,
                    331,
                    0
                ],
                "published": "2024-01-24T06:50:20Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    6,
                    50,
                    20,
                    2,
                    24,
                    0
                ],
                "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for\n  Personalized Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for\n  Personalized Dialogue Systems"
                },
                "summary": "Large Language Models (LLMs) has shown exceptional capabilities in many\nnatual language understanding and generation tasks. However, the\npersonalization issue still remains a much-coveted property, especially when it\ncomes to the multiple sources involved in the dialogue system. To better plan\nand incorporate the use of multiple sources in generating personalized\nresponse, we firstly decompose it into three sub-tasks: Knowledge Source\nSelection, Knowledge Retrieval, and Response Generation. We then propose a\nnovel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)\nSpecifically, we unify these three sub-tasks with different formulations into\nthe same sequence-to-sequence paradigm during the training, to adaptively\nretrieve evidences and evaluate the relevance on-demand using special tokens,\ncalled acting tokens and evaluation tokens. Enabling language models to\ngenerate acting tokens facilitates interaction with various knowledge sources,\nallowing them to adapt their behavior to diverse task requirements. Meanwhile,\nevaluation tokens gauge the relevance score between the dialogue context and\nthe retrieved evidence. In addition, we carefully design a self-refinement\nmechanism to iteratively refine the generated response considering 1) the\nconsistency scores between the generated response and retrieved evidence; and\n2) the relevance scores. Experiments on two personalized datasets (DuLeMon and\nKBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge\nsource selection and response generation task with itself as a retriever in a\nunified manner. Extensive analyses and discussions are provided for shedding\nsome new perspectives for personalized dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) has shown exceptional capabilities in many\nnatual language understanding and generation tasks. However, the\npersonalization issue still remains a much-coveted property, especially when it\ncomes to the multiple sources involved in the dialogue system. To better plan\nand incorporate the use of multiple sources in generating personalized\nresponse, we firstly decompose it into three sub-tasks: Knowledge Source\nSelection, Knowledge Retrieval, and Response Generation. We then propose a\nnovel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)\nSpecifically, we unify these three sub-tasks with different formulations into\nthe same sequence-to-sequence paradigm during the training, to adaptively\nretrieve evidences and evaluate the relevance on-demand using special tokens,\ncalled acting tokens and evaluation tokens. Enabling language models to\ngenerate acting tokens facilitates interaction with various knowledge sources,\nallowing them to adapt their behavior to diverse task requirements. Meanwhile,\nevaluation tokens gauge the relevance score between the dialogue context and\nthe retrieved evidence. In addition, we carefully design a self-refinement\nmechanism to iteratively refine the generated response considering 1) the\nconsistency scores between the generated response and retrieved evidence; and\n2) the relevance scores. Experiments on two personalized datasets (DuLeMon and\nKBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge\nsource selection and response generation task with itself as a retriever in a\nunified manner. Extensive analyses and discussions are provided for shedding\nsome new perspectives for personalized dialogue systems."
                },
                "authors": [
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Wenyu Huang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zezhong Wang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Fei Mi"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.13256v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13256v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17375v1",
                "updated": "2024-11-26T12:34:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    34,
                    52,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T12:34:52Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    34,
                    52,
                    1,
                    331,
                    0
                ],
                "title": "The Extractive-Abstractive Spectrum: Uncovering Verifiability Trade-offs\n  in LLM Generations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Extractive-Abstractive Spectrum: Uncovering Verifiability Trade-offs\n  in LLM Generations"
                },
                "summary": "Across all fields of academic study, experts cite their sources when sharing\ninformation. While large language models (LLMs) excel at synthesizing\ninformation, they do not provide reliable citation to sources, making it\ndifficult to trace and verify the origins of the information they present. In\ncontrast, search engines make sources readily accessible to users and place the\nburden of synthesizing information on the user. Through a survey, we find that\nusers prefer search engines over LLMs for high-stakes queries, where concerns\nregarding information provenance outweigh the perceived utility of LLM\nresponses. To examine the interplay between verifiability and utility of\ninformation-sharing tools, we introduce the extractive-abstractive spectrum, in\nwhich search engines and LLMs are extreme endpoints encapsulating multiple\nunexplored intermediate operating points. Search engines are extractive because\nthey respond to queries with snippets of sources with links (citations) to the\noriginal webpages. LLMs are abstractive because they address queries with\nanswers that synthesize and logically transform relevant information from\ntraining and in-context sources without reliable citation. We define five\noperating points that span the extractive-abstractive spectrum and conduct\nhuman evaluations on seven systems across four diverse query distributions that\nreflect real-world QA settings: web search, language simplification, multi-step\nreasoning, and medical advice. As outputs become more abstractive, we find that\nperceived utility improves by as much as 200%, while the proportion of properly\ncited sentences decreases by as much as 50% and users take up to 3 times as\nlong to verify cited information. Our findings recommend distinct operating\npoints for domain-specific LLM systems and our failure analysis informs\napproaches to high-utility LLM systems that empower users to verify\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across all fields of academic study, experts cite their sources when sharing\ninformation. While large language models (LLMs) excel at synthesizing\ninformation, they do not provide reliable citation to sources, making it\ndifficult to trace and verify the origins of the information they present. In\ncontrast, search engines make sources readily accessible to users and place the\nburden of synthesizing information on the user. Through a survey, we find that\nusers prefer search engines over LLMs for high-stakes queries, where concerns\nregarding information provenance outweigh the perceived utility of LLM\nresponses. To examine the interplay between verifiability and utility of\ninformation-sharing tools, we introduce the extractive-abstractive spectrum, in\nwhich search engines and LLMs are extreme endpoints encapsulating multiple\nunexplored intermediate operating points. Search engines are extractive because\nthey respond to queries with snippets of sources with links (citations) to the\noriginal webpages. LLMs are abstractive because they address queries with\nanswers that synthesize and logically transform relevant information from\ntraining and in-context sources without reliable citation. We define five\noperating points that span the extractive-abstractive spectrum and conduct\nhuman evaluations on seven systems across four diverse query distributions that\nreflect real-world QA settings: web search, language simplification, multi-step\nreasoning, and medical advice. As outputs become more abstractive, we find that\nperceived utility improves by as much as 200%, while the proportion of properly\ncited sentences decreases by as much as 50% and users take up to 3 times as\nlong to verify cited information. Our findings recommend distinct operating\npoints for domain-specific LLM systems and our failure analysis informs\napproaches to high-utility LLM systems that empower users to verify\ninformation."
                },
                "authors": [
                    {
                        "name": "Theodora Worledge"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    },
                    {
                        "name": "Carlos Guestrin"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Guestrin"
                },
                "author": "Carlos Guestrin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17367v1",
                "updated": "2024-11-26T12:20:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    20,
                    18,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T12:20:18Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    20,
                    18,
                    1,
                    331,
                    0
                ],
                "title": "Efficient Deployment of Transformer Models in Analog In-Memory Computing\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Deployment of Transformer Models in Analog In-Memory Computing\n  Hardware"
                },
                "summary": "Analog in-memory computing (AIMC) has emerged as a promising solution to\novercome the von Neumann bottleneck, accelerating neural network computations\nand improving computational efficiency. While AIMC has demonstrated success\nwith architectures such as CNNs, MLPs, and RNNs, deploying transformer-based\nmodels using AIMC presents unique challenges. Transformers are expected to\nhandle diverse downstream tasks and adapt to new user data or instructions\nafter deployment, which requires more flexible approaches to suit AIMC\nconstraints.\n  In this paper, we propose a novel method for deploying pre-trained\ntransformer models onto AIMC hardware. Unlike traditional approaches requiring\nhardware-aware training, our technique allows direct deployment without the\nneed for retraining the original model. Instead, we utilize lightweight,\nlow-rank adapters -- compact modules stored in digital cores -- to adapt the\nmodel to hardware constraints. We validate our approach on MobileBERT,\ndemonstrating accuracy on par with, or even exceeding, a traditional\nhardware-aware training approach. Our method is particularly appealing in\nmulti-task scenarios, as it enables a single analog model to be reused across\nmultiple tasks. Moreover, it supports on-chip adaptation to new hardware\nconstraints and tasks without updating analog weights, providing a flexible and\nversatile solution for real-world AI applications. Code is available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog in-memory computing (AIMC) has emerged as a promising solution to\novercome the von Neumann bottleneck, accelerating neural network computations\nand improving computational efficiency. While AIMC has demonstrated success\nwith architectures such as CNNs, MLPs, and RNNs, deploying transformer-based\nmodels using AIMC presents unique challenges. Transformers are expected to\nhandle diverse downstream tasks and adapt to new user data or instructions\nafter deployment, which requires more flexible approaches to suit AIMC\nconstraints.\n  In this paper, we propose a novel method for deploying pre-trained\ntransformer models onto AIMC hardware. Unlike traditional approaches requiring\nhardware-aware training, our technique allows direct deployment without the\nneed for retraining the original model. Instead, we utilize lightweight,\nlow-rank adapters -- compact modules stored in digital cores -- to adapt the\nmodel to hardware constraints. We validate our approach on MobileBERT,\ndemonstrating accuracy on par with, or even exceeding, a traditional\nhardware-aware training approach. Our method is particularly appealing in\nmulti-task scenarios, as it enables a single analog model to be reused across\nmultiple tasks. Moreover, it supports on-chip adaptation to new hardware\nconstraints and tasks without updating analog weights, providing a flexible and\nversatile solution for real-world AI applications. Code is available."
                },
                "authors": [
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Corey Lammie"
                    },
                    {
                        "name": "Manuel Le Gallo"
                    },
                    {
                        "name": "Bipin Rajendran"
                    }
                ],
                "author_detail": {
                    "name": "Bipin Rajendran"
                },
                "author": "Bipin Rajendran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15193v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15193v5",
                "updated": "2024-11-26T12:13:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    13,
                    21,
                    1,
                    331,
                    0
                ],
                "published": "2024-06-21T14:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    14,
                    35,
                    16,
                    4,
                    173,
                    0
                ],
                "title": "Inference Time Alignment with Reward-Guided Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference Time Alignment with Reward-Guided Tree Search"
                },
                "summary": "Inference-time computation methods enhance the performance of Large Language\nModels (LLMs) by leveraging additional computational resources to achieve\nsuperior results. Common techniques, such as Best-of-N sampling, Majority\nVoting, and variants of tree-search algorithms have proven to be effective in\nboosting the performance of LLMs. These approaches strategically trade\nincreased computational resources for improved model responses. In this work,\nwe proposed DARWIN, an inference-time alignment method that leverages the\nguidance of a reward model to achieve alignment through a reward-guided tree\nsearch. Empirical evidences indicates that our method outperforms other\ninference-time alignment methods such as Best-of-N and ARGS on two widely\naccepted alignment benchmarks AlpacaEval 2 and MT-Bench. Furthermore, we show\nthat our inference-time approach achieves performance comparable to\npreference-tuned models on both benchmarks, highlighting the effectiveness of\ntrading inference-time compute for enhanced performance during inference. We\nhave released our codes at https://github.com/declare-lab/darwin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time computation methods enhance the performance of Large Language\nModels (LLMs) by leveraging additional computational resources to achieve\nsuperior results. Common techniques, such as Best-of-N sampling, Majority\nVoting, and variants of tree-search algorithms have proven to be effective in\nboosting the performance of LLMs. These approaches strategically trade\nincreased computational resources for improved model responses. In this work,\nwe proposed DARWIN, an inference-time alignment method that leverages the\nguidance of a reward model to achieve alignment through a reward-guided tree\nsearch. Empirical evidences indicates that our method outperforms other\ninference-time alignment methods such as Best-of-N and ARGS on two widely\naccepted alignment benchmarks AlpacaEval 2 and MT-Bench. Furthermore, we show\nthat our inference-time approach achieves performance comparable to\npreference-tuned models on both benchmarks, highlighting the effectiveness of\ntrading inference-time compute for enhanced performance during inference. We\nhave released our codes at https://github.com/declare-lab/darwin."
                },
                "authors": [
                    {
                        "name": "Chia-Yu Hung"
                    },
                    {
                        "name": "Navonil Majumder"
                    },
                    {
                        "name": "Ambuj Mehrish"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15193v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15193v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00629v2",
                "updated": "2024-11-26T11:59:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    11,
                    59,
                    17,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-31T09:50:39Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    9,
                    50,
                    39,
                    6,
                    91,
                    0
                ],
                "title": "Against The Achilles' Heel: A Survey on Red Teaming for Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Against The Achilles' Heel: A Survey on Red Teaming for Generative\n  Models"
                },
                "summary": "Generative models are rapidly gaining popularity and being integrated into\neveryday applications, raising concerns over their safe use as various\nvulnerabilities are exposed. In light of this, the field of red teaming is\nundergoing fast-paced growth, highlighting the need for a comprehensive survey\ncovering the entire pipeline and addressing emerging topics. Our extensive\nsurvey, which examines over 120 papers, introduces a taxonomy of fine-grained\nattack strategies grounded in the inherent capabilities of language models.\nAdditionally, we have developed the \"searcher\" framework to unify various\nautomatic red teaming approaches. Moreover, our survey covers novel areas\nincluding multimodal attacks and defenses, risks around LLM-based agents,\noverkill of harmless queries, and the balance between harmlessness and\nhelpfulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models are rapidly gaining popularity and being integrated into\neveryday applications, raising concerns over their safe use as various\nvulnerabilities are exposed. In light of this, the field of red teaming is\nundergoing fast-paced growth, highlighting the need for a comprehensive survey\ncovering the entire pipeline and addressing emerging topics. Our extensive\nsurvey, which examines over 120 papers, introduces a taxonomy of fine-grained\nattack strategies grounded in the inherent capabilities of language models.\nAdditionally, we have developed the \"searcher\" framework to unify various\nautomatic red teaming approaches. Moreover, our survey covers novel areas\nincluding multimodal attacks and defenses, risks around LLM-based agents,\noverkill of harmless queries, and the balance between harmlessness and\nhelpfulness."
                },
                "authors": [
                    {
                        "name": "Lizhi Lin"
                    },
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Zenan Zhai"
                    },
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Junjie Gao"
                    },
                    {
                        "name": "Yixuan Zhang"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Haonan Li"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Li"
                },
                "author": "Haonan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.11914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.11914v3",
                "updated": "2024-11-26T11:39:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    11,
                    39,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2023-08-23T04:59:21Z",
                "published_parsed": [
                    2023,
                    8,
                    23,
                    4,
                    59,
                    21,
                    2,
                    235,
                    0
                ],
                "title": "Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge\n  Reasoning via Promoting Causal Consistency in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge\n  Reasoning via Promoting Causal Consistency in LLMs"
                },
                "summary": "Despite the progress of foundation models, knowledge-based reasoning remains\na persistent challenge due to their limited capacity for knowledge recall and\ninference. Existing methods primarily focus on encouraging these models to plan\nand solve problems or extensively sample reasoning chains independently.\nHowever, these methods often overlook conceptual errors and inferential\nfallacies, inevitably leading to a series of notorious issues such as\nmisleading conclusions, cognitive biases, and reduced decision quality. While\nexplicit modeling of causality is argued to hold promise in addressing these\nissues, contemporary research efforts have thus far fallen short in achieving\ncausality-based foundation models. Drawing inspiration from the orchestration\nof diverse specialized agents collaborating to tackle intricate tasks, we\npropose a framework named Causal-Consistency Chain-of-Thought (CaCo-CoT) that\nharnesses multi-agent collaboration to bolster the faithfulness and causality\nof foundation models, involving a set of reasoners and evaluators. These agents\ncollaboratively work within a reasoning-and-consensus paradigm to improve\nfaithfulness. The reasoners are tasked with generating reasoning chains for\nknowledge-intensive problems by mimicking human causal reasoning. Meanwhile,\nthe evaluator scrutinizes the causal consistency of a reasoner's reasoning\nchain from a non-causal and a counterfactual perspective. Our framework\ndemonstrates significant superiority over state-of-the-art methods through\nextensive and comprehensive evaluations across text-based and multi-modal\nknowledge reasoning tasks (e.g., science question answering and commonsense\nreasoning).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the progress of foundation models, knowledge-based reasoning remains\na persistent challenge due to their limited capacity for knowledge recall and\ninference. Existing methods primarily focus on encouraging these models to plan\nand solve problems or extensively sample reasoning chains independently.\nHowever, these methods often overlook conceptual errors and inferential\nfallacies, inevitably leading to a series of notorious issues such as\nmisleading conclusions, cognitive biases, and reduced decision quality. While\nexplicit modeling of causality is argued to hold promise in addressing these\nissues, contemporary research efforts have thus far fallen short in achieving\ncausality-based foundation models. Drawing inspiration from the orchestration\nof diverse specialized agents collaborating to tackle intricate tasks, we\npropose a framework named Causal-Consistency Chain-of-Thought (CaCo-CoT) that\nharnesses multi-agent collaboration to bolster the faithfulness and causality\nof foundation models, involving a set of reasoners and evaluators. These agents\ncollaboratively work within a reasoning-and-consensus paradigm to improve\nfaithfulness. The reasoners are tasked with generating reasoning chains for\nknowledge-intensive problems by mimicking human causal reasoning. Meanwhile,\nthe evaluator scrutinizes the causal consistency of a reasoner's reasoning\nchain from a non-causal and a counterfactual perspective. Our framework\ndemonstrates significant superiority over state-of-the-art methods through\nextensive and comprehensive evaluations across text-based and multi-modal\nknowledge reasoning tasks (e.g., science question answering and commonsense\nreasoning)."
                },
                "authors": [
                    {
                        "name": "Ziyi Tang"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Weixing Chen"
                    },
                    {
                        "name": "Keze Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liang Lin"
                },
                "author": "Liang Lin",
                "arxiv_comment": "8 pages, 3 figures. 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.11914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.11914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17338v1",
                "updated": "2024-11-26T11:32:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    11,
                    32,
                    43,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T11:32:43Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    11,
                    32,
                    43,
                    1,
                    331,
                    0
                ],
                "title": "Different Bias Under Different Criteria: Assessing Bias in LLMs with a\n  Fact-Based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Different Bias Under Different Criteria: Assessing Bias in LLMs with a\n  Fact-Based Approach"
                },
                "summary": "Large language models (LLMs) often reflect real-world biases, leading to\nefforts to mitigate these effects and make the models unbiased. Achieving this\ngoal requires defining clear criteria for an unbiased state, with any deviation\nfrom these criteria considered biased. Some studies define an unbiased state as\nequal treatment across diverse demographic groups, aiming for balanced outputs\nfrom LLMs. However, differing perspectives on equality and the importance of\npluralism make it challenging to establish a universal standard. Alternatively,\nother approaches propose using fact-based criteria for more consistent and\nobjective evaluations, though these methods have not yet been fully applied to\nLLM bias assessments. Thus, there is a need for a metric with objective\ncriteria that offers a distinct perspective from equality-based approaches.\nMotivated by this need, we introduce a novel metric to assess bias using\nfact-based criteria and real-world statistics. In this paper, we conducted a\nhuman survey demonstrating that humans tend to perceive LLM outputs more\npositively when they align closely with real-world demographic distributions.\nEvaluating various LLMs with our proposed metric reveals that model bias varies\ndepending on the criteria used, highlighting the need for multi-perspective\nassessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often reflect real-world biases, leading to\nefforts to mitigate these effects and make the models unbiased. Achieving this\ngoal requires defining clear criteria for an unbiased state, with any deviation\nfrom these criteria considered biased. Some studies define an unbiased state as\nequal treatment across diverse demographic groups, aiming for balanced outputs\nfrom LLMs. However, differing perspectives on equality and the importance of\npluralism make it challenging to establish a universal standard. Alternatively,\nother approaches propose using fact-based criteria for more consistent and\nobjective evaluations, though these methods have not yet been fully applied to\nLLM bias assessments. Thus, there is a need for a metric with objective\ncriteria that offers a distinct perspective from equality-based approaches.\nMotivated by this need, we introduce a novel metric to assess bias using\nfact-based criteria and real-world statistics. In this paper, we conducted a\nhuman survey demonstrating that humans tend to perceive LLM outputs more\npositively when they align closely with real-world demographic distributions.\nEvaluating various LLMs with our proposed metric reveals that model bias varies\ndepending on the criteria used, highlighting the need for multi-perspective\nassessment."
                },
                "authors": [
                    {
                        "name": "Changgeon Ko"
                    },
                    {
                        "name": "Jisu Shin"
                    },
                    {
                        "name": "Hoyun Song"
                    },
                    {
                        "name": "Jeongyeon Seo"
                    },
                    {
                        "name": "Jong C. Park"
                    }
                ],
                "author_detail": {
                    "name": "Jong C. Park"
                },
                "author": "Jong C. Park",
                "arxiv_comment": "Accepted in NeurIPS 2024 Workshop on Socially Responsible Language\n  Modelling Research (SoLaR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17309v1",
                "updated": "2024-11-26T10:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    54,
                    19,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T10:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    54,
                    19,
                    1,
                    331,
                    0
                ],
                "title": "PIM-AI: A Novel Architecture for High-Efficiency LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM-AI: A Novel Architecture for High-Efficiency LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have become essential in a variety of\napplications due to their advanced language understanding and generation\ncapabilities. However, their computational and memory requirements pose\nsignificant challenges to traditional hardware architectures.\nProcessing-in-Memory (PIM), which integrates computational units directly into\nmemory chips, offers several advantages for LLM inference, including reduced\ndata transfer bottlenecks and improved power efficiency.\n  This paper introduces PIM-AI, a novel DDR5/LPDDR5 PIM architecture designed\nfor LLM inference without modifying the memory controller or DDR/LPDDR memory\nPHY. We have developed a simulator to evaluate the performance of PIM-AI in\nvarious scenarios and demonstrate its significant advantages over conventional\narchitectures. In cloud-based scenarios, PIM-AI reduces the 3-year TCO per\nqueries-per-second by up to 6.94x compared to state-of-the-art GPUs, depending\non the LLM model used. In mobile scenarios, PIM-AI achieves a 10- to 20-fold\nreduction in energy per token compared to state-of-the-art mobile SoCs,\nresulting in 25 to 45~\\% more queries per second and 6.9x to 13.4x less energy\nper query, extending battery life and enabling more inferences per charge.\n  These results highlight PIM-AI's potential to revolutionize LLM deployments,\nmaking them more efficient, scalable, and sustainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become essential in a variety of\napplications due to their advanced language understanding and generation\ncapabilities. However, their computational and memory requirements pose\nsignificant challenges to traditional hardware architectures.\nProcessing-in-Memory (PIM), which integrates computational units directly into\nmemory chips, offers several advantages for LLM inference, including reduced\ndata transfer bottlenecks and improved power efficiency.\n  This paper introduces PIM-AI, a novel DDR5/LPDDR5 PIM architecture designed\nfor LLM inference without modifying the memory controller or DDR/LPDDR memory\nPHY. We have developed a simulator to evaluate the performance of PIM-AI in\nvarious scenarios and demonstrate its significant advantages over conventional\narchitectures. In cloud-based scenarios, PIM-AI reduces the 3-year TCO per\nqueries-per-second by up to 6.94x compared to state-of-the-art GPUs, depending\non the LLM model used. In mobile scenarios, PIM-AI achieves a 10- to 20-fold\nreduction in energy per token compared to state-of-the-art mobile SoCs,\nresulting in 25 to 45~\\% more queries per second and 6.9x to 13.4x less energy\nper query, extending battery life and enabling more inferences per charge.\n  These results highlight PIM-AI's potential to revolutionize LLM deployments,\nmaking them more efficient, scalable, and sustainable."
                },
                "authors": [
                    {
                        "name": "Cristobal Ortega"
                    },
                    {
                        "name": "Yann Falevoz"
                    },
                    {
                        "name": "Renaud Ayrignac"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Ayrignac"
                },
                "author": "Renaud Ayrignac",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17304v1",
                "updated": "2024-11-26T10:52:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    52,
                    8,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T10:52:08Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    52,
                    8,
                    1,
                    331,
                    0
                ],
                "title": "Meaningless is better: hashing bias-inducing words in LLM prompts\n  improves performance in logical reasoning and statistical learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meaningless is better: hashing bias-inducing words in LLM prompts\n  improves performance in logical reasoning and statistical learning"
                },
                "summary": "This paper introduces a novel method, referred to as \"hashing\", which\ninvolves masking potentially bias-inducing words in large language models\n(LLMs) with hash-like meaningless identifiers to reduce cognitive biases and\nreliance on external knowledge. The method was tested across three sets of\nexperiments involving a total of 490 prompts. Statistical analysis using\nchi-square tests showed significant improvements in all tested scenarios, which\ncovered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first\nexperiment, hashing decreased the fallacy rate in a modified version of the\n\"Linda\" problem aimed at evaluating susceptibility to cognitive biases. In the\nsecond experiment, it improved LLM results on the frequent itemset extraction\ntask. In the third experiment, we found hashing is also effective when the\nLinda problem is presented in a tabular format rather than text, indicating\nthat the technique works across various input representations. Overall, the\nmethod was shown to improve bias reduction and incorporation of external\nknowledge. Despite bias reduction, hallucination rates were inconsistently\nreduced across types of LLM models. These findings suggest that masking\nbias-inducing terms can improve LLM performance, although its effectiveness is\nmodel- and task-dependent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel method, referred to as \"hashing\", which\ninvolves masking potentially bias-inducing words in large language models\n(LLMs) with hash-like meaningless identifiers to reduce cognitive biases and\nreliance on external knowledge. The method was tested across three sets of\nexperiments involving a total of 490 prompts. Statistical analysis using\nchi-square tests showed significant improvements in all tested scenarios, which\ncovered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first\nexperiment, hashing decreased the fallacy rate in a modified version of the\n\"Linda\" problem aimed at evaluating susceptibility to cognitive biases. In the\nsecond experiment, it improved LLM results on the frequent itemset extraction\ntask. In the third experiment, we found hashing is also effective when the\nLinda problem is presented in a tabular format rather than text, indicating\nthat the technique works across various input representations. Overall, the\nmethod was shown to improve bias reduction and incorporation of external\nknowledge. Despite bias reduction, hallucination rates were inconsistently\nreduced across types of LLM models. These findings suggest that masking\nbias-inducing terms can improve LLM performance, although its effectiveness is\nmodel- and task-dependent."
                },
                "authors": [
                    {
                        "name": "Milena Chadimová"
                    },
                    {
                        "name": "Eduard Jurášek"
                    },
                    {
                        "name": "Tomáš Kliegr"
                    }
                ],
                "author_detail": {
                    "name": "Tomáš Kliegr"
                },
                "author": "Tomáš Kliegr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17301v1",
                "updated": "2024-11-26T10:48:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    48,
                    55,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T10:48:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    48,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "ER2Score: LLM-based Explainable and Customizable Metric for Assessing\n  Radiology Reports with Reward-Control Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ER2Score: LLM-based Explainable and Customizable Metric for Assessing\n  Radiology Reports with Reward-Control Loss"
                },
                "summary": "Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ER2Score, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ER2Score. Our experiments demonstrate ER2Score's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ER2Score, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ER2Score. Our experiments demonstrate ER2Score's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems."
                },
                "authors": [
                    {
                        "name": "Yunyi Liu"
                    },
                    {
                        "name": "Yingshu Li"
                    },
                    {
                        "name": "Zhanyu Wang"
                    },
                    {
                        "name": "Xinyu Liang"
                    },
                    {
                        "name": "Lingqiao Liu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Luping Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Luping Zhou"
                },
                "author": "Luping Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17284v1",
                "updated": "2024-11-26T10:13:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    13,
                    39,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T10:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    10,
                    13,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for Expert Prior Elicitation in Predictive\n  Modelling"
                },
                "summary": "Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes using LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. We compare LLM-elicited and uninformative priors,\nevaluate whether LLMs truthfully generate parameter distributions, and propose\na model selection strategy for in-context learning and prior elicitation. Our\nfindings show that LLM-elicited prior parameter distributions significantly\nreduce predictive error compared to uninformative priors in low-data settings.\nApplied to clinical problems, this translates to fewer required biological\nsamples, lowering cost and resources. Prior elicitation also consistently\noutperforms and proves more reliable than in-context learning at a lower cost,\nmaking it a preferred alternative in our setting. We demonstrate the utility of\nthis method across various use cases, including clinical applications. For\ninfection prediction, using LLM-elicited priors reduced the number of required\nlabels to achieve the same accuracy as an uninformative prior by 55%, at 200\ndays earlier in the study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), trained on diverse data effectively acquire a\nbreadth of information across various domains. However, their computational\ncomplexity, cost, and lack of transparency hinder their direct application for\nspecialised tasks. In fields such as clinical research, acquiring expert\nannotations or prior knowledge about predictive models is often costly and\ntime-consuming. This study proposes using LLMs to elicit expert prior\ndistributions for predictive models. This approach also provides an alternative\nto in-context learning, where language models are tasked with making\npredictions directly. We compare LLM-elicited and uninformative priors,\nevaluate whether LLMs truthfully generate parameter distributions, and propose\na model selection strategy for in-context learning and prior elicitation. Our\nfindings show that LLM-elicited prior parameter distributions significantly\nreduce predictive error compared to uninformative priors in low-data settings.\nApplied to clinical problems, this translates to fewer required biological\nsamples, lowering cost and resources. Prior elicitation also consistently\noutperforms and proves more reliable than in-context learning at a lower cost,\nmaking it a preferred alternative in our setting. We demonstrate the utility of\nthis method across various use cases, including clinical applications. For\ninfection prediction, using LLM-elicited priors reduced the number of required\nlabels to achieve the same accuracy as an uninformative prior by 55%, at 200\ndays earlier in the study."
                },
                "authors": [
                    {
                        "name": "Alexander Capstick"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    },
                    {
                        "name": "Payam Barnaghi"
                    }
                ],
                "author_detail": {
                    "name": "Payam Barnaghi"
                },
                "author": "Payam Barnaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17274v1",
                "updated": "2024-11-26T09:51:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    51,
                    55,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T09:51:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    51,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics"
                },
                "summary": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 11,632\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 11,632\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul."
                },
                "authors": [
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Yan Naing Tun"
                    },
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Tan Bui"
                    },
                    {
                        "name": "Ivana Clairine Irsan"
                    },
                    {
                        "name": "Yiran Cheng"
                    },
                    {
                        "name": "Xiang Lan"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13476v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13476v2",
                "updated": "2024-11-26T09:46:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    46,
                    25,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-20T17:22:31Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    22,
                    31,
                    2,
                    325,
                    0
                ],
                "title": "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context\n  Training"
                },
                "summary": "Extending context window sizes allows large language models (LLMs) to process\nlonger sequences and handle more complex tasks. Rotary Positional Embedding\n(RoPE) has become the de facto standard due to its relative positional encoding\nproperties that benefit long-context training. However, we observe that using\nRoPE with BFloat16 format results in numerical issues, causing it to deviate\nfrom its intended relative positional encoding, especially in long-context\nscenarios. This issue arises from BFloat16's limited precision and accumulates\nas context length increases, with the first token contributing significantly to\nthis problem. To address this, we develop AnchorAttention, a plug-and-play\nattention method that alleviates numerical issues caused by BFloat16, improves\nlong-context capabilities, and speeds up training. AnchorAttention reduces\nunnecessary attention computations, maintains semantic coherence, and boosts\ncomputational efficiency by treating the first token as a shared anchor with a\nconsistent position ID, making it visible to all documents within the training\ncontext. Experiments on three types of LLMs demonstrate that AnchorAttention\nsignificantly improves long-context performance and reduces training time by\nover 50\\% compared to standard full attention mechanisms, while preserving the\noriginal LLM's capabilities on general tasks. Our code is available at\nhttps://github.com/haonan3/AnchorContext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending context window sizes allows large language models (LLMs) to process\nlonger sequences and handle more complex tasks. Rotary Positional Embedding\n(RoPE) has become the de facto standard due to its relative positional encoding\nproperties that benefit long-context training. However, we observe that using\nRoPE with BFloat16 format results in numerical issues, causing it to deviate\nfrom its intended relative positional encoding, especially in long-context\nscenarios. This issue arises from BFloat16's limited precision and accumulates\nas context length increases, with the first token contributing significantly to\nthis problem. To address this, we develop AnchorAttention, a plug-and-play\nattention method that alleviates numerical issues caused by BFloat16, improves\nlong-context capabilities, and speeds up training. AnchorAttention reduces\nunnecessary attention computations, maintains semantic coherence, and boosts\ncomputational efficiency by treating the first token as a shared anchor with a\nconsistent position ID, making it visible to all documents within the training\ncontext. Experiments on three types of LLMs demonstrate that AnchorAttention\nsignificantly improves long-context performance and reduces training time by\nover 50\\% compared to standard full attention mechanisms, while preserving the\noriginal LLM's capabilities on general tasks. Our code is available at\nhttps://github.com/haonan3/AnchorContext."
                },
                "authors": [
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Tianyu Pang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Pang"
                },
                "author": "Tianyu Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13476v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17261v1",
                "updated": "2024-11-26T09:37:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    37,
                    59,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T09:37:59Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    37,
                    59,
                    1,
                    331,
                    0
                ],
                "title": "HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility\n  Evaluator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility\n  Evaluator"
                },
                "summary": "AIGC images are prevalent across various fields, yet they frequently suffer\nfrom quality issues like artifacts and unnatural textures. Specialized models\naim to predict defect region heatmaps but face two primary challenges: (1) lack\nof explainability, failing to provide reasons and analyses for subtle defects,\nand (2) inability to leverage common sense and logical reasoning, leading to\npoor generalization. Multimodal large language models (MLLMs) promise better\ncomprehension and reasoning but face their own challenges: (1) difficulty in\nfine-grained defect localization due to the limitations in capturing tiny\ndetails; and (2) constraints in providing pixel-wise outputs necessary for\nprecise heatmap generation. To address these challenges, we propose HEIE: a\nnovel MLLM-Based Hierarchical Explainable image Implausibility Evaluator. We\nintroduce the CoT-Driven Explainable Trinity Evaluator, which integrates\nheatmaps, scores, and explanation outputs, using CoT to decompose complex tasks\ninto subtasks of increasing difficulty and enhance interpretability. Our\nAdaptive Hierarchical Implausibility Mapper synergizes low-level image features\nwith high-level mapper tokens from LLMs, enabling precise local-to-global\nhierarchical heatmap predictions through an uncertainty-based adaptive token\napproach. Moreover, we propose a new dataset: Expl-AIGI-Eval, designed to\nfacilitate interpretable implausibility evaluation of AIGC images. Our method\ndemonstrates state-of-the-art performance through extensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIGC images are prevalent across various fields, yet they frequently suffer\nfrom quality issues like artifacts and unnatural textures. Specialized models\naim to predict defect region heatmaps but face two primary challenges: (1) lack\nof explainability, failing to provide reasons and analyses for subtle defects,\nand (2) inability to leverage common sense and logical reasoning, leading to\npoor generalization. Multimodal large language models (MLLMs) promise better\ncomprehension and reasoning but face their own challenges: (1) difficulty in\nfine-grained defect localization due to the limitations in capturing tiny\ndetails; and (2) constraints in providing pixel-wise outputs necessary for\nprecise heatmap generation. To address these challenges, we propose HEIE: a\nnovel MLLM-Based Hierarchical Explainable image Implausibility Evaluator. We\nintroduce the CoT-Driven Explainable Trinity Evaluator, which integrates\nheatmaps, scores, and explanation outputs, using CoT to decompose complex tasks\ninto subtasks of increasing difficulty and enhance interpretability. Our\nAdaptive Hierarchical Implausibility Mapper synergizes low-level image features\nwith high-level mapper tokens from LLMs, enabling precise local-to-global\nhierarchical heatmap predictions through an uncertainty-based adaptive token\napproach. Moreover, we propose a new dataset: Expl-AIGI-Eval, designed to\nfacilitate interpretable implausibility evaluation of AIGC images. Our method\ndemonstrates state-of-the-art performance through extensive experiments."
                },
                "authors": [
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Ru Zhen"
                    },
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haoxiang Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17255v1",
                "updated": "2024-11-26T09:31:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    31,
                    28,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T09:31:28Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    31,
                    28,
                    1,
                    331,
                    0
                ],
                "title": "APT: Architectural Planning and Text-to-Blueprint Construction Using\n  Large Language Models for Open-World Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APT: Architectural Planning and Text-to-Blueprint Construction Using\n  Large Language Models for Open-World Agents"
                },
                "summary": "We present APT, an advanced Large Language Model (LLM)-driven framework that\nenables autonomous agents to construct complex and creative structures within\nthe Minecraft environment. Unlike previous approaches that primarily\nconcentrate on skill-based open-world tasks or rely on image-based diffusion\nmodels for generating voxel-based structures, our method leverages the\nintrinsic spatial reasoning capabilities of LLMs. By employing chain-of-thought\ndecomposition along with multimodal inputs, the framework generates detailed\narchitectural layouts and blueprints that the agent can execute under zero-shot\nor few-shot learning scenarios. Our agent incorporates both memory and\nreflection modules to facilitate lifelong learning, adaptive refinement, and\nerror correction throughout the building process. To rigorously evaluate the\nagent's performance in this emerging research area, we introduce a\ncomprehensive benchmark consisting of diverse construction tasks designed to\ntest creativity, spatial reasoning, adherence to in-game rules, and the\neffective integration of multimodal instructions. Experimental results using\nvarious GPT-based LLM backends and agent configurations demonstrate the agent's\ncapacity to accurately interpret extensive instructions involving numerous\nitems, their positions, and orientations. The agent successfully produces\ncomplex structures complete with internal functionalities such as\nRedstone-powered systems. A/B testing indicates that the inclusion of a memory\nmodule leads to a significant increase in performance, emphasizing its role in\nenabling continuous learning and the reuse of accumulated experience.\nAdditionally, the agent's unexpected emergence of scaffolding behavior\nhighlights the potential of future LLM-driven agents to utilize subroutine\nplanning and leverage the emergence ability of LLMs to autonomously develop\nhuman-like problem-solving techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present APT, an advanced Large Language Model (LLM)-driven framework that\nenables autonomous agents to construct complex and creative structures within\nthe Minecraft environment. Unlike previous approaches that primarily\nconcentrate on skill-based open-world tasks or rely on image-based diffusion\nmodels for generating voxel-based structures, our method leverages the\nintrinsic spatial reasoning capabilities of LLMs. By employing chain-of-thought\ndecomposition along with multimodal inputs, the framework generates detailed\narchitectural layouts and blueprints that the agent can execute under zero-shot\nor few-shot learning scenarios. Our agent incorporates both memory and\nreflection modules to facilitate lifelong learning, adaptive refinement, and\nerror correction throughout the building process. To rigorously evaluate the\nagent's performance in this emerging research area, we introduce a\ncomprehensive benchmark consisting of diverse construction tasks designed to\ntest creativity, spatial reasoning, adherence to in-game rules, and the\neffective integration of multimodal instructions. Experimental results using\nvarious GPT-based LLM backends and agent configurations demonstrate the agent's\ncapacity to accurately interpret extensive instructions involving numerous\nitems, their positions, and orientations. The agent successfully produces\ncomplex structures complete with internal functionalities such as\nRedstone-powered systems. A/B testing indicates that the inclusion of a memory\nmodule leads to a significant increase in performance, emphasizing its role in\nenabling continuous learning and the reuse of accumulated experience.\nAdditionally, the agent's unexpected emergence of scaffolding behavior\nhighlights the potential of future LLM-driven agents to utilize subroutine\nplanning and leverage the emergence ability of LLMs to autonomously develop\nhuman-like problem-solving techniques."
                },
                "authors": [
                    {
                        "name": "Jun Yu Chen"
                    },
                    {
                        "name": "Tao Gao"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gao"
                },
                "author": "Tao Gao",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11211v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11211v4",
                "updated": "2024-11-26T09:28:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    28,
                    35,
                    1,
                    331,
                    0
                ],
                "published": "2024-07-15T19:53:02Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    19,
                    53,
                    2,
                    0,
                    197,
                    0
                ],
                "title": "Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer\n  from Text to Image via CLIP Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer\n  from Text to Image via CLIP Inversion"
                },
                "summary": "We introduce NOVIC, an innovative real-time uNconstrained Open Vocabulary\nImage Classifier that uses an autoregressive transformer to generatively output\nclassification labels as language. Leveraging the extensive knowledge of CLIP\nmodels, NOVIC harnesses the embedding space to enable zero-shot transfer from\npure text to images. Traditional CLIP models, despite their ability for open\nvocabulary classification, require an exhaustive prompt of potential class\nlabels, restricting their application to images of known content or context. To\naddress this, we propose an \"object decoder\" model that is trained on a\nlarge-scale 92M-target dataset of templated object noun sets and LLM-generated\ncaptions to always output the object noun in question. This effectively inverts\nthe CLIP text encoder and allows textual object labels from essentially the\nentire English language to be generated directly from image-derived embedding\nvectors, without requiring any a priori knowledge of the potential content of\nan image, and without any label biases. The trained decoders are tested on a\nmix of manually and web-curated datasets, as well as standard image\nclassification benchmarks, and achieve fine-grained prompt-free prediction\nscores of up to 87.5%, a strong result considering the model must work for any\nconceivable image and without any contextual clues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NOVIC, an innovative real-time uNconstrained Open Vocabulary\nImage Classifier that uses an autoregressive transformer to generatively output\nclassification labels as language. Leveraging the extensive knowledge of CLIP\nmodels, NOVIC harnesses the embedding space to enable zero-shot transfer from\npure text to images. Traditional CLIP models, despite their ability for open\nvocabulary classification, require an exhaustive prompt of potential class\nlabels, restricting their application to images of known content or context. To\naddress this, we propose an \"object decoder\" model that is trained on a\nlarge-scale 92M-target dataset of templated object noun sets and LLM-generated\ncaptions to always output the object noun in question. This effectively inverts\nthe CLIP text encoder and allows textual object labels from essentially the\nentire English language to be generated directly from image-derived embedding\nvectors, without requiring any a priori knowledge of the potential content of\nan image, and without any label biases. The trained decoders are tested on a\nmix of manually and web-curated datasets, as well as standard image\nclassification benchmarks, and achieve fine-grained prompt-free prediction\nscores of up to 87.5%, a strong result considering the model must work for any\nconceivable image and without any contextual clues."
                },
                "authors": [
                    {
                        "name": "Philipp Allgeuer"
                    },
                    {
                        "name": "Kyra Ahrens"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "Published at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11211v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11211v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15560v2",
                "updated": "2024-11-26T09:25:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    25,
                    22,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-23T13:34:50Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    13,
                    34,
                    50,
                    5,
                    328,
                    0
                ],
                "title": "Do LLMs Agree on the Creativity Evaluation of Alternative Uses?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Agree on the Creativity Evaluation of Alternative Uses?"
                },
                "summary": "This paper investigates whether large language models (LLMs) show agreement\nin assessing creativity in responses to the Alternative Uses Test (AUT). While\nLLMs are increasingly used to evaluate creative content, previous studies have\nprimarily focused on a single model assessing responses generated by the same\nmodel or humans. This paper explores whether LLMs can impartially and\naccurately evaluate creativity in outputs generated by both themselves and\nother models. Using an oracle benchmark set of AUT responses, categorized by\ncreativity level (common, creative, and highly creative), we experiment with\nfour state-of-the-art LLMs evaluating these outputs. We test both scoring and\nranking methods and employ two evaluation settings (comprehensive and\nsegmented) to examine if LLMs agree on the creativity evaluation of alternative\nuses. Results reveal high inter-model agreement, with Spearman correlations\naveraging above 0.7 across models and reaching over 0.77 with respect to the\noracle, indicating a high level of agreement and validating the reliability of\nLLMs in creativity assessment of alternative uses. Notably, models do not\nfavour their own responses, instead they provide similar creativity assessment\nscores or rankings for alternative uses generated by other models. These\nfindings suggest that LLMs exhibit impartiality and high alignment in\ncreativity evaluation, offering promising implications for their use in\nautomated creativity assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates whether large language models (LLMs) show agreement\nin assessing creativity in responses to the Alternative Uses Test (AUT). While\nLLMs are increasingly used to evaluate creative content, previous studies have\nprimarily focused on a single model assessing responses generated by the same\nmodel or humans. This paper explores whether LLMs can impartially and\naccurately evaluate creativity in outputs generated by both themselves and\nother models. Using an oracle benchmark set of AUT responses, categorized by\ncreativity level (common, creative, and highly creative), we experiment with\nfour state-of-the-art LLMs evaluating these outputs. We test both scoring and\nranking methods and employ two evaluation settings (comprehensive and\nsegmented) to examine if LLMs agree on the creativity evaluation of alternative\nuses. Results reveal high inter-model agreement, with Spearman correlations\naveraging above 0.7 across models and reaching over 0.77 with respect to the\noracle, indicating a high level of agreement and validating the reliability of\nLLMs in creativity assessment of alternative uses. Notably, models do not\nfavour their own responses, instead they provide similar creativity assessment\nscores or rankings for alternative uses generated by other models. These\nfindings suggest that LLMs exhibit impartiality and high alignment in\ncreativity evaluation, offering promising implications for their use in\nautomated creativity assessment."
                },
                "authors": [
                    {
                        "name": "Abdullah Al Rabeyah"
                    },
                    {
                        "name": "Fabrício Góes"
                    },
                    {
                        "name": "Marco Volpe"
                    },
                    {
                        "name": "Talles Medeiros"
                    }
                ],
                "author_detail": {
                    "name": "Talles Medeiros"
                },
                "author": "Talles Medeiros",
                "arxiv_comment": "19 pages, 7 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13577v2",
                "updated": "2024-11-26T09:20:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    20,
                    48,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-15T04:16:45Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    16,
                    45,
                    4,
                    320,
                    0
                ],
                "title": "WavChat: A Survey of Spoken Dialogue Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WavChat: A Survey of Spoken Dialogue Models"
                },
                "summary": "Recent advancements in spoken dialogue models, exemplified by systems like\nGPT-4o, have captured significant attention in the speech domain. Compared to\ntraditional three-tier cascaded spoken dialogue models that comprise speech\nrecognition (ASR), large language models (LLMs), and text-to-speech (TTS),\nmodern spoken dialogue models exhibit greater intelligence. These advanced\nspoken dialogue models not only comprehend audio, music, and other\nspeech-related features, but also capture stylistic and timbral characteristics\nin speech. Moreover, they generate high-quality, multi-turn speech responses\nwith low latency, enabling real-time interaction through simultaneous listening\nand speaking capability. Despite the progress in spoken dialogue systems, there\nis a lack of comprehensive surveys that systematically organize and analyze\nthese systems and the underlying technologies. To address this, we have first\ncompiled existing spoken dialogue systems in the chronological order and\ncategorized them into the cascaded and end-to-end paradigms. We then provide an\nin-depth overview of the core technologies in spoken dialogue models, covering\naspects such as speech representation, training paradigm, streaming, duplex,\nand interaction capabilities. Each section discusses the limitations of these\ntechnologies and outlines considerations for future research. Additionally, we\npresent a thorough review of relevant datasets, evaluation metrics, and\nbenchmarks from the perspectives of training and evaluating spoken dialogue\nsystems. We hope this survey will contribute to advancing both academic\nresearch and industrial applications in the field of spoken dialogue systems.\nThe related material is available at https://github.com/jishengpeng/WavChat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in spoken dialogue models, exemplified by systems like\nGPT-4o, have captured significant attention in the speech domain. Compared to\ntraditional three-tier cascaded spoken dialogue models that comprise speech\nrecognition (ASR), large language models (LLMs), and text-to-speech (TTS),\nmodern spoken dialogue models exhibit greater intelligence. These advanced\nspoken dialogue models not only comprehend audio, music, and other\nspeech-related features, but also capture stylistic and timbral characteristics\nin speech. Moreover, they generate high-quality, multi-turn speech responses\nwith low latency, enabling real-time interaction through simultaneous listening\nand speaking capability. Despite the progress in spoken dialogue systems, there\nis a lack of comprehensive surveys that systematically organize and analyze\nthese systems and the underlying technologies. To address this, we have first\ncompiled existing spoken dialogue systems in the chronological order and\ncategorized them into the cascaded and end-to-end paradigms. We then provide an\nin-depth overview of the core technologies in spoken dialogue models, covering\naspects such as speech representation, training paradigm, streaming, duplex,\nand interaction capabilities. Each section discusses the limitations of these\ntechnologies and outlines considerations for future research. Additionally, we\npresent a thorough review of relevant datasets, evaluation metrics, and\nbenchmarks from the perspectives of training and evaluating spoken dialogue\nsystems. We hope this survey will contribute to advancing both academic\nresearch and industrial applications in the field of spoken dialogue systems.\nThe related material is available at https://github.com/jishengpeng/WavChat."
                },
                "authors": [
                    {
                        "name": "Shengpeng Ji"
                    },
                    {
                        "name": "Yifu Chen"
                    },
                    {
                        "name": "Minghui Fang"
                    },
                    {
                        "name": "Jialong Zuo"
                    },
                    {
                        "name": "Jingyu Lu"
                    },
                    {
                        "name": "Hanting Wang"
                    },
                    {
                        "name": "Ziyue Jiang"
                    },
                    {
                        "name": "Long Zhou"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Xize Cheng"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Zehan Wang"
                    },
                    {
                        "name": "Qian Yang"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yidi Jiang"
                    },
                    {
                        "name": "Jingzhen He"
                    },
                    {
                        "name": "Yunfei Chu"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Zhou Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Zhao"
                },
                "author": "Zhou Zhao",
                "arxiv_comment": "60 papes, working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14128v2",
                "updated": "2024-11-26T09:12:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    12,
                    30,
                    1,
                    331,
                    0
                ],
                "published": "2024-09-21T12:46:17Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    12,
                    46,
                    17,
                    5,
                    265,
                    0
                ],
                "title": "Present and Future Generalization of Synthetic Image Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Present and Future Generalization of Synthetic Image Detectors"
                },
                "summary": "The continued release of increasingly realistic image generation models\ncreates a demand for synthetic image detectors. To build effective detectors we\nmust first understand how factors like data source diversity, training\nmethodologies and image alterations affect their generalization capabilities.\nThis work conducts a systematic analysis and uses its insights to develop\npractical guidelines for training robust synthetic image detectors. Model\ngeneralization capabilities are evaluated across different setups (e.g. scale,\nsources, transformations) including real-world deployment conditions. Through\nan extensive benchmarking of state-of-the-art detectors across diverse and\nrecent datasets, we show that while current approaches excel in specific\nscenarios, no single detector achieves universal effectiveness. Critical flaws\nare identified in detectors, and workarounds are proposed to enable the\ndeployment of real-world detector applications enhancing accuracy, reliability\nand robustness beyond the limitations of current systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The continued release of increasingly realistic image generation models\ncreates a demand for synthetic image detectors. To build effective detectors we\nmust first understand how factors like data source diversity, training\nmethodologies and image alterations affect their generalization capabilities.\nThis work conducts a systematic analysis and uses its insights to develop\npractical guidelines for training robust synthetic image detectors. Model\ngeneralization capabilities are evaluated across different setups (e.g. scale,\nsources, transformations) including real-world deployment conditions. Through\nan extensive benchmarking of state-of-the-art detectors across diverse and\nrecent datasets, we show that while current approaches excel in specific\nscenarios, no single detector achieves universal effectiveness. Critical flaws\nare identified in detectors, and workarounds are proposed to enable the\ndeployment of real-world detector applications enhancing accuracy, reliability\nand robustness beyond the limitations of current systems."
                },
                "authors": [
                    {
                        "name": "Pablo Bernabeu-Perez"
                    },
                    {
                        "name": "Enrique Lopez-Cuena"
                    },
                    {
                        "name": "Dario Garcia-Gasulla"
                    }
                ],
                "author_detail": {
                    "name": "Dario Garcia-Gasulla"
                },
                "author": "Dario Garcia-Gasulla",
                "arxiv_comment": "21 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17230v1",
                "updated": "2024-11-26T08:52:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    8,
                    52,
                    13,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T08:52:13Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    8,
                    52,
                    13,
                    1,
                    331,
                    0
                ],
                "title": "Fault Localization from the Semantic Code Search Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault Localization from the Semantic Code Search Perspective"
                },
                "summary": "The software development process is characterized by an iterative cycle of\ncontinuous functionality implementation and debugging, essential for the\nenhancement of software quality and adaptability to changing requirements. This\nprocess incorporates two isolatedly studied tasks: Code Search (CS), which\nretrieves reference code from a code corpus to aid in code implementation, and\nFault Localization (FL), which identifies code entities responsible for bugs\nwithin the software project to boost software debugging. These two tasks\nexhibit similarities since they both address search problems. Notably, CS\ntechniques have demonstrated greater effectiveness than FL ones, possibly\nbecause of the precise semantic details of the required code offered by natural\nlanguage queries, which are not readily accessible to FL methods. Drawing\ninspiration from this, we hypothesize that a fault localizer could achieve\ngreater proficiency if semantic information about the buggy methods were made\navailable. Based on this idea, we propose CosFL, an FL approach that decomposes\nthe FL task into two steps: query generation, which describes the functionality\nof the problematic code in natural language, and fault retrieval, which uses CS\nto find program elements semantically related to the query. Specifically, to\ndepict the buggy functionalities and generate high-quality queries, CosFL\nextensively harnesses the code analysis, semantic comprehension, and\ndecision-making capabilities of LLMs. Moreover, to enhance the accuracy of CS,\nCosFL captures varying levels of context information and employs a\nmulti-granularity code search strategy, which facilitates a more precise\nidentification of buggy methods from a holistic view. The evaluation on 835\nreal bugs from 23 Java projects shows that CosFL successfully localizes 324\nbugs within Top-1, which significantly outperforms the state-of-the-art\napproaches by 26.6%-57.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The software development process is characterized by an iterative cycle of\ncontinuous functionality implementation and debugging, essential for the\nenhancement of software quality and adaptability to changing requirements. This\nprocess incorporates two isolatedly studied tasks: Code Search (CS), which\nretrieves reference code from a code corpus to aid in code implementation, and\nFault Localization (FL), which identifies code entities responsible for bugs\nwithin the software project to boost software debugging. These two tasks\nexhibit similarities since they both address search problems. Notably, CS\ntechniques have demonstrated greater effectiveness than FL ones, possibly\nbecause of the precise semantic details of the required code offered by natural\nlanguage queries, which are not readily accessible to FL methods. Drawing\ninspiration from this, we hypothesize that a fault localizer could achieve\ngreater proficiency if semantic information about the buggy methods were made\navailable. Based on this idea, we propose CosFL, an FL approach that decomposes\nthe FL task into two steps: query generation, which describes the functionality\nof the problematic code in natural language, and fault retrieval, which uses CS\nto find program elements semantically related to the query. Specifically, to\ndepict the buggy functionalities and generate high-quality queries, CosFL\nextensively harnesses the code analysis, semantic comprehension, and\ndecision-making capabilities of LLMs. Moreover, to enhance the accuracy of CS,\nCosFL captures varying levels of context information and employs a\nmulti-granularity code search strategy, which facilitates a more precise\nidentification of buggy methods from a holistic view. The evaluation on 835\nreal bugs from 23 Java projects shows that CosFL successfully localizes 324\nbugs within Top-1, which significantly outperforms the state-of-the-art\napproaches by 26.6%-57.3%."
                },
                "authors": [
                    {
                        "name": "Yihao Qin"
                    },
                    {
                        "name": "Shangwen Wang"
                    },
                    {
                        "name": "Yan Lei"
                    },
                    {
                        "name": "Zhuo Zhang"
                    },
                    {
                        "name": "Bo Lin"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Liqian Chen"
                    },
                    {
                        "name": "Xiaoguang Mao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoguang Mao"
                },
                "author": "Xiaoguang Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08903v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08903v3",
                "updated": "2024-11-26T08:50:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    8,
                    50,
                    52,
                    1,
                    331,
                    0
                ],
                "published": "2024-06-13T07:57:27Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    7,
                    57,
                    27,
                    3,
                    165,
                    0
                ],
                "title": "Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for\n  Large Language Models"
                },
                "summary": "Fine-tuning is a crucial process for adapting large language models (LLMs) to\ndiverse applications. In certain scenarios, such as multi-tenant serving,\ndeploying multiple LLMs becomes necessary to meet complex demands. Recent\nstudies suggest decomposing a fine-tuned LLM into a base model and\ncorresponding delta weights, which are then compressed using low-rank or\nlow-bit approaches to reduce costs. In this work, we observe that existing\nlow-rank and low-bit compression methods can significantly harm the model\nperformance for task-specific fine-tuned LLMs (e.g., WizardMath for math\nproblems). Motivated by the long-tail distribution of singular values in the\ndelta weights, we propose a delta quantization approach using mixed-precision.\nThis method employs higher-bit representation for singular vectors\ncorresponding to larger singular values. We evaluate our approach on various\nfine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs.\nExperimental results demonstrate that our approach performs comparably to full\nfine-tuned LLMs, surpassing both low-rank and low-bit baselines by a\nconsiderable margin. Additionally, we show that our method is compatible with\nvarious backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning is a crucial process for adapting large language models (LLMs) to\ndiverse applications. In certain scenarios, such as multi-tenant serving,\ndeploying multiple LLMs becomes necessary to meet complex demands. Recent\nstudies suggest decomposing a fine-tuned LLM into a base model and\ncorresponding delta weights, which are then compressed using low-rank or\nlow-bit approaches to reduce costs. In this work, we observe that existing\nlow-rank and low-bit compression methods can significantly harm the model\nperformance for task-specific fine-tuned LLMs (e.g., WizardMath for math\nproblems). Motivated by the long-tail distribution of singular values in the\ndelta weights, we propose a delta quantization approach using mixed-precision.\nThis method employs higher-bit representation for singular vectors\ncorresponding to larger singular values. We evaluate our approach on various\nfine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs.\nExperimental results demonstrate that our approach performs comparably to full\nfine-tuned LLMs, surpassing both low-rank and low-bit baselines by a\nconsiderable margin. Additionally, we show that our method is compatible with\nvarious backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Bowen Ping"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08903v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08903v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05399v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05399v2",
                "updated": "2024-11-26T08:48:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    8,
                    48,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-07-07T14:55:04Z",
                "published_parsed": [
                    2024,
                    7,
                    7,
                    14,
                    55,
                    4,
                    6,
                    189,
                    0
                ],
                "title": "IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning"
                },
                "summary": "Legal systems worldwide are inundated with exponential growth in cases and\ndocuments. There is an imminent need to develop NLP and ML techniques for\nautomatically processing and understanding legal documents to streamline the\nlegal system. However, evaluating and comparing various NLP models designed\nspecifically for the legal domain is challenging. This paper addresses this\nchallenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding\nand Reasoning. IL-TUR contains monolingual (English, Hindi) and multi-lingual\n(9 Indian languages) domain-specific tasks that address different aspects of\nthe legal system from the point of view of understanding and reasoning over\nIndian legal documents. We present baseline models (including LLM-based) for\neach task, outlining the gap between models and the ground truth. To foster\nfurther research in the legal domain, we create a leaderboard (available at:\nhttps://exploration-lab.github.io/IL-TUR/) where the research community can\nupload and compare legal text understanding systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal systems worldwide are inundated with exponential growth in cases and\ndocuments. There is an imminent need to develop NLP and ML techniques for\nautomatically processing and understanding legal documents to streamline the\nlegal system. However, evaluating and comparing various NLP models designed\nspecifically for the legal domain is challenging. This paper addresses this\nchallenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding\nand Reasoning. IL-TUR contains monolingual (English, Hindi) and multi-lingual\n(9 Indian languages) domain-specific tasks that address different aspects of\nthe legal system from the point of view of understanding and reasoning over\nIndian legal documents. We present baseline models (including LLM-based) for\neach task, outlining the gap between models and the ground truth. To foster\nfurther research in the legal domain, we create a leaderboard (available at:\nhttps://exploration-lab.github.io/IL-TUR/) where the research community can\nupload and compare legal text understanding systems."
                },
                "authors": [
                    {
                        "name": "Abhinav Joshi"
                    },
                    {
                        "name": "Shounak Paul"
                    },
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Pawan Goyal"
                    },
                    {
                        "name": "Saptarshi Ghosh"
                    },
                    {
                        "name": "Ashutosh Modi"
                    }
                ],
                "author_detail": {
                    "name": "Ashutosh Modi"
                },
                "author": "Ashutosh Modi",
                "arxiv_comment": "Accepted at ACL 2024 Main Conference; 40 Pages (9 Pages + References\n  + Appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05399v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05399v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00231v2",
                "updated": "2024-11-26T08:37:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    8,
                    37,
                    54,
                    1,
                    331,
                    0
                ],
                "published": "2024-05-31T23:29:42Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    23,
                    29,
                    42,
                    4,
                    152,
                    0
                ],
                "title": "LLM-RankFusion: Mitigating Intrinsic Inconsistency in LLM-based Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-RankFusion: Mitigating Intrinsic Inconsistency in LLM-based Ranking"
                },
                "summary": "Ranking passages by prompting a large language model (LLM) can achieve\npromising performance in modern information retrieval (IR) systems. A common\napproach to sort the ranking list is by prompting LLMs for a pairwise or\nsetwise comparison which often relies on sorting algorithms. However,\nsorting-based methods require consistent comparisons to correctly sort the\npassages, which we show that LLMs often violate. We identify two kinds of\nintrinsic inconsistency in LLM-based pairwise comparisons: order inconsistency\nwhich leads to conflicting results when switching the passage order, and\ntransitive inconsistency which leads to non-transitive triads among all\npreference pairs. Our study of these inconsistencies is relevant for\nunderstanding and improving the stability of any ranking scheme based on\nrelative preferences. In this paper, we propose LLM-RankFusion, an LLM-based\nranking framework that mitigates these inconsistencies and produces a robust\nranking list. LLM-RankFusion mitigates order inconsistency using in-context\nlearning (ICL) to demonstrate order-agnostic comparisons and calibration to\nestimate the underlying preference probability between two passages. We then\naddress transitive inconsistency by aggregating the ranking results from\nmultiple rankers. In our experiments, we empirically show that LLM-RankFusion\ncan significantly reduce inconsistent comparison results, improving the ranking\nquality by making the final ranking list more robust. Our code is available at\n\\href{https://github.com/XHMY/LLM-RankFusion}{https://github.com/XHMY/LLM-RankFusion}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ranking passages by prompting a large language model (LLM) can achieve\npromising performance in modern information retrieval (IR) systems. A common\napproach to sort the ranking list is by prompting LLMs for a pairwise or\nsetwise comparison which often relies on sorting algorithms. However,\nsorting-based methods require consistent comparisons to correctly sort the\npassages, which we show that LLMs often violate. We identify two kinds of\nintrinsic inconsistency in LLM-based pairwise comparisons: order inconsistency\nwhich leads to conflicting results when switching the passage order, and\ntransitive inconsistency which leads to non-transitive triads among all\npreference pairs. Our study of these inconsistencies is relevant for\nunderstanding and improving the stability of any ranking scheme based on\nrelative preferences. In this paper, we propose LLM-RankFusion, an LLM-based\nranking framework that mitigates these inconsistencies and produces a robust\nranking list. LLM-RankFusion mitigates order inconsistency using in-context\nlearning (ICL) to demonstrate order-agnostic comparisons and calibration to\nestimate the underlying preference probability between two passages. We then\naddress transitive inconsistency by aggregating the ranking results from\nmultiple rankers. In our experiments, we empirically show that LLM-RankFusion\ncan significantly reduce inconsistent comparison results, improving the ranking\nquality by making the final ranking list more robust. Our code is available at\n\\href{https://github.com/XHMY/LLM-RankFusion}{https://github.com/XHMY/LLM-RankFusion}"
                },
                "authors": [
                    {
                        "name": "Yifan Zeng"
                    },
                    {
                        "name": "Ojas Tendolkar"
                    },
                    {
                        "name": "Raymond Baartmans"
                    },
                    {
                        "name": "Qingyun Wu"
                    },
                    {
                        "name": "Lizhong Chen"
                    },
                    {
                        "name": "Huazheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huazheng Wang"
                },
                "author": "Huazheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17204v1",
                "updated": "2024-11-26T08:21:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    8,
                    21,
                    24,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T08:21:24Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    8,
                    21,
                    24,
                    1,
                    331,
                    0
                ],
                "title": "Strategic Prompting for Conversational Tasks: A Comparative Analysis of\n  Large Language Models Across Diverse Conversational Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Prompting for Conversational Tasks: A Comparative Analysis of\n  Large Language Models Across Diverse Conversational Tasks"
                },
                "summary": "Given the advancements in conversational artificial intelligence, the\nevaluation and assessment of Large Language Models (LLMs) play a crucial role\nin ensuring optimal performance across various conversational tasks. In this\npaper, we present a comprehensive study that thoroughly evaluates the\ncapabilities and limitations of five prevalent LLMs: Llama, OPT, Falcon,\nAlpaca, and MPT. The study encompasses various conversational tasks, including\nreservation, empathetic response generation, mental health and legal\ncounseling, persuasion, and negotiation. To conduct the evaluation, an\nextensive test setup is employed, utilizing multiple evaluation criteria that\nspan from automatic to human evaluation. This includes using generic and\ntask-specific metrics to gauge the LMs' performance accurately. From our\nevaluation, no single model emerges as universally optimal for all tasks.\nInstead, their performance varies significantly depending on the specific\nrequirements of each task. While some models excel in certain tasks, they may\ndemonstrate comparatively poorer performance in others. These findings\nemphasize the importance of considering task-specific requirements and\ncharacteristics when selecting the most suitable LM for conversational\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the advancements in conversational artificial intelligence, the\nevaluation and assessment of Large Language Models (LLMs) play a crucial role\nin ensuring optimal performance across various conversational tasks. In this\npaper, we present a comprehensive study that thoroughly evaluates the\ncapabilities and limitations of five prevalent LLMs: Llama, OPT, Falcon,\nAlpaca, and MPT. The study encompasses various conversational tasks, including\nreservation, empathetic response generation, mental health and legal\ncounseling, persuasion, and negotiation. To conduct the evaluation, an\nextensive test setup is employed, utilizing multiple evaluation criteria that\nspan from automatic to human evaluation. This includes using generic and\ntask-specific metrics to gauge the LMs' performance accurately. From our\nevaluation, no single model emerges as universally optimal for all tasks.\nInstead, their performance varies significantly depending on the specific\nrequirements of each task. While some models excel in certain tasks, they may\ndemonstrate comparatively poorer performance in others. These findings\nemphasize the importance of considering task-specific requirements and\ncharacteristics when selecting the most suitable LM for conversational\napplications."
                },
                "authors": [
                    {
                        "name": "Ratnesh Kumar Joshi"
                    },
                    {
                        "name": "Priyanshu Priya"
                    },
                    {
                        "name": "Vishesh Desai"
                    },
                    {
                        "name": "Saurav Dudhate"
                    },
                    {
                        "name": "Siddhant Senapati"
                    },
                    {
                        "name": "Asif Ekbal"
                    },
                    {
                        "name": "Roshni Ramnani"
                    },
                    {
                        "name": "Anutosh Maitra"
                    }
                ],
                "author_detail": {
                    "name": "Anutosh Maitra"
                },
                "author": "Anutosh Maitra",
                "arxiv_comment": "37 pages, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08208v2",
                "updated": "2024-11-26T08:07:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    8,
                    7,
                    8,
                    1,
                    331,
                    0
                ],
                "published": "2024-08-15T15:18:46Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    18,
                    46,
                    3,
                    228,
                    0
                ],
                "title": "LLM4DSR: Leveraing Large Language Model for Denoising Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4DSR: Leveraing Large Language Model for Denoising Sequential\n  Recommendation"
                },
                "summary": "Sequential Recommenders generate recommendations based on users' historical\ninteraction sequences. However, in practice, these collected sequences are\noften contaminated by noisy interactions, which significantly impairs\nrecommendation performance. Accurately identifying such noisy interactions\nwithout additional information is particularly challenging due to the absence\nof explicit supervisory signals indicating noise. Large Language Models (LLMs),\nequipped with extensive open knowledge and semantic reasoning abilities, offer\na promising avenue to bridge this information gap. However, employing LLMs for\ndenoising in sequential recommendation presents notable challenges: 1) Direct\napplication of pretrained LLMs may not be competent for the denoising task,\nfrequently generating nonsensical responses; 2) Even after fine-tuning, the\nreliability of LLM outputs remains questionable, especially given the\ncomplexity of the denoising task and the inherent hallucinatory issue of LLMs.\n  To tackle these challenges, we propose LLM4DSR, a tailored approach for\ndenoising sequential recommendation using LLMs. We constructed a\nself-supervised fine-tuning task to activate LLMs' capabilities to identify\nnoisy items and suggest replacements. Furthermore, we developed an uncertainty\nestimation module that ensures only high-confidence responses are utilized for\nsequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing corrected\nsequences to be flexibly applied across various recommendation models.\nExtensive experiments validate the superiority of LLM4DSR over existing\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Recommenders generate recommendations based on users' historical\ninteraction sequences. However, in practice, these collected sequences are\noften contaminated by noisy interactions, which significantly impairs\nrecommendation performance. Accurately identifying such noisy interactions\nwithout additional information is particularly challenging due to the absence\nof explicit supervisory signals indicating noise. Large Language Models (LLMs),\nequipped with extensive open knowledge and semantic reasoning abilities, offer\na promising avenue to bridge this information gap. However, employing LLMs for\ndenoising in sequential recommendation presents notable challenges: 1) Direct\napplication of pretrained LLMs may not be competent for the denoising task,\nfrequently generating nonsensical responses; 2) Even after fine-tuning, the\nreliability of LLM outputs remains questionable, especially given the\ncomplexity of the denoising task and the inherent hallucinatory issue of LLMs.\n  To tackle these challenges, we propose LLM4DSR, a tailored approach for\ndenoising sequential recommendation using LLMs. We constructed a\nself-supervised fine-tuning task to activate LLMs' capabilities to identify\nnoisy items and suggest replacements. Furthermore, we developed an uncertainty\nestimation module that ensures only high-confidence responses are utilized for\nsequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing corrected\nsequences to be flexibly applied across various recommendation models.\nExtensive experiments validate the superiority of LLM4DSR over existing\nmethods."
                },
                "authors": [
                    {
                        "name": "Bohao Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Changwang Zhang"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Yudi Wu"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Xingyu Lou"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yan Feng"
                    },
                    {
                        "name": "Chun Chen"
                    },
                    {
                        "name": "Can Wang"
                    }
                ],
                "author_detail": {
                    "name": "Can Wang"
                },
                "author": "Can Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17178v1",
                "updated": "2024-11-26T07:32:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    7,
                    32,
                    36,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T07:32:36Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    7,
                    32,
                    36,
                    1,
                    331,
                    0
                ],
                "title": "LiteVAR: Compressing Visual Autoregressive Modelling with Efficient\n  Attention and Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteVAR: Compressing Visual Autoregressive Modelling with Efficient\n  Attention and Quantization"
                },
                "summary": "Visual Autoregressive (VAR) has emerged as a promising approach in image\ngeneration, offering competitive potential and performance comparable to\ndiffusion-based models. However, current AR-based visual generation models\nrequire substantial computational resources, limiting their applicability on\nresource-constrained devices. To address this issue, we conducted analysis and\nidentified significant redundancy in three dimensions of the VAR model: (1) the\nattention map, (2) the attention outputs when using classifier free guidance,\nand (3) the data precision. Correspondingly, we proposed efficient attention\nmechanism and low-bit quantization method to enhance the efficiency of VAR\nmodels while maintaining performance. With negligible performance lost (less\nthan 0.056 FID increase), we could achieve 85.2% reduction in attention\ncomputation, 50% reduction in overall memory and 1.5x latency reduction. To\nensure deployment feasibility, we developed efficient training-free compression\ntechniques and analyze the deployment feasibility and efficiency gain of each\ntechnique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) has emerged as a promising approach in image\ngeneration, offering competitive potential and performance comparable to\ndiffusion-based models. However, current AR-based visual generation models\nrequire substantial computational resources, limiting their applicability on\nresource-constrained devices. To address this issue, we conducted analysis and\nidentified significant redundancy in three dimensions of the VAR model: (1) the\nattention map, (2) the attention outputs when using classifier free guidance,\nand (3) the data precision. Correspondingly, we proposed efficient attention\nmechanism and low-bit quantization method to enhance the efficiency of VAR\nmodels while maintaining performance. With negligible performance lost (less\nthan 0.056 FID increase), we could achieve 85.2% reduction in attention\ncomputation, 50% reduction in overall memory and 1.5x latency reduction. To\nensure deployment feasibility, we developed efficient training-free compression\ntechniques and analyze the deployment feasibility and efficiency gain of each\ntechnique."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Tianchen Zhao"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Rui Wan"
                    },
                    {
                        "name": "Wenxi Gao"
                    },
                    {
                        "name": "Zhenhua Zhu"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17150v1",
                "updated": "2024-11-26T06:34:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    6,
                    34,
                    48,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T06:34:48Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    6,
                    34,
                    48,
                    1,
                    331,
                    0
                ],
                "title": "Distilling Spectral Graph for Object-Context Aware Open-Vocabulary\n  Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Spectral Graph for Object-Context Aware Open-Vocabulary\n  Semantic Segmentation"
                },
                "summary": "Open-Vocabulary Semantic Segmentation (OVSS) has advanced with recent\nvision-language models (VLMs), enabling segmentation beyond predefined\ncategories through various learning schemes. Notably, training-free methods\noffer scalable, easily deployable solutions for handling unseen data, a key\ngoal of OVSS. Yet, a critical issue persists: lack of object-level context\nconsideration when segmenting complex objects in the challenging environment of\nOVSS based on arbitrary query prompts. This oversight limits models' ability to\ngroup semantically consistent elements within object and map them precisely to\nuser-defined arbitrary classes. In this work, we introduce a novel approach\nthat overcomes this limitation by incorporating object-level contextual\nknowledge within images. Specifically, our model enhances intra-object\nconsistency by distilling spectral-driven features from vision foundation\nmodels into the attention mechanism of the visual encoder, enabling\nsemantically coherent components to form a single object mask. Additionally, we\nrefine the text embeddings with zero-shot object presence likelihood to ensure\naccurate alignment with the specific objects represented in the images. By\nleveraging object-level contextual knowledge, our proposed approach achieves\nstate-of-the-art performance with strong generalizability across diverse\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Vocabulary Semantic Segmentation (OVSS) has advanced with recent\nvision-language models (VLMs), enabling segmentation beyond predefined\ncategories through various learning schemes. Notably, training-free methods\noffer scalable, easily deployable solutions for handling unseen data, a key\ngoal of OVSS. Yet, a critical issue persists: lack of object-level context\nconsideration when segmenting complex objects in the challenging environment of\nOVSS based on arbitrary query prompts. This oversight limits models' ability to\ngroup semantically consistent elements within object and map them precisely to\nuser-defined arbitrary classes. In this work, we introduce a novel approach\nthat overcomes this limitation by incorporating object-level contextual\nknowledge within images. Specifically, our model enhances intra-object\nconsistency by distilling spectral-driven features from vision foundation\nmodels into the attention mechanism of the visual encoder, enabling\nsemantically coherent components to form a single object mask. Additionally, we\nrefine the text embeddings with zero-shot object presence likelihood to ensure\naccurate alignment with the specific objects represented in the images. By\nleveraging object-level contextual knowledge, our proposed approach achieves\nstate-of-the-art performance with strong generalizability across diverse\ndatasets."
                },
                "authors": [
                    {
                        "name": "Chanyoung Kim"
                    },
                    {
                        "name": "Dayun Ju"
                    },
                    {
                        "name": "Woojung Han"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "name": "Seong Jae Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Seong Jae Hwang"
                },
                "author": "Seong Jae Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02884v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02884v3",
                "updated": "2024-11-26T06:29:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    6,
                    29,
                    12,
                    1,
                    331,
                    0
                ],
                "published": "2024-06-05T03:05:52Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    3,
                    5,
                    52,
                    2,
                    157,
                    0
                ],
                "title": "PosterLLaVa: Constructing a Unified Multi-modal Layout Generator with\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PosterLLaVa: Constructing a Unified Multi-modal Layout Generator with\n  LLM"
                },
                "summary": "Layout generation is the keystone in achieving automated graphic design,\nrequiring arranging the position and size of various multi-modal design\nelements in a visually pleasing and constraint-following manner. Previous\napproaches are either inefficient for large-scale applications or lack\nflexibility for varying design requirements. Our research introduces a unified\nframework for automated graphic layout generation, leveraging the multi-modal\nlarge language model (MLLM) to accommodate diverse design tasks. In contrast,\nour data-driven method employs structured text (JSON format) and visual\ninstruction tuning to generate layouts under specific visual and textual\nconstraints, including user-defined natural language specifications. We\nconducted extensive experiments and achieved state-of-the-art (SOTA)\nperformance on public multi-modal layout generation benchmarks, demonstrating\nthe effectiveness of our method. Moreover, recognizing existing datasets'\nlimitations in capturing the complexity of real-world graphic designs, we\npropose two new datasets for much more challenging tasks (user-constrained\ngeneration and complicated poster), further validating our model's utility in\nreal-life settings. Marking by its superior accessibility and adaptability,\nthis approach further automates large-scale graphic design tasks. Finally, we\ndevelop an automated text-to-poster system that generates editable SVG posters\nbased on users' design intentions, bridging the gap between layout generation\nand real-world graphic design applications. This system integrates our proposed\nlayout generation method as the core component, demonstrating its effectiveness\nin practical scenarios. The code and datasets are open-sourced on\nhttps://github.com/posterllava/PosterLLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layout generation is the keystone in achieving automated graphic design,\nrequiring arranging the position and size of various multi-modal design\nelements in a visually pleasing and constraint-following manner. Previous\napproaches are either inefficient for large-scale applications or lack\nflexibility for varying design requirements. Our research introduces a unified\nframework for automated graphic layout generation, leveraging the multi-modal\nlarge language model (MLLM) to accommodate diverse design tasks. In contrast,\nour data-driven method employs structured text (JSON format) and visual\ninstruction tuning to generate layouts under specific visual and textual\nconstraints, including user-defined natural language specifications. We\nconducted extensive experiments and achieved state-of-the-art (SOTA)\nperformance on public multi-modal layout generation benchmarks, demonstrating\nthe effectiveness of our method. Moreover, recognizing existing datasets'\nlimitations in capturing the complexity of real-world graphic designs, we\npropose two new datasets for much more challenging tasks (user-constrained\ngeneration and complicated poster), further validating our model's utility in\nreal-life settings. Marking by its superior accessibility and adaptability,\nthis approach further automates large-scale graphic design tasks. Finally, we\ndevelop an automated text-to-poster system that generates editable SVG posters\nbased on users' design intentions, bridging the gap between layout generation\nand real-world graphic design applications. This system integrates our proposed\nlayout generation method as the core component, demonstrating its effectiveness\nin practical scenarios. The code and datasets are open-sourced on\nhttps://github.com/posterllava/PosterLLaVA."
                },
                "authors": [
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Yingmin Luo"
                    },
                    {
                        "name": "Zhongang Qi"
                    },
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Chang Wen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chang Wen Chen"
                },
                "author": "Chang Wen Chen",
                "arxiv_comment": "13 pages; with PosterGen as extension; IEEE template",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02884v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02884v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16205v2",
                "updated": "2024-11-26T06:28:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    6,
                    28,
                    54,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-25T09:05:36Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    5,
                    36,
                    0,
                    330,
                    0
                ],
                "title": "MH-MoE: Multi-Head Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MH-MoE: Multi-Head Mixture-of-Experts"
                },
                "summary": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet."
                },
                "authors": [
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "7 pages, 0 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13439v2",
                "updated": "2024-11-26T06:18:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    6,
                    18,
                    16,
                    1,
                    331,
                    0
                ],
                "published": "2024-06-19T10:59:48Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    10,
                    59,
                    48,
                    2,
                    171,
                    0
                ],
                "title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists"
                },
                "summary": "Large Language Models (LLMs) are increasingly relied upon to evaluate text\noutputs of other LLMs, thereby influencing leaderboards and development\ndecisions. However, concerns persist over the accuracy of these assessments and\nthe potential for misleading conclusions. In this work, we investigate the\neffectiveness of LLMs as evaluators for text generation tasks. We propose FBI,\na novel framework designed to examine the proficiency of Evaluator LLMs in\nassessing four critical abilities in other LLMs: factual accuracy, instruction\nfollowing, coherence in long-form writing, and reasoning proficiency. By\nintroducing targeted perturbations in answers generated by LLMs, that clearly\nimpact one of these key capabilities, we test whether an Evaluator LLM can\ndetect these quality drops. By creating a total of 2400 perturbed answers\ncovering 22 perturbation categories, we conduct a comprehensive study using\ndifferent evaluation strategies on five prominent LLMs commonly used as\nevaluators in the literature. Our findings reveal significant shortcomings in\ncurrent Evaluator LLMs, which failed to identify quality drops in over 50\\% of\ncases on average. Single-answer and pairwise evaluations demonstrated notable\nlimitations, whereas reference-based evaluations showed comparatively better\nperformance. These results underscore the unreliable nature of current\nEvaluator LLMs and advocate for cautious implementation in practical\napplications. Code and data are available at https://github.com/AI4Bharat/FBI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly relied upon to evaluate text\noutputs of other LLMs, thereby influencing leaderboards and development\ndecisions. However, concerns persist over the accuracy of these assessments and\nthe potential for misleading conclusions. In this work, we investigate the\neffectiveness of LLMs as evaluators for text generation tasks. We propose FBI,\na novel framework designed to examine the proficiency of Evaluator LLMs in\nassessing four critical abilities in other LLMs: factual accuracy, instruction\nfollowing, coherence in long-form writing, and reasoning proficiency. By\nintroducing targeted perturbations in answers generated by LLMs, that clearly\nimpact one of these key capabilities, we test whether an Evaluator LLM can\ndetect these quality drops. By creating a total of 2400 perturbed answers\ncovering 22 perturbation categories, we conduct a comprehensive study using\ndifferent evaluation strategies on five prominent LLMs commonly used as\nevaluators in the literature. Our findings reveal significant shortcomings in\ncurrent Evaluator LLMs, which failed to identify quality drops in over 50\\% of\ncases on average. Single-answer and pairwise evaluations demonstrated notable\nlimitations, whereas reference-based evaluations showed comparatively better\nperformance. These results underscore the unreliable nature of current\nEvaluator LLMs and advocate for cautious implementation in practical\napplications. Code and data are available at https://github.com/AI4Bharat/FBI."
                },
                "authors": [
                    {
                        "name": "Sumanth Doddapaneni"
                    },
                    {
                        "name": "Mohammed Safi Ur Rahman Khan"
                    },
                    {
                        "name": "Sshubam Verma"
                    },
                    {
                        "name": "Mitesh M. Khapra"
                    }
                ],
                "author_detail": {
                    "name": "Mitesh M. Khapra"
                },
                "author": "Mitesh M. Khapra",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17135v1",
                "updated": "2024-11-26T06:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    6,
                    4,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T06:04:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    6,
                    4,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "LLM-Based Offline Learning for Embodied Agents via Consistency-Guided\n  Reward Ensemble",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Offline Learning for Embodied Agents via Consistency-Guided\n  Reward Ensemble"
                },
                "summary": "Employing large language models (LLMs) to enable embodied agents has become\npopular, yet it presents several limitations in practice. In this work, rather\nthan using LLMs directly as agents, we explore their use as tools for embodied\nagent learning. Specifically, to train separate agents via offline\nreinforcement learning (RL), an LLM is used to provide dense reward feedback on\nindividual actions in training datasets. In doing so, we present a\nconsistency-guided reward ensemble framework (CoREN), designed for tackling\ndifficulties in grounding LLM-generated estimates to the target environment\ndomain. The framework employs an adaptive ensemble of spatio-temporally\nconsistent rewards to derive domain-grounded rewards in the training datasets,\nthus enabling effective offline learning of embodied agents in different\nenvironment domains. Experiments with the VirtualHome benchmark demonstrate\nthat CoREN significantly outperforms other offline RL agents, and it also\nachieves comparable performance to state-of-the-art LLM-based agents with 8B\nparameters, despite CoREN having only 117M parameters for the agent policy\nnetwork and using LLMs only for training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing large language models (LLMs) to enable embodied agents has become\npopular, yet it presents several limitations in practice. In this work, rather\nthan using LLMs directly as agents, we explore their use as tools for embodied\nagent learning. Specifically, to train separate agents via offline\nreinforcement learning (RL), an LLM is used to provide dense reward feedback on\nindividual actions in training datasets. In doing so, we present a\nconsistency-guided reward ensemble framework (CoREN), designed for tackling\ndifficulties in grounding LLM-generated estimates to the target environment\ndomain. The framework employs an adaptive ensemble of spatio-temporally\nconsistent rewards to derive domain-grounded rewards in the training datasets,\nthus enabling effective offline learning of embodied agents in different\nenvironment domains. Experiments with the VirtualHome benchmark demonstrate\nthat CoREN significantly outperforms other offline RL agents, and it also\nachieves comparable performance to state-of-the-art LLM-based agents with 8B\nparameters, despite CoREN having only 117M parameters for the agent policy\nnetwork and using LLMs only for training."
                },
                "authors": [
                    {
                        "name": "Yujeong Lee"
                    },
                    {
                        "name": "Sangwoo Shin"
                    },
                    {
                        "name": "Wei-Jin Park"
                    },
                    {
                        "name": "Honguk Woo"
                    }
                ],
                "author_detail": {
                    "name": "Honguk Woo"
                },
                "author": "Honguk Woo",
                "arxiv_comment": "Findings of EMNLP-2024 Camera Ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14491v2",
                "updated": "2024-11-26T06:03:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    6,
                    3,
                    8,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-20T12:34:44Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    34,
                    44,
                    2,
                    325,
                    0
                ],
                "title": "A Survey on Human-Centric LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Human-Centric LLMs"
                },
                "summary": "The rapid evolution of large language models (LLMs) and their capacity to\nsimulate human cognition and behavior has given rise to LLM-based frameworks\nand tools that are evaluated and applied based on their ability to perform\ntasks traditionally performed by humans, namely those involving cognition,\ndecision-making, and social interaction. This survey provides a comprehensive\nexamination of such human-centric LLM capabilities, focusing on their\nperformance in both individual tasks (where an LLM acts as a stand-in for a\nsingle human) and collective tasks (where multiple LLMs coordinate to mimic\ngroup dynamics). We first evaluate LLM competencies across key areas including\nreasoning, perception, and social cognition, comparing their abilities to\nhuman-like skills. Then, we explore real-world applications of LLMs in\nhuman-centric domains such as behavioral science, political science, and\nsociology, assessing their effectiveness in replicating human behaviors and\ninteractions. Finally, we identify challenges and future research directions,\nsuch as improving LLM adaptability, emotional intelligence, and cultural\nsensitivity, while addressing inherent biases and enhancing frameworks for\nhuman-AI collaboration. This survey aims to provide a foundational\nunderstanding of LLMs from a human-centric perspective, offering insights into\ntheir current capabilities and potential for future development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) and their capacity to\nsimulate human cognition and behavior has given rise to LLM-based frameworks\nand tools that are evaluated and applied based on their ability to perform\ntasks traditionally performed by humans, namely those involving cognition,\ndecision-making, and social interaction. This survey provides a comprehensive\nexamination of such human-centric LLM capabilities, focusing on their\nperformance in both individual tasks (where an LLM acts as a stand-in for a\nsingle human) and collective tasks (where multiple LLMs coordinate to mimic\ngroup dynamics). We first evaluate LLM competencies across key areas including\nreasoning, perception, and social cognition, comparing their abilities to\nhuman-like skills. Then, we explore real-world applications of LLMs in\nhuman-centric domains such as behavioral science, political science, and\nsociology, assessing their effectiveness in replicating human behaviors and\ninteractions. Finally, we identify challenges and future research directions,\nsuch as improving LLM adaptability, emotional intelligence, and cultural\nsensitivity, while addressing inherent biases and enhancing frameworks for\nhuman-AI collaboration. This survey aims to provide a foundational\nunderstanding of LLMs from a human-centric perspective, offering insights into\ntheir current capabilities and potential for future development."
                },
                "authors": [
                    {
                        "name": "Jing Yi Wang"
                    },
                    {
                        "name": "Nicholas Sukiennik"
                    },
                    {
                        "name": "Tong Li"
                    },
                    {
                        "name": "Weikang Su"
                    },
                    {
                        "name": "Qianyue Hao"
                    },
                    {
                        "name": "Jingbo Xu"
                    },
                    {
                        "name": "Zihan Huang"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17130v1",
                "updated": "2024-11-26T05:49:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    49,
                    25,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:49:25Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    49,
                    25,
                    1,
                    331,
                    0
                ],
                "title": "TechCoach: Towards Technical Keypoint-Aware Descriptive Action Coaching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TechCoach: Towards Technical Keypoint-Aware Descriptive Action Coaching"
                },
                "summary": "To guide a learner to master the action skills, it is crucial for a coach to\n1) reason through the learner's action execution and technical keypoints, and\n2) provide detailed, understandable feedback on what is done well and what can\nbe improved. However, existing score-based action assessment methods are still\nfar from this practical scenario. To bridge this gap, we investigate a new task\ntermed Descriptive Action Coaching (DAC) which requires a model to provide\ndetailed commentary on what is done well and what can be improved beyond a\nquality score from an action execution. To this end, we construct a new dataset\nnamed EE4D-DAC. With an LLM-based annotation pipeline, our dataset goes beyond\nthe existing action assessment datasets by providing the hierarchical coaching\ncommentary at both keypoint and instance levels. Furthermore, we propose\nTechCoach, a new framework that explicitly incorporates keypoint-level\nreasoning into the DAC process. The central to our method lies in the\nContext-aware Keypoint Reasoner, which enables TechCoach to learn\nkeypoint-related quality representations by querying visual context under the\nsupervision of keypoint-level coaching commentary. Prompted by the visual\ncontext and the keypoint-related quality representations, a unified\nKeypoint-aware Action Assessor is then employed to provide the overall coaching\ncommentary together with the quality score. Combining all of these, we build a\nnew benchmark for DAC and evaluate the effectiveness of our method through\nextensive experiments. Data and code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To guide a learner to master the action skills, it is crucial for a coach to\n1) reason through the learner's action execution and technical keypoints, and\n2) provide detailed, understandable feedback on what is done well and what can\nbe improved. However, existing score-based action assessment methods are still\nfar from this practical scenario. To bridge this gap, we investigate a new task\ntermed Descriptive Action Coaching (DAC) which requires a model to provide\ndetailed commentary on what is done well and what can be improved beyond a\nquality score from an action execution. To this end, we construct a new dataset\nnamed EE4D-DAC. With an LLM-based annotation pipeline, our dataset goes beyond\nthe existing action assessment datasets by providing the hierarchical coaching\ncommentary at both keypoint and instance levels. Furthermore, we propose\nTechCoach, a new framework that explicitly incorporates keypoint-level\nreasoning into the DAC process. The central to our method lies in the\nContext-aware Keypoint Reasoner, which enables TechCoach to learn\nkeypoint-related quality representations by querying visual context under the\nsupervision of keypoint-level coaching commentary. Prompted by the visual\ncontext and the keypoint-related quality representations, a unified\nKeypoint-aware Action Assessor is then employed to provide the overall coaching\ncommentary together with the quality score. Combining all of these, we build a\nnew benchmark for DAC and evaluate the effectiveness of our method through\nextensive experiments. Data and code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yuan-Ming Li"
                    },
                    {
                        "name": "An-Lan Wang"
                    },
                    {
                        "name": "Kun-Yu Lin"
                    },
                    {
                        "name": "Yu-Ming Tang"
                    },
                    {
                        "name": "Ling-An Zeng"
                    },
                    {
                        "name": "Jian-Fang Hu"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Shi Zheng"
                },
                "author": "Wei-Shi Zheng",
                "arxiv_comment": "19 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17123v1",
                "updated": "2024-11-26T05:29:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    29,
                    18,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:29:18Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    29,
                    18,
                    1,
                    331,
                    0
                ],
                "title": "Advancing Content Moderation: Evaluating Large Language Models for\n  Detecting Sensitive Content Across Text, Images, and Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Content Moderation: Evaluating Large Language Models for\n  Detecting Sensitive Content Across Text, Images, and Videos"
                },
                "summary": "The widespread dissemination of hate speech, harassment, harmful and sexual\ncontent, and violence across websites and media platforms presents substantial\nchallenges and provokes widespread concern among different sectors of society.\nGovernments, educators, and parents are often at odds with media platforms\nabout how to regulate, control, and limit the spread of such content.\nTechnologies for detecting and censoring the media contents are a key solution\nto addressing these challenges. Techniques from natural language processing and\ncomputer vision have been used widely to automatically identify and filter out\nsensitive content such as offensive languages, violence, nudity, and addiction\nin both text, images, and videos, enabling platforms to enforce content\npolicies at scale. However, existing methods still have limitations in\nachieving high detection accuracy with fewer false positives and false\nnegatives. Therefore, more sophisticated algorithms for understanding the\ncontext of both text and image may open rooms for improvement in content\ncensorship to build a more efficient censorship system. In this paper, we\nevaluate existing LLM-based content moderation solutions such as OpenAI\nmoderation model and Llama-Guard3 and study their capabilities to detect\nsensitive contents. Additionally, we explore recent LLMs such as GPT, Gemini,\nand Llama in identifying inappropriate contents across media outlets. Various\ntextual and visual datasets like X tweets, Amazon reviews, news articles, human\nphotos, cartoons, sketches, and violence videos have been utilized for\nevaluation and comparison. The results demonstrate that LLMs outperform\ntraditional techniques by achieving higher accuracy and lower false positive\nand false negative rates. This highlights the potential to integrate LLMs into\nwebsites, social media platforms, and video-sharing services for regulatory and\ncontent moderation purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread dissemination of hate speech, harassment, harmful and sexual\ncontent, and violence across websites and media platforms presents substantial\nchallenges and provokes widespread concern among different sectors of society.\nGovernments, educators, and parents are often at odds with media platforms\nabout how to regulate, control, and limit the spread of such content.\nTechnologies for detecting and censoring the media contents are a key solution\nto addressing these challenges. Techniques from natural language processing and\ncomputer vision have been used widely to automatically identify and filter out\nsensitive content such as offensive languages, violence, nudity, and addiction\nin both text, images, and videos, enabling platforms to enforce content\npolicies at scale. However, existing methods still have limitations in\nachieving high detection accuracy with fewer false positives and false\nnegatives. Therefore, more sophisticated algorithms for understanding the\ncontext of both text and image may open rooms for improvement in content\ncensorship to build a more efficient censorship system. In this paper, we\nevaluate existing LLM-based content moderation solutions such as OpenAI\nmoderation model and Llama-Guard3 and study their capabilities to detect\nsensitive contents. Additionally, we explore recent LLMs such as GPT, Gemini,\nand Llama in identifying inappropriate contents across media outlets. Various\ntextual and visual datasets like X tweets, Amazon reviews, news articles, human\nphotos, cartoons, sketches, and violence videos have been utilized for\nevaluation and comparison. The results demonstrate that LLMs outperform\ntraditional techniques by achieving higher accuracy and lower false positive\nand false negative rates. This highlights the potential to integrate LLMs into\nwebsites, social media platforms, and video-sharing services for regulatory and\ncontent moderation purposes."
                },
                "authors": [
                    {
                        "name": "Nouar AlDahoul"
                    },
                    {
                        "name": "Myles Joshua Toledo Tan"
                    },
                    {
                        "name": "Harishwar Reddy Kasireddy"
                    },
                    {
                        "name": "Yasir Zaki"
                    }
                ],
                "author_detail": {
                    "name": "Yasir Zaki"
                },
                "author": "Yasir Zaki",
                "arxiv_comment": "55 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13766v2",
                "updated": "2024-11-26T05:12:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    12,
                    26,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-21T00:29:58Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    0,
                    29,
                    58,
                    3,
                    326,
                    0
                ],
                "title": "Tiny-Align: Bridging Automatic Speech Recognition and Large Language\n  Model on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiny-Align: Bridging Automatic Speech Recognition and Large Language\n  Model on the Edge"
                },
                "summary": "The combination of Large Language Models (LLM) and Automatic Speech\nRecognition (ASR), when deployed on edge devices (called edge ASR-LLM), can\nserve as a powerful personalized assistant to enable audio-based interaction\nfor users. Compared to text-based interaction, edge ASR-LLM allows accessible\nand natural audio interactions. Unfortunately, existing ASR-LLM models are\nmainly trained in high-performance computing environments and produce\nsubstantial model weights, making them difficult to deploy on edge devices.\nMore importantly, to better serve users' personalized needs, the ASR-LLM must\nbe able to learn from each distinct user, given that audio input often contains\nhighly personalized characteristics that necessitate personalized on-device\ntraining. Since individually fine-tuning the ASR or LLM often leads to\nsuboptimal results due to modality-specific limitations, end-to-end training\nensures seamless integration of audio features and language understanding\n(cross-modal alignment), ultimately enabling a more personalized and efficient\nadaptation on edge devices. However, due to the complex training requirements\nand substantial computational demands of existing approaches, cross-modal\nalignment between ASR audio and LLM can be challenging on edge devices. In this\nwork, we propose a resource-efficient cross-modal alignment framework that\nbridges ASR and LLMs on edge devices to handle personalized audio input. Our\nframework enables efficient ASR-LLM alignment on resource-constrained devices\nlike NVIDIA Jetson Orin (8GB RAM), achieving 50x training time speedup while\nimproving the alignment quality by more than 50\\%. To the best of our\nknowledge, this is the first work to study efficient ASR-LLM alignment on\nresource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combination of Large Language Models (LLM) and Automatic Speech\nRecognition (ASR), when deployed on edge devices (called edge ASR-LLM), can\nserve as a powerful personalized assistant to enable audio-based interaction\nfor users. Compared to text-based interaction, edge ASR-LLM allows accessible\nand natural audio interactions. Unfortunately, existing ASR-LLM models are\nmainly trained in high-performance computing environments and produce\nsubstantial model weights, making them difficult to deploy on edge devices.\nMore importantly, to better serve users' personalized needs, the ASR-LLM must\nbe able to learn from each distinct user, given that audio input often contains\nhighly personalized characteristics that necessitate personalized on-device\ntraining. Since individually fine-tuning the ASR or LLM often leads to\nsuboptimal results due to modality-specific limitations, end-to-end training\nensures seamless integration of audio features and language understanding\n(cross-modal alignment), ultimately enabling a more personalized and efficient\nadaptation on edge devices. However, due to the complex training requirements\nand substantial computational demands of existing approaches, cross-modal\nalignment between ASR audio and LLM can be challenging on edge devices. In this\nwork, we propose a resource-efficient cross-modal alignment framework that\nbridges ASR and LLMs on edge devices to handle personalized audio input. Our\nframework enables efficient ASR-LLM alignment on resource-constrained devices\nlike NVIDIA Jetson Orin (8GB RAM), achieving 50x training time speedup while\nimproving the alignment quality by more than 50\\%. To the best of our\nknowledge, this is the first work to study efficient ASR-LLM alignment on\nresource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Ruiyang Qin"
                    },
                    {
                        "name": "Dancheng Liu"
                    },
                    {
                        "name": "Gelei Xu"
                    },
                    {
                        "name": "Zheyu Yan"
                    },
                    {
                        "name": "Chenhui Xu"
                    },
                    {
                        "name": "Yuting Hu"
                    },
                    {
                        "name": "X. Sharon Hu"
                    },
                    {
                        "name": "Jinjun Xiong"
                    },
                    {
                        "name": "Yiyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Yiyu Shi"
                },
                "author": "Yiyu Shi",
                "arxiv_comment": "7 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v1",
                "updated": "2024-11-26T05:10:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17110v1",
                "updated": "2024-11-26T05:00:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    0,
                    23,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:00:23Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    0,
                    23,
                    1,
                    331,
                    0
                ],
                "title": "TabulaX: Leveraging Large Language Models for Multi-Class Table\n  Transformations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabulaX: Leveraging Large Language Models for Multi-Class Table\n  Transformations"
                },
                "summary": "The integration of tabular data from diverse sources is often hindered by\ninconsistencies in formatting and representation, posing significant challenges\nfor data analysts and personal digital assistants. Existing methods for\nautomating tabular data transformations are limited in scope, often focusing on\nspecific types of transformations or lacking interpretability. In this paper,\nwe introduce TabulaX, a novel framework that leverages Large Language Models\n(LLMs) for multi-class tabular transformations. TabulaX first classifies input\ntables into four transformation classes (string-based, numerical, algorithmic,\nand general) and then applies tailored methods to generate human-interpretable\ntransformation functions, such as numeric formulas or programming code. This\napproach enhances transparency and allows users to understand and modify the\nmappings. Through extensive experiments on real-world datasets from various\ndomains, we demonstrate that TabulaX outperforms existing state-of-the-art\napproaches in terms of accuracy, supports a broader class of transformations,\nand generates interpretable transformations that can be efficiently applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of tabular data from diverse sources is often hindered by\ninconsistencies in formatting and representation, posing significant challenges\nfor data analysts and personal digital assistants. Existing methods for\nautomating tabular data transformations are limited in scope, often focusing on\nspecific types of transformations or lacking interpretability. In this paper,\nwe introduce TabulaX, a novel framework that leverages Large Language Models\n(LLMs) for multi-class tabular transformations. TabulaX first classifies input\ntables into four transformation classes (string-based, numerical, algorithmic,\nand general) and then applies tailored methods to generate human-interpretable\ntransformation functions, such as numeric formulas or programming code. This\napproach enhances transparency and allows users to understand and modify the\nmappings. Through extensive experiments on real-world datasets from various\ndomains, we demonstrate that TabulaX outperforms existing state-of-the-art\napproaches in terms of accuracy, supports a broader class of transformations,\nand generates interpretable transformations that can be efficiently applied."
                },
                "authors": [
                    {
                        "name": "Arash Dargahi Nobari"
                    },
                    {
                        "name": "Davood Rafiei"
                    }
                ],
                "author_detail": {
                    "name": "Davood Rafiei"
                },
                "author": "Davood Rafiei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17106v1",
                "updated": "2024-11-26T04:49:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    49,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:49:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    49,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "PassionSR: Post-Training Quantization with Adaptive Scale in One-Step\n  Diffusion based Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PassionSR: Post-Training Quantization with Adaptive Scale in One-Step\n  Diffusion based Image Super-Resolution"
                },
                "summary": "Diffusion-based image super-resolution (SR) models have shown superior\nperformance at the cost of multiple denoising steps. However, even though the\ndenoising step has been reduced to one, they require high computational costs\nand storage requirements, making it difficult for deployment on hardware\ndevices. To address these issues, we propose a novel post-training quantization\napproach with adaptive scale in one-step diffusion (OSD) image SR, PassionSR.\nFirst, we simplify OSD model to two core components, UNet and Variational\nAutoencoder (VAE) by removing the CLIPEncoder. Secondly, we propose Learnable\nBoundary Quantizer (LBQ) and Learnable Equivalent Transformation (LET) to\noptimize the quantization process and manipulate activation distributions for\nbetter quantization. Finally, we design a Distributed Quantization Calibration\n(DQC) strategy that stabilizes the training of quantized parameters for rapid\nconvergence. Comprehensive experiments demonstrate that PassionSR with 8-bit\nand 6-bit obtains comparable visual results with full-precision model.\nMoreover, our PassionSR achieves significant advantages over recent leading\nlow-bit quantization methods for image SR. Our code will be at\nhttps://github.com/libozhu03/PassionSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image super-resolution (SR) models have shown superior\nperformance at the cost of multiple denoising steps. However, even though the\ndenoising step has been reduced to one, they require high computational costs\nand storage requirements, making it difficult for deployment on hardware\ndevices. To address these issues, we propose a novel post-training quantization\napproach with adaptive scale in one-step diffusion (OSD) image SR, PassionSR.\nFirst, we simplify OSD model to two core components, UNet and Variational\nAutoencoder (VAE) by removing the CLIPEncoder. Secondly, we propose Learnable\nBoundary Quantizer (LBQ) and Learnable Equivalent Transformation (LET) to\noptimize the quantization process and manipulate activation distributions for\nbetter quantization. Finally, we design a Distributed Quantization Calibration\n(DQC) strategy that stabilizes the training of quantized parameters for rapid\nconvergence. Comprehensive experiments demonstrate that PassionSR with 8-bit\nand 6-bit obtains comparable visual results with full-precision model.\nMoreover, our PassionSR achieves significant advantages over recent leading\nlow-bit quantization methods for image SR. Our code will be at\nhttps://github.com/libozhu03/PassionSR."
                },
                "authors": [
                    {
                        "name": "Libo Zhu"
                    },
                    {
                        "name": "Jianze Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Yong Guo"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "https://github.com/libozhu03/PassionSR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17102v1",
                "updated": "2024-11-26T04:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    39,
                    46,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:39:46Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    39,
                    46,
                    1,
                    331,
                    0
                ],
                "title": "Scholar Name Disambiguation with Search-enhanced LLM Across Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scholar Name Disambiguation with Search-enhanced LLM Across Language"
                },
                "summary": "The task of scholar name disambiguation is crucial in various real-world\nscenarios, including bibliometric-based candidate evaluation for awards,\napplication material anti-fraud measures, and more. Despite significant\nadvancements, current methods face limitations due to the complexity of\nheterogeneous data, often necessitating extensive human intervention. This\npaper proposes a novel approach by leveraging search-enhanced language models\nacross multiple languages to improve name disambiguation. By utilizing the\npowerful query rewriting, intent recognition, and data indexing capabilities of\nsearch engines, our method can gather richer information for distinguishing\nbetween entities and extracting profiles, resulting in a more comprehensive\ndata dimension. Given the strong cross-language capabilities of large language\nmodels(LLMs), optimizing enhanced retrieval methods with this technology offers\nsubstantial potential for high-efficiency information retrieval and\nutilization. Our experiments demonstrate that incorporating local languages\nsignificantly enhances disambiguation performance, particularly for scholars\nfrom diverse geographic regions. This multi-lingual, search-enhanced\nmethodology offers a promising direction for more efficient and accurate active\nscholar name disambiguation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of scholar name disambiguation is crucial in various real-world\nscenarios, including bibliometric-based candidate evaluation for awards,\napplication material anti-fraud measures, and more. Despite significant\nadvancements, current methods face limitations due to the complexity of\nheterogeneous data, often necessitating extensive human intervention. This\npaper proposes a novel approach by leveraging search-enhanced language models\nacross multiple languages to improve name disambiguation. By utilizing the\npowerful query rewriting, intent recognition, and data indexing capabilities of\nsearch engines, our method can gather richer information for distinguishing\nbetween entities and extracting profiles, resulting in a more comprehensive\ndata dimension. Given the strong cross-language capabilities of large language\nmodels(LLMs), optimizing enhanced retrieval methods with this technology offers\nsubstantial potential for high-efficiency information retrieval and\nutilization. Our experiments demonstrate that incorporating local languages\nsignificantly enhances disambiguation performance, particularly for scholars\nfrom diverse geographic regions. This multi-lingual, search-enhanced\nmethodology offers a promising direction for more efficient and accurate active\nscholar name disambiguation."
                },
                "authors": [
                    {
                        "name": "Renyu Zhao"
                    },
                    {
                        "name": "Yunxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Chen"
                },
                "author": "Yunxin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08067v2",
                "updated": "2024-11-26T04:05:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    5,
                    34,
                    1,
                    331,
                    0
                ],
                "published": "2024-10-10T16:01:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    1,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"
                },
                "summary": "Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses. Striving to maximize\nthe implicit reward gap between the chosen and the slightly inferior rejected\nresponses can cause overfitting and unnecessary unlearning of the high-quality\nrejected responses. The unawareness of the reward scores also drives the LLM to\nindiscriminately favor the low-quality chosen responses and fail to generalize\nto responses with the highest rewards, which are sparse in data. To overcome\nthese shortcomings, our study introduces reward-conditioned LLM policies that\ndiscern and learn from the entire spectrum of response quality within the\ndataset, helping extrapolate to more optimal regions. We propose an effective\nyet simple data relabeling method that conditions the preference pairs on\nquality scores to construct a reward-augmented dataset. This dataset is easily\nintegrated with existing direct alignment algorithms and is applicable to any\npreference dataset. The experimental results across instruction-following\nbenchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate that\nour approach consistently boosts the performance of DPO by a considerable\nmargin across diverse models. Additionally, our method improves the average\naccuracy on various academic benchmarks. When applying our method to on-policy\ndata, the resulting DPO model achieves SOTA results on AlpacaEval. Through\nablation studies, we demonstrate that our method not only maximizes the utility\nof preference data but also mitigates the issue of unlearning, demonstrating\nits broad effectiveness beyond mere dataset expansion. Our code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses. Striving to maximize\nthe implicit reward gap between the chosen and the slightly inferior rejected\nresponses can cause overfitting and unnecessary unlearning of the high-quality\nrejected responses. The unawareness of the reward scores also drives the LLM to\nindiscriminately favor the low-quality chosen responses and fail to generalize\nto responses with the highest rewards, which are sparse in data. To overcome\nthese shortcomings, our study introduces reward-conditioned LLM policies that\ndiscern and learn from the entire spectrum of response quality within the\ndataset, helping extrapolate to more optimal regions. We propose an effective\nyet simple data relabeling method that conditions the preference pairs on\nquality scores to construct a reward-augmented dataset. This dataset is easily\nintegrated with existing direct alignment algorithms and is applicable to any\npreference dataset. The experimental results across instruction-following\nbenchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate that\nour approach consistently boosts the performance of DPO by a considerable\nmargin across diverse models. Additionally, our method improves the average\naccuracy on various academic benchmarks. When applying our method to on-policy\ndata, the resulting DPO model achieves SOTA results on AlpacaEval. Through\nablation studies, we demonstrate that our method not only maximizes the utility\nof preference data but also mitigates the issue of unlearning, demonstrating\nits broad effectiveness beyond mere dataset expansion. Our code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference."
                },
                "authors": [
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Zhaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoran Wang"
                },
                "author": "Zhaoran Wang",
                "arxiv_comment": "Temporarily modified the author list pending verification from\n  companies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v1",
                "updated": "2024-11-26T04:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17767v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17767v3",
                "updated": "2024-11-26T03:54:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    3,
                    54,
                    9,
                    1,
                    331,
                    0
                ],
                "published": "2024-05-28T02:46:11Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    2,
                    46,
                    11,
                    1,
                    149,
                    0
                ],
                "title": "Linguistic Collapse: Neural Collapse in (Large) Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic Collapse: Neural Collapse in (Large) Language Models"
                },
                "summary": "Neural collapse ($\\mathcal{NC}$) is a phenomenon observed in classification\ntasks where top-layer representations collapse into their class means, which\nbecome equinorm, equiangular and aligned with the classifiers. These behaviours\n-- associated with generalization and robustness -- would manifest under\nspecific conditions: models are trained towards zero loss, with noise-free\nlabels belonging to balanced classes, which do not outnumber the model's hidden\ndimension. Recent studies have explored $\\mathcal{NC}$ in the absence of one or\nmore of these conditions to extend and capitalize on the associated benefits of\nideal geometries. Language modelling presents a curious frontier, as\n\\textit{training by token prediction} constitutes a classification task where\nnone of the conditions exist: the vocabulary is imbalanced and exceeds the\nembedding dimension; different tokens might correspond to similar contextual\nembeddings; and large language models (LLMs) in particular are typically only\ntrained for a few epochs. This paper empirically investigates the impact of\nscaling the architectures and training of causal language models (CLMs) on\ntheir progression towards $\\mathcal{NC}$. We find that $\\mathcal{NC}$\nproperties that develop with scale (and regularization) are linked to\ngeneralization. Moreover, there is evidence of some relationship between\n$\\mathcal{NC}$ and generalization independent of scale. Our work thereby\nunderscores the generality of $\\mathcal{NC}$ as it extends to the novel and\nmore challenging setting of language modelling. Downstream, we seek to inspire\nfurther research on the phenomenon to deepen our understanding of LLMs -- and\nneural networks at large -- and improve existing architectures based on\n$\\mathcal{NC}$-related properties. Our code is hosted on GitHub at\nhttps://github.com/rhubarbwu/linguistic-collapse .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural collapse ($\\mathcal{NC}$) is a phenomenon observed in classification\ntasks where top-layer representations collapse into their class means, which\nbecome equinorm, equiangular and aligned with the classifiers. These behaviours\n-- associated with generalization and robustness -- would manifest under\nspecific conditions: models are trained towards zero loss, with noise-free\nlabels belonging to balanced classes, which do not outnumber the model's hidden\ndimension. Recent studies have explored $\\mathcal{NC}$ in the absence of one or\nmore of these conditions to extend and capitalize on the associated benefits of\nideal geometries. Language modelling presents a curious frontier, as\n\\textit{training by token prediction} constitutes a classification task where\nnone of the conditions exist: the vocabulary is imbalanced and exceeds the\nembedding dimension; different tokens might correspond to similar contextual\nembeddings; and large language models (LLMs) in particular are typically only\ntrained for a few epochs. This paper empirically investigates the impact of\nscaling the architectures and training of causal language models (CLMs) on\ntheir progression towards $\\mathcal{NC}$. We find that $\\mathcal{NC}$\nproperties that develop with scale (and regularization) are linked to\ngeneralization. Moreover, there is evidence of some relationship between\n$\\mathcal{NC}$ and generalization independent of scale. Our work thereby\nunderscores the generality of $\\mathcal{NC}$ as it extends to the novel and\nmore challenging setting of language modelling. Downstream, we seek to inspire\nfurther research on the phenomenon to deepen our understanding of LLMs -- and\nneural networks at large -- and improve existing architectures based on\n$\\mathcal{NC}$-related properties. Our code is hosted on GitHub at\nhttps://github.com/rhubarbwu/linguistic-collapse ."
                },
                "authors": [
                    {
                        "name": "Robert Wu"
                    },
                    {
                        "name": "Vardan Papyan"
                    }
                ],
                "author_detail": {
                    "name": "Vardan Papyan"
                },
                "author": "Vardan Papyan",
                "arxiv_comment": "NeurIPS 2024; 35 pages; 30 figures; reverted to log mean norms for\n  NC2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17767v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17767v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07 (Primary) 68T50 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17080v1",
                "updated": "2024-11-26T03:41:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    3,
                    41,
                    1,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T03:41:01Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    3,
                    41,
                    1,
                    1,
                    331,
                    0
                ],
                "title": "DeepMDV: Learning Global Matching for Multi-depot Vehicle Routing\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepMDV: Learning Global Matching for Multi-depot Vehicle Routing\n  Problems"
                },
                "summary": "Due to the substantial rise in online retail and e-commerce in recent years,\nthe demand for efficient and fast solutions to Vehicle Routing Problems (VRP)\nhas become critical. To manage the increasing demand, companies have adopted\nthe strategy of adding more depots. However, the presence of multiple depots\nintroduces additional complexities, making existing VRP solutions suboptimal\nfor addressing the Multi-depot Vehicle Routing Problem (MDVRP). Traditional\nmethods for solving the MDVRP often require significant computation time,\nmaking them unsuitable for large-scale instances. Additionally, existing\nlearning-based solutions for the MDVRP struggle with generalizability and fail\nto deliver high-quality results for scenarios involving a large number of\ncustomers. In this paper, we propose a novel solution for MDVRP. Our approach\nemploys an attention mechanism, featuring a decoder with two key layers: one\nlayer to consider the states of all vehicles and learn to select the most\nsuitable vehicle based on the proximity of unassigned customers, and another\nlayer to focus on assigning a customer to the selected vehicle. This approach\ndelivers high-quality solutions for large-scale MDVRP instances and\ndemonstrates remarkable generalizability across varying numbers of customers\nand depots. Its adaptability and performance make it a practical and deployable\nsolution for real-world logistics challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the substantial rise in online retail and e-commerce in recent years,\nthe demand for efficient and fast solutions to Vehicle Routing Problems (VRP)\nhas become critical. To manage the increasing demand, companies have adopted\nthe strategy of adding more depots. However, the presence of multiple depots\nintroduces additional complexities, making existing VRP solutions suboptimal\nfor addressing the Multi-depot Vehicle Routing Problem (MDVRP). Traditional\nmethods for solving the MDVRP often require significant computation time,\nmaking them unsuitable for large-scale instances. Additionally, existing\nlearning-based solutions for the MDVRP struggle with generalizability and fail\nto deliver high-quality results for scenarios involving a large number of\ncustomers. In this paper, we propose a novel solution for MDVRP. Our approach\nemploys an attention mechanism, featuring a decoder with two key layers: one\nlayer to consider the states of all vehicles and learn to select the most\nsuitable vehicle based on the proximity of unassigned customers, and another\nlayer to focus on assigning a customer to the selected vehicle. This approach\ndelivers high-quality solutions for large-scale MDVRP instances and\ndemonstrates remarkable generalizability across varying numbers of customers\nand depots. Its adaptability and performance make it a practical and deployable\nsolution for real-world logistics challenges."
                },
                "authors": [
                    {
                        "name": "Saeed Nasehi"
                    },
                    {
                        "name": "Farhana Choudhury"
                    },
                    {
                        "name": "Egemen Tanin"
                    }
                ],
                "author_detail": {
                    "name": "Egemen Tanin"
                },
                "author": "Egemen Tanin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.06062v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.06062v4",
                "updated": "2024-11-26T03:19:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    3,
                    19,
                    15,
                    1,
                    331,
                    0
                ],
                "published": "2023-11-10T13:55:05Z",
                "published_parsed": [
                    2023,
                    11,
                    10,
                    13,
                    55,
                    5,
                    4,
                    314,
                    0
                ],
                "title": "Practical Membership Inference Attacks against Fine-tuned Large Language\n  Models via Self-prompt Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Membership Inference Attacks against Fine-tuned Large Language\n  Models via Self-prompt Calibration"
                },
                "summary": "Membership Inference Attacks (MIA) aim to infer whether a target data record\nhas been utilized for model training or not. Existing MIAs designed for large\nlanguage models (LLMs) can be bifurcated into two types: reference-free and\nreference-based attacks. Although reference-based attacks appear promising\nperformance by calibrating the probability measured on the target model with\nreference models, this illusion of privacy risk heavily depends on a reference\ndataset that closely resembles the training set. Both two types of attacks are\npredicated on the hypothesis that training records consistently maintain a\nhigher probability of being sampled. However, this hypothesis heavily relies on\nthe overfitting of target models, which will be mitigated by multiple\nregularization methods and the generalization of LLMs. Thus, these reasons lead\nto high false-positive rates of MIAs in practical scenarios. We propose a\nMembership Inference Attack based on Self-calibrated Probabilistic Variation\n(SPV-MIA). Specifically, we introduce a self-prompt approach, which constructs\nthe dataset to fine-tune the reference model by prompting the target LLM\nitself. In this manner, the adversary can collect a dataset with a similar\ndistribution from public APIs. Furthermore, we introduce probabilistic\nvariation, a more reliable membership signal based on LLM memorization rather\nthan overfitting, from which we rediscover the neighbour attack with\ntheoretical grounding. Comprehensive evaluation conducted on three datasets and\nfour exemplary LLMs shows that SPV-MIA raises the AUC of MIAs from 0.7 to a\nsignificantly high level of 0.9. Our code and dataset are available at:\nhttps://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks (MIA) aim to infer whether a target data record\nhas been utilized for model training or not. Existing MIAs designed for large\nlanguage models (LLMs) can be bifurcated into two types: reference-free and\nreference-based attacks. Although reference-based attacks appear promising\nperformance by calibrating the probability measured on the target model with\nreference models, this illusion of privacy risk heavily depends on a reference\ndataset that closely resembles the training set. Both two types of attacks are\npredicated on the hypothesis that training records consistently maintain a\nhigher probability of being sampled. However, this hypothesis heavily relies on\nthe overfitting of target models, which will be mitigated by multiple\nregularization methods and the generalization of LLMs. Thus, these reasons lead\nto high false-positive rates of MIAs in practical scenarios. We propose a\nMembership Inference Attack based on Self-calibrated Probabilistic Variation\n(SPV-MIA). Specifically, we introduce a self-prompt approach, which constructs\nthe dataset to fine-tune the reference model by prompting the target LLM\nitself. In this manner, the adversary can collect a dataset with a similar\ndistribution from public APIs. Furthermore, we introduce probabilistic\nvariation, a more reliable membership signal based on LLM memorization rather\nthan overfitting, from which we rediscover the neighbour attack with\ntheoretical grounding. Comprehensive evaluation conducted on three datasets and\nfour exemplary LLMs shows that SPV-MIA raises the AUC of MIAs from 0.7 to a\nsignificantly high level of 0.9. Our code and dataset are available at:\nhttps://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA"
                },
                "authors": [
                    {
                        "name": "Wenjie Fu"
                    },
                    {
                        "name": "Huandong Wang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Guanghua Liu"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Tao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Jiang"
                },
                "author": "Tao Jiang",
                "arxiv_comment": "Repo: https://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA",
                "arxiv_journal_ref": "The Thirty-eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.06062v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.06062v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17066v1",
                "updated": "2024-11-26T03:06:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    3,
                    6,
                    52,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T03:06:52Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    3,
                    6,
                    52,
                    1,
                    331,
                    0
                ],
                "title": "Relations, Negations, and Numbers: Looking for Logic in Generative\n  Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relations, Negations, and Numbers: Looking for Logic in Generative\n  Text-to-Image Models"
                },
                "summary": "Despite remarkable progress in multi-modal AI research, there is a salient\ndomain in which modern AI continues to lag considerably behind even human\nchildren: the reliable deployment of logical operators. Here, we examine three\nforms of logical operators: relations, negations, and discrete numbers. We\nasked human respondents (N=178 in total) to evaluate images generated by a\nstate-of-the-art image-generating AI (DALL-E 3) prompted with these `logical\nprobes', and find that none reliably produce human agreement scores greater\nthan 50\\%. The negation probes and numbers (beyond 3) fail most frequently. In\na 4th experiment, we assess a `grounded diffusion' pipeline that leverages\ntargeted prompt engineering and structured intermediate representations for\ngreater compositional control, but find its performance is judged even worse\nthan that of DALL-E 3 across prompts. To provide further clarity on potential\nsources of success and failure in these text-to-image systems, we supplement\nour 4 core experiments with multiple auxiliary analyses and schematic diagrams,\ndirectly quantifying, for example, the relationship between the N-gram\nfrequency of relational prompts and the average match to generated images; the\nsuccess rates for 3 different prompt modification strategies in the rendering\nof negation prompts; and the scalar variability / ratio dependence\n(`approximate numeracy') of prompts involving integers. We conclude by\ndiscussing the limitations inherent to `grounded' multimodal learning systems\nwhose grounding relies heavily on vector-based semantics (e.g. DALL-E 3), or\nunder-specified syntactical constraints (e.g. `grounded diffusion'), and\npropose minimal modifications (inspired by development, based in imagery) that\ncould help to bridge the lingering compositional gap between scale and\nstructure. All data and code is available at\nhttps://github.com/ColinConwell/T2I-Probology",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite remarkable progress in multi-modal AI research, there is a salient\ndomain in which modern AI continues to lag considerably behind even human\nchildren: the reliable deployment of logical operators. Here, we examine three\nforms of logical operators: relations, negations, and discrete numbers. We\nasked human respondents (N=178 in total) to evaluate images generated by a\nstate-of-the-art image-generating AI (DALL-E 3) prompted with these `logical\nprobes', and find that none reliably produce human agreement scores greater\nthan 50\\%. The negation probes and numbers (beyond 3) fail most frequently. In\na 4th experiment, we assess a `grounded diffusion' pipeline that leverages\ntargeted prompt engineering and structured intermediate representations for\ngreater compositional control, but find its performance is judged even worse\nthan that of DALL-E 3 across prompts. To provide further clarity on potential\nsources of success and failure in these text-to-image systems, we supplement\nour 4 core experiments with multiple auxiliary analyses and schematic diagrams,\ndirectly quantifying, for example, the relationship between the N-gram\nfrequency of relational prompts and the average match to generated images; the\nsuccess rates for 3 different prompt modification strategies in the rendering\nof negation prompts; and the scalar variability / ratio dependence\n(`approximate numeracy') of prompts involving integers. We conclude by\ndiscussing the limitations inherent to `grounded' multimodal learning systems\nwhose grounding relies heavily on vector-based semantics (e.g. DALL-E 3), or\nunder-specified syntactical constraints (e.g. `grounded diffusion'), and\npropose minimal modifications (inspired by development, based in imagery) that\ncould help to bridge the lingering compositional gap between scale and\nstructure. All data and code is available at\nhttps://github.com/ColinConwell/T2I-Probology"
                },
                "authors": [
                    {
                        "name": "Colin Conwell"
                    },
                    {
                        "name": "Rupert Tawiah-Quashie"
                    },
                    {
                        "name": "Tomer Ullman"
                    }
                ],
                "author_detail": {
                    "name": "Tomer Ullman"
                },
                "author": "Tomer Ullman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17065v1",
                "updated": "2024-11-26T03:06:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    3,
                    6,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T03:06:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    3,
                    6,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Creative Agents: Simulating the Systems Model of Creativity with\n  Generative Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative Agents: Simulating the Systems Model of Creativity with\n  Generative Agents"
                },
                "summary": "With the growing popularity of generative AI for images, video, and music, we\nwitnessed models rapidly improve in quality and performance. However, not much\nattention is paid towards enabling AI's ability to \"be creative\". In this\nstudy, we implemented and simulated the systems model of creativity (proposed\nby Csikszentmihalyi) using virtual agents utilizing large language models\n(LLMs) and text prompts. For comparison, the simulations were conducted with\nthe \"virtual artists\" being: 1)isolated and 2)placed in a multi-agent system.\nBoth scenarios were compared by analyzing the variations and overall\n\"creativity\" in the generated artifacts (measured via a user study and LLM).\nOur results suggest that the generative agents may perform better in the\nframework of the systems model of creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing popularity of generative AI for images, video, and music, we\nwitnessed models rapidly improve in quality and performance. However, not much\nattention is paid towards enabling AI's ability to \"be creative\". In this\nstudy, we implemented and simulated the systems model of creativity (proposed\nby Csikszentmihalyi) using virtual agents utilizing large language models\n(LLMs) and text prompts. For comparison, the simulations were conducted with\nthe \"virtual artists\" being: 1)isolated and 2)placed in a multi-agent system.\nBoth scenarios were compared by analyzing the variations and overall\n\"creativity\" in the generated artifacts (measured via a user study and LLM).\nOur results suggest that the generative agents may perform better in the\nframework of the systems model of creativity."
                },
                "authors": [
                    {
                        "name": "Naomi Imasato"
                    },
                    {
                        "name": "Kazuki Miyazawa"
                    },
                    {
                        "name": "Takayuki Nagai"
                    },
                    {
                        "name": "Takato Horii"
                    }
                ],
                "author_detail": {
                    "name": "Takato Horii"
                },
                "author": "Takato Horii",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17058v1",
                "updated": "2024-11-26T02:57:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    2,
                    57,
                    28,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T02:57:28Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    2,
                    57,
                    28,
                    1,
                    331,
                    0
                ],
                "title": "ThreatModeling-LLM: Automating Threat Modeling using Large Language\n  Models for Banking System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThreatModeling-LLM: Automating Threat Modeling using Large Language\n  Models for Banking System"
                },
                "summary": "Threat modeling is a crucial component of cybersecurity, particularly for\nindustries such as banking, where the security of financial data is paramount.\nTraditional threat modeling approaches require expert intervention and manual\neffort, often leading to inefficiencies and human error. The advent of Large\nLanguage Models (LLMs) offers a promising avenue for automating these\nprocesses, enhancing both efficiency and efficacy. However, this transition is\nnot straightforward due to three main challenges: (1) the lack of publicly\navailable, domain-specific datasets, (2) the need for tailored models to handle\ncomplex banking system architectures, and (3) the requirement for real-time,\nadaptive mitigation strategies that align with compliance standards like NIST\n800-53.\n  In this paper, we introduce ThreatModeling-LLM, a novel and adaptable\nframework that automates threat modeling for banking systems using LLMs.\nThreatModeling-LLM operates in three stages: 1) dataset creation, 2) prompt\nengineering and 3) model fine-tuning. We first generate a benchmark dataset\nusing Microsoft Threat Modeling Tool (TMT). Then, we apply Chain of Thought\n(CoT) and Optimization by PROmpting (OPRO) on the pre-trained LLMs to optimize\nthe initial prompt. Lastly, we fine-tune the LLM using Low-Rank Adaptation\n(LoRA) based on the benchmark dataset and the optimized prompt to improve the\nthreat identification and mitigation generation capabilities of pre-trained\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Threat modeling is a crucial component of cybersecurity, particularly for\nindustries such as banking, where the security of financial data is paramount.\nTraditional threat modeling approaches require expert intervention and manual\neffort, often leading to inefficiencies and human error. The advent of Large\nLanguage Models (LLMs) offers a promising avenue for automating these\nprocesses, enhancing both efficiency and efficacy. However, this transition is\nnot straightforward due to three main challenges: (1) the lack of publicly\navailable, domain-specific datasets, (2) the need for tailored models to handle\ncomplex banking system architectures, and (3) the requirement for real-time,\nadaptive mitigation strategies that align with compliance standards like NIST\n800-53.\n  In this paper, we introduce ThreatModeling-LLM, a novel and adaptable\nframework that automates threat modeling for banking systems using LLMs.\nThreatModeling-LLM operates in three stages: 1) dataset creation, 2) prompt\nengineering and 3) model fine-tuning. We first generate a benchmark dataset\nusing Microsoft Threat Modeling Tool (TMT). Then, we apply Chain of Thought\n(CoT) and Optimization by PROmpting (OPRO) on the pre-trained LLMs to optimize\nthe initial prompt. Lastly, we fine-tune the LLM using Low-Rank Adaptation\n(LoRA) based on the benchmark dataset and the optimized prompt to improve the\nthreat identification and mitigation generation capabilities of pre-trained\nLLMs."
                },
                "authors": [
                    {
                        "name": "Shuiqiao Yang"
                    },
                    {
                        "name": "Tingmin Wu"
                    },
                    {
                        "name": "Shigang Liu"
                    },
                    {
                        "name": "David Nguyen"
                    },
                    {
                        "name": "Seung Jang"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    }
                ],
                "author_detail": {
                    "name": "Alsharif Abuadbba"
                },
                "author": "Alsharif Abuadbba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13789v2",
                "updated": "2024-11-26T02:54:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    2,
                    54,
                    7,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-21T02:22:35Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    22,
                    35,
                    3,
                    326,
                    0
                ],
                "title": "LEADRE: Multi-Faceted Knowledge Enhanced LLM Empowered Display\n  Advertisement Recommender System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEADRE: Multi-Faceted Knowledge Enhanced LLM Empowered Display\n  Advertisement Recommender System"
                },
                "summary": "Display advertising provides significant value to advertisers, publishers,\nand users. Traditional display advertising systems utilize a multi-stage\narchitecture consisting of retrieval, coarse ranking, and final ranking.\nHowever, conventional retrieval methods rely on ID-based learning to rank\nmechanisms and fail to adequately utilize the content information of ads, which\nhampers their ability to provide diverse recommendation lists.\n  To address this limitation, we propose leveraging the extensive world\nknowledge of LLMs. However, three key challenges arise when attempting to\nmaximize the effectiveness of LLMs: \"How to capture user interests\", \"How to\nbridge the knowledge gap between LLMs and advertising system\", and \"How to\nefficiently deploy LLMs\". To overcome these challenges, we introduce a novel\nLLM-based framework called LLM Empowered Display ADvertisement REcommender\nsystem (LEADRE). LEADRE consists of three core modules: (1) The Intent-Aware\nPrompt Engineering introduces multi-faceted knowledge and designs intent-aware\n<Prompt, Response> pairs that fine-tune LLMs to generate ads tailored to users'\npersonal interests. (2) The Advertising-Specific Knowledge Alignment\nincorporates auxiliary fine-tuning tasks and Direct Preference Optimization\n(DPO) to align LLMs with ad semantic and business value. (3) The Efficient\nSystem Deployment deploys LEADRE in an online environment by integrating both\nlatency-tolerant and latency-sensitive service. Extensive offline experiments\ndemonstrate the effectiveness of LEADRE and validate the contributions of\nindividual modules. Online A/B test shows that LEADRE leads to a 1.57% and\n1.17% GMV lift for serviced users on WeChat Channels and Moments separately.\nLEADRE has been deployed on both platforms, serving tens of billions of\nrequests each day.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Display advertising provides significant value to advertisers, publishers,\nand users. Traditional display advertising systems utilize a multi-stage\narchitecture consisting of retrieval, coarse ranking, and final ranking.\nHowever, conventional retrieval methods rely on ID-based learning to rank\nmechanisms and fail to adequately utilize the content information of ads, which\nhampers their ability to provide diverse recommendation lists.\n  To address this limitation, we propose leveraging the extensive world\nknowledge of LLMs. However, three key challenges arise when attempting to\nmaximize the effectiveness of LLMs: \"How to capture user interests\", \"How to\nbridge the knowledge gap between LLMs and advertising system\", and \"How to\nefficiently deploy LLMs\". To overcome these challenges, we introduce a novel\nLLM-based framework called LLM Empowered Display ADvertisement REcommender\nsystem (LEADRE). LEADRE consists of three core modules: (1) The Intent-Aware\nPrompt Engineering introduces multi-faceted knowledge and designs intent-aware\n<Prompt, Response> pairs that fine-tune LLMs to generate ads tailored to users'\npersonal interests. (2) The Advertising-Specific Knowledge Alignment\nincorporates auxiliary fine-tuning tasks and Direct Preference Optimization\n(DPO) to align LLMs with ad semantic and business value. (3) The Efficient\nSystem Deployment deploys LEADRE in an online environment by integrating both\nlatency-tolerant and latency-sensitive service. Extensive offline experiments\ndemonstrate the effectiveness of LEADRE and validate the contributions of\nindividual modules. Online A/B test shows that LEADRE leads to a 1.57% and\n1.17% GMV lift for serviced users on WeChat Channels and Moments separately.\nLEADRE has been deployed on both platforms, serving tens of billions of\nrequests each day."
                },
                "authors": [
                    {
                        "name": "Fengxin Li"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Chao Zhou"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Xiaoxiang Deng"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Dapeng Liu"
                    },
                    {
                        "name": "Lei Xiao"
                    },
                    {
                        "name": "Haijie Gu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Hongyan Liu"
                    },
                    {
                        "name": "Biao Qin"
                    },
                    {
                        "name": "Jun He"
                    }
                ],
                "author_detail": {
                    "name": "Jun He"
                },
                "author": "Jun He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14575v3",
                "updated": "2024-11-26T02:30:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    2,
                    30,
                    41,
                    1,
                    331,
                    0
                ],
                "published": "2024-08-26T18:48:51Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    18,
                    48,
                    51,
                    0,
                    239,
                    0
                ],
                "title": "EVINCE: Optimizing Adversarial LLM Dialogues via Conditional Statistics\n  and Information Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVINCE: Optimizing Adversarial LLM Dialogues via Conditional Statistics\n  and Information Theory"
                },
                "summary": "This paper introduces EVINCE (Entropy and Variation IN Conditional\nExchanges), a framework that optimizes multi-LLM dialogues using conditional\nstatistics and information theory. EVINCE introduces dual entropy optimization\nto balance perspective diversity with prior knowledge, providing quantitative\nmeasures for modulating LLM interactions. Through information-theoretic metrics\nand mutual information optimization, the framework demonstrates consistent\nimprovement over single-LLM performance in applications ranging from disease\ndiagnosis to news debiasing. We present theoretical foundations and empirical\nvalidation for this structured approach to LLM collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces EVINCE (Entropy and Variation IN Conditional\nExchanges), a framework that optimizes multi-LLM dialogues using conditional\nstatistics and information theory. EVINCE introduces dual entropy optimization\nto balance perspective diversity with prior knowledge, providing quantitative\nmeasures for modulating LLM interactions. Through information-theoretic metrics\nand mutual information optimization, the framework demonstrates consistent\nimprovement over single-LLM performance in applications ranging from disease\ndiagnosis to news debiasing. We present theoretical foundations and empirical\nvalidation for this structured approach to LLM collaboration."
                },
                "authors": [
                    {
                        "name": "Edward Y. Chang"
                    }
                ],
                "author_detail": {
                    "name": "Edward Y. Chang"
                },
                "author": "Edward Y. Chang",
                "arxiv_comment": "32 pages, 9 figures, 10 tables. arXiv admin note: substantial text\n  overlap with arXiv:2405.15808",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17045v1",
                "updated": "2024-11-26T02:23:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    2,
                    23,
                    30,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T02:23:30Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    2,
                    23,
                    30,
                    1,
                    331,
                    0
                ],
                "title": "Redefining Crowdsourced Test Report Prioritization: An Innovative\n  Approach with Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining Crowdsourced Test Report Prioritization: An Innovative\n  Approach with Large Language Model"
                },
                "summary": "Context: Crowdsourced testing has gained popularity in software testing,\nespecially for mobile app testing, due to its ability to bring diversity and\ntackle fragmentation issues. However, the openness of crowdsourced testing\npresents challenges, particularly in the manual review of numerous test\nreports, which is time-consuming and labor-intensive. Objective: The primary\ngoal of this research is to improve the efficiency of review processes in\ncrowdsourced testing. Traditional approaches to test report prioritization lack\na deep understanding of semantic information in textual descriptions of these\nreports. This paper introduces LLMPrior, a novel approach for prioritizing\ncrowdsourced test reports using large language models (LLMs). Method: LLMPrior\nleverages LLMs for the analysis and clustering of crowdsourced test reports\nbased on the types of bugs revealed in their textual descriptions. This\ninvolves using prompt engineering techniques to enhance the performance of\nLLMs. Following the clustering, a recurrent selection algorithm is applied to\nprioritize the reports. Results: Empirical experiments are conducted to\nevaluate the effectiveness of LLMPrior. The findings indicate that LLMPrior not\nonly surpasses current state-of-the-art approaches in terms of performance but\nalso proves to be more feasible, efficient, and reliable. This success is\nattributed to the use of prompt engineering techniques and the cluster-based\nprioritization strategy. Conclusion: LLMPrior represents a significant\nadvancement in crowdsourced test report prioritization. By effectively\nutilizing large language models and a cluster-based strategy, it addresses the\nchallenges in traditional prioritization approaches, offering a more efficient\nand reliable solution for app developers dealing with crowdsourced test\nreports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Crowdsourced testing has gained popularity in software testing,\nespecially for mobile app testing, due to its ability to bring diversity and\ntackle fragmentation issues. However, the openness of crowdsourced testing\npresents challenges, particularly in the manual review of numerous test\nreports, which is time-consuming and labor-intensive. Objective: The primary\ngoal of this research is to improve the efficiency of review processes in\ncrowdsourced testing. Traditional approaches to test report prioritization lack\na deep understanding of semantic information in textual descriptions of these\nreports. This paper introduces LLMPrior, a novel approach for prioritizing\ncrowdsourced test reports using large language models (LLMs). Method: LLMPrior\nleverages LLMs for the analysis and clustering of crowdsourced test reports\nbased on the types of bugs revealed in their textual descriptions. This\ninvolves using prompt engineering techniques to enhance the performance of\nLLMs. Following the clustering, a recurrent selection algorithm is applied to\nprioritize the reports. Results: Empirical experiments are conducted to\nevaluate the effectiveness of LLMPrior. The findings indicate that LLMPrior not\nonly surpasses current state-of-the-art approaches in terms of performance but\nalso proves to be more feasible, efficient, and reliable. This success is\nattributed to the use of prompt engineering techniques and the cluster-based\nprioritization strategy. Conclusion: LLMPrior represents a significant\nadvancement in crowdsourced test report prioritization. By effectively\nutilizing large language models and a cluster-based strategy, it addresses the\nchallenges in traditional prioritization approaches, offering a more efficient\nand reliable solution for app developers dealing with crowdsourced test\nreports."
                },
                "authors": [
                    {
                        "name": "Yuchen Ling"
                    },
                    {
                        "name": "Shengcheng Yu"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Guobin Pan"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Jia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jia Liu"
                },
                "author": "Jia Liu",
                "arxiv_comment": "Accepted by Information and Software Technology in Nov 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17044v1",
                "updated": "2024-11-26T02:22:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    2,
                    22,
                    7,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T02:22:07Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    2,
                    22,
                    7,
                    1,
                    331,
                    0
                ],
                "title": "4D Scaffold Gaussian Splatting for Memory Efficient Dynamic Scene\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "4D Scaffold Gaussian Splatting for Memory Efficient Dynamic Scene\n  Reconstruction"
                },
                "summary": "Existing 4D Gaussian methods for dynamic scene reconstruction offer high\nvisual fidelity and fast rendering. However, these methods suffer from\nexcessive memory and storage demands, which limits their practical deployment.\nThis paper proposes a 4D anchor-based framework that retains visual quality and\nrendering speed of 4D Gaussians while significantly reducing storage costs. Our\nmethod extends 3D scaffolding to 4D space, and leverages sparse 4D grid-aligned\nanchors with compressed feature vectors. Each anchor models a set of neural 4D\nGaussians, each of which represent a local spatiotemporal region. In addition,\nwe introduce a temporal coverage-aware anchor growing strategy to effectively\nassign additional anchors to under-reconstructed dynamic regions. Our method\nadjusts the accumulated gradients based on Gaussians' temporal coverage,\nimproving reconstruction quality in dynamic regions. To reduce the number of\nanchors, we further present enhanced formulations of neural 4D Gaussians. These\ninclude the neural velocity, and the temporal opacity derived from a\ngeneralized Gaussian distribution. Experimental results demonstrate that our\nmethod achieves state-of-the-art visual quality and 97.8% storage reduction\nover 4DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing 4D Gaussian methods for dynamic scene reconstruction offer high\nvisual fidelity and fast rendering. However, these methods suffer from\nexcessive memory and storage demands, which limits their practical deployment.\nThis paper proposes a 4D anchor-based framework that retains visual quality and\nrendering speed of 4D Gaussians while significantly reducing storage costs. Our\nmethod extends 3D scaffolding to 4D space, and leverages sparse 4D grid-aligned\nanchors with compressed feature vectors. Each anchor models a set of neural 4D\nGaussians, each of which represent a local spatiotemporal region. In addition,\nwe introduce a temporal coverage-aware anchor growing strategy to effectively\nassign additional anchors to under-reconstructed dynamic regions. Our method\nadjusts the accumulated gradients based on Gaussians' temporal coverage,\nimproving reconstruction quality in dynamic regions. To reduce the number of\nanchors, we further present enhanced formulations of neural 4D Gaussians. These\ninclude the neural velocity, and the temporal opacity derived from a\ngeneralized Gaussian distribution. Experimental results demonstrate that our\nmethod achieves state-of-the-art visual quality and 97.8% storage reduction\nover 4DGS."
                },
                "authors": [
                    {
                        "name": "Woong Oh Cho"
                    },
                    {
                        "name": "In Cho"
                    },
                    {
                        "name": "Seoha Kim"
                    },
                    {
                        "name": "Jeongmin Bae"
                    },
                    {
                        "name": "Youngjung Uh"
                    },
                    {
                        "name": "Seon Joo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seon Joo Kim"
                },
                "author": "Seon Joo Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17017v1",
                "updated": "2024-11-26T01:00:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    1,
                    0,
                    9,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T01:00:09Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    1,
                    0,
                    9,
                    1,
                    331,
                    0
                ],
                "title": "TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On"
                },
                "summary": "Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional\nefficacy in generating realistic images and preserving garment details, largely\nattributed to the robust generative capabilities of text-to-image (T2I)\ndiffusion backbones. However, the T2I models that underpin these methods have\nbecome outdated, thereby limiting the potential for further improvement in VTO.\nAdditionally, current methods face notable challenges in accurately rendering\ntext on garments without distortion and preserving fine-grained details, such\nas textures and material fidelity. The emergence of Diffusion Transformer (DiT)\nbased T2I models has showcased impressive performance and offers a promising\nopportunity for advancing VTO. Directly applying existing VTO techniques to\ntransformer-based T2I models is ineffective due to substantial architectural\ndifferences, which hinder their ability to fully leverage the models' advanced\ncapabilities for improved text generation. To address these challenges and\nunlock the full potential of DiT-based T2I models for VTO, we propose\nTED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter\nfor enhancing garment-specific features, a Text Preservation Loss to ensure\naccurate and distortion-free text rendering, and a constraint mechanism to\ngenerate prompts by optimizing Large Language Model (LLM). These innovations\nenable state-of-the-art (SOTA) performance in visual quality and text fidelity,\nestablishing a new benchmark for VTO task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional\nefficacy in generating realistic images and preserving garment details, largely\nattributed to the robust generative capabilities of text-to-image (T2I)\ndiffusion backbones. However, the T2I models that underpin these methods have\nbecome outdated, thereby limiting the potential for further improvement in VTO.\nAdditionally, current methods face notable challenges in accurately rendering\ntext on garments without distortion and preserving fine-grained details, such\nas textures and material fidelity. The emergence of Diffusion Transformer (DiT)\nbased T2I models has showcased impressive performance and offers a promising\nopportunity for advancing VTO. Directly applying existing VTO techniques to\ntransformer-based T2I models is ineffective due to substantial architectural\ndifferences, which hinder their ability to fully leverage the models' advanced\ncapabilities for improved text generation. To address these challenges and\nunlock the full potential of DiT-based T2I models for VTO, we propose\nTED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter\nfor enhancing garment-specific features, a Text Preservation Loss to ensure\naccurate and distortion-free text rendering, and a constraint mechanism to\ngenerate prompts by optimizing Large Language Model (LLM). These innovations\nenable state-of-the-art (SOTA) performance in visual quality and text fidelity,\nestablishing a new benchmark for VTO task."
                },
                "authors": [
                    {
                        "name": "Zhenchen Wan"
                    },
                    {
                        "name": "Yanwu Xu"
                    },
                    {
                        "name": "Zhaoqing Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Mingming Gong"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Gong"
                },
                "author": "Mingming Gong",
                "arxiv_comment": "10 pages, 6 figures, 3 tables, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17015v1",
                "updated": "2024-11-26T00:53:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    0,
                    53,
                    36,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T00:53:36Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    0,
                    53,
                    36,
                    1,
                    331,
                    0
                ],
                "title": "Trinity: Synchronizing Verbal, Nonverbal, and Visual Channels to Support\n  Academic Oral Presentation Delivery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trinity: Synchronizing Verbal, Nonverbal, and Visual Channels to Support\n  Academic Oral Presentation Delivery"
                },
                "summary": "Academic Oral Presentation (AOP) allows English-As-Foreign-Language (EFL)\nstudents to express ideas, engage in academic discourse, and present research\nfindings. However, while previous efforts focus on training efficiency or\nspeech assistance, EFL students often face the challenge of seamlessly\nintegrating verbal, nonverbal, and visual elements into their presentations to\navoid coming across as monotonous and unappealing. Based on a need-finding\nsurvey, a design study, and an expert interview, we introduce Trinity, a hybrid\nmobile-centric delivery support system that provides guidance for multichannel\ndelivery on-the-fly. On the desktop side, Trinity facilitates script refinement\nand offers customizable delivery support based on large language models (LLMs).\nBased on the desktop configuration, Trinity App enables a remote mobile visual\ncontrol, multi-level speech pace modulation, and integrated delivery prompts\nfor synchronized delivery. A controlled between-subject user study suggests\nthat Trinity effectively supports AOP delivery and is perceived as\nsignificantly more helpful than baselines, without excessive cognitive load.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic Oral Presentation (AOP) allows English-As-Foreign-Language (EFL)\nstudents to express ideas, engage in academic discourse, and present research\nfindings. However, while previous efforts focus on training efficiency or\nspeech assistance, EFL students often face the challenge of seamlessly\nintegrating verbal, nonverbal, and visual elements into their presentations to\navoid coming across as monotonous and unappealing. Based on a need-finding\nsurvey, a design study, and an expert interview, we introduce Trinity, a hybrid\nmobile-centric delivery support system that provides guidance for multichannel\ndelivery on-the-fly. On the desktop side, Trinity facilitates script refinement\nand offers customizable delivery support based on large language models (LLMs).\nBased on the desktop configuration, Trinity App enables a remote mobile visual\ncontrol, multi-level speech pace modulation, and integrated delivery prompts\nfor synchronized delivery. A controlled between-subject user study suggests\nthat Trinity effectively supports AOP delivery and is perceived as\nsignificantly more helpful than baselines, without excessive cognitive load."
                },
                "authors": [
                    {
                        "name": "Yuchen Wu"
                    },
                    {
                        "name": "Shengxin Li"
                    },
                    {
                        "name": "Shizhen Zhang"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Quan Li"
                    }
                ],
                "author_detail": {
                    "name": "Quan Li"
                },
                "author": "Quan Li",
                "arxiv_comment": "In Proceedings of Chinese CHI 2024 (Best Paper Award)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13611v2",
                "updated": "2024-11-26T00:45:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    0,
                    45,
                    22,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-20T02:03:16Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    3,
                    16,
                    2,
                    325,
                    0
                ],
                "title": "DSTC: Direct Preference Learning with Only Self-Generated Tests and Code\n  to Improve Code LMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSTC: Direct Preference Learning with Only Self-Generated Tests and Code\n  to Improve Code LMs"
                },
                "summary": "Direct preference learning offers a promising and computation-efficient\nbeyond supervised fine-tuning (SFT) for improving code generation in coding\nlarge language models (LMs). However, the scarcity of reliable preference data\nis a bottleneck for the performance of direct preference learning to improve\nthe coding accuracy of code LMs. In this paper, we introduce\n\\underline{\\textbf{D}}irect Preference Learning with Only\n\\underline{\\textbf{S}}elf-Generated \\underline{\\textbf{T}}ests and\n\\underline{\\textbf{C}}ode (DSTC), a framework that leverages only\nself-generated code snippets and tests to construct reliable preference pairs\nsuch that direct preference learning can improve LM coding accuracy without\nexternal annotations. DSTC combines a minimax selection process and test-code\nconcatenation to improve preference pair quality, reducing the influence of\nincorrect self-generated tests and enhancing model performance without the need\nfor costly reward models. When applied with direct preference learning methods\nsuch as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization\n(KTO), DSTC yields stable improvements in coding accuracy (pass@1 score) across\ndiverse coding benchmarks, including HumanEval, MBPP, and BigCodeBench,\ndemonstrating both its effectiveness and scalability for models of various\nsizes. This approach autonomously enhances code generation accuracy across LLMs\nof varying sizes, reducing reliance on expensive annotated coding datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct preference learning offers a promising and computation-efficient\nbeyond supervised fine-tuning (SFT) for improving code generation in coding\nlarge language models (LMs). However, the scarcity of reliable preference data\nis a bottleneck for the performance of direct preference learning to improve\nthe coding accuracy of code LMs. In this paper, we introduce\n\\underline{\\textbf{D}}irect Preference Learning with Only\n\\underline{\\textbf{S}}elf-Generated \\underline{\\textbf{T}}ests and\n\\underline{\\textbf{C}}ode (DSTC), a framework that leverages only\nself-generated code snippets and tests to construct reliable preference pairs\nsuch that direct preference learning can improve LM coding accuracy without\nexternal annotations. DSTC combines a minimax selection process and test-code\nconcatenation to improve preference pair quality, reducing the influence of\nincorrect self-generated tests and enhancing model performance without the need\nfor costly reward models. When applied with direct preference learning methods\nsuch as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization\n(KTO), DSTC yields stable improvements in coding accuracy (pass@1 score) across\ndiverse coding benchmarks, including HumanEval, MBPP, and BigCodeBench,\ndemonstrating both its effectiveness and scalability for models of various\nsizes. This approach autonomously enhances code generation accuracy across LLMs\nof varying sizes, reducing reliance on expensive annotated coding datasets."
                },
                "authors": [
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Zhaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoran Wang"
                },
                "author": "Zhaoran Wang",
                "arxiv_comment": "We temporarily modified the author list because of the pending\n  verification from the company",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16991v1",
                "updated": "2024-11-25T23:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    23,
                    37,
                    48,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T23:37:48Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    23,
                    37,
                    48,
                    0,
                    330,
                    0
                ],
                "title": "Dynamic Self-Distillation via Previous Mini-batches for Fine-tuning\n  Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Self-Distillation via Previous Mini-batches for Fine-tuning\n  Small Language Models"
                },
                "summary": "Knowledge distillation (KD) has become a widely adopted approach for\ncompressing large language models (LLMs) to reduce computational costs and\nmemory footprints. However, the availability of complex teacher models is a\nprerequisite for running most KD pipelines. Thus, the traditional KD procedure\ncan be unachievable or budget-unfriendly, particularly when relying on\ncommercial LLMs like GPT4. In this regard, Self-distillation (SelfD) emerges as\nan advisable alternative, enabling student models to learn without teachers'\nguidance. Nonetheless, existing SelfD approaches for LMs often involve\narchitectural modifications, assuming the models are open-source, which may not\nalways be practical. In this work, we introduce a model-agnostic and\ntask-agnostic method named dynamic SelfD from the previous minibatch (DynSDPB),\nwhich realizes current iterations' distillation from the last ones' generated\nlogits. Additionally, to address prediction inaccuracies during the early\niterations, we dynamically adjust the distillation influence and temperature\nvalues to enhance the adaptability of fine-tuning. Furthermore, DynSDPB is a\nnovel fine-tuning policy that facilitates the seamless integration of existing\nself-correction and self-training techniques for small language models (SLMs)\nbecause they all require updating SLMs' parameters. We demonstrate the superior\nperformance of DynSDPB on both encoder-only LMs (e.g., BERT model families) and\ndecoder-only LMs (e.g., LLaMA model families), validating its effectiveness\nacross natural language understanding (NLU) and natural language generation\n(NLG) benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) has become a widely adopted approach for\ncompressing large language models (LLMs) to reduce computational costs and\nmemory footprints. However, the availability of complex teacher models is a\nprerequisite for running most KD pipelines. Thus, the traditional KD procedure\ncan be unachievable or budget-unfriendly, particularly when relying on\ncommercial LLMs like GPT4. In this regard, Self-distillation (SelfD) emerges as\nan advisable alternative, enabling student models to learn without teachers'\nguidance. Nonetheless, existing SelfD approaches for LMs often involve\narchitectural modifications, assuming the models are open-source, which may not\nalways be practical. In this work, we introduce a model-agnostic and\ntask-agnostic method named dynamic SelfD from the previous minibatch (DynSDPB),\nwhich realizes current iterations' distillation from the last ones' generated\nlogits. Additionally, to address prediction inaccuracies during the early\niterations, we dynamically adjust the distillation influence and temperature\nvalues to enhance the adaptability of fine-tuning. Furthermore, DynSDPB is a\nnovel fine-tuning policy that facilitates the seamless integration of existing\nself-correction and self-training techniques for small language models (SLMs)\nbecause they all require updating SLMs' parameters. We demonstrate the superior\nperformance of DynSDPB on both encoder-only LMs (e.g., BERT model families) and\ndecoder-only LMs (e.g., LLaMA model families), validating its effectiveness\nacross natural language understanding (NLU) and natural language generation\n(NLG) benchmarks."
                },
                "authors": [
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Yin Yu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Runchao Li"
                    },
                    {
                        "name": "Xianxuan Long"
                    },
                    {
                        "name": "Haotian Yu"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16990v1",
                "updated": "2024-11-25T23:36:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    23,
                    36,
                    24,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T23:36:24Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    23,
                    36,
                    24,
                    0,
                    330,
                    0
                ],
                "title": "Enabling Skip Graphs to Process K-Dimensional Range Queries in a Mobile\n  Sensor Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Skip Graphs to Process K-Dimensional Range Queries in a Mobile\n  Sensor Network"
                },
                "summary": "A skip graph is a resilient application-layer routing structure that supports\nrange queries of distributed k-dimensional data. By sorting deterministic keys\ninto groups based on locally computed random membership vectors, nodes in a\nstandard skip graph can optimize range query performance in mobile networks\nsuch as unmanned aerial vehicle swarms. We propose a skip graph extension that\ninverts the key and membership vector roles and bases group membership on\ndeterministic vectors derived from the z-ordering of k-dimensional data and\nsorting within groups is based on locally computed random keys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A skip graph is a resilient application-layer routing structure that supports\nrange queries of distributed k-dimensional data. By sorting deterministic keys\ninto groups based on locally computed random membership vectors, nodes in a\nstandard skip graph can optimize range query performance in mobile networks\nsuch as unmanned aerial vehicle swarms. We propose a skip graph extension that\ninverts the key and membership vector roles and bases group membership on\ndeterministic vectors derived from the z-ordering of k-dimensional data and\nsorting within groups is based on locally computed random keys."
                },
                "authors": [
                    {
                        "name": "Gregory J. Brault"
                    },
                    {
                        "name": "Christopher James Augeri"
                    },
                    {
                        "name": "Barry E. Mullins"
                    },
                    {
                        "name": "Rusty O. Baldwin"
                    },
                    {
                        "name": "Christopher B. Mayer"
                    }
                ],
                "author_detail": {
                    "name": "Christopher B. Mayer"
                },
                "author": "Christopher B. Mayer",
                "arxiv_doi": "10.1109/NCA.2007.18",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/NCA.2007.18",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.16990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "archival pre-print of this k-D distributed prefix-free distributed\n  encoding technique ahead of another arXiv release which will cite this work,\n  specifically with respect to using a more extensive prefix-free encoding\n  technique to localize token partitions in an arbitrary input/output context\n  of an LLM, SSM, or other k-D tokenized model",
                "arxiv_journal_ref": "Proceedings of the 6th IEEE International Symposium on Network\n  Computing and Applications NCA, Cambridge, MA, IEEE, 12-14 July 2007",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.2; E.1; E.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]