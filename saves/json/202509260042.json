[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.03090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v2",
                "updated": "2025-09-24T16:56:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    56,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective"
                },
                "summary": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19729v1",
                "updated": "2025-09-24T03:15:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T03:15:37Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "title": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference"
                },
                "summary": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Haoyu Chen"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Kun Qian"
                    },
                    {
                        "name": "Yu Guan"
                    },
                    {
                        "name": "Jin Zhao"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "12 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v2",
                "updated": "2025-09-24T01:32:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    32,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_doi": "10.1145/3731599.3767498",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731599.3767498",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19599v1",
                "updated": "2025-09-23T21:46:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    46,
                    38,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T21:46:38Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    46,
                    38,
                    1,
                    266,
                    0
                ],
                "title": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing."
                },
                "authors": [
                    {
                        "name": "Danilo Trombino"
                    },
                    {
                        "name": "Vincenzo Pecorella"
                    },
                    {
                        "name": "Alessandro de Giulii"
                    },
                    {
                        "name": "Davide Tresoldi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Tresoldi"
                },
                "author": "Davide Tresoldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v4",
                "updated": "2025-09-23T21:08:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    8,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "CVPR 2025. Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v3",
                "updated": "2025-09-23T20:25:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    20,
                    25,
                    15,
                    1,
                    266,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19459v1",
                "updated": "2025-09-23T18:14:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    18,
                    14,
                    21,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T18:14:21Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    18,
                    14,
                    21,
                    1,
                    266,
                    0
                ],
                "title": "Automated Insertion of Flushes and Fences for Persistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Insertion of Flushes and Fences for Persistency"
                },
                "summary": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations."
                },
                "authors": [
                    {
                        "name": "Yutong Guo"
                    },
                    {
                        "name": "Weiyu Luo"
                    },
                    {
                        "name": "Brian Demsky"
                    }
                ],
                "author_detail": {
                    "name": "Brian Demsky"
                },
                "author": "Brian Demsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19260v1",
                "updated": "2025-09-23T17:18:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    18,
                    59,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:18:59Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    18,
                    59,
                    1,
                    266,
                    0
                ],
                "title": "Reconstruction of a potential parameter in time-fractional diffusion\n  problems via a Kohn--Vogelius type functional: Theoretical aspects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstruction of a potential parameter in time-fractional diffusion\n  problems via a Kohn--Vogelius type functional: Theoretical aspects"
                },
                "summary": "Of concern is the problem of reconstructing a space-dependent potential from\nboundary observations in the Caputo time-fractional diffusion equation,\nutilizing a stable and robust recovery method. We develop an algorithm to\nminimize the Kohn-Vogelius (KV) cost function, which measures the difference\nbetween the solutions of two excitations. The inverse potential problem is\nrecast into an optimization problem, where the objective is to minimize a\nKohn-Vogelius-type functional within a set of admissible potentials. We\nestablish the well-posedness of this optimization problem by proving the\nexistence and uniqueness of a minimizer and demonstrating its stability with\nrespect to perturbations in the boundary data. Furthermore, we analyze the\nFr\\'echet differentiability of the KV functional and prove the Lipschitz\ncontinuity of its gradient. These theoretical results enable the development of\na convergent conjugate gradient algorithm for numerical reconstruction. The\neffectiveness and robustness of the proposed method are confirmed through\nseveral numerical examples in both one and two dimensions, including cases with\nnoisy data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Of concern is the problem of reconstructing a space-dependent potential from\nboundary observations in the Caputo time-fractional diffusion equation,\nutilizing a stable and robust recovery method. We develop an algorithm to\nminimize the Kohn-Vogelius (KV) cost function, which measures the difference\nbetween the solutions of two excitations. The inverse potential problem is\nrecast into an optimization problem, where the objective is to minimize a\nKohn-Vogelius-type functional within a set of admissible potentials. We\nestablish the well-posedness of this optimization problem by proving the\nexistence and uniqueness of a minimizer and demonstrating its stability with\nrespect to perturbations in the boundary data. Furthermore, we analyze the\nFr\\'echet differentiability of the KV functional and prove the Lipschitz\ncontinuity of its gradient. These theoretical results enable the development of\na convergent conjugate gradient algorithm for numerical reconstruction. The\neffectiveness and robustness of the proposed method are confirmed through\nseveral numerical examples in both one and two dimensions, including cases with\nnoisy data."
                },
                "authors": [
                    {
                        "name": "Hamza Kahlaoui"
                    },
                    {
                        "name": "Mourad Hrizi"
                    },
                    {
                        "name": "Abdessamad Oulmelk"
                    },
                    {
                        "name": "Xiangcheng Zheng"
                    },
                    {
                        "name": "Ahmed Hendy"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Hendy"
                },
                "author": "Ahmed Hendy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19228v1",
                "updated": "2025-09-23T16:49:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:49:43Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "title": "CompLLM: Compression for Long Context Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompLLM: Compression for Long Context Q&A"
                },
                "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility."
                },
                "authors": [
                    {
                        "name": "Gabriele Berton"
                    },
                    {
                        "name": "Jayakrishnan Unnikrishnan"
                    },
                    {
                        "name": "Son Tran"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19061v1",
                "updated": "2025-09-23T14:25:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:25:13Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "title": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics"
                },
                "summary": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators."
                },
                "authors": [
                    {
                        "name": "Marcel Ferrari"
                    },
                    {
                        "name": "Cyrill Püntener"
                    },
                    {
                        "name": "Alexander Sotoudeh"
                    },
                    {
                        "name": "Niklas Viebig"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Viebig"
                },
                "author": "Niklas Viebig",
                "arxiv_comment": "15 pages, 5 figures, appendix has 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F08, 65N55, 65N22, 76M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.8; F.2.1; D.1.3; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18909v1",
                "updated": "2025-09-23T12:32:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T12:32:51Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "title": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation"
                },
                "summary": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge."
                },
                "authors": [
                    {
                        "name": "Jan Wichelmann"
                    },
                    {
                        "name": "Anja Rabich"
                    },
                    {
                        "name": "Anna P\"atschke"
                    },
                    {
                        "name": "Thomas Eisenbarth"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Eisenbarth"
                },
                "author": "Thomas Eisenbarth",
                "arxiv_doi": "10.1109/SP54263.2024.00261",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP54263.2024.00261",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.18909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 IEEE Symposium on Security and Privacy (SP), San Francisco,\n  CA, USA, 2024, pp. 4182-4199",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v3",
                "updated": "2025-09-23T08:31:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    31,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v2",
                "updated": "2025-09-23T08:24:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    24,
                    7,
                    1,
                    266,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18684v1",
                "updated": "2025-09-23T06:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T06:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "title": "Static Estimation of Reuse Profiles for Arrays in Nested Loops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Estimation of Reuse Profiles for Arrays in Nested Loops"
                },
                "summary": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "This paper is accepted at the MEMSYS 2025 conference, 11th\n  International Symposium on Memory Systems, Washington D.C., October 7 -\n  October 8, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18670v1",
                "updated": "2025-09-23T05:39:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T05:39:47Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "title": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases"
                },
                "summary": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Kyuri Park"
                    },
                    {
                        "name": "Youngjae Kim"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "arxiv_comment": "11 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18592v1",
                "updated": "2025-09-23T03:23:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T03:23:03Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation"
                },
                "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/."
                },
                "authors": [
                    {
                        "name": "Neel P. Bhatt"
                    },
                    {
                        "name": "Yunhao Yang"
                    },
                    {
                        "name": "Rohan Siva"
                    },
                    {
                        "name": "Pranay Samineni"
                    },
                    {
                        "name": "Daniel Milan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Ufuk Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Ufuk Topcu"
                },
                "author": "Ufuk Topcu",
                "arxiv_comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v2",
                "updated": "2025-09-23T01:58:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    1,
                    58,
                    7,
                    1,
                    266,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00329v2",
                "updated": "2025-09-22T19:20:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    20,
                    33,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-31T00:52:17Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    52,
                    17,
                    5,
                    151,
                    0
                ],
                "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation"
                },
                "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}."
                },
                "authors": [
                    {
                        "name": "Muhammad Adnan"
                    },
                    {
                        "name": "Nithesh Kurella"
                    },
                    {
                        "name": "Akhil Arunkumar"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "arxiv_comment": "Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18344v1",
                "updated": "2025-09-22T19:08:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T19:08:57Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding"
                },
                "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit)."
                },
                "authors": [
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Chun-Che Yang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18307v1",
                "updated": "2025-09-22T18:32:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    18,
                    32,
                    32,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T18:32:32Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    18,
                    32,
                    32,
                    0,
                    265,
                    0
                ],
                "title": "Comparison of Adaptive plan doses using Velocity generated synthetic CT\n  with KV CBCT and re-planning CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparison of Adaptive plan doses using Velocity generated synthetic CT\n  with KV CBCT and re-planning CT"
                },
                "summary": "Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated\nthrough Velocity workstation and compare the target and normal tissue doses\nwith Adaptive plan CT doses.\n  Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation\nTherapy (ART) were included in this retrospective study. Initially, patient\nunderwent treatment with the primary plan. After subsequent indications of\nmajor changes in patients' physicality and anatomy adaptive CT scans were\nacquired as per institutional protocol. Both the primary planning CT and the\nindicative cone-beam CT (CBCT) last acquired before the commencement of the\nadaptive treatment were imported into Velocity workstation. Rigid and\ndeformable image registration techniques were used for the generation of a\nSynthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT)\nfor adaptive plan execution. The primary plan dose was subsequently mapped and\ndeformed onto the Synthetic CT in Velocity workstation, allowing for a\ncomparative dosimetric analysis between the sCT and rCT plan doses. This\ncomparison was conducted in both Velocity and Eclipse, focusing on dose\nvariations across different organs at risk (OARs) and the planning target\nvolume (PTV). Additionally, dosimetric indices were evaluated to assess and\nvalidate the accuracy and quality of the synthetic CT-based dose mapping\nrelative to adaptive planning.\n  Results: The dosimetric comparison between sCT and rCT stated that Mean dose\nfor OARs and PTVs were found to be similar in the two planning and the level of\nconfidence by using T-statistics. Collaborative research has the potential to\neliminate the need of rCT as a standard requirement.\n  Conclusion: The sCT shows comparable CT numbers and doses to the replanning\nCT, suggesting it's potential as a replacement pending clinical correlation and\ncontour adjustments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated\nthrough Velocity workstation and compare the target and normal tissue doses\nwith Adaptive plan CT doses.\n  Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation\nTherapy (ART) were included in this retrospective study. Initially, patient\nunderwent treatment with the primary plan. After subsequent indications of\nmajor changes in patients' physicality and anatomy adaptive CT scans were\nacquired as per institutional protocol. Both the primary planning CT and the\nindicative cone-beam CT (CBCT) last acquired before the commencement of the\nadaptive treatment were imported into Velocity workstation. Rigid and\ndeformable image registration techniques were used for the generation of a\nSynthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT)\nfor adaptive plan execution. The primary plan dose was subsequently mapped and\ndeformed onto the Synthetic CT in Velocity workstation, allowing for a\ncomparative dosimetric analysis between the sCT and rCT plan doses. This\ncomparison was conducted in both Velocity and Eclipse, focusing on dose\nvariations across different organs at risk (OARs) and the planning target\nvolume (PTV). Additionally, dosimetric indices were evaluated to assess and\nvalidate the accuracy and quality of the synthetic CT-based dose mapping\nrelative to adaptive planning.\n  Results: The dosimetric comparison between sCT and rCT stated that Mean dose\nfor OARs and PTVs were found to be similar in the two planning and the level of\nconfidence by using T-statistics. Collaborative research has the potential to\neliminate the need of rCT as a standard requirement.\n  Conclusion: The sCT shows comparable CT numbers and doses to the replanning\nCT, suggesting it's potential as a replacement pending clinical correlation and\ncontour adjustments."
                },
                "authors": [
                    {
                        "name": "Sudam Masanta"
                    },
                    {
                        "name": "Gurvinder Singh"
                    },
                    {
                        "name": "Shefali Pahwa"
                    },
                    {
                        "name": "Shekhar Dwivedi"
                    },
                    {
                        "name": "Devaraju Sampathirao"
                    },
                    {
                        "name": "Ramandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Ramandeep Singh"
                },
                "author": "Ramandeep Singh",
                "arxiv_comment": "8 pages; comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v1",
                "updated": "2025-09-22T17:58:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00919v2",
                "updated": "2025-09-22T16:16:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    16,
                    25,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-02T21:15:07Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    21,
                    15,
                    7,
                    6,
                    33,
                    0
                ],
                "title": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings"
                },
                "summary": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining."
                },
                "authors": [
                    {
                        "name": "Stephen Zhang"
                    },
                    {
                        "name": "Mustafa Khan"
                    },
                    {
                        "name": "Vardan Papyan"
                    }
                ],
                "author_detail": {
                    "name": "Vardan Papyan"
                },
                "author": "Vardan Papyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v2",
                "updated": "2025-09-22T12:28:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    28,
                    41,
                    0,
                    265,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "MaoXun Huang"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "13 pages, accepted as a main conference paper at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v3",
                "updated": "2025-09-22T12:03:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    3,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17650v1",
                "updated": "2025-09-22T11:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T11:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers"
                },
                "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical."
                },
                "authors": [
                    {
                        "name": "Soroush Mahdi"
                    },
                    {
                        "name": "Fardin Ayar"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    },
                    {
                        "name": "Mahdi Javanmardi"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Javanmardi"
                },
                "author": "Mahdi Javanmardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v2",
                "updated": "2025-09-25T10:24:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    24,
                    14,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v1",
                "updated": "2025-09-22T06:52:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v5",
                "updated": "2025-09-25T09:49:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    9,
                    49,
                    59,
                    3,
                    268,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17360v1",
                "updated": "2025-09-22T05:24:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T05:24:22Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "title": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access"
                },
                "summary": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads."
                },
                "authors": [
                    {
                        "name": "Chaoyi Ruan"
                    },
                    {
                        "name": "Chao Bi"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Ziji Shi"
                    },
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Jialin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Li"
                },
                "author": "Jialin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17257v1",
                "updated": "2025-09-21T22:14:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T22:14:56Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "title": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices"
                },
                "summary": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper."
                },
                "authors": [
                    {
                        "name": "Sven Christophersen"
                    }
                ],
                "author_detail": {
                    "name": "Sven Christophersen"
                },
                "author": "Sven Christophersen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F55, 65F08, 65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v1",
                "updated": "2025-09-21T21:05:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07639v2",
                "updated": "2025-09-21T11:48:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    11,
                    48,
                    15,
                    6,
                    264,
                    0
                ],
                "published": "2025-06-09T11:04:13Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse"
                },
                "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment."
                },
                "authors": [
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Shikai Geng"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v2",
                "updated": "2025-09-21T07:03:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    7,
                    3,
                    46,
                    6,
                    264,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11815v2",
                "updated": "2025-09-21T03:35:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    3,
                    35,
                    36,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-15T11:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Fast Speculative Decoding in Vision-Language Models"
                },
                "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM."
                },
                "authors": [
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Fuwei Yang"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Pengju Ren"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16857v1",
                "updated": "2025-09-21T00:59:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T00:59:45Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching"
                },
                "summary": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput."
                },
                "authors": [
                    {
                        "name": "Xingyu Xiang"
                    },
                    {
                        "name": "Raj Joshi"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Chenxingyu Zhao"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Eddie Kohler"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v2",
                "updated": "2025-09-20T13:54:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    54,
                    37,
                    5,
                    263,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "17 pages, 13 figures, the second version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16686v1",
                "updated": "2025-09-20T13:27:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T13:27:13Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "title": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs"
                },
                "summary": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs."
                },
                "authors": [
                    {
                        "name": "Zhengge Cai"
                    },
                    {
                        "name": "Haowen Hou"
                    }
                ],
                "author_detail": {
                    "name": "Haowen Hou"
                },
                "author": "Haowen Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16630v1",
                "updated": "2025-09-20T11:09:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T11:09:01Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation"
                },
                "summary": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/."
                },
                "authors": [
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Hongfa Wang"
                    },
                    {
                        "name": "Heng Pan"
                    },
                    {
                        "name": "Yingqing He"
                    },
                    {
                        "name": "Junkun Yuan"
                    },
                    {
                        "name": "Ailing Zeng"
                    },
                    {
                        "name": "Chengfei Cai"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Zhifeng Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qifeng Chen"
                },
                "author": "Qifeng Chen",
                "arxiv_comment": "accepted by IJCV2025. project\n  page:https://follow-your-emoji.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16495v1",
                "updated": "2025-09-20T01:56:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T01:56:25Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "title": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads"
                },
                "summary": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads."
                },
                "authors": [
                    {
                        "name": "Mert Hidayetoglu"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Michael Wyatt"
                    },
                    {
                        "name": "Jeff Rasley"
                    },
                    {
                        "name": "Yuxiong He"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    }
                ],
                "author_detail": {
                    "name": "Samyam Rajbhandari"
                },
                "author": "Samyam Rajbhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16471v1",
                "updated": "2025-09-19T23:46:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T23:46:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes"
                },
                "summary": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts"
                },
                "authors": [
                    {
                        "name": "Sima Zeinali Danalou"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Niher R. Sarker"
                    },
                    {
                        "name": "Hooman Chamani"
                    },
                    {
                        "name": "Jane Y. Howe"
                    },
                    {
                        "name": "Patrick C. Lee"
                    },
                    {
                        "name": "Jay R. Werber"
                    }
                ],
                "author_detail": {
                    "name": "Jay R. Werber"
                },
                "author": "Jay R. Werber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16407v1",
                "updated": "2025-09-19T20:31:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T20:31:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"
                },
                "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."
                },
                "authors": [
                    {
                        "name": "Hunter McCoy"
                    },
                    {
                        "name": "Prashant Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Pandey"
                },
                "author": "Prashant Pandey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v2",
                "updated": "2025-09-19T17:18:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    18,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts"
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "EMNLP 2025 Main Conference. Mingyuan, Jize, and Haozhen contributed\n  equally, while Minjia, Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v2",
                "updated": "2025-09-19T15:19:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    19,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v2",
                "updated": "2025-09-19T14:14:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    14,
                    32,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped the first prototype of a compact line-focus X-ray tube (LFXT) with\ntechnology potentially suitable for clinical translation of minibeams and\nmicrobeams. We give an overview of the commissioning process preceding first\noperation, present optical and radiological focal spot characterization\nmethods, and dosimetric measurements. Additionally, we report on first\npreclinical in vitro cell and in vivo mouse brain irradiations conducted with\nthe LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The\nfocal spot characterization resulted in a strongly eccentric electron\ndistribution with a width of 72.3 $\\mu$m. Dosimetry showed sharp microbeam dose\nprofiles with steep lateral penumbras and a peak-to-valley dose ratio above 10\nthroughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was\nmeasured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at\n150 mm distance from the focal spot. In vitro and in vivo experiments\ndemonstrated the feasibility of the LFXT for minibeam and microbeam\napplications with field sizes of 1.5-2 cm. The mice displayed no observable\nside effects after whole-brain 260 $\\mu$m-minibeam irradiation. We successfully\nconstructed and commissioned the first proof-of-concept LFXT prototype.\nDosimetric characterizations of the achieved microbeam field showed the\nsuperiority of the LFXT compared to conventional X-ray tubes in terms of beam\nquality. In future developments, the remaining limitations of the prototype\nwill be addressed for improved minibeam and first ever microbeam radiation\ntherapy in a clinical setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped the first prototype of a compact line-focus X-ray tube (LFXT) with\ntechnology potentially suitable for clinical translation of minibeams and\nmicrobeams. We give an overview of the commissioning process preceding first\noperation, present optical and radiological focal spot characterization\nmethods, and dosimetric measurements. Additionally, we report on first\npreclinical in vitro cell and in vivo mouse brain irradiations conducted with\nthe LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The\nfocal spot characterization resulted in a strongly eccentric electron\ndistribution with a width of 72.3 $\\mu$m. Dosimetry showed sharp microbeam dose\nprofiles with steep lateral penumbras and a peak-to-valley dose ratio above 10\nthroughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was\nmeasured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at\n150 mm distance from the focal spot. In vitro and in vivo experiments\ndemonstrated the feasibility of the LFXT for minibeam and microbeam\napplications with field sizes of 1.5-2 cm. The mice displayed no observable\nside effects after whole-brain 260 $\\mu$m-minibeam irradiation. We successfully\nconstructed and commissioned the first proof-of-concept LFXT prototype.\nDosimetric characterizations of the achieved microbeam field showed the\nsuperiority of the LFXT compared to conventional X-ray tubes in terms of beam\nquality. In future developments, the remaining limitations of the prototype\nwill be addressed for improved minibeam and first ever microbeam radiation\ntherapy in a clinical setting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15763v1",
                "updated": "2025-09-19T08:47:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    47,
                    37,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T08:47:37Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    47,
                    37,
                    4,
                    262,
                    0
                ],
                "title": "UniGist: Towards General and Hardware-aligned Sequence-level Long\n  Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGist: Towards General and Hardware-aligned Sequence-level Long\n  Context Compression"
                },
                "summary": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling."
                },
                "authors": [
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v2",
                "updated": "2025-09-19T06:20:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    6,
                    20,
                    14,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: A Cache-Assisted Parallel Speculative Decoding Framework via\n  Query-and-Correct Paradigm for Accelerating LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: A Cache-Assisted Parallel Speculative Decoding Framework via\n  Query-and-Correct Paradigm for Accelerating LLM Inference"
                },
                "summary": "Speculative decoding (SD), where a draft model provides multiple candidate\ntokens for the target model to verify in parallel, has demonstrated significant\npotential for accelerating LLM inference. Yet, existing SD approaches adhere to\na strict draft-then-verify paradigm, enforcing a sequential process that\nhampers performance and constrains the draft model's capacity. Moreover,\nrejecting a token in the candidate sequence invalidates all subsequent tokens,\nleading to wasted computation during drafting. To overcome these limitations,\nwe propose a cache-assisted parallel speculative decoding framework called\nCARD, which employs a novel query-and-correct paradigm. Our approach decouples\ndrafting from verification: the draft model populates a shared cache with\ncandidate tokens, while the target model concurrently refines the draft's\ntrajectory. This enables inference at near-draft-speed, effectively leveraging\nthe draft model's efficiency without additional fine-tuning. Experimental\nresults show that CARD significantly outperforms existing state-of-the-art\nmethods, achieving up to a 4.83x acceleration over vanilla autoregressive\ndecoding, with no fine-tuning required for either models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where a draft model provides multiple candidate\ntokens for the target model to verify in parallel, has demonstrated significant\npotential for accelerating LLM inference. Yet, existing SD approaches adhere to\na strict draft-then-verify paradigm, enforcing a sequential process that\nhampers performance and constrains the draft model's capacity. Moreover,\nrejecting a token in the candidate sequence invalidates all subsequent tokens,\nleading to wasted computation during drafting. To overcome these limitations,\nwe propose a cache-assisted parallel speculative decoding framework called\nCARD, which employs a novel query-and-correct paradigm. Our approach decouples\ndrafting from verification: the draft model populates a shared cache with\ncandidate tokens, while the target model concurrently refines the draft's\ntrajectory. This enables inference at near-draft-speed, effectively leveraging\nthe draft model's efficiency without additional fine-tuning. Experimental\nresults show that CARD significantly outperforms existing state-of-the-art\nmethods, achieving up to a 4.83x acceleration over vanilla autoregressive\ndecoding, with no fine-tuning required for either models."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15529v1",
                "updated": "2025-09-19T02:27:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    2,
                    27,
                    1,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T02:27:01Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    2,
                    27,
                    1,
                    4,
                    262,
                    0
                ],
                "title": "Optimization techniques for SQL+ML queries: A performance analysis of\n  real-time feature computation in OpenMLDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization techniques for SQL+ML queries: A performance analysis of\n  real-time feature computation in OpenMLDB"
                },
                "summary": "In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source\ndatabase that seamlessly integrates offline and online feature computations.\nThe work used feature-rich synthetic dataset experiments in Docker, which acted\nlike production environments that processed 100 to 500 records per batch and 6\nto 12 requests per batch in parallel. Efforts have been concentrated in the\nareas of better query plans, cached execution plans, parallel processing, and\nresource management. The experimental results show that OpenMLDB can support\napproximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL\nand ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This\nstudy assessed the impact of optimization and showed that query plan\noptimization accounted for 35% of the performance gains, caching for 25%, and\nparallel processing for 20%. These results illustrate OpenMLDB's capability for\ntime-sensitive ML use cases, such as fraud detection, personalized\nrecommendation, and time series forecasting. The system's modular optimization\nframework, which combines batch and stream processing without interference,\ncontributes to its significant performance gain over traditional database\nsystems, particularly in applications that require real-time feature\ncomputation and serving. This study contributes to the understanding and design\nof high-performance SQL+ML systems and highlights the need for specialized SQL\noptimization for ML workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source\ndatabase that seamlessly integrates offline and online feature computations.\nThe work used feature-rich synthetic dataset experiments in Docker, which acted\nlike production environments that processed 100 to 500 records per batch and 6\nto 12 requests per batch in parallel. Efforts have been concentrated in the\nareas of better query plans, cached execution plans, parallel processing, and\nresource management. The experimental results show that OpenMLDB can support\napproximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL\nand ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This\nstudy assessed the impact of optimization and showed that query plan\noptimization accounted for 35% of the performance gains, caching for 25%, and\nparallel processing for 20%. These results illustrate OpenMLDB's capability for\ntime-sensitive ML use cases, such as fraud detection, personalized\nrecommendation, and time series forecasting. The system's modular optimization\nframework, which combines batch and stream processing without interference,\ncontributes to its significant performance gain over traditional database\nsystems, particularly in applications that require real-time feature\ncomputation and serving. This study contributes to the understanding and design\nof high-performance SQL+ML systems and highlights the need for specialized SQL\noptimization for ML workloads."
                },
                "authors": [
                    {
                        "name": "Mashkhal A. Sidiq"
                    },
                    {
                        "name": "Aras A. Salih"
                    },
                    {
                        "name": "Samrand M. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Samrand M. Hassan"
                },
                "author": "Samrand M. Hassan",
                "arxiv_doi": "10.5121/ijdms.2025.17501",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5121/ijdms.2025.17501",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.15529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 4 figures, 1 Table",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15515v1",
                "updated": "2025-09-19T01:39:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    1,
                    39,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T01:39:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    1,
                    39,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for\n  Cost-Effective LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for\n  Cost-Effective LLM Inference"
                },
                "summary": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%."
                },
                "authors": [
                    {
                        "name": "Hantao Yang"
                    },
                    {
                        "name": "Hong Xie"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v3",
                "updated": "2025-09-18T23:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    23,
                    34,
                    50,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16278v1",
                "updated": "2025-09-18T17:38:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    38,
                    48,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:38:48Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    38,
                    48,
                    3,
                    261,
                    0
                ],
                "title": "Language Modeling with Learned Meta-Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Modeling with Learned Meta-Tokens"
                },
                "summary": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization."
                },
                "authors": [
                    {
                        "name": "Alok N. Shah"
                    },
                    {
                        "name": "Khush Gupta"
                    },
                    {
                        "name": "Keshav Ramji"
                    },
                    {
                        "name": "Pratik Chaudhari"
                    }
                ],
                "author_detail": {
                    "name": "Pratik Chaudhari"
                },
                "author": "Pratik Chaudhari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15038v1",
                "updated": "2025-09-18T15:04:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:04:06Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "title": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition"
                },
                "summary": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Siddhant Chaudhary"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15024v1",
                "updated": "2025-09-18T14:51:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:51:13Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "title": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering"
                },
                "summary": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xuanting Xie"
                    },
                    {
                        "name": "Bingheng Li"
                    },
                    {
                        "name": "Erlin Pan"
                    },
                    {
                        "name": "Rui Hou"
                    },
                    {
                        "name": "Wenyu Chen"
                    },
                    {
                        "name": "Zhao Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Kang"
                },
                "author": "Zhao Kang",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13789v2",
                "updated": "2025-09-18T04:57:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    4,
                    57,
                    32,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-17T07:58:36Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    58,
                    36,
                    2,
                    260,
                    0
                ],
                "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching"
                },
                "summary": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality."
                },
                "authors": [
                    {
                        "name": "Hanshuai Cui"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Zhifei Xu"
                    },
                    {
                        "name": "Zhi Yao"
                    },
                    {
                        "name": "Wenyi Zeng"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14403v1",
                "updated": "2025-09-17T20:08:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    8,
                    53,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T20:08:53Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    8,
                    53,
                    2,
                    260,
                    0
                ],
                "title": "Kilovolt-Class $β-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with\n  MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt-Class $β-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with\n  MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers"
                },
                "summary": "We report on the growth optimization of intentionally low-doped ($10^{15}$\n$cm^{-3}$) high-quality $\\beta-Ga_2O_3$ drift layers up to 10 $\\mu m$ thick via\nMOCVD and the fabrication of kilovolt-class field plated Schottky barrier\ndiodes on these thick drift layers. Homoepitaxial growth was performed on (010)\n$10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth\nparameters were systematically optimized to determine the best conditions for\nhigh quality thick growths with the given reactor geometry. Chamber pressure\nwas found to improve the growth rate, mobility, and roughness of the samples.\nGrowth rates of up to 7.2 $\\mu m$/hr., thicknesses of up to 10 $\\mu m$, Hall\nmobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID\nconcentrations as low as $2 \\times$ $10^{15}$ $cm^{-3}$, and controllable\nintentional doping down to $3 \\times$ $10^{15}$ $cm^{-3}$ were achieved. Field\nplated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \\times$\n$10^{15}$ $cm^{-3}$ intentionally doped 10 $\\mu m$ thick film to determine the\nelectrical performance of the MOCVD-grown material. The FP-SBD was found to\nhave current density $>$100 A/$cm^2$ at 3 V forward bias with a specific\ndifferential on resistance ($R_{on,sp}$) of 16.22 m$\\Omega$.$cm^2$ and a turn\non voltage of 1 V. The diodes were found to have high quality anode\nmetal/semiconductor interfaces with an ideality factor of 1.04, close to unity.\nDiodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through\nmaximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art\nresult for SBDs on MOCVD-grown (010) drift layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on the growth optimization of intentionally low-doped ($10^{15}$\n$cm^{-3}$) high-quality $\\beta-Ga_2O_3$ drift layers up to 10 $\\mu m$ thick via\nMOCVD and the fabrication of kilovolt-class field plated Schottky barrier\ndiodes on these thick drift layers. Homoepitaxial growth was performed on (010)\n$10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth\nparameters were systematically optimized to determine the best conditions for\nhigh quality thick growths with the given reactor geometry. Chamber pressure\nwas found to improve the growth rate, mobility, and roughness of the samples.\nGrowth rates of up to 7.2 $\\mu m$/hr., thicknesses of up to 10 $\\mu m$, Hall\nmobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID\nconcentrations as low as $2 \\times$ $10^{15}$ $cm^{-3}$, and controllable\nintentional doping down to $3 \\times$ $10^{15}$ $cm^{-3}$ were achieved. Field\nplated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \\times$\n$10^{15}$ $cm^{-3}$ intentionally doped 10 $\\mu m$ thick film to determine the\nelectrical performance of the MOCVD-grown material. The FP-SBD was found to\nhave current density $>$100 A/$cm^2$ at 3 V forward bias with a specific\ndifferential on resistance ($R_{on,sp}$) of 16.22 m$\\Omega$.$cm^2$ and a turn\non voltage of 1 V. The diodes were found to have high quality anode\nmetal/semiconductor interfaces with an ideality factor of 1.04, close to unity.\nDiodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through\nmaximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art\nresult for SBDs on MOCVD-grown (010) drift layers."
                },
                "authors": [
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Rachel Kahler"
                    },
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Akhila Mattapalli"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14347v1",
                "updated": "2025-09-17T18:26:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    18,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T18:26:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    18,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "On the Illusion of Success: An Empirical Study of Build Reruns and\n  Silent Failures in Industrial CI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Illusion of Success: An Empirical Study of Build Reruns and\n  Silent Failures in Industrial CI"
                },
                "summary": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Francis Bordeleau"
                    },
                    {
                        "name": "Ali Tizghadam"
                    }
                ],
                "author_detail": {
                    "name": "Ali Tizghadam"
                },
                "author": "Ali Tizghadam",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14093v1",
                "updated": "2025-09-17T15:33:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:33:44Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework"
                },
                "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints."
                },
                "authors": [
                    {
                        "name": "Kerui Huang"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Tongtong Xu"
                    },
                    {
                        "name": "Lingfeng Bao"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14041v1",
                "updated": "2025-09-17T14:42:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:42:38Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "title": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching"
                },
                "summary": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO."
                },
                "authors": [
                    {
                        "name": "Henry Kao"
                    },
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Prabhdeep Singh Soni"
                    },
                    {
                        "name": "Ali Sedaghati"
                    },
                    {
                        "name": "Fang Su"
                    },
                    {
                        "name": "Bryan Chan"
                    },
                    {
                        "name": "Maziar Goudarzi"
                    },
                    {
                        "name": "Reza Azimi"
                    }
                ],
                "author_detail": {
                    "name": "Reza Azimi"
                },
                "author": "Reza Azimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18172v1",
                "updated": "2025-09-17T13:51:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    51,
                    27,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:51:27Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    51,
                    27,
                    2,
                    260,
                    0
                ],
                "title": "SBVR: Summation of BitVector Representation for Efficient LLM\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SBVR: Summation of BitVector Representation for Efficient LLM\n  Quantization"
                },
                "summary": "With the advent of large language models (LLMs), numerous Post-Training\nQuantization (PTQ) strategies have been proposed to alleviate deployment\nbarriers created by their enormous parameter counts. Quantization achieves\ncompression by limiting the number of representable points in the data.\nTherefore, the key to achieving efficient quantization is selecting the optimal\ncombination of representation points, or codes, for the given data. Existing\nPTQ solutions adopt two major approaches to this problem: Round-To-Nearest\n(RTN)-based methods and codebook-based methods. RTN-based methods map LLM\nweights onto uniformly distributed integer grids, failing to account for the\nGaussian-like weight distribution of LLM weights. Codebook-based methods\nmitigate this issue by constructing distribution-aware codebooks; however, they\nsuffer from random and strided memory access patterns, resulting in degraded\ninference speed that is exacerbated by the limited size of GPU L1 cache. To\novercome these limitations, we propose a novel LLM quantization method, SBVR\n(Summation of BitVector Representation), that enables Gaussian-like code\nrepresentation in a hardware-friendly manner for fast inference. SBVR maps\nweight values to non-uniform representation points whose distribution follows\nthe actual distribution of LLM weights, enabling more accurate compression.\nAdditionally, we design a custom CUDA kernel that allows matrix-vector\nmultiplication directly in the SBVR format without decompression, thereby\nenabling high-performance execution of SBVR-compressed models. Our evaluations\nof SBVR on various models demonstrate state-of-the-art perplexity and accuracy\nbenchmark performance while delivering a 2.21x- 3.04x end-to-end\ntoken-generation speedup over naive FP16 models in the 4-bit quantization\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of large language models (LLMs), numerous Post-Training\nQuantization (PTQ) strategies have been proposed to alleviate deployment\nbarriers created by their enormous parameter counts. Quantization achieves\ncompression by limiting the number of representable points in the data.\nTherefore, the key to achieving efficient quantization is selecting the optimal\ncombination of representation points, or codes, for the given data. Existing\nPTQ solutions adopt two major approaches to this problem: Round-To-Nearest\n(RTN)-based methods and codebook-based methods. RTN-based methods map LLM\nweights onto uniformly distributed integer grids, failing to account for the\nGaussian-like weight distribution of LLM weights. Codebook-based methods\nmitigate this issue by constructing distribution-aware codebooks; however, they\nsuffer from random and strided memory access patterns, resulting in degraded\ninference speed that is exacerbated by the limited size of GPU L1 cache. To\novercome these limitations, we propose a novel LLM quantization method, SBVR\n(Summation of BitVector Representation), that enables Gaussian-like code\nrepresentation in a hardware-friendly manner for fast inference. SBVR maps\nweight values to non-uniform representation points whose distribution follows\nthe actual distribution of LLM weights, enabling more accurate compression.\nAdditionally, we design a custom CUDA kernel that allows matrix-vector\nmultiplication directly in the SBVR format without decompression, thereby\nenabling high-performance execution of SBVR-compressed models. Our evaluations\nof SBVR on various models demonstrate state-of-the-art perplexity and accuracy\nbenchmark performance while delivering a 2.21x- 3.04x end-to-end\ntoken-generation speedup over naive FP16 models in the 4-bit quantization\nregime."
                },
                "authors": [
                    {
                        "name": "Wonjun Bang"
                    },
                    {
                        "name": "Jongseok Park"
                    },
                    {
                        "name": "Hongseung Yu"
                    },
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Kyunghan Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyunghan Lee"
                },
                "author": "Kyunghan Lee",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13848v1",
                "updated": "2025-09-17T09:24:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:24:40Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation"
                },
                "summary": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference."
                },
                "authors": [
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Yongkang Zhou"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13604v1",
                "updated": "2025-09-17T00:28:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T00:28:49Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "title": "A Framework for Multi-source Prefetching Through Adaptive Weight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Multi-source Prefetching Through Adaptive Weight"
                },
                "summary": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web."
                },
                "authors": [
                    {
                        "name": "Yoseph Berhanu Alebachew"
                    },
                    {
                        "name": "Mulugeta Libsie"
                    }
                ],
                "author_detail": {
                    "name": "Mulugeta Libsie"
                },
                "author": "Mulugeta Libsie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v3",
                "updated": "2025-09-16T23:56:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    56,
                    55,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08256v2",
                "updated": "2025-09-16T23:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    15,
                    44,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-28T05:22:44Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    5,
                    22,
                    44,
                    2,
                    148,
                    0
                ],
                "title": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference"
                },
                "summary": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER"
                },
                "authors": [
                    {
                        "name": "Dongwei Wang"
                    },
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Yuxin Ren"
                    },
                    {
                        "name": "Jianing Deng"
                    },
                    {
                        "name": "Jingtong Hu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Huanrui Yang"
                    }
                ],
                "author_detail": {
                    "name": "Huanrui Yang"
                },
                "author": "Huanrui Yang",
                "arxiv_comment": "EMNLP2025 Camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v3",
                "updated": "2025-09-16T10:33:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    33,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_doi": "10.1145/3744916.3764523",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764523",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ICSE '26 (The 48th IEEE/ACM International Conference on\n  Software Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12900v1",
                "updated": "2025-09-16T09:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "title": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis"
                },
                "summary": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Bálint Hartmann"
                    },
                    {
                        "name": "Michelle T. Cirunay"
                    }
                ],
                "author_detail": {
                    "name": "Michelle T. Cirunay"
                },
                "author": "Michelle T. Cirunay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12867v1",
                "updated": "2025-09-16T09:22:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:22:21Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "title": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1."
                },
                "authors": [
                    {
                        "name": "Yabo Zhang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Qingyun Li"
                    },
                    {
                        "name": "Zhen Hu"
                    },
                    {
                        "name": "Kavin Han"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19341v1",
                "updated": "2025-09-16T09:14:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    14,
                    15,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:14:15Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    14,
                    15,
                    1,
                    259,
                    0
                ],
                "title": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks"
                },
                "summary": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework."
                },
                "authors": [
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Peng Qin"
                    },
                    {
                        "name": "Yueyue Zhang"
                    },
                    {
                        "name": "Yifei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Wang"
                },
                "author": "Yifei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12817v1",
                "updated": "2025-09-16T08:36:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T08:36:05Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "title": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention"
                },
                "summary": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Dong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Wang"
                },
                "author": "Dong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11156v2",
                "updated": "2025-09-16T07:49:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    7,
                    49,
                    41,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-14T08:22:37Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    22,
                    37,
                    6,
                    257,
                    0
                ],
                "title": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud"
                },
                "summary": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems."
                },
                "authors": [
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Aadarshraj Sah"
                    },
                    {
                        "name": "Poddutoori Sweeya Reddy"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v2",
                "updated": "2025-09-15T14:40:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    40,
                    16,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "IEEE Computer Architecture Letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11628v1",
                "updated": "2025-09-15T06:46:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T06:46:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching"
                },
                "summary": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Fei Ren"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_doi": "10.1145/3746027.3755331",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755331",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.11628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 9 figures, ACM Multimedia 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v2",
                "updated": "2025-09-15T01:15:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    1,
                    15,
                    50,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v2",
                "updated": "2025-09-15T00:51:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    0,
                    51,
                    47,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11239v1",
                "updated": "2025-09-14T12:29:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T12:29:49Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "title": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks"
                },
                "summary": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments."
                },
                "authors": [
                    {
                        "name": "Zhekun Huang"
                    },
                    {
                        "name": "Milena Radenkovic"
                    }
                ],
                "author_detail": {
                    "name": "Milena Radenkovic"
                },
                "author": "Milena Radenkovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11181v1",
                "updated": "2025-09-14T09:26:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T09:26:44Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "title": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study"
                },
                "summary": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior."
                },
                "authors": [
                    {
                        "name": "Alexander Frisch"
                    },
                    {
                        "name": "Daniel Isaia"
                    },
                    {
                        "name": "Oliver Preuß"
                    },
                    {
                        "name": "Xufei Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xufei Fang"
                },
                "author": "Xufei Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11155v1",
                "updated": "2025-09-14T08:20:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T08:20:48Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "title": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs"
                },
                "summary": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable."
                },
                "authors": [
                    {
                        "name": "Santhosh G S"
                    },
                    {
                        "name": "Saurav Prakash"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10798v1",
                "updated": "2025-09-13T03:34:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "published": "2025-09-13T03:34:12Z",
                "published_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction"
                },
                "summary": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10372v1",
                "updated": "2025-09-12T16:05:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:05:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness"
                },
                "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively."
                },
                "authors": [
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Zichuan Wang"
                    },
                    {
                        "name": "Zhiheng Yue"
                    },
                    {
                        "name": "Yousheng Long"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Jianxun Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10312v1",
                "updated": "2025-09-12T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching"
                },
                "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion."
                },
                "authors": [
                    {
                        "name": "Zhixin Zheng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10251v1",
                "updated": "2025-09-12T13:49:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:49:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing"
                },
                "summary": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs."
                },
                "authors": [
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Jieming Yin"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Diyu Zhou"
                    },
                    {
                        "name": "Zhenlin Wang"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10016v1",
                "updated": "2025-09-12T07:20:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:20:53Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "title": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report"
                },
                "summary": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago."
                },
                "authors": [
                    {
                        "name": "M. Müller"
                    },
                    {
                        "name": "J. Rabault"
                    },
                    {
                        "name": "C. Palerme"
                    },
                    {
                        "name": "J. Tjernström"
                    }
                ],
                "author_detail": {
                    "name": "J. Tjernström"
                },
                "author": "J. Tjernström",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09754v1",
                "updated": "2025-09-11T16:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation"
                },
                "summary": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava."
                },
                "authors": [
                    {
                        "name": "Yiqun Shen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Zhengze Zhang"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Nguyen Cam-Tu"
                    }
                ],
                "author_detail": {
                    "name": "Nguyen Cam-Tu"
                },
                "author": "Nguyen Cam-Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09525v1",
                "updated": "2025-09-11T15:06:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:06:03Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "title": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes"
                },
                "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."
                },
                "authors": [
                    {
                        "name": "Jialiang Huang"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Sixing Lin"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Xia Liao"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Mengting Lu"
                    },
                    {
                        "name": "Tao Ma"
                    },
                    {
                        "name": "Haifeng Gong"
                    },
                    {
                        "name": "Mingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxing Zhang"
                },
                "author": "Mingxing Zhang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09494v1",
                "updated": "2025-09-11T14:34:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:34:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding"
                },
                "summary": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v2",
                "updated": "2025-09-11T12:06:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    49,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v2",
                "updated": "2025-09-11T10:20:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    20,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_doi": "10.1109/CLOUD67622.2025.00051",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CLOUD67622.2025.00051",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.19880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 2025 IEEE 18th International Conference on Cloud\n  Computing (CLOUD)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v3",
                "updated": "2025-09-11T06:45:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    45,
                    58,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v3",
                "updated": "2025-09-11T06:16:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    16,
                    31,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09094v1",
                "updated": "2025-09-11T02:00:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T02:00:27Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "title": "Coherence-Aware Task Graph Modeling for Realistic Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence-Aware Task Graph Modeling for Realistic Application"
                },
                "summary": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3742875.3754678",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3742875.3754678",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by MEMOCODE'25, 10 pages",
                "arxiv_journal_ref": "International Symposium on Formal Methods and Models for System\n  Design (MEMOCODE '25), September 28-October 3, 2025, Taipei, Taiwan",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v2",
                "updated": "2025-09-10T17:59:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08696v1",
                "updated": "2025-09-10T15:41:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:41:15Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching"
                },
                "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ ."
                },
                "authors": [
                    {
                        "name": "Siratish Sakpiboonchit"
                    }
                ],
                "author_detail": {
                    "name": "Siratish Sakpiboonchit"
                },
                "author": "Siratish Sakpiboonchit",
                "arxiv_comment": "9 pages, 2 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08542v1",
                "updated": "2025-09-10T12:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:46:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference"
                },
                "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ASP-DAC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08342v1",
                "updated": "2025-09-10T07:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:28:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08315v1",
                "updated": "2025-09-10T06:32:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T06:32:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolKV: Evolutionary KV Cache Compression for LLM Inference"
                },
                "summary": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation."
                },
                "authors": [
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Yekun Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yekun Chai"
                },
                "author": "Yekun Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v3",
                "updated": "2025-09-09T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    30,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07379v1",
                "updated": "2025-09-09T04:00:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T04:00:43Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "title": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize."
                },
                "authors": [
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Grant Pinkert"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Yanli Li"
                    },
                    {
                        "name": "Dong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yuan"
                },
                "author": "Dong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v2",
                "updated": "2025-09-09T00:15:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    0,
                    15,
                    5,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v4",
                "updated": "2025-09-08T13:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    34,
                    54,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06579v1",
                "updated": "2025-09-08T11:49:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:49:51Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis"
                },
                "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html."
                },
                "authors": [
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Daniel Watson"
                    },
                    {
                        "name": "Yannick Strümpler"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.20359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20359v1",
                "updated": "2025-09-24T17:59:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    59,
                    19,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:59:19Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    59,
                    19,
                    2,
                    267,
                    0
                ],
                "title": "Stars and ionized gas in UGCA 320: a nearby gas-rich, dwarf Irregular\n  galaxy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stars and ionized gas in UGCA 320: a nearby gas-rich, dwarf Irregular\n  galaxy"
                },
                "summary": "UGCA 320 is a gas-rich dwarf irregular galaxy which belongs to a nearby,\nrelatively isolated group of dwarf galaxies. Here, we combine multi-band HST\nimaging data with deep long-slit SALT/RSS and integral-field VLT/MUSE spectral\ndata to study the stellar and ionized gas components of UGCA 320. Our imaging\ndata analysis reveals a very blue (V-I~0.1 mag), flattened radial colour\nprofile. We detect an abundance of ionized gas in UGCA 320 powered mostly by\nrecent star formation. The stellar disc in UGCA 320 is populated predominantly\nby young (~120 Myr) and metal-poor (~15-30 per cent solar metallicity) stars\nand it rotates in the same sense as the ionized gas disc but with higher\nrotation velocities, and possibly in different planes. Our analysis reveals a\nsharp transition in the kinematic properties of the discs at radius ~10\" (~0.3\nkpc) and distortions in the outer disc region. We show that these features are\nconsistent with a recent tidal interaction most likely with its close neighbour\n- UGCA 319. We discuss our results in the context of interacting dwarf galaxies\nand also show that similar inferences can be made independently from the\nlong-slit data analysis as with the integral-field data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UGCA 320 is a gas-rich dwarf irregular galaxy which belongs to a nearby,\nrelatively isolated group of dwarf galaxies. Here, we combine multi-band HST\nimaging data with deep long-slit SALT/RSS and integral-field VLT/MUSE spectral\ndata to study the stellar and ionized gas components of UGCA 320. Our imaging\ndata analysis reveals a very blue (V-I~0.1 mag), flattened radial colour\nprofile. We detect an abundance of ionized gas in UGCA 320 powered mostly by\nrecent star formation. The stellar disc in UGCA 320 is populated predominantly\nby young (~120 Myr) and metal-poor (~15-30 per cent solar metallicity) stars\nand it rotates in the same sense as the ionized gas disc but with higher\nrotation velocities, and possibly in different planes. Our analysis reveals a\nsharp transition in the kinematic properties of the discs at radius ~10\" (~0.3\nkpc) and distortions in the outer disc region. We show that these features are\nconsistent with a recent tidal interaction most likely with its close neighbour\n- UGCA 319. We discuss our results in the context of interacting dwarf galaxies\nand also show that similar inferences can be made independently from the\nlong-slit data analysis as with the integral-field data."
                },
                "authors": [
                    {
                        "name": "Adebusola B. Alabi"
                    },
                    {
                        "name": "S. Ilani Loubser"
                    },
                    {
                        "name": "Moses K. Mogotsi"
                    },
                    {
                        "name": "N. Zabel"
                    }
                ],
                "author_detail": {
                    "name": "N. Zabel"
                },
                "author": "N. Zabel",
                "arxiv_comment": "Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13364v2",
                "updated": "2025-09-24T17:53:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    53,
                    11,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-19T17:10:22Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    10,
                    22,
                    0,
                    139,
                    0
                ],
                "title": "Modeling Innovation Ecosystem Dynamics through Interacting Reinforced\n  Bernoulli Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Innovation Ecosystem Dynamics through Interacting Reinforced\n  Bernoulli Processes"
                },
                "summary": "Understanding how capabilities evolve into core capabilities-and how core\ncapabilities may ossify into rigidities-is central to innovation strategy\n(Leonard-Barton 1992, Teece 2009). A major challenge in formalizing this\nprocess lies in the interactive nature of innovation: successes in one domain\noften reshape others, endogenizing specialization and complicating isolated\nmodeling. This is especially true in ecosystems where firm capabilities and\ninnovation outcomes hinge on managing interdependencies and complementarities\n(Jacobides, Cennamo and Gawer 2018, 2024).\n  To address this, we propose a novel formal model based on interacting\nreinforced Bernoulli processes. This framework captures how patent successes\npropagate across technological categories and how these categories co-evolve.\nThe model is able to jointly account for several stylized facts in the\nempirical innovation literature, including sublinear success growth\n(successprobability decay), convergence of success shares across fields, and\ndiminishing cross-category correlations over time.\n  Empirical validation using GLOBAL PATSTAT (1980-2018) supports the\ntheoretical predictions. We estimate the structural parameters of the\ninteraction matrix and we also propose a statistical procedure to make\ninference on the intensity of cross-category interactions under the mean-field\nassumption.\n  By endogenizing technological specialization, our model provides a strategic\ntool for policymakers and managers, supporting decision-making in complex,\nco-evolving innovation ecosystems-where targeted interventions can produce\nsystemic effects, influencing competitive trajectories and shaping long-term\npatterns of specialization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how capabilities evolve into core capabilities-and how core\ncapabilities may ossify into rigidities-is central to innovation strategy\n(Leonard-Barton 1992, Teece 2009). A major challenge in formalizing this\nprocess lies in the interactive nature of innovation: successes in one domain\noften reshape others, endogenizing specialization and complicating isolated\nmodeling. This is especially true in ecosystems where firm capabilities and\ninnovation outcomes hinge on managing interdependencies and complementarities\n(Jacobides, Cennamo and Gawer 2018, 2024).\n  To address this, we propose a novel formal model based on interacting\nreinforced Bernoulli processes. This framework captures how patent successes\npropagate across technological categories and how these categories co-evolve.\nThe model is able to jointly account for several stylized facts in the\nempirical innovation literature, including sublinear success growth\n(successprobability decay), convergence of success shares across fields, and\ndiminishing cross-category correlations over time.\n  Empirical validation using GLOBAL PATSTAT (1980-2018) supports the\ntheoretical predictions. We estimate the structural parameters of the\ninteraction matrix and we also propose a statistical procedure to make\ninference on the intensity of cross-category interactions under the mean-field\nassumption.\n  By endogenizing technological specialization, our model provides a strategic\ntool for policymakers and managers, supporting decision-making in complex,\nco-evolving innovation ecosystems-where targeted interventions can produce\nsystemic effects, influencing competitive trajectories and shaping long-term\npatterns of specialization."
                },
                "authors": [
                    {
                        "name": "Giacomo Aletti"
                    },
                    {
                        "name": "Irene Crimaldi"
                    },
                    {
                        "name": "Andrea Ghiglietti"
                    },
                    {
                        "name": "Federico Nutarelli"
                    }
                ],
                "author_detail": {
                    "name": "Federico Nutarelli"
                },
                "author": "Federico Nutarelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04344v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04344v3",
                "updated": "2025-09-24T17:48:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    48,
                    25,
                    2,
                    267,
                    0
                ],
                "published": "2025-03-06T11:41:36Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    41,
                    36,
                    3,
                    65,
                    0
                ],
                "title": "LEDiT: Your Length-Extrapolatable Diffusion Transformer without\n  Positional Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEDiT: Your Length-Extrapolatable Diffusion Transformer without\n  Positional Encoding"
                },
                "summary": "Diffusion transformers (DiTs) struggle to generate images at resolutions\nhigher than their training resolutions. The primary obstacle is that the\nexplicit positional encodings(PE), such as RoPE, need extrapolating to unseen\npositions which degrades performance when the inference resolution differs from\ntraining. In this paper, We propose a Length-Extrapolatable Diffusion\nTransformer~(LEDiT) to overcome this limitation. LEDiT needs no explicit PEs,\nthereby avoiding PE extrapolation. The key innovation of LEDiT lies in the use\nof causal attention. We demonstrate that causal attention can implicitly encode\nglobal positional information and show that such information facilitates\nextrapolation. We further introduce a locality enhancement module, which\ncaptures fine-grained local information to complement the global coarse-grained\nposition information encoded by causal attention. Experimental results on both\nconditional and text-to-image generation tasks demonstrate that LEDiT supports\nup to 4x resolution scaling (e.g., from 256x256 to 512x512), achieving better\nimage quality compared to the state-of-the-art length extrapolation methods. We\nbelieve that LEDiT marks a departure from the standard RoPE-based methods and\noffers a promising insight into length extrapolation. Project page:\nhttps://shenzhang2145.github.io/ledit/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers (DiTs) struggle to generate images at resolutions\nhigher than their training resolutions. The primary obstacle is that the\nexplicit positional encodings(PE), such as RoPE, need extrapolating to unseen\npositions which degrades performance when the inference resolution differs from\ntraining. In this paper, We propose a Length-Extrapolatable Diffusion\nTransformer~(LEDiT) to overcome this limitation. LEDiT needs no explicit PEs,\nthereby avoiding PE extrapolation. The key innovation of LEDiT lies in the use\nof causal attention. We demonstrate that causal attention can implicitly encode\nglobal positional information and show that such information facilitates\nextrapolation. We further introduce a locality enhancement module, which\ncaptures fine-grained local information to complement the global coarse-grained\nposition information encoded by causal attention. Experimental results on both\nconditional and text-to-image generation tasks demonstrate that LEDiT supports\nup to 4x resolution scaling (e.g., from 256x256 to 512x512), achieving better\nimage quality compared to the state-of-the-art length extrapolation methods. We\nbelieve that LEDiT marks a departure from the standard RoPE-based methods and\noffers a promising insight into length extrapolation. Project page:\nhttps://shenzhang2145.github.io/ledit/"
                },
                "authors": [
                    {
                        "name": "Shen Zhang"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Yaning Tan"
                    },
                    {
                        "name": "Zhaowei Chen"
                    },
                    {
                        "name": "Linze Li"
                    },
                    {
                        "name": "Ge Wu"
                    },
                    {
                        "name": "Yuhao Chen"
                    },
                    {
                        "name": "Shuheng Li"
                    },
                    {
                        "name": "Zhenyu Zhao"
                    },
                    {
                        "name": "Caihua Chen"
                    },
                    {
                        "name": "Jiajun Liang"
                    },
                    {
                        "name": "Yao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yao Tang"
                },
                "author": "Yao Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04344v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04344v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05935v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05935v4",
                "updated": "2025-09-24T17:46:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    46,
                    21,
                    2,
                    267,
                    0
                ],
                "published": "2025-02-09T15:32:46Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    15,
                    32,
                    46,
                    6,
                    40,
                    0
                ],
                "title": "Interactive Inference: A Neuromorphic Theory of Human-Computer\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Inference: A Neuromorphic Theory of Human-Computer\n  Interaction"
                },
                "summary": "Neuromorphic Human-Computer Interaction (HCI) is a theoretical approach to\ndesigning better user experiences (UX) motivated by advances in the\nunderstanding of the neurophysiology of the brain. Inspired by the\nneuroscientific theory of Active Inference, Interactive Inference is a first\nexample of such approach. It offers a simplified interpretation of Active\nInference that allows designers to more readily apply this theory to design and\nevaluation. In Interactive Inference, user behaviour is modeled as Bayesian\ninference on progress and goal distributions that predicts the next action. We\nshow how the error between goal and progress distributions, or Bayesian\nsurprise, can be modeled as a simple mean square error of the signal-to-noise\nratio (SNR) of a task. The problem is that the user's capacity to process\nBayesian surprise follows the logarithm of this SNR. This means errors rise\nquickly once average capacity is exceeded. Our model allows the quantitative\nanalysis of performance and error using one framework that can provide\nreal-time estimates of the mental load in users that needs to be minimized by\ndesign. We show how three basic laws of HCI, Hick's Law, Fitts' Law and the\nPower Law can be expressed using our model. We then test the validity of the\nmodel by empirically measuring how well it predicts human performance and error\nin a car following task. Results suggest that driver processing capacity indeed\nis a logarithmic function of the SNR of the distance to a lead car. This result\nprovides initial evidence that Interactive Interference can be useful as a new\ntheoretical design tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic Human-Computer Interaction (HCI) is a theoretical approach to\ndesigning better user experiences (UX) motivated by advances in the\nunderstanding of the neurophysiology of the brain. Inspired by the\nneuroscientific theory of Active Inference, Interactive Inference is a first\nexample of such approach. It offers a simplified interpretation of Active\nInference that allows designers to more readily apply this theory to design and\nevaluation. In Interactive Inference, user behaviour is modeled as Bayesian\ninference on progress and goal distributions that predicts the next action. We\nshow how the error between goal and progress distributions, or Bayesian\nsurprise, can be modeled as a simple mean square error of the signal-to-noise\nratio (SNR) of a task. The problem is that the user's capacity to process\nBayesian surprise follows the logarithm of this SNR. This means errors rise\nquickly once average capacity is exceeded. Our model allows the quantitative\nanalysis of performance and error using one framework that can provide\nreal-time estimates of the mental load in users that needs to be minimized by\ndesign. We show how three basic laws of HCI, Hick's Law, Fitts' Law and the\nPower Law can be expressed using our model. We then test the validity of the\nmodel by empirically measuring how well it predicts human performance and error\nin a car following task. Results suggest that driver processing capacity indeed\nis a logarithmic function of the SNR of the distance to a lead car. This result\nprovides initial evidence that Interactive Interference can be useful as a new\ntheoretical design tool."
                },
                "authors": [
                    {
                        "name": "Roel Vertegaal"
                    },
                    {
                        "name": "Timothy Merritt"
                    },
                    {
                        "name": "Saul Greenberg"
                    },
                    {
                        "name": "Aneesh P. Tarun"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    }
                ],
                "author_detail": {
                    "name": "Zafeirios Fountas"
                },
                "author": "Zafeirios Fountas",
                "arxiv_comment": "18 pages, 7 figures, 1 table, 37 mathematical formulas, in press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05935v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05935v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20345v1",
                "updated": "2025-09-24T17:37:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    37,
                    14,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:37:14Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    37,
                    14,
                    2,
                    267,
                    0
                ],
                "title": "Statistical Inference Leveraging Synthetic Data with Distribution-Free\n  Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference Leveraging Synthetic Data with Distribution-Free\n  Guarantees"
                },
                "summary": "The rapid proliferation of high-quality synthetic data -- generated by\nadvanced AI models or collected as auxiliary data from related tasks --\npresents both opportunities and challenges for statistical inference. This\npaper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that\nwraps around any statistical inference procedure to safely enhance sample\nefficiency by combining synthetic and real data. Our framework leverages\nhigh-quality synthetic data to boost statistical power, yet adaptively defaults\nto the standard inference method using only real data when synthetic data is of\nlow quality. The error of our method remains below a user-specified bound\nwithout any distributional assumptions on the synthetic data, and decreases as\nthe quality of the synthetic data improves. This flexibility enables seamless\nintegration with conformal prediction, risk control, hypothesis testing, and\nmultiple testing procedures, all without modifying the base inference method.\nWe demonstrate the benefits of our method on challenging tasks with limited\nlabeled data, including AlphaFold protein structure prediction, and comparing\nlarge reasoning models on complex math problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of high-quality synthetic data -- generated by\nadvanced AI models or collected as auxiliary data from related tasks --\npresents both opportunities and challenges for statistical inference. This\npaper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that\nwraps around any statistical inference procedure to safely enhance sample\nefficiency by combining synthetic and real data. Our framework leverages\nhigh-quality synthetic data to boost statistical power, yet adaptively defaults\nto the standard inference method using only real data when synthetic data is of\nlow quality. The error of our method remains below a user-specified bound\nwithout any distributional assumptions on the synthetic data, and decreases as\nthe quality of the synthetic data improves. This flexibility enables seamless\nintegration with conformal prediction, risk control, hypothesis testing, and\nmultiple testing procedures, all without modifying the base inference method.\nWe demonstrate the benefits of our method on challenging tasks with limited\nlabeled data, including AlphaFold protein structure prediction, and comparing\nlarge reasoning models on complex math problems."
                },
                "authors": [
                    {
                        "name": "Meshi Bashari"
                    },
                    {
                        "name": "Yonghoon Lee"
                    },
                    {
                        "name": "Roy Maor Lotan"
                    },
                    {
                        "name": "Edgar Dobriban"
                    },
                    {
                        "name": "Yaniv Romano"
                    }
                ],
                "author_detail": {
                    "name": "Yaniv Romano"
                },
                "author": "Yaniv Romano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15074v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15074v3",
                "updated": "2025-09-24T17:25:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    25,
                    12,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-21T03:43:29Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    3,
                    43,
                    29,
                    2,
                    141,
                    0
                ],
                "title": "DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware\n  Reinforcement Learning on Imbalanced Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware\n  Reinforcement Learning on Imbalanced Data"
                },
                "summary": "Large Language Models (LLMs) are increasingly aligned with human preferences\nthrough Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,\nGroup Relative Policy Optimization (GRPO) has gained attention for its\nsimplicity and strong performance, notably eliminating the need for a learned\nvalue function. However, GRPO implicitly assumes a balanced domain distribution\nand uniform semantic alignment across groups, assumptions that rarely hold in\nreal-world datasets. When applied to multi-domain, imbalanced data, GRPO\ndisproportionately optimizes for dominant domains, neglecting underrepresented\nones and resulting in poor generalization and fairness. We propose\nDomain-Informed Self-Consistency Policy Optimization (DISCO), a principled\nextension to GRPO that addresses inter-group imbalance with two key\ninnovations. Domain-aware reward scaling counteracts frequency bias by\nreweighting optimization based on domain prevalence. Difficulty-aware reward\nscaling leverages prompt-level self-consistency to identify and prioritize\nuncertain prompts that offer greater learning value. Together, these strategies\npromote more equitable and effective policy learning across domains. Extensive\nexperiments across multiple LLMs and skewed training distributions show that\nDISCO improves generalization, outperforms existing GRPO variants by 5% on\nQwen3 models, and sets new state-of-the-art results on multi-domain alignment\nbenchmarks. Our code and data are available at\nhttps://github.com/Tonyzhou98/disco_grpo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly aligned with human preferences\nthrough Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,\nGroup Relative Policy Optimization (GRPO) has gained attention for its\nsimplicity and strong performance, notably eliminating the need for a learned\nvalue function. However, GRPO implicitly assumes a balanced domain distribution\nand uniform semantic alignment across groups, assumptions that rarely hold in\nreal-world datasets. When applied to multi-domain, imbalanced data, GRPO\ndisproportionately optimizes for dominant domains, neglecting underrepresented\nones and resulting in poor generalization and fairness. We propose\nDomain-Informed Self-Consistency Policy Optimization (DISCO), a principled\nextension to GRPO that addresses inter-group imbalance with two key\ninnovations. Domain-aware reward scaling counteracts frequency bias by\nreweighting optimization based on domain prevalence. Difficulty-aware reward\nscaling leverages prompt-level self-consistency to identify and prioritize\nuncertain prompts that offer greater learning value. Together, these strategies\npromote more equitable and effective policy learning across domains. Extensive\nexperiments across multiple LLMs and skewed training distributions show that\nDISCO improves generalization, outperforms existing GRPO variants by 5% on\nQwen3 models, and sets new state-of-the-art results on multi-domain alignment\nbenchmarks. Our code and data are available at\nhttps://github.com/Tonyzhou98/disco_grpo."
                },
                "authors": [
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Jing Zhu"
                    },
                    {
                        "name": "Shengyi Qian"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Xiyao Wang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Paiheng Xu"
                    },
                    {
                        "name": "Wei Ai"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang",
                "arxiv_comment": "Accepted by EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15074v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15074v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20336v1",
                "updated": "2025-09-24T17:25:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    25,
                    5,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:25:05Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    25,
                    5,
                    2,
                    267,
                    0
                ],
                "title": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit\n  Tracing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit\n  Tracing"
                },
                "summary": "Transformer-based LLMs demonstrate strong performance on graph reasoning\ntasks, yet their internal mechanisms remain underexplored. To uncover these\nreasoning process mechanisms in a fundamental and unified view, we set the\nbasic decoder-only transformers and explain them using the circuit-tracer\nframework. Through this lens, we visualize reasoning traces and identify two\ncore mechanisms in graph reasoning: token merging and structural memorization,\nwhich underlie both path reasoning and substructure extraction tasks. We\nfurther quantify these behaviors and analyze how they are influenced by graph\ndensity and model size. Our study provides a unified interpretability framework\nfor understanding structural reasoning in decoder-only Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs demonstrate strong performance on graph reasoning\ntasks, yet their internal mechanisms remain underexplored. To uncover these\nreasoning process mechanisms in a fundamental and unified view, we set the\nbasic decoder-only transformers and explain them using the circuit-tracer\nframework. Through this lens, we visualize reasoning traces and identify two\ncore mechanisms in graph reasoning: token merging and structural memorization,\nwhich underlie both path reasoning and substructure extraction tasks. We\nfurther quantify these behaviors and analyze how they are influenced by graph\ndensity and model size. Our study provides a unified interpretability framework\nfor understanding structural reasoning in decoder-only Transformers."
                },
                "authors": [
                    {
                        "name": "Xinnan Dai"
                    },
                    {
                        "name": "Chung-Hsiang Lo"
                    },
                    {
                        "name": "Kai Guo"
                    },
                    {
                        "name": "Shenglai Zeng"
                    },
                    {
                        "name": "Dongsheng Luo"
                    },
                    {
                        "name": "Jiliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Tang"
                },
                "author": "Jiliang Tang",
                "arxiv_comment": "Accepted by the Workshop on Efficient Reasoning, Neurips 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02876v2",
                "updated": "2025-09-24T17:23:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    23,
                    48,
                    2,
                    267,
                    0
                ],
                "published": "2025-04-02T00:19:05Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    0,
                    19,
                    5,
                    2,
                    92,
                    0
                ],
                "title": "Multimodal Reference Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Reference Visual Grounding"
                },
                "summary": "Visual grounding focuses on detecting objects from images based on language\nexpressions. Recent Large Vision-Language Models (LVLMs) have significantly\nadvanced visual grounding performance by training large models with large-scale\ndatasets. However, the problem remains challenging, especially when similar\nobjects appear in the input image. For example, an LVLM may not be able to\ndifferentiate Diet Coke and regular Coke in an image. In this case, if\nadditional reference images of Diet Coke and regular Coke are available, it can\nhelp the visual grounding of similar objects.\n  In this work, we introduce a new task named Multimodal Reference Visual\nGrounding (MRVG). In this task, a model has access to a set of reference images\nof objects in a database. Based on these reference images and a language\nexpression, the model is required to detect a target object from a query image.\nWe first introduce a new dataset to study the MRVG problem. Then we introduce a\nnovel method, named MRVG-Net, to solve this visual grounding problem. We show\nthat by efficiently using reference images with few-shot object detection and\nusing Large Language Models (LLMs) for object matching, our method achieves\nsuperior visual grounding performance compared to the state-of-the-art LVLMs\nsuch as Qwen2.5-VL-72B. Our approach bridges the gap between few-shot detection\nand visual grounding, unlocking new capabilities for visual understanding,\nwhich has wide applications in robotics. Project page with our video, code, and\ndataset: https://irvlutd.github.io/MultiGrounding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding focuses on detecting objects from images based on language\nexpressions. Recent Large Vision-Language Models (LVLMs) have significantly\nadvanced visual grounding performance by training large models with large-scale\ndatasets. However, the problem remains challenging, especially when similar\nobjects appear in the input image. For example, an LVLM may not be able to\ndifferentiate Diet Coke and regular Coke in an image. In this case, if\nadditional reference images of Diet Coke and regular Coke are available, it can\nhelp the visual grounding of similar objects.\n  In this work, we introduce a new task named Multimodal Reference Visual\nGrounding (MRVG). In this task, a model has access to a set of reference images\nof objects in a database. Based on these reference images and a language\nexpression, the model is required to detect a target object from a query image.\nWe first introduce a new dataset to study the MRVG problem. Then we introduce a\nnovel method, named MRVG-Net, to solve this visual grounding problem. We show\nthat by efficiently using reference images with few-shot object detection and\nusing Large Language Models (LLMs) for object matching, our method achieves\nsuperior visual grounding performance compared to the state-of-the-art LVLMs\nsuch as Qwen2.5-VL-72B. Our approach bridges the gap between few-shot detection\nand visual grounding, unlocking new capabilities for visual understanding,\nwhich has wide applications in robotics. Project page with our video, code, and\ndataset: https://irvlutd.github.io/MultiGrounding"
                },
                "authors": [
                    {
                        "name": "Yangxiao Lu"
                    },
                    {
                        "name": "Ruosen Li"
                    },
                    {
                        "name": "Liqiang Jing"
                    },
                    {
                        "name": "Jikai Wang"
                    },
                    {
                        "name": "Xinya Du"
                    },
                    {
                        "name": "Yunhui Guo"
                    },
                    {
                        "name": "Nicholas Ruozzi"
                    },
                    {
                        "name": "Yu Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Xiang"
                },
                "author": "Yu Xiang",
                "arxiv_comment": "Project page with our code and dataset:\n  https://irvlutd.github.io/MultiGrounding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20328v1",
                "updated": "2025-09-24T17:17:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    17,
                    27,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:17:27Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    17,
                    27,
                    2,
                    267,
                    0
                ],
                "title": "Video models are zero-shot learners and reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video models are zero-shot learners and reasoners"
                },
                "summary": "The remarkable zero-shot capabilities of Large Language Models (LLMs) have\npropelled natural language processing from task-specific models to unified,\ngeneralist foundation models. This transformation emerged from simple\nprimitives: large, generative models trained on web-scale data. Curiously, the\nsame primitives apply to today's generative video models. Could video models be\non a trajectory towards general-purpose vision understanding, much like LLMs\ndeveloped general-purpose language understanding? We demonstrate that Veo 3 can\nsolve a broad variety of tasks it wasn't explicitly trained for: segmenting\nobjects, detecting edges, editing images, understanding physical properties,\nrecognizing object affordances, simulating tool use, and more. These abilities\nto perceive, model, and manipulate the visual world enable early forms of\nvisual reasoning like maze and symmetry solving. Veo's emergent zero-shot\ncapabilities indicate that video models are on a path to becoming unified,\ngeneralist vision foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable zero-shot capabilities of Large Language Models (LLMs) have\npropelled natural language processing from task-specific models to unified,\ngeneralist foundation models. This transformation emerged from simple\nprimitives: large, generative models trained on web-scale data. Curiously, the\nsame primitives apply to today's generative video models. Could video models be\non a trajectory towards general-purpose vision understanding, much like LLMs\ndeveloped general-purpose language understanding? We demonstrate that Veo 3 can\nsolve a broad variety of tasks it wasn't explicitly trained for: segmenting\nobjects, detecting edges, editing images, understanding physical properties,\nrecognizing object affordances, simulating tool use, and more. These abilities\nto perceive, model, and manipulate the visual world enable early forms of\nvisual reasoning like maze and symmetry solving. Veo's emergent zero-shot\ncapabilities indicate that video models are on a path to becoming unified,\ngeneralist vision foundation models."
                },
                "authors": [
                    {
                        "name": "Thaddäus Wiedemer"
                    },
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Paul Vicol"
                    },
                    {
                        "name": "Shixiang Shane Gu"
                    },
                    {
                        "name": "Nick Matarese"
                    },
                    {
                        "name": "Kevin Swersky"
                    },
                    {
                        "name": "Been Kim"
                    },
                    {
                        "name": "Priyank Jaini"
                    },
                    {
                        "name": "Robert Geirhos"
                    }
                ],
                "author_detail": {
                    "name": "Robert Geirhos"
                },
                "author": "Robert Geirhos",
                "arxiv_comment": "Project page: https://video-zero-shot.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20324v1",
                "updated": "2025-09-24T17:11:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    11,
                    35,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:11:35Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    11,
                    35,
                    2,
                    267,
                    0
                ],
                "title": "RAG Security and Privacy: Formalizing the Threat Model and Attack\n  Surface",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG Security and Privacy: Formalizing the Threat Model and Attack\n  Surface"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is an emerging approach in natural\nlanguage processing that combines large language models (LLMs) with external\ndocument retrieval to produce more accurate and grounded responses. While RAG\nhas shown strong potential in reducing hallucinations and improving factual\nconsistency, it also introduces new privacy and security challenges that differ\nfrom those faced by traditional LLMs. Existing research has demonstrated that\nLLMs can leak sensitive information through training data memorization or\nadversarial prompts, and RAG systems inherit many of these vulnerabilities. At\nthe same time, reliance of RAG on an external knowledge base opens new attack\nsurfaces, including the potential for leaking information about the presence or\ncontent of retrieved documents, or for injecting malicious content to\nmanipulate model behavior. Despite these risks, there is currently no formal\nframework that defines the threat landscape for RAG systems. In this paper, we\naddress a critical gap in the literature by proposing, to the best of our\nknowledge, the first formal threat model for retrieval-RAG systems. We\nintroduce a structured taxonomy of adversary types based on their access to\nmodel components and data, and we formally define key threat vectors such as\ndocument-level membership inference and data poisoning, which pose serious\nprivacy and integrity risks in real-world deployments. By establishing formal\ndefinitions and attack models, our work lays the foundation for a more rigorous\nand principled understanding of privacy and security in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is an emerging approach in natural\nlanguage processing that combines large language models (LLMs) with external\ndocument retrieval to produce more accurate and grounded responses. While RAG\nhas shown strong potential in reducing hallucinations and improving factual\nconsistency, it also introduces new privacy and security challenges that differ\nfrom those faced by traditional LLMs. Existing research has demonstrated that\nLLMs can leak sensitive information through training data memorization or\nadversarial prompts, and RAG systems inherit many of these vulnerabilities. At\nthe same time, reliance of RAG on an external knowledge base opens new attack\nsurfaces, including the potential for leaking information about the presence or\ncontent of retrieved documents, or for injecting malicious content to\nmanipulate model behavior. Despite these risks, there is currently no formal\nframework that defines the threat landscape for RAG systems. In this paper, we\naddress a critical gap in the literature by proposing, to the best of our\nknowledge, the first formal threat model for retrieval-RAG systems. We\nintroduce a structured taxonomy of adversary types based on their access to\nmodel components and data, and we formally define key threat vectors such as\ndocument-level membership inference and data poisoning, which pose serious\nprivacy and integrity risks in real-world deployments. By establishing formal\ndefinitions and attack models, our work lays the foundation for a more rigorous\nand principled understanding of privacy and security in RAG systems."
                },
                "authors": [
                    {
                        "name": "Atousa Arzanipour"
                    },
                    {
                        "name": "Rouzbeh Behnia"
                    },
                    {
                        "name": "Reza Ebrahimi"
                    },
                    {
                        "name": "Kaushik Dutta"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Dutta"
                },
                "author": "Kaushik Dutta",
                "arxiv_comment": "Accepted at the 5th ICDM Workshop on September 20, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20321v1",
                "updated": "2025-09-24T17:08:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    8,
                    12,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:08:12Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    8,
                    12,
                    2,
                    267,
                    0
                ],
                "title": "DRES: Benchmarking LLMs for Disfluency Removal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRES: Benchmarking LLMs for Disfluency Removal"
                },
                "summary": "Disfluencies -- such as \"um,\" \"uh,\" interjections, parentheticals, and edited\nstatements -- remain a persistent challenge for speech-driven systems,\ndegrading accuracy in command interpretation, summarization, and conversational\nagents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled\ntext-level benchmark that establishes a reproducible semantic upper bound for\nthis task. DRES builds on human-annotated Switchboard transcripts, isolating\ndisfluency removal from ASR errors and acoustic variability. We systematically\nevaluate proprietary and open-source LLMs across scales, prompting strategies,\nand architectures. Our results reveal that (i) simple segmentation consistently\nimproves performance, even for long-context models; (ii) reasoning-oriented\nmodels tend to over-delete fluent tokens; and (iii) fine-tuning achieves near\nstate-of-the-art precision and recall but harms generalization abilities. We\nfurther present a set of LLM-specific error modes and offer nine practical\nrecommendations (R1-R9) for deploying disfluency removal in speech-driven\npipelines. DRES provides a reproducible, model-agnostic foundation for\nadvancing robust spoken-language systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disfluencies -- such as \"um,\" \"uh,\" interjections, parentheticals, and edited\nstatements -- remain a persistent challenge for speech-driven systems,\ndegrading accuracy in command interpretation, summarization, and conversational\nagents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled\ntext-level benchmark that establishes a reproducible semantic upper bound for\nthis task. DRES builds on human-annotated Switchboard transcripts, isolating\ndisfluency removal from ASR errors and acoustic variability. We systematically\nevaluate proprietary and open-source LLMs across scales, prompting strategies,\nand architectures. Our results reveal that (i) simple segmentation consistently\nimproves performance, even for long-context models; (ii) reasoning-oriented\nmodels tend to over-delete fluent tokens; and (iii) fine-tuning achieves near\nstate-of-the-art precision and recall but harms generalization abilities. We\nfurther present a set of LLM-specific error modes and offer nine practical\nrecommendations (R1-R9) for deploying disfluency removal in speech-driven\npipelines. DRES provides a reproducible, model-agnostic foundation for\nadvancing robust spoken-language systems."
                },
                "authors": [
                    {
                        "name": "Maria Teleki"
                    },
                    {
                        "name": "Sai Janjur"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Oliver Grabner"
                    },
                    {
                        "name": "Ketan Verma"
                    },
                    {
                        "name": "Thomas Docog"
                    },
                    {
                        "name": "Xiangjue Dong"
                    },
                    {
                        "name": "Lingfeng Shi"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Stephanie Birkelbach"
                    },
                    {
                        "name": "Jason Kim"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "James Caverlee"
                    }
                ],
                "author_detail": {
                    "name": "James Caverlee"
                },
                "author": "James Caverlee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13739v2",
                "updated": "2025-09-24T17:02:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    2,
                    50,
                    2,
                    267,
                    0
                ],
                "published": "2025-08-19T11:23:09Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    23,
                    9,
                    1,
                    231,
                    0
                ],
                "title": "Enhancing Targeted Adversarial Attacks on Large Vision-Language Models\n  via Intermediate Projector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Targeted Adversarial Attacks on Large Vision-Language Models\n  via Intermediate Projector"
                },
                "summary": "The growing deployment of Large Vision-Language Models (VLMs) raises safety\nconcerns, as adversaries may exploit model vulnerabilities to induce harmful\noutputs, with targeted black-box adversarial attacks posing a particularly\nsevere threat. However, existing methods primarily maximize encoder-level\nglobal similarity, which lacks the granularity for stealthy and practical\nfine-grained attacks, where only specific target should be altered (e.g.,\nmodifying a car while preserving its background). Moreover, they largely\nneglect the projector, a key semantic bridge in VLMs for multimodal alignment.\nTo address these limitations, we propose a novel black-box targeted attack\nframework that leverages the projector. Specifically, we utilize the widely\nadopted Querying Transformer (Q-Former) which transforms global image\nembeddings into fine-grained query outputs, to enhance attack effectiveness and\ngranularity. For standard global targeted attack scenarios, we propose the\nIntermediate Projector Guided Attack (IPGA), which aligns Q-Former fine-grained\nquery outputs with the target to enhance attack strength and exploits the\nintermediate pretrained Q-Former that is not fine-tuned for any specific Large\nLanguage Model (LLM) to improve attack transferability. For fine-grained attack\nscenarios, we augment IPGA with the Residual Query Alignment (RQA) module,\nwhich preserves unrelated content by constraining non-target query outputs to\nenhance attack granularity. Extensive experiments demonstrate that IPGA\nsignificantly outperforms baselines in global targeted attacks, and IPGA with\nRQA (IPGA-R) attains superior success rates and unrelated content preservation\nover baselines in fine-grained attacks. Our method also transfers effectively\nto commercial VLMs such as Google Gemini and OpenAI GPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing deployment of Large Vision-Language Models (VLMs) raises safety\nconcerns, as adversaries may exploit model vulnerabilities to induce harmful\noutputs, with targeted black-box adversarial attacks posing a particularly\nsevere threat. However, existing methods primarily maximize encoder-level\nglobal similarity, which lacks the granularity for stealthy and practical\nfine-grained attacks, where only specific target should be altered (e.g.,\nmodifying a car while preserving its background). Moreover, they largely\nneglect the projector, a key semantic bridge in VLMs for multimodal alignment.\nTo address these limitations, we propose a novel black-box targeted attack\nframework that leverages the projector. Specifically, we utilize the widely\nadopted Querying Transformer (Q-Former) which transforms global image\nembeddings into fine-grained query outputs, to enhance attack effectiveness and\ngranularity. For standard global targeted attack scenarios, we propose the\nIntermediate Projector Guided Attack (IPGA), which aligns Q-Former fine-grained\nquery outputs with the target to enhance attack strength and exploits the\nintermediate pretrained Q-Former that is not fine-tuned for any specific Large\nLanguage Model (LLM) to improve attack transferability. For fine-grained attack\nscenarios, we augment IPGA with the Residual Query Alignment (RQA) module,\nwhich preserves unrelated content by constraining non-target query outputs to\nenhance attack granularity. Extensive experiments demonstrate that IPGA\nsignificantly outperforms baselines in global targeted attacks, and IPGA with\nRQA (IPGA-R) attains superior success rates and unrelated content preservation\nover baselines in fine-grained attacks. Our method also transfers effectively\nto commercial VLMs such as Google Gemini and OpenAI GPT."
                },
                "authors": [
                    {
                        "name": "Yiming Cao"
                    },
                    {
                        "name": "Yanjie Li"
                    },
                    {
                        "name": "Kaisheng Liang"
                    },
                    {
                        "name": "Bin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Xiao"
                },
                "author": "Bin Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20319v1",
                "updated": "2025-09-24T17:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    2,
                    39,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    2,
                    39,
                    2,
                    267,
                    0
                ],
                "title": "Z-Scores: A Metric for Linguistically Assessing Disfluency Removal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Z-Scores: A Metric for Linguistically Assessing Disfluency Removal"
                },
                "summary": "Evaluating disfluency removal in speech requires more than aggregate\ntoken-level scores. Traditional word-based metrics such as precision, recall,\nand F1 (E-Scores) capture overall performance but cannot reveal why models\nsucceed or fail. We introduce Z-Scores, a span-level linguistically-grounded\nevaluation metric that categorizes system behavior across distinct disfluency\ntypes (EDITED, INTJ, PRN). Our deterministic alignment module enables robust\nmapping between generated text and disfluent transcripts, allowing Z-Scores to\nexpose systematic weaknesses that word-level metrics obscure. By providing\ncategory-specific diagnostics, Z-Scores enable researchers to identify model\nfailure modes and design targeted interventions -- such as tailored prompts or\ndata augmentation -- yielding measurable performance improvements. A case study\nwith LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies\nhidden in aggregate F1, directly informing model refinement strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating disfluency removal in speech requires more than aggregate\ntoken-level scores. Traditional word-based metrics such as precision, recall,\nand F1 (E-Scores) capture overall performance but cannot reveal why models\nsucceed or fail. We introduce Z-Scores, a span-level linguistically-grounded\nevaluation metric that categorizes system behavior across distinct disfluency\ntypes (EDITED, INTJ, PRN). Our deterministic alignment module enables robust\nmapping between generated text and disfluent transcripts, allowing Z-Scores to\nexpose systematic weaknesses that word-level metrics obscure. By providing\ncategory-specific diagnostics, Z-Scores enable researchers to identify model\nfailure modes and design targeted interventions -- such as tailored prompts or\ndata augmentation -- yielding measurable performance improvements. A case study\nwith LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies\nhidden in aggregate F1, directly informing model refinement strategies."
                },
                "authors": [
                    {
                        "name": "Maria Teleki"
                    },
                    {
                        "name": "Sai Janjur"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Oliver Grabner"
                    },
                    {
                        "name": "Ketan Verma"
                    },
                    {
                        "name": "Thomas Docog"
                    },
                    {
                        "name": "Xiangjue Dong"
                    },
                    {
                        "name": "Lingfeng Shi"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Stephanie Birkelbach"
                    },
                    {
                        "name": "Jason Kim"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "James Caverlee"
                    }
                ],
                "author_detail": {
                    "name": "James Caverlee"
                },
                "author": "James Caverlee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20318v1",
                "updated": "2025-09-24T17:01:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    1,
                    50,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:01:50Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    1,
                    50,
                    2,
                    267,
                    0
                ],
                "title": "A Comprehensive Evaluation of YOLO-based Deer Detection Performance on\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation of YOLO-based Deer Detection Performance on\n  Edge Devices"
                },
                "summary": "The escalating economic losses in agriculture due to deer intrusion,\nestimated to be in the hundreds of millions of dollars annually in the U.S.,\nhighlight the inadequacy of traditional mitigation strategies since these\nmethods are often labor-intensive, costly, and ineffective for modern farming\nsystems. To overcome this, there is a critical need for intelligent, autonomous\nsolutions which require accurate and efficient deer detection. But the progress\nin this field is impeded by a significant gap in the literature, mainly the\nlack of a domain-specific, practical dataset and limited study on the on-field\ndeployability of deer detection systems. Addressing this gap, this study\npresents a comprehensive evaluation of state-of-the-art deep learning models\nfor deer detection in challenging real-world scenarios. The contributions of\nthis work are threefold. First, we introduce a curated, publicly available\ndataset of 3,095 annotated images with bounding-box annotations of deer,\nderived from the Idaho Cameratraps project. Second, we provide an extensive\ncomparative analysis of 12 model variants across four recent YOLO\narchitectures(v8, v9, v10, and v11). Finally, we benchmarked performance on a\nhigh-end NVIDIA RTX 5090 GPU and evaluated on two representative edge computing\nplatforms: Raspberry Pi 5 and NVIDIA Jetson AGX Xavier. Results show that the\nreal-time detection is not feasible in Raspberry Pi without hardware-specific\nmodel optimization, while NVIDIA Jetson provides greater than 30 FPS with\nGPU-accelerated inference on 's' and 'n' series models. This study also reveals\nthat smaller, architecturally advanced models such as YOLOv11n, YOLOv8s, and\nYOLOv9s offer the optimal balance of high accuracy (AP@.5 > 0.85) and\ncomputational efficiency (FPS > 30). To support further research, both the\nsource code and datasets are publicly available at\nhttps://github.com/WinnerBishal/track-the-deer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The escalating economic losses in agriculture due to deer intrusion,\nestimated to be in the hundreds of millions of dollars annually in the U.S.,\nhighlight the inadequacy of traditional mitigation strategies since these\nmethods are often labor-intensive, costly, and ineffective for modern farming\nsystems. To overcome this, there is a critical need for intelligent, autonomous\nsolutions which require accurate and efficient deer detection. But the progress\nin this field is impeded by a significant gap in the literature, mainly the\nlack of a domain-specific, practical dataset and limited study on the on-field\ndeployability of deer detection systems. Addressing this gap, this study\npresents a comprehensive evaluation of state-of-the-art deep learning models\nfor deer detection in challenging real-world scenarios. The contributions of\nthis work are threefold. First, we introduce a curated, publicly available\ndataset of 3,095 annotated images with bounding-box annotations of deer,\nderived from the Idaho Cameratraps project. Second, we provide an extensive\ncomparative analysis of 12 model variants across four recent YOLO\narchitectures(v8, v9, v10, and v11). Finally, we benchmarked performance on a\nhigh-end NVIDIA RTX 5090 GPU and evaluated on two representative edge computing\nplatforms: Raspberry Pi 5 and NVIDIA Jetson AGX Xavier. Results show that the\nreal-time detection is not feasible in Raspberry Pi without hardware-specific\nmodel optimization, while NVIDIA Jetson provides greater than 30 FPS with\nGPU-accelerated inference on 's' and 'n' series models. This study also reveals\nthat smaller, architecturally advanced models such as YOLOv11n, YOLOv8s, and\nYOLOv9s offer the optimal balance of high accuracy (AP@.5 > 0.85) and\ncomputational efficiency (FPS > 30). To support further research, both the\nsource code and datasets are publicly available at\nhttps://github.com/WinnerBishal/track-the-deer."
                },
                "authors": [
                    {
                        "name": "Bishal Adhikari"
                    },
                    {
                        "name": "Jiajia Li"
                    },
                    {
                        "name": "Eric S. Michel"
                    },
                    {
                        "name": "Jacob Dykes"
                    },
                    {
                        "name": "Te-Ming Paul Tseng"
                    },
                    {
                        "name": "Mary Love Tagert"
                    },
                    {
                        "name": "Dong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Dong Chen"
                },
                "author": "Dong Chen",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20317v1",
                "updated": "2025-09-24T17:01:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    1,
                    32,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:01:32Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    1,
                    32,
                    2,
                    267,
                    0
                ],
                "title": "SIM-CoT: Supervised Implicit Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIM-CoT: Supervised Implicit Chain-of-Thought"
                },
                "summary": "Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient\nalternative to explicit CoT reasoning in Large Language Models (LLMs), but a\npersistent performance gap has limited the application of implicit CoT. We\nidentify a core latent instability issue by scaling the computational budget of\nimplicit CoT approaches: as we increase the number of implicit reasoning tokens\nto enhance performance, the training process often becomes unstable and\ncollapses. Our analysis reveals that this instability arises from the latent\nrepresentations becoming homogeneous and losing their semantic diversity, a\nfailure caused by insufficient step-level supervision in existing implicit CoT\napproaches. To address this issue, we propose SIM-CoT, a plug-and-play training\nmodule that introduces step-level supervision to stabilize and enrich the\nlatent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder\nduring training to align each implicit token with its corresponding explicit\nreasoning step, ensuring that latent states capture distinct and meaningful\ninformation. The proposed auxiliary decoder is removed during inference,\npreserving the computational efficiency of implicit CoT methods with no added\noverhead. In addition, the auxiliary decoder affords interpretability of\nimplicit reasoning by projecting each latent token onto an explicit reasoning\nvocabulary, enabling per-step visualization of semantic roles and diagnosis.\nSIM-CoT significantly enhances both the in-domain accuracy and out-of-domain\nstability of various implicit CoT methods, boosting baselines like Coconut by\n+8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong\nscalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1%\nwith 2.3\\times greater token efficiency, while substantially closing the\nperformance gap on larger models like LLaMA-3.1 8B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient\nalternative to explicit CoT reasoning in Large Language Models (LLMs), but a\npersistent performance gap has limited the application of implicit CoT. We\nidentify a core latent instability issue by scaling the computational budget of\nimplicit CoT approaches: as we increase the number of implicit reasoning tokens\nto enhance performance, the training process often becomes unstable and\ncollapses. Our analysis reveals that this instability arises from the latent\nrepresentations becoming homogeneous and losing their semantic diversity, a\nfailure caused by insufficient step-level supervision in existing implicit CoT\napproaches. To address this issue, we propose SIM-CoT, a plug-and-play training\nmodule that introduces step-level supervision to stabilize and enrich the\nlatent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder\nduring training to align each implicit token with its corresponding explicit\nreasoning step, ensuring that latent states capture distinct and meaningful\ninformation. The proposed auxiliary decoder is removed during inference,\npreserving the computational efficiency of implicit CoT methods with no added\noverhead. In addition, the auxiliary decoder affords interpretability of\nimplicit reasoning by projecting each latent token onto an explicit reasoning\nvocabulary, enabling per-step visualization of semantic roles and diagnosis.\nSIM-CoT significantly enhances both the in-domain accuracy and out-of-domain\nstability of various implicit CoT methods, boosting baselines like Coconut by\n+8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong\nscalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1%\nwith 2.3\\times greater token efficiency, while substantially closing the\nperformance gap on larger models like LLaMA-3.1 8B."
                },
                "authors": [
                    {
                        "name": "Xilin Wei"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01841v2",
                "updated": "2025-09-24T17:00:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    0,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2024-09-27T23:05:02Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    23,
                    5,
                    2,
                    4,
                    271,
                    0
                ],
                "title": "A GEN AI Framework for Medical Note Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A GEN AI Framework for Medical Note Generation"
                },
                "summary": "The increasing administrative burden of medical documentation, particularly\nthrough Electronic Health Records (EHR), significantly reduces the time\navailable for direct patient care and contributes to physician burnout. To\naddress this issue, we propose MediNotes, an advanced generative AI framework\ndesigned to automate the creation of SOAP (Subjective, Objective, Assessment,\nPlan) notes from medical conversations. MediNotes integrates Large Language\nModels (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech\nRecognition (ASR) to capture and process both text and voice inputs in real\ntime or from recorded audio, generating structured and contextually accurate\nmedical notes. The framework also incorporates advanced techniques like\nQuantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning\n(PEFT) for efficient model fine-tuning in resource-constrained environments.\nAdditionally, MediNotes offers a query-based retrieval system, allowing\nhealthcare providers and patients to access relevant medical information\nquickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate\nthat MediNotes significantly improves the accuracy, efficiency, and usability\nof automated medical documentation, offering a robust solution to reduce the\nadministrative burden on healthcare professionals while improving the quality\nof clinical workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing administrative burden of medical documentation, particularly\nthrough Electronic Health Records (EHR), significantly reduces the time\navailable for direct patient care and contributes to physician burnout. To\naddress this issue, we propose MediNotes, an advanced generative AI framework\ndesigned to automate the creation of SOAP (Subjective, Objective, Assessment,\nPlan) notes from medical conversations. MediNotes integrates Large Language\nModels (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech\nRecognition (ASR) to capture and process both text and voice inputs in real\ntime or from recorded audio, generating structured and contextually accurate\nmedical notes. The framework also incorporates advanced techniques like\nQuantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning\n(PEFT) for efficient model fine-tuning in resource-constrained environments.\nAdditionally, MediNotes offers a query-based retrieval system, allowing\nhealthcare providers and patients to access relevant medical information\nquickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate\nthat MediNotes significantly improves the accuracy, efficiency, and usability\nof automated medical documentation, offering a robust solution to reduce the\nadministrative burden on healthcare professionals while improving the quality\nof clinical workflows."
                },
                "authors": [
                    {
                        "name": "Hui Yi Leong"
                    },
                    {
                        "name": "Yi Fan Gao"
                    },
                    {
                        "name": "Shuai Ji"
                    },
                    {
                        "name": "Bora Kalaycioglu"
                    },
                    {
                        "name": "Uktu Pamuksuz"
                    }
                ],
                "author_detail": {
                    "name": "Uktu Pamuksuz"
                },
                "author": "Uktu Pamuksuz",
                "arxiv_comment": "8 Figures, 7 page, IEEE standard research paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09324v3",
                "updated": "2025-09-24T16:59:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    59,
                    19,
                    2,
                    267,
                    0
                ],
                "published": "2024-09-14T06:02:17Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    6,
                    2,
                    17,
                    5,
                    258,
                    0
                ],
                "title": "Efficient Fine-Tuning of Large Language Models for Automated Medical\n  Documentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Fine-Tuning of Large Language Models for Automated Medical\n  Documentation"
                },
                "summary": "Scientific research indicates that for every hour spent in direct patient\ncare, physicians spend nearly two additional hours on administrative tasks,\nparticularly on electronic health records (EHRs) and desk work. This excessive\nadministrative burden not only reduces the time available for patient care but\nalso contributes to physician burnout and inefficiencies in healthcare\ndelivery. To address these challenges, this study introduces MediGen, a\nfine-tuned large language model (LLM) designed to automate the generation of\nmedical reports from medical dialogues. By leveraging state-of-the-art\nmethodologies for fine-tuning open-source pretrained models, including\nLLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing\nclinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising\nresults, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating\nits effectiveness in generating accurate and clinically relevant medical\nreports. These findings suggest that MediGen has the potential to significantly\nreduce the administrative workload on physicians, improving both healthcare\nefficiency and physician well-being.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific research indicates that for every hour spent in direct patient\ncare, physicians spend nearly two additional hours on administrative tasks,\nparticularly on electronic health records (EHRs) and desk work. This excessive\nadministrative burden not only reduces the time available for patient care but\nalso contributes to physician burnout and inefficiencies in healthcare\ndelivery. To address these challenges, this study introduces MediGen, a\nfine-tuned large language model (LLM) designed to automate the generation of\nmedical reports from medical dialogues. By leveraging state-of-the-art\nmethodologies for fine-tuning open-source pretrained models, including\nLLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing\nclinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising\nresults, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating\nits effectiveness in generating accurate and clinically relevant medical\nreports. These findings suggest that MediGen has the potential to significantly\nreduce the administrative workload on physicians, improving both healthcare\nefficiency and physician well-being."
                },
                "authors": [
                    {
                        "name": "Hui Yi Leong"
                    },
                    {
                        "name": "Yi Fan Gao"
                    },
                    {
                        "name": "Ji Shuai"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Uktu Pamuksuz"
                    }
                ],
                "author_detail": {
                    "name": "Uktu Pamuksuz"
                },
                "author": "Uktu Pamuksuz",
                "arxiv_doi": "10.13140/RG.2.2.26884.74881",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.26884.74881",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.09324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "4 pages, 3 Figures, 3 Tables. The final version will be published in\n  the proceedings of the IEEE conference",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v2",
                "updated": "2025-09-24T16:56:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    56,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective"
                },
                "summary": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08540v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08540v3",
                "updated": "2025-09-24T16:51:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    51,
                    16,
                    2,
                    267,
                    0
                ],
                "published": "2025-07-11T12:39:25Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    39,
                    25,
                    4,
                    192,
                    0
                ],
                "title": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection"
                },
                "summary": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications."
                },
                "authors": [
                    {
                        "name": "Ioannis Lamprou"
                    },
                    {
                        "name": "Alexander Shevtsov"
                    },
                    {
                        "name": "Ioannis Arapakis"
                    },
                    {
                        "name": "Sotiris Ioannidis"
                    }
                ],
                "author_detail": {
                    "name": "Sotiris Ioannidis"
                },
                "author": "Sotiris Ioannidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08540v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08540v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22323v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22323v2",
                "updated": "2025-09-24T16:48:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    48,
                    33,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-28T13:09:47Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    9,
                    47,
                    2,
                    148,
                    0
                ],
                "title": "Advancing Expert Specialization for Better MoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Expert Specialization for Better MoE"
                },
                "summary": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community."
                },
                "authors": [
                    {
                        "name": "Hongcan Guo"
                    },
                    {
                        "name": "Haolang Lu"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Bolun Chu"
                    },
                    {
                        "name": "Jialin Zhuang"
                    },
                    {
                        "name": "Yuan Yang"
                    },
                    {
                        "name": "Wenhao Che"
                    },
                    {
                        "name": "Sicong Leng"
                    },
                    {
                        "name": "Qimei Cui"
                    },
                    {
                        "name": "Xudong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xudong Jiang"
                },
                "author": "Xudong Jiang",
                "arxiv_comment": "33pages, 6figures(Accepted by Neurips 2026 Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22323v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22323v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22931v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22931v3",
                "updated": "2025-09-24T16:41:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    41,
                    40,
                    2,
                    267,
                    0
                ],
                "published": "2025-07-24T13:46:51Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    46,
                    51,
                    3,
                    205,
                    0
                ],
                "title": "Enhancing RAG Efficiency with Adaptive Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing RAG Efficiency with Adaptive Context Compression"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy."
                },
                "authors": [
                    {
                        "name": "Shuyu Guo"
                    },
                    {
                        "name": "Shuo Zhang"
                    },
                    {
                        "name": "Zhaochun Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhaochun Ren"
                },
                "author": "Zhaochun Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22931v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22931v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09873v2",
                "updated": "2025-09-24T16:38:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    38,
                    38,
                    2,
                    267,
                    0
                ],
                "published": "2024-11-15T01:48:08Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    48,
                    8,
                    4,
                    320,
                    0
                ],
                "title": "LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing\n  Online Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing\n  Online Learners"
                },
                "summary": "Intelligent tutoring systems (ITS) using artificial intelligence (AI)\ntechnology have shown promise in supporting learners with diverse abilities.\nLarge language models (LLMs) provide new opportunities to incorporate personas\nto AI-based tutors and support dynamic interactive dialogue. This paper\nexplores how DHH learners interact with LLM-powered AI tutors with different\nexperiences in DHH education as personas to identify their accessibility\npreferences. A user study with 16 DHH participants showed that they asked\nDHH-related questions based on background information and evaluated the AI\ntutors' cultural knowledge of the DHH communities in their responses.\nParticipants suggested providing more transparency in each AI tutor's position\nwithin the DHH community. Participants also pointed out the lack of support in\nthe multimodality of sign language in current LLMs. We discuss design\nimplications to support the diverse needs in interaction between DHH users and\nthe LLMs, such as offering supports in tuning language styles of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent tutoring systems (ITS) using artificial intelligence (AI)\ntechnology have shown promise in supporting learners with diverse abilities.\nLarge language models (LLMs) provide new opportunities to incorporate personas\nto AI-based tutors and support dynamic interactive dialogue. This paper\nexplores how DHH learners interact with LLM-powered AI tutors with different\nexperiences in DHH education as personas to identify their accessibility\npreferences. A user study with 16 DHH participants showed that they asked\nDHH-related questions based on background information and evaluated the AI\ntutors' cultural knowledge of the DHH communities in their responses.\nParticipants suggested providing more transparency in each AI tutor's position\nwithin the DHH community. Participants also pointed out the lack of support in\nthe multimodality of sign language in current LLMs. We discuss design\nimplications to support the diverse needs in interaction between DHH users and\nthe LLMs, such as offering supports in tuning language styles of LLMs."
                },
                "authors": [
                    {
                        "name": "Haocong Cheng"
                    },
                    {
                        "name": "Si Chen"
                    },
                    {
                        "name": "Christopher Perdriau"
                    },
                    {
                        "name": "Shriya Mokkapati"
                    },
                    {
                        "name": "Yun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yun Huang"
                },
                "author": "Yun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20293v1",
                "updated": "2025-09-24T16:26:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    26,
                    47,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T16:26:47Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    26,
                    47,
                    2,
                    267,
                    0
                ],
                "title": "When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks\n  Silently Undermine Validity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks\n  Silently Undermine Validity"
                },
                "summary": "LLM-judged benchmarks are increasingly used to evaluate complex model\nbehaviors, yet their design introduces failure modes absent in conventional\nground-truth based benchmarks. We argue that without tight objectives and\nverifiable constructions, benchmark rankings can produce high-confidence\nrankings that are in fact largely noise. We introduce two mechanisms to\ndiagnose these issues. Schematic adherence quantifies how much of a judge's\noverall verdict is explained by the explicit evaluation schema, revealing\nunexplained variance when judges deviate from their own rubric. Psychometric\nvalidity aggregates internal consistency and discriminant validity signals to\nquantify irreducible uncertainty in any benchmarking run. Applying these tools\nto Arena-Hard Auto, we find severe schema incoherence and factor collapse\nacross popular judges: for example, unexplained variance exceeding 90 percent\nfor DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We\nalso show that the ELO-style aggregation used by Arena-Hard Auto collapses and\nmasks genuine ranking uncertainty. Our results highlight design failures that\nundermine validity and offer actionable principles for building better-scoped,\nreliability-aware LLM-judged benchmarks. We release our code at\nhttps://anonymous.4open.science/r/judgment-to-noise-947D/README.md",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-judged benchmarks are increasingly used to evaluate complex model\nbehaviors, yet their design introduces failure modes absent in conventional\nground-truth based benchmarks. We argue that without tight objectives and\nverifiable constructions, benchmark rankings can produce high-confidence\nrankings that are in fact largely noise. We introduce two mechanisms to\ndiagnose these issues. Schematic adherence quantifies how much of a judge's\noverall verdict is explained by the explicit evaluation schema, revealing\nunexplained variance when judges deviate from their own rubric. Psychometric\nvalidity aggregates internal consistency and discriminant validity signals to\nquantify irreducible uncertainty in any benchmarking run. Applying these tools\nto Arena-Hard Auto, we find severe schema incoherence and factor collapse\nacross popular judges: for example, unexplained variance exceeding 90 percent\nfor DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We\nalso show that the ELO-style aggregation used by Arena-Hard Auto collapses and\nmasks genuine ranking uncertainty. Our results highlight design failures that\nundermine validity and offer actionable principles for building better-scoped,\nreliability-aware LLM-judged benchmarks. We release our code at\nhttps://anonymous.4open.science/r/judgment-to-noise-947D/README.md"
                },
                "authors": [
                    {
                        "name": "Benjamin Feuer"
                    },
                    {
                        "name": "Chiung-Yi Tseng"
                    },
                    {
                        "name": "Astitwa Sarthak Lathe"
                    },
                    {
                        "name": "Oussama Elachqar"
                    },
                    {
                        "name": "John P Dickerson"
                    }
                ],
                "author_detail": {
                    "name": "John P Dickerson"
                },
                "author": "John P Dickerson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20278v1",
                "updated": "2025-09-24T16:15:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    15,
                    26,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T16:15:26Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    15,
                    26,
                    2,
                    267,
                    0
                ],
                "title": "Instruction Boundary: Quantifying Biases in LLM Reasoning under Various\n  Coverage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Boundary: Quantifying Biases in LLM Reasoning under Various\n  Coverage"
                },
                "summary": "Large-language-model (LLM) reasoning has long been regarded as a powerful\ntool for problem solving across domains, providing non-experts with valuable\nadvice. However, their limitations - especially those stemming from prompt\ndesign - remain underexplored. Because users may supply biased or incomplete\nprompts - often unintentionally - LLMs can be misled, undermining reliability\nand creating risks. We refer to this vulnerability as the Instruction Boundary.\nTo investigate the phenomenon, we distill it into eight concrete facets and\nintroduce BiasDetector, a framework that measures biases arising from three\ninstruction types: complete, redundant, and insufficient. We evaluate several\nmainstream LLMs and find that, despite high headline accuracy, substantial\nbiases persist in many downstream tasks as a direct consequence of prompt\ncoverage. Our empirical study confirms that LLM reasoning reliability can still\nbe significantly improved. We analyze the practical impact of these biases and\noutline mitigation strategies. Our findings underscore the need for developers\nto tackle biases and for users to craft options carefully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-language-model (LLM) reasoning has long been regarded as a powerful\ntool for problem solving across domains, providing non-experts with valuable\nadvice. However, their limitations - especially those stemming from prompt\ndesign - remain underexplored. Because users may supply biased or incomplete\nprompts - often unintentionally - LLMs can be misled, undermining reliability\nand creating risks. We refer to this vulnerability as the Instruction Boundary.\nTo investigate the phenomenon, we distill it into eight concrete facets and\nintroduce BiasDetector, a framework that measures biases arising from three\ninstruction types: complete, redundant, and insufficient. We evaluate several\nmainstream LLMs and find that, despite high headline accuracy, substantial\nbiases persist in many downstream tasks as a direct consequence of prompt\ncoverage. Our empirical study confirms that LLM reasoning reliability can still\nbe significantly improved. We analyze the practical impact of these biases and\noutline mitigation strategies. Our findings underscore the need for developers\nto tackle biases and for users to craft options carefully."
                },
                "authors": [
                    {
                        "name": "Zipeng Ling"
                    },
                    {
                        "name": "Yuehao Tang"
                    },
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Shuliang Liu"
                    },
                    {
                        "name": "Gaoyang Jiang"
                    },
                    {
                        "name": "Shenghong Fu"
                    },
                    {
                        "name": "Junqi Yang"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Jiawan Zhang"
                    },
                    {
                        "name": "Kejia Huang"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20277v1",
                "updated": "2025-09-24T16:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    15,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T16:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    15,
                    17,
                    2,
                    267,
                    0
                ],
                "title": "Investigating Security Implications of Automatically Generated Code on\n  the Software Supply Chain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Security Implications of Automatically Generated Code on\n  the Software Supply Chain"
                },
                "summary": "In recent years, various software supply chain (SSC) attacks have posed\nsignificant risks to the global community. Severe consequences may arise if\ndevelopers integrate insecure code snippets that are vulnerable to SSC attacks\ninto their products. Particularly, code generation techniques, such as large\nlanguage models (LLMs), have been widely utilized in the developer community.\nHowever, LLMs are known to suffer from inherent issues when generating code,\nincluding fabrication, misinformation, and reliance on outdated training data,\nall of which can result in serious software supply chain threats. In this\npaper, we investigate the security threats to the SSC that arise from these\ninherent issues. We examine three categories of threats, including eleven\npotential SSC-related threats, related to external components in source code,\nand continuous integration configuration files. We find some threats in\nLLM-generated code could enable attackers to hijack software and workflows,\nwhile some others might cause potential hidden threats that compromise the\nsecurity of the software over time. To understand these security impacts and\nseverity, we design a tool, SSCGuard, to generate 439,138 prompts based on\nSSC-related questions collected online, and analyze the responses of four\npopular LLMs from GPT and Llama. Our results show that all identified\nSSC-related threats persistently exist. To mitigate these risks, we propose a\nnovel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce\nfabrication, and a middleware-based defense that informs users of various SSC\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, various software supply chain (SSC) attacks have posed\nsignificant risks to the global community. Severe consequences may arise if\ndevelopers integrate insecure code snippets that are vulnerable to SSC attacks\ninto their products. Particularly, code generation techniques, such as large\nlanguage models (LLMs), have been widely utilized in the developer community.\nHowever, LLMs are known to suffer from inherent issues when generating code,\nincluding fabrication, misinformation, and reliance on outdated training data,\nall of which can result in serious software supply chain threats. In this\npaper, we investigate the security threats to the SSC that arise from these\ninherent issues. We examine three categories of threats, including eleven\npotential SSC-related threats, related to external components in source code,\nand continuous integration configuration files. We find some threats in\nLLM-generated code could enable attackers to hijack software and workflows,\nwhile some others might cause potential hidden threats that compromise the\nsecurity of the software over time. To understand these security impacts and\nseverity, we design a tool, SSCGuard, to generate 439,138 prompts based on\nSSC-related questions collected online, and analyze the responses of four\npopular LLMs from GPT and Llama. Our results show that all identified\nSSC-related threats persistently exist. To mitigate these risks, we propose a\nnovel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce\nfabrication, and a middleware-based defense that informs users of various SSC\nthreats."
                },
                "authors": [
                    {
                        "name": "Xiaofan Li"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03148v2",
                "updated": "2025-09-24T16:07:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    7,
                    19,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-03T08:57:10Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    57,
                    10,
                    2,
                    246,
                    0
                ],
                "title": "Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan,\n  Sutsilvan, Surmiran, Puter, and Vallader",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan,\n  Sutsilvan, Surmiran, Puter, and Vallader"
                },
                "summary": "The Romansh language, spoken in Switzerland, has limited resources for\nmachine translation evaluation. In this paper, we present a benchmark for six\nvarieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five\nregional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our\nreference translations were created by human translators based on the WMT24++\nbenchmark, which ensures parallelism with more than 55 other languages. An\nautomatic evaluation of existing MT systems and LLMs shows that translation out\nof Romansh into German is handled relatively well for all the varieties, but\ntranslation into Romansh is still challenging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Romansh language, spoken in Switzerland, has limited resources for\nmachine translation evaluation. In this paper, we present a benchmark for six\nvarieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five\nregional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our\nreference translations were created by human translators based on the WMT24++\nbenchmark, which ensures parallelism with more than 55 other languages. An\nautomatic evaluation of existing MT systems and LLMs shows that translation out\nof Romansh into German is handled relatively well for all the varieties, but\ntranslation into Romansh is still challenging."
                },
                "authors": [
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Ignacio Pérez Prat"
                    },
                    {
                        "name": "Not Battesta Soliva"
                    },
                    {
                        "name": "Sandra Baltermia-Guetg"
                    },
                    {
                        "name": "Andrina Beeli"
                    },
                    {
                        "name": "Simona Beeli"
                    },
                    {
                        "name": "Madlaina Capeder"
                    },
                    {
                        "name": "Laura Decurtins"
                    },
                    {
                        "name": "Gian Peder Gregori"
                    },
                    {
                        "name": "Flavia Hobi"
                    },
                    {
                        "name": "Gabriela Holderegger"
                    },
                    {
                        "name": "Arina Lazzarini"
                    },
                    {
                        "name": "Viviana Lazzarini"
                    },
                    {
                        "name": "Walter Rosselli"
                    },
                    {
                        "name": "Bettina Vital"
                    },
                    {
                        "name": "Anna Rutkiewicz"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "WMT25 (Open Language Data Initiative Shared Task)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20270v1",
                "updated": "2025-09-24T16:04:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    4,
                    11,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T16:04:11Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    4,
                    11,
                    2,
                    267,
                    0
                ],
                "title": "Scan-do Attitude: Towards Autonomous CT Protocol Management using a\n  Large Language Model Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scan-do Attitude: Towards Autonomous CT Protocol Management using a\n  Large Language Model Agent"
                },
                "summary": "Managing scan protocols in Computed Tomography (CT), which includes adjusting\nacquisition parameters or configuring reconstructions, as well as selecting\npostprocessing tools in a patient-specific manner, is time-consuming and\nrequires clinical as well as technical expertise. At the same time, we observe\nan increasing shortage of skilled workforce in radiology. To address this\nissue, a Large Language Model (LLM)-based agent framework is proposed to assist\nwith the interpretation and execution of protocol configuration requests given\nin natural language or a structured, device-independent format, aiming to\nimprove the workflow efficiency and reduce technologists' workload. The agent\ncombines in-context-learning, instruction-following, and structured toolcalling\nabilities to identify relevant protocol elements and apply accurate\nmodifications. In a systematic evaluation, experimental results indicate that\nthe agent can effectively retrieve protocol components, generate device\ncompatible protocol definition files, and faithfully implement user requests.\nDespite demonstrating feasibility in principle, the approach faces limitations\nregarding syntactic and semantic validity due to lack of a unified device API,\nand challenges with ambiguous or complex requests. In summary, the findings\nshow a clear path towards LLM-based agents for supporting scan protocol\nmanagement in CT imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing scan protocols in Computed Tomography (CT), which includes adjusting\nacquisition parameters or configuring reconstructions, as well as selecting\npostprocessing tools in a patient-specific manner, is time-consuming and\nrequires clinical as well as technical expertise. At the same time, we observe\nan increasing shortage of skilled workforce in radiology. To address this\nissue, a Large Language Model (LLM)-based agent framework is proposed to assist\nwith the interpretation and execution of protocol configuration requests given\nin natural language or a structured, device-independent format, aiming to\nimprove the workflow efficiency and reduce technologists' workload. The agent\ncombines in-context-learning, instruction-following, and structured toolcalling\nabilities to identify relevant protocol elements and apply accurate\nmodifications. In a systematic evaluation, experimental results indicate that\nthe agent can effectively retrieve protocol components, generate device\ncompatible protocol definition files, and faithfully implement user requests.\nDespite demonstrating feasibility in principle, the approach faces limitations\nregarding syntactic and semantic validity due to lack of a unified device API,\nand challenges with ambiguous or complex requests. In summary, the findings\nshow a clear path towards LLM-based agents for supporting scan protocol\nmanagement in CT imaging."
                },
                "authors": [
                    {
                        "name": "Xingjian Kang"
                    },
                    {
                        "name": "Linda Vorberg"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Alexander Katzmann"
                    },
                    {
                        "name": "Oliver Taubmann"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Taubmann"
                },
                "author": "Oliver Taubmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.11123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.11123v2",
                "updated": "2025-09-24T15:57:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    57,
                    10,
                    2,
                    267,
                    0
                ],
                "published": "2023-06-19T18:54:51Z",
                "published_parsed": [
                    2023,
                    6,
                    19,
                    18,
                    54,
                    51,
                    0,
                    170,
                    0
                ],
                "title": "To Fold or Not to Fold: Graph Regularized Tensor Train for Visual Data\n  Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Fold or Not to Fold: Graph Regularized Tensor Train for Visual Data\n  Completion"
                },
                "summary": "Tensor train (TT) representation has achieved tremendous success in visual\ndata completion tasks, especially when it is combined with tensor folding.\nHowever, folding an image or video tensor breaks the original data structure,\nleading to local information loss as nearby pixels may be assigned into\ndifferent dimensions and become far away from each other. In this paper, to\nfully preserve the local information of the original visual data, we explore\nnot folding the data tensor, and at the same time adopt graph information to\nregularize local similarity between nearby entries. To overcome the high\ncomputational complexity introduced by the graph-based regularization in the TT\ncompletion problem, we propose to break the original problem into multiple\nsub-problems with respect to each TT core fiber, instead of each TT core as in\ntraditional methods. Furthermore, to avoid heavy parameter tuning, a sparsity\npromoting probabilistic model is built based on the generalized inverse\nGaussian (GIG) prior, and an inference algorithm is derived under the\nmean-field approximation. Experiments on both synthetic data and real-world\nvisual data show the superiority of the proposed methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor train (TT) representation has achieved tremendous success in visual\ndata completion tasks, especially when it is combined with tensor folding.\nHowever, folding an image or video tensor breaks the original data structure,\nleading to local information loss as nearby pixels may be assigned into\ndifferent dimensions and become far away from each other. In this paper, to\nfully preserve the local information of the original visual data, we explore\nnot folding the data tensor, and at the same time adopt graph information to\nregularize local similarity between nearby entries. To overcome the high\ncomputational complexity introduced by the graph-based regularization in the TT\ncompletion problem, we propose to break the original problem into multiple\nsub-problems with respect to each TT core fiber, instead of each TT core as in\ntraditional methods. Furthermore, to avoid heavy parameter tuning, a sparsity\npromoting probabilistic model is built based on the generalized inverse\nGaussian (GIG) prior, and an inference algorithm is derived under the\nmean-field approximation. Experiments on both synthetic data and real-world\nvisual data show the superiority of the proposed methods."
                },
                "authors": [
                    {
                        "name": "Le Xu"
                    },
                    {
                        "name": "Lei Cheng"
                    },
                    {
                        "name": "Ngai Wong"
                    },
                    {
                        "name": "Yik-Chung Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yik-Chung Wu"
                },
                "author": "Yik-Chung Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.11123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.11123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14264v2",
                "updated": "2025-09-24T15:47:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    47,
                    49,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-20T12:13:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    12,
                    13,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "AAPO: Enhancing the Reasoning Capabilities of LLMs with Advantage\n  Momentum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AAPO: Enhancing the Reasoning Capabilities of LLMs with Advantage\n  Momentum"
                },
                "summary": "Reinforcement learning (RL) has emerged as an effective approach for\nenhancing the reasoning capabilities of large language models (LLMs),\nespecially in scenarios where supervised fine-tuning (SFT) falls short due to\nlimited chain-of-thought (CoT) data. Among RL-based post-training methods,\ngroup relative advantage estimation, as exemplified by Group Relative Policy\nOptimization (GRPO), has attracted considerable attention for eliminating the\ndependency on the value model, thereby simplifying training compared to\ntraditional approaches like Proximal Policy Optimization (PPO). However, we\nobserve that exsiting group relative advantage estimation method still suffers\nfrom training inefficiencies, particularly when the estimated advantage\napproaches zero. To address this limitation, we propose Advantage-Augmented\nPolicy Optimization (AAPO), a novel RL algorithm that optimizes the\ncross-entropy (CE) loss using advantages enhanced through a momentum-based\nestimation scheme. This approach effectively mitigates the inefficiencies\nassociated with group relative advantage estimation. Experimental results on\nmultiple mathematical reasoning benchmarks demonstrate the superior performance\nof AAPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has emerged as an effective approach for\nenhancing the reasoning capabilities of large language models (LLMs),\nespecially in scenarios where supervised fine-tuning (SFT) falls short due to\nlimited chain-of-thought (CoT) data. Among RL-based post-training methods,\ngroup relative advantage estimation, as exemplified by Group Relative Policy\nOptimization (GRPO), has attracted considerable attention for eliminating the\ndependency on the value model, thereby simplifying training compared to\ntraditional approaches like Proximal Policy Optimization (PPO). However, we\nobserve that exsiting group relative advantage estimation method still suffers\nfrom training inefficiencies, particularly when the estimated advantage\napproaches zero. To address this limitation, we propose Advantage-Augmented\nPolicy Optimization (AAPO), a novel RL algorithm that optimizes the\ncross-entropy (CE) loss using advantages enhanced through a momentum-based\nestimation scheme. This approach effectively mitigates the inefficiencies\nassociated with group relative advantage estimation. Experimental results on\nmultiple mathematical reasoning benchmarks demonstrate the superior performance\nof AAPO."
                },
                "authors": [
                    {
                        "name": "Jian Xiong"
                    },
                    {
                        "name": "Jingbo Zhou"
                    },
                    {
                        "name": "Jingyong Ye"
                    },
                    {
                        "name": "Qiang Huang"
                    },
                    {
                        "name": "Dejing Dou"
                    }
                ],
                "author_detail": {
                    "name": "Dejing Dou"
                },
                "author": "Dejing Dou",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03547v2",
                "updated": "2025-09-24T15:41:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    41,
                    44,
                    2,
                    267,
                    0
                ],
                "published": "2025-08-05T15:15:35Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    15,
                    15,
                    35,
                    1,
                    217,
                    0
                ],
                "title": "Guided Reality: Generating Visually-Enriched AR Task Guidance with LLMs\n  and Vision Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided Reality: Generating Visually-Enriched AR Task Guidance with LLMs\n  and Vision Models"
                },
                "summary": "Large language models (LLMs) have enabled the automatic generation of\nstep-by-step augmented reality (AR) instructions for a wide range of physical\ntasks. However, existing LLM-based AR guidance often lacks rich visual\naugmentations to effectively embed instructions into spatial context for a\nbetter user understanding. We present Guided Reality, a fully automated AR\nsystem that generates embedded and dynamic visual guidance based on\nstep-by-step instructions. Our system integrates LLMs and vision models to: 1)\ngenerate multi-step instructions from user queries, 2) identify appropriate\ntypes of visual guidance, 3) extract spatial information about key interaction\npoints in the real world, and 4) embed visual guidance in physical space to\nsupport task execution. Drawing from a corpus of user manuals, we define five\ncategories of visual guidance and propose an identification strategy based on\nthe current step. We evaluate the system through a user study (N=16),\ncompleting real-world tasks and exploring the system in the wild. Additionally,\nfour instructors shared insights on how Guided Reality could be integrated into\ntheir training workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have enabled the automatic generation of\nstep-by-step augmented reality (AR) instructions for a wide range of physical\ntasks. However, existing LLM-based AR guidance often lacks rich visual\naugmentations to effectively embed instructions into spatial context for a\nbetter user understanding. We present Guided Reality, a fully automated AR\nsystem that generates embedded and dynamic visual guidance based on\nstep-by-step instructions. Our system integrates LLMs and vision models to: 1)\ngenerate multi-step instructions from user queries, 2) identify appropriate\ntypes of visual guidance, 3) extract spatial information about key interaction\npoints in the real world, and 4) embed visual guidance in physical space to\nsupport task execution. Drawing from a corpus of user manuals, we define five\ncategories of visual guidance and propose an identification strategy based on\nthe current step. We evaluate the system through a user study (N=16),\ncompleting real-world tasks and exploring the system in the wild. Additionally,\nfour instructors shared insights on how Guided Reality could be integrated into\ntheir training workflows."
                },
                "authors": [
                    {
                        "name": "Ada Yi Zhao"
                    },
                    {
                        "name": "Aditya Gunturu"
                    },
                    {
                        "name": "Ellen Yi-Luen Do"
                    },
                    {
                        "name": "Ryo Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Ryo Suzuki"
                },
                "author": "Ryo Suzuki",
                "arxiv_doi": "10.1145/3746059.3747784",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746059.3747784",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.03547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear at UIST 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21179v2",
                "updated": "2025-09-24T15:38:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    38,
                    14,
                    2,
                    267,
                    0
                ],
                "published": "2025-07-26T15:50:08Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    50,
                    8,
                    5,
                    207,
                    0
                ],
                "title": "CANDLE: A Cross-Modal Agentic Knowledge Distillation Framework for\n  Interpretable Sarcopenia Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CANDLE: A Cross-Modal Agentic Knowledge Distillation Framework for\n  Interpretable Sarcopenia Diagnosis"
                },
                "summary": "Background and Aims: Large language models (LLMs) have shown remarkable\ngeneralization and transfer capabilities by learning from vast corpora of text\nand web data. Their semantic representations allow cross-task knowledge\ntransfer and reasoning, offering promising opportunities for data-scarce and\nheterogeneous domains such as clinical medicine. Yet, in diagnostic tasks like\nsarcopenia, major challenges remain: interpretability, transparency, and\ndeployment efficiency. Traditional machine learning (TML) models provide stable\nperformance and feature-level attribution, ensuring traceable and auditable\ndecision logic, but lack semantic breadth. Conversely, LLMs enable flexible\ninference but often function as opaque predictors. Existing integration\nstrategies remain shallow, rarely embedding the structured reasoning of TML\ninto LLM inference. Methods: Using sarcopenia diagnosis as a case study,\nSHapley Additive exPlanations (SHAP) were extracted from a baseline XGBoost\nmodel and transformed into structured, LLM-compatible representations. An\nactor-critic reinforcement learning (RL) strategy guided the LLM to reason over\nthese SHAP-based inputs, producing calibrated rationales and refined decision\nrules. The distilled reasoning was consolidated into a structured knowledge\nrepository and deployed via retrieval-augmented generation (RAG) for case-based\ninference. Results: (Omitted here.) Conclusion: By coupling SHAP-derived\nstatistical evidence with reinforcement-trained LLM reasoning, CANDLE mitigates\nthe interpretability-performance trade-off, enhances predictive accuracy, and\npreserves high decision consistency. The framework offers a scalable approach\nto knowledge assetization of TML models, enabling interpretable, reproducible,\nand clinically aligned decision support in sarcopenia and potentially broader\nmedical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background and Aims: Large language models (LLMs) have shown remarkable\ngeneralization and transfer capabilities by learning from vast corpora of text\nand web data. Their semantic representations allow cross-task knowledge\ntransfer and reasoning, offering promising opportunities for data-scarce and\nheterogeneous domains such as clinical medicine. Yet, in diagnostic tasks like\nsarcopenia, major challenges remain: interpretability, transparency, and\ndeployment efficiency. Traditional machine learning (TML) models provide stable\nperformance and feature-level attribution, ensuring traceable and auditable\ndecision logic, but lack semantic breadth. Conversely, LLMs enable flexible\ninference but often function as opaque predictors. Existing integration\nstrategies remain shallow, rarely embedding the structured reasoning of TML\ninto LLM inference. Methods: Using sarcopenia diagnosis as a case study,\nSHapley Additive exPlanations (SHAP) were extracted from a baseline XGBoost\nmodel and transformed into structured, LLM-compatible representations. An\nactor-critic reinforcement learning (RL) strategy guided the LLM to reason over\nthese SHAP-based inputs, producing calibrated rationales and refined decision\nrules. The distilled reasoning was consolidated into a structured knowledge\nrepository and deployed via retrieval-augmented generation (RAG) for case-based\ninference. Results: (Omitted here.) Conclusion: By coupling SHAP-derived\nstatistical evidence with reinforcement-trained LLM reasoning, CANDLE mitigates\nthe interpretability-performance trade-off, enhances predictive accuracy, and\npreserves high decision consistency. The framework offers a scalable approach\nto knowledge assetization of TML models, enabling interpretable, reproducible,\nand clinically aligned decision support in sarcopenia and potentially broader\nmedical domains."
                },
                "authors": [
                    {
                        "name": "Yuqi Jin"
                    },
                    {
                        "name": "Zhenhao Shuai"
                    },
                    {
                        "name": "Zihan Hu"
                    },
                    {
                        "name": "Weiteng Zhang"
                    },
                    {
                        "name": "Weihao Xie"
                    },
                    {
                        "name": "Jianwei Shuai"
                    },
                    {
                        "name": "Xian Shen"
                    },
                    {
                        "name": "Zhen Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Feng"
                },
                "author": "Zhen Feng",
                "arxiv_comment": "11 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20249v1",
                "updated": "2025-09-24T15:35:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    35,
                    26,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:35:26Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    35,
                    26,
                    2,
                    267,
                    0
                ],
                "title": "Indirect Statistical Inference with Guaranteed Necessity and Sufficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indirect Statistical Inference with Guaranteed Necessity and Sufficiency"
                },
                "summary": "This paper develops a new framework for indirect statistical inference with\nguaranteed necessity and sufficiency, applicable to continuous random\nvariables. We prove that when comparing exponentially transformed order\nstatistics from an assumed distribution with those from simulated unit\nexponential samples, the ranked quotients exhibit distinct asymptotics: the\nleft segment converges to a non-degenerate distribution, while the middle and\nright segments degenerate to one. This yields a necessary and sufficient\ncondition in probability for two sequences of continuous random variables to\nfollow the same distribution. Building on this, we propose an optimization\ncriterion based on relative errors between ordered samples. The criterion\nachieves its minimum if and only if the assumed and true distributions\ncoincide, providing a second necessary and sufficient condition in\noptimization. These dual NS properties, rare in the literature, establish a\nfundamentally stronger inference framework than existing methods. Unlike\nclassical approaches based on absolute errors (e.g., Kolmogorov-Smirnov), NSE\nexploits relative errors to ensure faster convergence, requires only mild\napproximability of the cumulative distribution function, and provides both\npoint and interval estimates. Simulations and real-data applications confirm\nNSE's superior performance in preserving distributional assumptions where\ntraditional methods fail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops a new framework for indirect statistical inference with\nguaranteed necessity and sufficiency, applicable to continuous random\nvariables. We prove that when comparing exponentially transformed order\nstatistics from an assumed distribution with those from simulated unit\nexponential samples, the ranked quotients exhibit distinct asymptotics: the\nleft segment converges to a non-degenerate distribution, while the middle and\nright segments degenerate to one. This yields a necessary and sufficient\ncondition in probability for two sequences of continuous random variables to\nfollow the same distribution. Building on this, we propose an optimization\ncriterion based on relative errors between ordered samples. The criterion\nachieves its minimum if and only if the assumed and true distributions\ncoincide, providing a second necessary and sufficient condition in\noptimization. These dual NS properties, rare in the literature, establish a\nfundamentally stronger inference framework than existing methods. Unlike\nclassical approaches based on absolute errors (e.g., Kolmogorov-Smirnov), NSE\nexploits relative errors to ensure faster convergence, requires only mild\napproximability of the cumulative distribution function, and provides both\npoint and interval estimates. Simulations and real-data applications confirm\nNSE's superior performance in preserving distributional assumptions where\ntraditional methods fail."
                },
                "authors": [
                    {
                        "name": "Z Zhang"
                    },
                    {
                        "name": "X Hu"
                    },
                    {
                        "name": "C Lu"
                    },
                    {
                        "name": "T Liu"
                    }
                ],
                "author_detail": {
                    "name": "T Liu"
                },
                "author": "T Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20241v1",
                "updated": "2025-09-24T15:32:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    32,
                    1,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:32:01Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    32,
                    1,
                    2,
                    267,
                    0
                ],
                "title": "Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute"
                },
                "summary": "As AI inference scales to billions of queries and emerging reasoning and\nagentic workflows increase token demand, reliable estimates of per-query energy\nuse are increasingly important for capacity planning, emissions accounting, and\nefficiency prioritization. Many public estimates are inconsistent and overstate\nenergy use, because they extrapolate from limited benchmarks and fail to\nreflect efficiency gains achievable at scale. In this perspective, we introduce\na bottom-up methodology to estimate the per-query energy of large-scale LLM\nsystems based on token throughput. For models running on an H100 node under\nrealistic workloads, GPU utilization and PUE constraints, we estimate a median\nenergy per query of 0.34 Wh (IQR: 0.18-0.67) for frontier-scale models (>200\nbillion parameters). These results are consistent with measurements using\nproduction-scale configurations and show that non-production estimates and\nassumptions can overstate energy use by 4-20x. Extending to test-time scaling\nscenarios with 15x more tokens per typical query, the median energy rises 13x\nto 4.32 Wh, indicating that targeting efficiency in this regime will deliver\nthe largest fleet-wide savings. We quantify achievable efficiency gains at the\nmodel, serving platform, and hardware levels, finding individual median\nreductions of 1.5-3.5x in energy per query, while combined advances can\nplausibly deliver 8-20x reductions. To illustrate the system-level impact, we\nestimate the baseline daily energy use of a deployment serving 1 billion\nqueries to be 0.8 GWh/day. If 10% are long queries, demand could grow to 1.8\nGWh/day. With targeted efficiency interventions, it falls to 0.9 GWh/day,\nsimilar to the energy footprint of web search at that scale. This echoes how\ndata centers historically tempered energy growth through efficiency gains\nduring the internet and cloud build-up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI inference scales to billions of queries and emerging reasoning and\nagentic workflows increase token demand, reliable estimates of per-query energy\nuse are increasingly important for capacity planning, emissions accounting, and\nefficiency prioritization. Many public estimates are inconsistent and overstate\nenergy use, because they extrapolate from limited benchmarks and fail to\nreflect efficiency gains achievable at scale. In this perspective, we introduce\na bottom-up methodology to estimate the per-query energy of large-scale LLM\nsystems based on token throughput. For models running on an H100 node under\nrealistic workloads, GPU utilization and PUE constraints, we estimate a median\nenergy per query of 0.34 Wh (IQR: 0.18-0.67) for frontier-scale models (>200\nbillion parameters). These results are consistent with measurements using\nproduction-scale configurations and show that non-production estimates and\nassumptions can overstate energy use by 4-20x. Extending to test-time scaling\nscenarios with 15x more tokens per typical query, the median energy rises 13x\nto 4.32 Wh, indicating that targeting efficiency in this regime will deliver\nthe largest fleet-wide savings. We quantify achievable efficiency gains at the\nmodel, serving platform, and hardware levels, finding individual median\nreductions of 1.5-3.5x in energy per query, while combined advances can\nplausibly deliver 8-20x reductions. To illustrate the system-level impact, we\nestimate the baseline daily energy use of a deployment serving 1 billion\nqueries to be 0.8 GWh/day. If 10% are long queries, demand could grow to 1.8\nGWh/day. With targeted efficiency interventions, it falls to 0.9 GWh/day,\nsimilar to the energy footprint of web search at that scale. This echoes how\ndata centers historically tempered energy growth through efficiency gains\nduring the internet and cloud build-up."
                },
                "authors": [
                    {
                        "name": "Felipe Oviedo"
                    },
                    {
                        "name": "Fiodar Kazhamiaka"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Allen Kim"
                    },
                    {
                        "name": "Amy Luers"
                    },
                    {
                        "name": "Melanie Nakagawa"
                    },
                    {
                        "name": "Ricardo Bianchini"
                    },
                    {
                        "name": "Juan M. Lavista Ferres"
                    }
                ],
                "author_detail": {
                    "name": "Juan M. Lavista Ferres"
                },
                "author": "Juan M. Lavista Ferres",
                "arxiv_comment": "A preprint version with DOI is available at Zenodo:\n  https://doi.org/10.5281/zenodo.17188770",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14190v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14190v3",
                "updated": "2025-09-25T08:38:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    8,
                    38,
                    41,
                    3,
                    268,
                    0
                ],
                "published": "2025-04-19T05:34:12Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    5,
                    34,
                    12,
                    5,
                    109,
                    0
                ],
                "title": "Matter Dipole and Hubble Tension due to Large Wavelength Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matter Dipole and Hubble Tension due to Large Wavelength Perturbations"
                },
                "summary": "We theoretically analyze the dipole anisotropy observed in the quasar\ndistribution from the CatWISE2020 catalog. The catalog data shows a peak around\n$z\\approx 1$, suggesting the presence of a large-scale dipole component. We\nexplore the possibility that this dipole could be driven by primordial density\nfluctuations from modes that were superhorizon at the time of CMB decoupling\nbut have since entered the horizon and become subhorizon. In particular, we\nconsider the impact of adiabatic modes with wave numbers $k$ in the range\n$(10^{-4} - 4 \\times 10^{-3})~\\mathrm{Mpc}^{-1} $, corresponding to wavelength\nscales of several Gpc. Such modes can create large-scale density variations,\nlikely causing anisotropies in the distribution of matter and, as a result,\naffecting the number density of observed quasars. We find that these can lead\nto a significant contribution to the dipole for sources up to redshifts of\nabout 1, but are unable to explain the observed dipole. We also demonstrate\nthat a superhorizon curvature perturbations mode, with a comoving wavenumber\n$k\\lesssim0.3H_0$ can lead to a significant enhancement in the locally inferred\nHubble constant. This effect offers a viable explanation for the observed\ndiscrepancy between local and CMB inferred measurements of $H_0$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We theoretically analyze the dipole anisotropy observed in the quasar\ndistribution from the CatWISE2020 catalog. The catalog data shows a peak around\n$z\\approx 1$, suggesting the presence of a large-scale dipole component. We\nexplore the possibility that this dipole could be driven by primordial density\nfluctuations from modes that were superhorizon at the time of CMB decoupling\nbut have since entered the horizon and become subhorizon. In particular, we\nconsider the impact of adiabatic modes with wave numbers $k$ in the range\n$(10^{-4} - 4 \\times 10^{-3})~\\mathrm{Mpc}^{-1} $, corresponding to wavelength\nscales of several Gpc. Such modes can create large-scale density variations,\nlikely causing anisotropies in the distribution of matter and, as a result,\naffecting the number density of observed quasars. We find that these can lead\nto a significant contribution to the dipole for sources up to redshifts of\nabout 1, but are unable to explain the observed dipole. We also demonstrate\nthat a superhorizon curvature perturbations mode, with a comoving wavenumber\n$k\\lesssim0.3H_0$ can lead to a significant enhancement in the locally inferred\nHubble constant. This effect offers a viable explanation for the observed\ndiscrepancy between local and CMB inferred measurements of $H_0$."
                },
                "authors": [
                    {
                        "name": "Gopal Kashyap"
                    },
                    {
                        "name": "Naveen K. Singh"
                    },
                    {
                        "name": "Pankaj Jain"
                    }
                ],
                "author_detail": {
                    "name": "Pankaj Jain"
                },
                "author": "Pankaj Jain",
                "arxiv_comment": "11 pages, 6 figures, accepted in PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14190v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14190v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18920v2",
                "updated": "2025-09-24T15:30:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    30,
                    3,
                    2,
                    267,
                    0
                ],
                "published": "2024-07-09T07:02:57Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    7,
                    2,
                    57,
                    1,
                    191,
                    0
                ],
                "title": "Context-Masked Meta-Prompting for Privacy-Preserving LLM Adaptation in\n  Finance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Masked Meta-Prompting for Privacy-Preserving LLM Adaptation in\n  Finance"
                },
                "summary": "The increasing reliance on Large Language Models (LLMs) in sensitive domains\nlike finance necessitates robust methods for privacy preservation and\nregulatory compliance. This paper presents an iterative meta-prompting\nmethodology designed to optimise hard prompts without exposing proprietary or\nconfidential context to the LLM. Through a novel regeneration process involving\nfeeder and propagation methods, we demonstrate significant improvements in\nprompt efficacy. Evaluated on public datasets serving as proxies for financial\ntasks such as SQuAD for extractive financial Q&A, CNN/DailyMail for news\nsummarisation, and SAMSum for client interaction summarisation, our approach,\nutilising GPT-3.5 Turbo, achieved a 103.87% improvement in ROUGE-L F1 for\nquestion answering. This work highlights a practical, low-cost strategy for\nadapting LLMs to financial applications while upholding critical privacy and\nauditability standards, offering a compelling case for its relevance in the\nevolving landscape of generative AI in finance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing reliance on Large Language Models (LLMs) in sensitive domains\nlike finance necessitates robust methods for privacy preservation and\nregulatory compliance. This paper presents an iterative meta-prompting\nmethodology designed to optimise hard prompts without exposing proprietary or\nconfidential context to the LLM. Through a novel regeneration process involving\nfeeder and propagation methods, we demonstrate significant improvements in\nprompt efficacy. Evaluated on public datasets serving as proxies for financial\ntasks such as SQuAD for extractive financial Q&A, CNN/DailyMail for news\nsummarisation, and SAMSum for client interaction summarisation, our approach,\nutilising GPT-3.5 Turbo, achieved a 103.87% improvement in ROUGE-L F1 for\nquestion answering. This work highlights a practical, low-cost strategy for\nadapting LLMs to financial applications while upholding critical privacy and\nauditability standards, offering a compelling case for its relevance in the\nevolving landscape of generative AI in finance."
                },
                "authors": [
                    {
                        "name": "Sayash Raaj Hiraou"
                    }
                ],
                "author_detail": {
                    "name": "Sayash Raaj Hiraou"
                },
                "author": "Sayash Raaj Hiraou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20238v1",
                "updated": "2025-09-24T15:28:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    28,
                    32,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:28:32Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    28,
                    32,
                    2,
                    267,
                    0
                ],
                "title": "Velocity model building from seismic images using a Convolutional Neural\n  Operator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Velocity model building from seismic images using a Convolutional Neural\n  Operator"
                },
                "summary": "The success of building a high-resolution velocity model using machine\nlearning is hampered by generalization limitations that often limit the success\nof the approach on field data. This is especially true when relying on neural\noperators for the mapping. Thus, we propose a novel inversion framework that\nrelies on learning to map the velocity model to a seismic image using a\nConvolutional Neural Operator (CNO), and then we use optimization to invert for\nthe velocity that matches the image. The key to the success of our network is\nthat we use the initial and true velocity models as input in the training, then\nwe invert for the true velocity starting from the initial velocity at\ninference. Specifically, we first train a neural operator to accurately learn\nthe forward mapping from seismic velocity models to RTM images, using synthetic\ndatasets that include high-frequency structural information. Once trained, the\nneural operator is embedded into an inversion loop, where its differentiable\nnature enables efficient gradient computation via automatic differentiation.\nThis allows us to progressively inject high-wavenumber information from RTM\nimages into the background velocity model, thereby improving resolution without\nthe need for traditional adjoint-state solvers. The proposed framework is\nvalidated on both synthetic and field data. Results demonstrate that the neural\noperator generalizes well to real seismic scenarios, maintains high inversion\naccuracy, and significantly reduces computational cost. This work highlights\nthe potential of neural operators as flexible and scalable tools for efficient,\ndata-driven seismic imaging and inversion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of building a high-resolution velocity model using machine\nlearning is hampered by generalization limitations that often limit the success\nof the approach on field data. This is especially true when relying on neural\noperators for the mapping. Thus, we propose a novel inversion framework that\nrelies on learning to map the velocity model to a seismic image using a\nConvolutional Neural Operator (CNO), and then we use optimization to invert for\nthe velocity that matches the image. The key to the success of our network is\nthat we use the initial and true velocity models as input in the training, then\nwe invert for the true velocity starting from the initial velocity at\ninference. Specifically, we first train a neural operator to accurately learn\nthe forward mapping from seismic velocity models to RTM images, using synthetic\ndatasets that include high-frequency structural information. Once trained, the\nneural operator is embedded into an inversion loop, where its differentiable\nnature enables efficient gradient computation via automatic differentiation.\nThis allows us to progressively inject high-wavenumber information from RTM\nimages into the background velocity model, thereby improving resolution without\nthe need for traditional adjoint-state solvers. The proposed framework is\nvalidated on both synthetic and field data. Results demonstrate that the neural\noperator generalizes well to real seismic scenarios, maintains high inversion\naccuracy, and significantly reduces computational cost. This work highlights\nthe potential of neural operators as flexible and scalable tools for efficient,\ndata-driven seismic imaging and inversion."
                },
                "authors": [
                    {
                        "name": "Xiao Ma"
                    },
                    {
                        "name": "Tariq Alkhalifah"
                    }
                ],
                "author_detail": {
                    "name": "Tariq Alkhalifah"
                },
                "author": "Tariq Alkhalifah",
                "arxiv_comment": "17 pages,16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20235v1",
                "updated": "2025-09-24T15:25:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    25,
                    11,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:25:11Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    25,
                    11,
                    2,
                    267,
                    0
                ],
                "title": "$S_8$ from Tully-Fisher, fundamental plane, and supernova distances\n  agree with Planck",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$S_8$ from Tully-Fisher, fundamental plane, and supernova distances\n  agree with Planck"
                },
                "summary": "Peculiar velocity measurements constrain the parameter combination\n$f\\sigma_8$, the product of the linear growth rate $f$ and the fluctuation\namplitude $\\sigma_8$. Under the approximation that $f$ is a monotonic function\nof $\\Omega_{\\rm m}$, this can be related to $S_8 \\equiv \\sigma_8\n\\sqrt{\\Omega_{\\rm m}/0.3}$, enabling direct comparison with weak lensing and\ncosmic microwave background results. We exploit this by using three classes of\ndirect-distance tracers -- the Tully-Fisher relation, the fundamental plane,\nand Type~Ia supernovae -- to infer peculiar velocities. A unified hierarchical\nforward model jointly calibrates each distance indicator and a linear theory\nreconstruction of the local Universe. This is the first consistent Bayesian\nanalysis to combine all three major classes of distance indicators within a\ncommon framework, enabling cross-checks of systematics across diverse galaxy\npopulations. All three tracers yield consistent values of $S_8$ that are also\nin agreement with Planck. Our joint constraint is $S_8 = 0.819 \\pm 0.030$, with\nthe uncertainty dominated by the 2M++ galaxy field. These results demonstrate\nthat peculiar velocity surveys provide a robust, consistent measurement of\n$S_8$, and support concordance with the cosmic microwave background.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peculiar velocity measurements constrain the parameter combination\n$f\\sigma_8$, the product of the linear growth rate $f$ and the fluctuation\namplitude $\\sigma_8$. Under the approximation that $f$ is a monotonic function\nof $\\Omega_{\\rm m}$, this can be related to $S_8 \\equiv \\sigma_8\n\\sqrt{\\Omega_{\\rm m}/0.3}$, enabling direct comparison with weak lensing and\ncosmic microwave background results. We exploit this by using three classes of\ndirect-distance tracers -- the Tully-Fisher relation, the fundamental plane,\nand Type~Ia supernovae -- to infer peculiar velocities. A unified hierarchical\nforward model jointly calibrates each distance indicator and a linear theory\nreconstruction of the local Universe. This is the first consistent Bayesian\nanalysis to combine all three major classes of distance indicators within a\ncommon framework, enabling cross-checks of systematics across diverse galaxy\npopulations. All three tracers yield consistent values of $S_8$ that are also\nin agreement with Planck. Our joint constraint is $S_8 = 0.819 \\pm 0.030$, with\nthe uncertainty dominated by the 2M++ galaxy field. These results demonstrate\nthat peculiar velocity surveys provide a robust, consistent measurement of\n$S_8$, and support concordance with the cosmic microwave background."
                },
                "authors": [
                    {
                        "name": "Richard Stiskalek"
                    }
                ],
                "author_detail": {
                    "name": "Richard Stiskalek"
                },
                "author": "Richard Stiskalek",
                "arxiv_comment": "9 pages, 6 figures. To be submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04931v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04931v3",
                "updated": "2025-09-24T15:24:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    24,
                    43,
                    2,
                    267,
                    0
                ],
                "published": "2023-12-08T09:48:36Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    9,
                    48,
                    36,
                    4,
                    342,
                    0
                ],
                "title": "Long Video Understanding with Learnable Retrieval in Video-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Video Understanding with Learnable Retrieval in Video-Language\n  Models"
                },
                "summary": "The remarkable natural language understanding, reasoning, and generation\ncapabilities of large language models (LLMs) have made them attractive for\napplication to video understanding, utilizing video tokens as contextual input.\nHowever, employing LLMs for long video understanding presents significant\nchallenges. The extensive number of video tokens leads to considerable\ncomputational costs for LLMs while using aggregated tokens results in loss of\nvision details. Moreover, the presence of abundant question-irrelevant tokens\nintroduces noise to the video reasoning process. To address these issues, we\nintroduce a simple yet effective learnable retrieval-based video-language model\n(R-VLM) for efficient long video understanding. Specifically, given a question\n(query) and a long video, our model identifies and selects the most relevant K\nvideo chunks and uses their associated visual tokens to serve as context for\nthe LLM inference. This effectively reduces the number of video tokens,\neliminates noise interference, and enhances system performance. We achieve this\nby incorporating a learnable lightweight MLP block to facilitate the efficient\nretrieval of question-relevant chunks, through the end-to-end training of our\nvideo-language model with a proposed soft matching loss. Our experimental\nresults on multiple zero-shot video question answering datasets validate the\neffectiveness of our framework for comprehending long videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable natural language understanding, reasoning, and generation\ncapabilities of large language models (LLMs) have made them attractive for\napplication to video understanding, utilizing video tokens as contextual input.\nHowever, employing LLMs for long video understanding presents significant\nchallenges. The extensive number of video tokens leads to considerable\ncomputational costs for LLMs while using aggregated tokens results in loss of\nvision details. Moreover, the presence of abundant question-irrelevant tokens\nintroduces noise to the video reasoning process. To address these issues, we\nintroduce a simple yet effective learnable retrieval-based video-language model\n(R-VLM) for efficient long video understanding. Specifically, given a question\n(query) and a long video, our model identifies and selects the most relevant K\nvideo chunks and uses their associated visual tokens to serve as context for\nthe LLM inference. This effectively reduces the number of video tokens,\neliminates noise interference, and enhances system performance. We achieve this\nby incorporating a learnable lightweight MLP block to facilitate the efficient\nretrieval of question-relevant chunks, through the end-to-end training of our\nvideo-language model with a proposed soft matching loss. Our experimental\nresults on multiple zero-shot video question answering datasets validate the\neffectiveness of our framework for comprehending long videos."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xu"
                    },
                    {
                        "name": "Cuiling Lan"
                    },
                    {
                        "name": "Wenxuan Xie"
                    },
                    {
                        "name": "Xuejin Chen"
                    },
                    {
                        "name": "Yan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Lu"
                },
                "author": "Yan Lu",
                "arxiv_comment": "Accepted by IEEE Transactions on Multimedia (TMM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04931v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04931v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20230v1",
                "updated": "2025-09-24T15:23:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    23,
                    46,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:23:46Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    23,
                    46,
                    2,
                    267,
                    0
                ],
                "title": "Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided\n  Multi-Point Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided\n  Multi-Point Optimization"
                },
                "summary": "Current LLM unlearning methods face a critical security vulnerability that\nundermines their fundamental purpose: while they appear to successfully remove\nsensitive or harmful knowledge, this ``forgotten\" information remains\nprecariously recoverable through relearning attacks. We identify that the root\ncause is that conventional methods optimizing the forgetting loss at individual\ndata points will drive model parameters toward sharp minima in the loss\nlandscape. In these unstable regions, even minimal parameter perturbations can\ndrastically alter the model's behaviors. Consequently, relearning attacks\nexploit this vulnerability by using just a few fine-tuning samples to navigate\nthe steep gradients surrounding these unstable regions, thereby rapidly\nrecovering knowledge that was supposedly erased. This exposes a critical\nrobustness gap between apparent unlearning and actual knowledge removal. To\naddress this issue, we propose StableUN, a bi-level feedback-guided\noptimization framework that explicitly seeks more stable parameter regions via\nneighborhood-aware optimization. It integrates forgetting feedback, which uses\nadversarial perturbations to probe parameter neighborhoods, with remembering\nfeedback to preserve model utility, aligning the two objectives through\ngradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that\nour method is significantly more robust against both relearning and\njailbreaking attacks while maintaining competitive utility performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM unlearning methods face a critical security vulnerability that\nundermines their fundamental purpose: while they appear to successfully remove\nsensitive or harmful knowledge, this ``forgotten\" information remains\nprecariously recoverable through relearning attacks. We identify that the root\ncause is that conventional methods optimizing the forgetting loss at individual\ndata points will drive model parameters toward sharp minima in the loss\nlandscape. In these unstable regions, even minimal parameter perturbations can\ndrastically alter the model's behaviors. Consequently, relearning attacks\nexploit this vulnerability by using just a few fine-tuning samples to navigate\nthe steep gradients surrounding these unstable regions, thereby rapidly\nrecovering knowledge that was supposedly erased. This exposes a critical\nrobustness gap between apparent unlearning and actual knowledge removal. To\naddress this issue, we propose StableUN, a bi-level feedback-guided\noptimization framework that explicitly seeks more stable parameter regions via\nneighborhood-aware optimization. It integrates forgetting feedback, which uses\nadversarial perturbations to probe parameter neighborhoods, with remembering\nfeedback to preserve model utility, aligning the two objectives through\ngradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that\nour method is significantly more robust against both relearning and\njailbreaking attacks while maintaining competitive utility performance."
                },
                "authors": [
                    {
                        "name": "Wenhan Wu"
                    },
                    {
                        "name": "Zheyuan Liu"
                    },
                    {
                        "name": "Chongyang Gao"
                    },
                    {
                        "name": "Ren Wang"
                    },
                    {
                        "name": "Kaize Ding"
                    }
                ],
                "author_detail": {
                    "name": "Kaize Ding"
                },
                "author": "Kaize Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00433v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00433v3",
                "updated": "2025-09-24T15:22:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    22,
                    22,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-31T07:28:32Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    7,
                    28,
                    32,
                    5,
                    151,
                    0
                ],
                "title": "Latent Wavelet Diffusion For Ultra-High-Resolution Image Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Wavelet Diffusion For Ultra-High-Resolution Image Synthesis"
                },
                "summary": "High-resolution image synthesis remains a core challenge in generative\nmodeling, particularly in balancing computational efficiency with the\npreservation of fine-grained visual detail. We present Latent Wavelet Diffusion\n(LWD), a lightweight training framework that significantly improves detail and\ntexture fidelity in ultra-high-resolution (2K-4K) image synthesis. LWD\nintroduces a novel, frequency-aware masking strategy derived from wavelet\nenergy maps, which dynamically focuses the training process on detail-rich\nregions of the latent space. This is complemented by a scale-consistent VAE\nobjective to ensure high spectral fidelity. The primary advantage of our\napproach is its efficiency: LWD requires no architectural modifications and\nadds zero additional cost during inference, making it a practical solution for\nscaling existing models. Across multiple strong baselines, LWD consistently\nimproves perceptual quality and FID scores, demonstrating the power of\nsignal-driven supervision as a principled and efficient path toward\nhigh-resolution generative modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution image synthesis remains a core challenge in generative\nmodeling, particularly in balancing computational efficiency with the\npreservation of fine-grained visual detail. We present Latent Wavelet Diffusion\n(LWD), a lightweight training framework that significantly improves detail and\ntexture fidelity in ultra-high-resolution (2K-4K) image synthesis. LWD\nintroduces a novel, frequency-aware masking strategy derived from wavelet\nenergy maps, which dynamically focuses the training process on detail-rich\nregions of the latent space. This is complemented by a scale-consistent VAE\nobjective to ensure high spectral fidelity. The primary advantage of our\napproach is its efficiency: LWD requires no architectural modifications and\nadds zero additional cost during inference, making it a practical solution for\nscaling existing models. Across multiple strong baselines, LWD consistently\nimproves perceptual quality and FID scores, demonstrating the power of\nsignal-driven supervision as a principled and efficient path toward\nhigh-resolution generative modeling."
                },
                "authors": [
                    {
                        "name": "Luigi Sigillo"
                    },
                    {
                        "name": "Shengfeng He"
                    },
                    {
                        "name": "Danilo Comminiello"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Comminiello"
                },
                "author": "Danilo Comminiello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00433v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00433v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20223v1",
                "updated": "2025-09-24T15:16:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    16,
                    9,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:16:09Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    16,
                    9,
                    2,
                    267,
                    0
                ],
                "title": "An Empirical Analysis of Secure Federated Learning for Autonomous\n  Vehicle Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Analysis of Secure Federated Learning for Autonomous\n  Vehicle Applications"
                },
                "summary": "Federated Learning lends itself as a promising paradigm in enabling\ndistributed learning for autonomous vehicles applications and ensuring data\nprivacy while enhancing and refining predictive model performance through\ncollaborative training on edge client vehicles. However, it remains vulnerable\nto various categories of cyber-attacks, necessitating more robust security\nmeasures to effectively mitigate potential threats. Poisoning attacks and\ninference attacks are commonly initiated within the federated learning\nenvironment to compromise secure system performance. Secure aggregation can\nlimit the disclosure of sensitive information from outsider and insider\nattackers of the federated learning environment. In this study, our aim is to\nconduct an empirical analysis on the transportation image dataset (e.g., LISA\ntraffic light) using various secure aggregation techniques and multiparty\ncomputation in the presence of diverse categories of cyber-attacks. Multiparty\ncomputation serves as a state-of-the-art security mechanism, offering standard\nprivacy for secure aggregation of edge autonomous vehicles local model updates\nthrough various security protocols. The presence of adversaries can mislead the\nautonomous vehicle learning model, leading to the misclassification of traffic\nlights, and resulting in detrimental impacts. This empirical study explores the\nresilience of various secure federated learning aggregation techniques and\nmultiparty computation in safeguarding autonomous vehicle applications against\nvarious cyber threats during both training and inference times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning lends itself as a promising paradigm in enabling\ndistributed learning for autonomous vehicles applications and ensuring data\nprivacy while enhancing and refining predictive model performance through\ncollaborative training on edge client vehicles. However, it remains vulnerable\nto various categories of cyber-attacks, necessitating more robust security\nmeasures to effectively mitigate potential threats. Poisoning attacks and\ninference attacks are commonly initiated within the federated learning\nenvironment to compromise secure system performance. Secure aggregation can\nlimit the disclosure of sensitive information from outsider and insider\nattackers of the federated learning environment. In this study, our aim is to\nconduct an empirical analysis on the transportation image dataset (e.g., LISA\ntraffic light) using various secure aggregation techniques and multiparty\ncomputation in the presence of diverse categories of cyber-attacks. Multiparty\ncomputation serves as a state-of-the-art security mechanism, offering standard\nprivacy for secure aggregation of edge autonomous vehicles local model updates\nthrough various security protocols. The presence of adversaries can mislead the\nautonomous vehicle learning model, leading to the misclassification of traffic\nlights, and resulting in detrimental impacts. This empirical study explores the\nresilience of various secure federated learning aggregation techniques and\nmultiparty computation in safeguarding autonomous vehicle applications against\nvarious cyber threats during both training and inference times."
                },
                "authors": [
                    {
                        "name": "Md Jueal Mia"
                    },
                    {
                        "name": "M. Hadi Amini"
                    }
                ],
                "author_detail": {
                    "name": "M. Hadi Amini"
                },
                "author": "M. Hadi Amini",
                "arxiv_comment": "i3CE 2024, 2024 ASCE International Conference on Computing in Civil\n  Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20215v1",
                "updated": "2025-09-24T15:12:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    12,
                    21,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:12:21Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    12,
                    21,
                    2,
                    267,
                    0
                ],
                "title": "The Cream Rises to the Top: Efficient Reranking Method for Verilog Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Cream Rises to the Top: Efficient Reranking Method for Verilog Code\n  Generation"
                },
                "summary": "LLMs face significant challenges in Verilog generation due to limited\ndomain-specific knowledge. While sampling techniques improve pass@k metrics,\nhardware engineers need one trustworthy solution rather than uncertain\ncandidates. To bridge this gap, we formulate it as a semantic alignment problem\nbetween requirements and Verilog implementations, and propose VCD-RNK, a\ndiscriminator model tailored for efficient Verilog code reranking.\nSpecifically, VCD-RNKincorporates Verilog-specific reasoning by distilling\nexpert knowledge across three dimensions: code semantic analysis, test case\ngeneration, and functional correctness assessment. By explicitly simulating the\nabove reasoning processes during inference, VCD-RNK effectively avoids\ncomputationally intensive test execution in existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs face significant challenges in Verilog generation due to limited\ndomain-specific knowledge. While sampling techniques improve pass@k metrics,\nhardware engineers need one trustworthy solution rather than uncertain\ncandidates. To bridge this gap, we formulate it as a semantic alignment problem\nbetween requirements and Verilog implementations, and propose VCD-RNK, a\ndiscriminator model tailored for efficient Verilog code reranking.\nSpecifically, VCD-RNKincorporates Verilog-specific reasoning by distilling\nexpert knowledge across three dimensions: code semantic analysis, test case\ngeneration, and functional correctness assessment. By explicitly simulating the\nabove reasoning processes during inference, VCD-RNK effectively avoids\ncomputationally intensive test execution in existing methods."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Wei Zheng"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Fengji Zhang"
                    },
                    {
                        "name": "Terry Yue Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Terry Yue Zhuo"
                },
                "author": "Terry Yue Zhuo",
                "arxiv_comment": "Under review ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20214v1",
                "updated": "2025-09-24T15:10:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    10,
                    44,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:10:44Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    10,
                    44,
                    2,
                    267,
                    0
                ],
                "title": "Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for\n  Efficient LLM Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for\n  Efficient LLM Deployment"
                },
                "summary": "We study weight-only post-training quantization (PTQ), which quantizes the\nweights of a large language model (LLM) without retraining, using little or no\ncalibration data. Weight-only PTQ is crucial for reducing the memory footprint\nand latency of LLM inference, especially in memory-bound, small-batch inference\nscenarios, such as personalized inference on edge devices. Despite its\nimportance, irregular weight distributions with heavy-tailed outliers in LLMs\ncomplicate quantization, recently motivating rotation-based methods that\ntransform weights into near-Gaussian distributions, which are more regular with\nfewer outliers, thereby reducing quantization error. In this work, we first\nderive the information-theoretically optimal bit allocation for Gaussianized\nweights under given bit budgets, revealing that fine-grained fractional-bit\nquantizers approaching the Gaussian distortion-rate bound are essential to\nachieve near-optimal quantization performance. To bridge this theoretical\ninsight and practical implementation, we introduce Q-Palette, a versatile\ncollection of fractional-bit quantizers that range from trellis-coded\nquantizers offering near-optimal distortion to simpler vector and scalar\nquantizers optimized for faster inference, all efficiently implemented with\noptimized CUDA kernels across various bitwidths. Furthermore, leveraging\nQ-Palette as a foundational component, we propose a novel mixed-scheme\nquantization framework, jointly optimizing quantizer choices and layer fusion\ndecisions given resource constraints. The code is available at\nhttps://github.com/snu-mllab/Q-Palette.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study weight-only post-training quantization (PTQ), which quantizes the\nweights of a large language model (LLM) without retraining, using little or no\ncalibration data. Weight-only PTQ is crucial for reducing the memory footprint\nand latency of LLM inference, especially in memory-bound, small-batch inference\nscenarios, such as personalized inference on edge devices. Despite its\nimportance, irregular weight distributions with heavy-tailed outliers in LLMs\ncomplicate quantization, recently motivating rotation-based methods that\ntransform weights into near-Gaussian distributions, which are more regular with\nfewer outliers, thereby reducing quantization error. In this work, we first\nderive the information-theoretically optimal bit allocation for Gaussianized\nweights under given bit budgets, revealing that fine-grained fractional-bit\nquantizers approaching the Gaussian distortion-rate bound are essential to\nachieve near-optimal quantization performance. To bridge this theoretical\ninsight and practical implementation, we introduce Q-Palette, a versatile\ncollection of fractional-bit quantizers that range from trellis-coded\nquantizers offering near-optimal distortion to simpler vector and scalar\nquantizers optimized for faster inference, all efficiently implemented with\noptimized CUDA kernels across various bitwidths. Furthermore, leveraging\nQ-Palette as a foundational component, we propose a novel mixed-scheme\nquantization framework, jointly optimizing quantizer choices and layer fusion\ndecisions given resource constraints. The code is available at\nhttps://github.com/snu-mllab/Q-Palette."
                },
                "authors": [
                    {
                        "name": "Deokjae Lee"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20211v1",
                "updated": "2025-09-24T15:04:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    4,
                    25,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:04:25Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    4,
                    25,
                    2,
                    267,
                    0
                ],
                "title": "Practical do-Shapley Explanations with Estimand-Agnostic Causal\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical do-Shapley Explanations with Estimand-Agnostic Causal\n  Inference"
                },
                "summary": "Among explainability techniques, SHAP stands out as one of the most popular,\nbut often overlooks the causal structure of the problem. In response, do-SHAP\nemploys interventional queries, but its reliance on estimands hinders its\npractical application. To address this problem, we propose the use of\nestimand-agnostic approaches, which allow for the estimation of any\nidentifiable query from a single model, making do-SHAP feasible on complex\ngraphs. We also develop a novel algorithm to significantly accelerate its\ncomputation at a negligible cost, as well as a method to explain inaccessible\nData Generating Processes. We demonstrate the estimation and computational\nperformance of our approach, and validate it on two real-world datasets,\nhighlighting its potential in obtaining reliable explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Among explainability techniques, SHAP stands out as one of the most popular,\nbut often overlooks the causal structure of the problem. In response, do-SHAP\nemploys interventional queries, but its reliance on estimands hinders its\npractical application. To address this problem, we propose the use of\nestimand-agnostic approaches, which allow for the estimation of any\nidentifiable query from a single model, making do-SHAP feasible on complex\ngraphs. We also develop a novel algorithm to significantly accelerate its\ncomputation at a negligible cost, as well as a method to explain inaccessible\nData Generating Processes. We demonstrate the estimation and computational\nperformance of our approach, and validate it on two real-world datasets,\nhighlighting its potential in obtaining reliable explanations."
                },
                "authors": [
                    {
                        "name": "Álvaro Parafita"
                    },
                    {
                        "name": "Tomas Garriga"
                    },
                    {
                        "name": "Axel Brando"
                    },
                    {
                        "name": "Francisco J. Cazorla"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Cazorla"
                },
                "author": "Francisco J. Cazorla",
                "arxiv_comment": "Accepted for publication at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20208v1",
                "updated": "2025-09-24T15:02:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    2,
                    33,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:02:33Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    2,
                    33,
                    2,
                    267,
                    0
                ],
                "title": "Play by the Type Rules: Inferring Constraints for LLM Functions in\n  Declarative Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Play by the Type Rules: Inferring Constraints for LLM Functions in\n  Declarative Programs"
                },
                "summary": "Integrating LLM powered operators in declarative query languages allows for\nthe combination of cheap and interpretable functions with powerful,\ngeneralizable language model reasoning. However, in order to benefit from the\noptimized execution of a database query language like SQL, generated outputs\nmust align with the rules enforced by both type checkers and database contents.\nCurrent approaches address this challenge with orchestrations consisting of\nmany LLM-based post-processing calls to ensure alignment between generated\noutputs and database values, introducing performance bottlenecks. We perform a\nstudy on the ability of various sized open-source language models to both parse\nand execute functions within a query language based on SQL, showing that small\nlanguage models can excel as function executors over hybrid data sources. Then,\nwe propose an efficient solution to enforce the well-typedness of LLM\nfunctions, demonstrating 7% accuracy improvement on a multi-hop question\nanswering dataset with 53% improvement in latency over comparable solutions. We\nmake our implementation available at https://github.com/parkervg/blendsql",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating LLM powered operators in declarative query languages allows for\nthe combination of cheap and interpretable functions with powerful,\ngeneralizable language model reasoning. However, in order to benefit from the\noptimized execution of a database query language like SQL, generated outputs\nmust align with the rules enforced by both type checkers and database contents.\nCurrent approaches address this challenge with orchestrations consisting of\nmany LLM-based post-processing calls to ensure alignment between generated\noutputs and database values, introducing performance bottlenecks. We perform a\nstudy on the ability of various sized open-source language models to both parse\nand execute functions within a query language based on SQL, showing that small\nlanguage models can excel as function executors over hybrid data sources. Then,\nwe propose an efficient solution to enforce the well-typedness of LLM\nfunctions, demonstrating 7% accuracy improvement on a multi-hop question\nanswering dataset with 53% improvement in latency over comparable solutions. We\nmake our implementation available at https://github.com/parkervg/blendsql"
                },
                "authors": [
                    {
                        "name": "Parker Glenn"
                    },
                    {
                        "name": "Alfy Samuel"
                    },
                    {
                        "name": "Daben Liu"
                    }
                ],
                "author_detail": {
                    "name": "Daben Liu"
                },
                "author": "Daben Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20205v1",
                "updated": "2025-09-24T15:01:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    1,
                    25,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:01:25Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    1,
                    25,
                    2,
                    267,
                    0
                ],
                "title": "Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge\n  Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge\n  Accelerators"
                },
                "summary": "The proliferation of GPU accelerated edge devices like Nvidia Jetsons and the\nrise in privacy concerns are placing an emphasis on concurrent DNN training and\ninferencing on edge devices. Inference and training have different computing\nand QoS goals. But edge accelerators like Jetson do not support native GPU\nsharing and expose 1000s of power modes. This requires careful time-sharing of\nconcurrent workloads to meet power--performance goals, while limiting costly\nprofiling. In this paper, we design an intelligent time-slicing approach for\nconcurrent DNN training and inferencing on Jetsons. We formulate an\noptimization problem to interleave training and inferencing minibatches, and\ndecide the device power mode and inference minibatch size, while maximizing the\ntraining throughput and staying within latency and power budgets, with modest\nprofiling costs. We propose GMD, an efficient multi-dimensional gradient\ndescent search which profiles just $15$ power modes; and ALS, an Active\nLearning technique which identifies reusable Pareto-optimal power modes, but\nprofiles $50$--$150$ power modes. We evaluate these within our Fulcrum\nscheduler for $273,000+$ configurations across $15$ DNN workloads. We also\nevaluate our strategies on dynamic arrival inference and concurrent inferences.\nALS and GMD outperform simpler and more complex baselines with larger-scale\nprofiling. Their solutions satisfy the latency and power budget for $>97\\%$ of\nour runs, and on average are within $7\\%$ of the optimal throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of GPU accelerated edge devices like Nvidia Jetsons and the\nrise in privacy concerns are placing an emphasis on concurrent DNN training and\ninferencing on edge devices. Inference and training have different computing\nand QoS goals. But edge accelerators like Jetson do not support native GPU\nsharing and expose 1000s of power modes. This requires careful time-sharing of\nconcurrent workloads to meet power--performance goals, while limiting costly\nprofiling. In this paper, we design an intelligent time-slicing approach for\nconcurrent DNN training and inferencing on Jetsons. We formulate an\noptimization problem to interleave training and inferencing minibatches, and\ndecide the device power mode and inference minibatch size, while maximizing the\ntraining throughput and staying within latency and power budgets, with modest\nprofiling costs. We propose GMD, an efficient multi-dimensional gradient\ndescent search which profiles just $15$ power modes; and ALS, an Active\nLearning technique which identifies reusable Pareto-optimal power modes, but\nprofiles $50$--$150$ power modes. We evaluate these within our Fulcrum\nscheduler for $273,000+$ configurations across $15$ DNN workloads. We also\nevaluate our strategies on dynamic arrival inference and concurrent inferences.\nALS and GMD outperform simpler and more complex baselines with larger-scale\nprofiling. Their solutions satisfy the latency and power budget for $>97\\%$ of\nour runs, and on average are within $7\\%$ of the optimal throughput."
                },
                "authors": [
                    {
                        "name": "Prashanthi S. K."
                    },
                    {
                        "name": "Saisamarth Taluri"
                    },
                    {
                        "name": "Pranav Gupta"
                    },
                    {
                        "name": "Amartya Ranjan Saikia"
                    },
                    {
                        "name": "Kunal Kumar Sahoo"
                    },
                    {
                        "name": "Atharva Vinay Joshi"
                    },
                    {
                        "name": "Lakshya Karwa"
                    },
                    {
                        "name": "Kedar Dhule"
                    },
                    {
                        "name": "Yogesh Simmhan"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh Simmhan"
                },
                "author": "Yogesh Simmhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05926v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05926v3",
                "updated": "2025-09-24T14:58:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    58,
                    32,
                    2,
                    267,
                    0
                ],
                "published": "2025-01-10T12:46:39Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    46,
                    39,
                    4,
                    10,
                    0
                ],
                "title": "LLMs Reproduce Stereotypes of Sexual and Gender Minorities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Reproduce Stereotypes of Sexual and Gender Minorities"
                },
                "summary": "A large body of research has found substantial gender bias in NLP systems.\nMost of this research takes a binary, essentialist view of gender: limiting its\nvariation to the categories _men_ and _women_, conflating gender with sex, and\nignoring different sexual identities. But gender and sexuality exist on a\nspectrum, so in this paper we study the biases of large language models (LLMs)\ntowards sexual and gender minorities beyond binary categories. Grounding our\nstudy in a widely used social psychology model -- the Stereotype Content Model\n-- we demonstrate that English-language survey questions about social\nperceptions elicit more negative stereotypes of sexual and gender minorities\nfrom both humans and LLMs. We then extend this framework to a more realistic\nuse case: text generation. Our analysis shows that LLMs generate stereotyped\nrepresentations of sexual and gender minorities in this setting, showing that\nthey amplify representational harms in creative writing, a widely advertised\nuse for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large body of research has found substantial gender bias in NLP systems.\nMost of this research takes a binary, essentialist view of gender: limiting its\nvariation to the categories _men_ and _women_, conflating gender with sex, and\nignoring different sexual identities. But gender and sexuality exist on a\nspectrum, so in this paper we study the biases of large language models (LLMs)\ntowards sexual and gender minorities beyond binary categories. Grounding our\nstudy in a widely used social psychology model -- the Stereotype Content Model\n-- we demonstrate that English-language survey questions about social\nperceptions elicit more negative stereotypes of sexual and gender minorities\nfrom both humans and LLMs. We then extend this framework to a more realistic\nuse case: text generation. Our analysis shows that LLMs generate stereotyped\nrepresentations of sexual and gender minorities in this setting, showing that\nthey amplify representational harms in creative writing, a widely advertised\nuse for LLMs."
                },
                "authors": [
                    {
                        "name": "Ruby Ostrow"
                    },
                    {
                        "name": "Adam Lopez"
                    }
                ],
                "author_detail": {
                    "name": "Adam Lopez"
                },
                "author": "Adam Lopez",
                "arxiv_comment": "13 pages, 5 figures, 9 tables (including bibliography and appendix).\n  Accepted to Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05926v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05926v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11045v2",
                "updated": "2025-09-24T14:57:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    57,
                    25,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-21T14:10:06Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    10,
                    6,
                    2,
                    141,
                    0
                ],
                "title": "Procedural Environment Generation for Tool-Use Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Environment Generation for Tool-Use Agents"
                },
                "summary": "Although the power of LLM tool-use agents has ignited a flurry of recent\nresearch in this area, the curation of tool-use training data remains an open\nproblem$-$especially for online RL training. Existing approaches to synthetic\ntool-use data generation tend to be non-interactive, and/or non-compositional.\nWe introduce RandomWorld, a pipeline for the procedural generation of\ninteractive tools and compositional tool-use data. We show that models tuned\nvia SFT and RL on synthetic RandomWorld data improve on a range of tool-use\nbenchmarks, and set the new SoTA for two metrics on the NESTFUL dataset.\nFurther experiments show that downstream performance scales with the amount of\nRandomWorld-generated training data, opening up the possibility of further\nimprovement through the use of entirely synthetic data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the power of LLM tool-use agents has ignited a flurry of recent\nresearch in this area, the curation of tool-use training data remains an open\nproblem$-$especially for online RL training. Existing approaches to synthetic\ntool-use data generation tend to be non-interactive, and/or non-compositional.\nWe introduce RandomWorld, a pipeline for the procedural generation of\ninteractive tools and compositional tool-use data. We show that models tuned\nvia SFT and RL on synthetic RandomWorld data improve on a range of tool-use\nbenchmarks, and set the new SoTA for two metrics on the NESTFUL dataset.\nFurther experiments show that downstream performance scales with the amount of\nRandomWorld-generated training data, opening up the possibility of further\nimprovement through the use of entirely synthetic data."
                },
                "authors": [
                    {
                        "name": "Michael Sullivan"
                    },
                    {
                        "name": "Mareike Hartmann"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "arxiv_comment": "16 pages, 3 figures; accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20194v1",
                "updated": "2025-09-24T14:49:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    49,
                    8,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:49:08Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    49,
                    8,
                    2,
                    267,
                    0
                ],
                "title": "Identification and Semiparametric Estimation of Conditional Means from\n  Aggregate Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification and Semiparametric Estimation of Conditional Means from\n  Aggregate Data"
                },
                "summary": "We introduce a new method for estimating the mean of an outcome variable\nwithin groups when researchers only observe the average of the outcome and\ngroup indicators across a set of aggregation units, such as geographical areas.\nExisting methods for this problem, also known as ecological inference,\nimplicitly make strong assumptions about the aggregation process. We first\nformalize weaker conditions for identification, which motivates estimators that\ncan efficiently control for many covariates. We propose a debiased machine\nlearning estimator that is based on nuisance functions restricted to a\npartially linear form. Our estimator also admits a semiparametric sensitivity\nanalysis for violations of the key identifying assumption, as well as\nasymptotically valid confidence intervals for local, unit-level estimates under\nadditional assumptions. Simulations and validation on real-world data where\nground truth is available demonstrate the advantages of our approach over\nexisting methods. Open-source software is available which implements the\nproposed methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new method for estimating the mean of an outcome variable\nwithin groups when researchers only observe the average of the outcome and\ngroup indicators across a set of aggregation units, such as geographical areas.\nExisting methods for this problem, also known as ecological inference,\nimplicitly make strong assumptions about the aggregation process. We first\nformalize weaker conditions for identification, which motivates estimators that\ncan efficiently control for many covariates. We propose a debiased machine\nlearning estimator that is based on nuisance functions restricted to a\npartially linear form. Our estimator also admits a semiparametric sensitivity\nanalysis for violations of the key identifying assumption, as well as\nasymptotically valid confidence intervals for local, unit-level estimates under\nadditional assumptions. Simulations and validation on real-world data where\nground truth is available demonstrate the advantages of our approach over\nexisting methods. Open-source software is available which implements the\nproposed methods."
                },
                "authors": [
                    {
                        "name": "Cory McCartan"
                    },
                    {
                        "name": "Shiro Kuriwaki"
                    }
                ],
                "author_detail": {
                    "name": "Shiro Kuriwaki"
                },
                "author": "Shiro Kuriwaki",
                "arxiv_comment": "24 pages, plus references and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04137v2",
                "updated": "2025-09-24T14:48:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    48,
                    30,
                    2,
                    267,
                    0
                ],
                "published": "2025-07-05T19:20:59Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    19,
                    20,
                    59,
                    5,
                    186,
                    0
                ],
                "title": "Detecting Token-Level Hallucinations Using Variance Signals: A\n  Reference-Free Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Token-Level Hallucinations Using Variance Signals: A\n  Reference-Free Approach"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive generative\ncapabilities across diverse tasks but remain susceptible to hallucinations,\nconfidently generated yet factually incorrect outputs. We introduce a\nreference-free, token-level hallucination detection framework that leverages\nthe variance in token log-probabilities across multiple stochastic generations.\nUnlike prior methods that require ground-truth references or sentence-level\nverification, our approach is model-agnostic, interpretable, and suited for\nreal-time or post-hoc analysis. We evaluate our method on unanswerable question\nprompts from the SQuAD v2 dataset and benchmark across three autoregressive\nmodels of varying scales: GPT-Neo 125M, Falcon 1B, and Mistral 7B. Through both\nquantitative metrics and visual diagnostics, we show that token-level variance\nreliably highlights instability in model outputs and correlates with\nhallucination patterns. Our framework is lightweight, reproducible, and\nadaptable to multiple domains, offering a valuable diagnostic tool for\nanalyzing generative reliability in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive generative\ncapabilities across diverse tasks but remain susceptible to hallucinations,\nconfidently generated yet factually incorrect outputs. We introduce a\nreference-free, token-level hallucination detection framework that leverages\nthe variance in token log-probabilities across multiple stochastic generations.\nUnlike prior methods that require ground-truth references or sentence-level\nverification, our approach is model-agnostic, interpretable, and suited for\nreal-time or post-hoc analysis. We evaluate our method on unanswerable question\nprompts from the SQuAD v2 dataset and benchmark across three autoregressive\nmodels of varying scales: GPT-Neo 125M, Falcon 1B, and Mistral 7B. Through both\nquantitative metrics and visual diagnostics, we show that token-level variance\nreliably highlights instability in model outputs and correlates with\nhallucination patterns. Our framework is lightweight, reproducible, and\nadaptable to multiple domains, offering a valuable diagnostic tool for\nanalyzing generative reliability in LLMs."
                },
                "authors": [
                    {
                        "name": "Keshav Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Keshav Kumar"
                },
                "author": "Keshav Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20190v1",
                "updated": "2025-09-24T14:46:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    46,
                    42,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:46:42Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    46,
                    42,
                    2,
                    267,
                    0
                ],
                "title": "STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test\n  Generation"
                },
                "summary": "In modern automotive development, security testing is critical for\nsafeguarding systems against increasingly advanced threats. Attack trees are\nwidely used to systematically represent potential attack vectors, but\ngenerating comprehensive test cases from these trees remains a labor-intensive,\nerror-prone task that has seen limited automation in the context of testing\nvehicular systems. This paper introduces STAF (Security Test Automation\nFramework), a novel approach to automating security test case generation.\nLeveraging Large Language Models (LLMs) and a four-step self-corrective\nRetrieval-Augmented Generation (RAG) framework, STAF automates the generation\nof executable security test cases from attack trees, providing an end-to-end\nsolution that encompasses the entire attack surface. We particularly show the\nelements and processes needed to provide an LLM to actually produce sensible\nand executable automotive security test suites, along with the integration with\nan automated testing framework. We further compare our tailored approach with\ngeneral purpose (vanilla) LLMs and the performance of different LLMs (namely\nGPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our\noperation step-by-step in a concrete case study. Our results show significant\nimprovements in efficiency, accuracy, scalability, and easy integration in any\nworkflow, marking a substantial advancement in automating automotive security\ntesting methodologies. Using TARAs as an input for verfication tests, we create\nsynergies by connecting two vital elements of a secure automotive development\nprocess.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern automotive development, security testing is critical for\nsafeguarding systems against increasingly advanced threats. Attack trees are\nwidely used to systematically represent potential attack vectors, but\ngenerating comprehensive test cases from these trees remains a labor-intensive,\nerror-prone task that has seen limited automation in the context of testing\nvehicular systems. This paper introduces STAF (Security Test Automation\nFramework), a novel approach to automating security test case generation.\nLeveraging Large Language Models (LLMs) and a four-step self-corrective\nRetrieval-Augmented Generation (RAG) framework, STAF automates the generation\nof executable security test cases from attack trees, providing an end-to-end\nsolution that encompasses the entire attack surface. We particularly show the\nelements and processes needed to provide an LLM to actually produce sensible\nand executable automotive security test suites, along with the integration with\nan automated testing framework. We further compare our tailored approach with\ngeneral purpose (vanilla) LLMs and the performance of different LLMs (namely\nGPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our\noperation step-by-step in a concrete case study. Our results show significant\nimprovements in efficiency, accuracy, scalability, and easy integration in any\nworkflow, marking a substantial advancement in automating automotive security\ntesting methodologies. Using TARAs as an input for verfication tests, we create\nsynergies by connecting two vital elements of a secure automotive development\nprocess."
                },
                "authors": [
                    {
                        "name": "Tanmay Khule"
                    },
                    {
                        "name": "Stefan Marksteiner"
                    },
                    {
                        "name": "Jose Alguindigue"
                    },
                    {
                        "name": "Hannes Fuchs"
                    },
                    {
                        "name": "Sebastian Fischmeister"
                    },
                    {
                        "name": "Apurva Narayan"
                    }
                ],
                "author_detail": {
                    "name": "Apurva Narayan"
                },
                "author": "Apurva Narayan",
                "arxiv_comment": "18 pages, 2 figures, accepted for 23rd escar Europe (Nov 05-06, 2025,\n  Frankfurt, Germany)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20189v1",
                "updated": "2025-09-24T14:46:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    46,
                    7,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:46:07Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    46,
                    7,
                    2,
                    267,
                    0
                ],
                "title": "Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge\n  Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge\n  Accelerators"
                },
                "summary": "Edge accelerators such as Nvidia Jetsons are becoming an integral part of the\ncomputing continuum, and are often used for DNN inferencing and training.\nNvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power\nenvelope and offer $1000$s of power modes to customize CPU, GPU and memory\nfrequencies. Their widely varying power--performance trade-offs can be\nexploited for energy and power-constrained deployments. While data-driven\nmethods to predict the power and latency of DNN workloads for edge devices\nexist, there is a lack of principled study to understand why edge accelerators\nand their power modes perform the way they do. We develop a time roofline and a\nnovel energy roofline model for the Jetson Orin AGX for diverse power modes,\nand couple it with an analytical model of the compute (FLOP) and memory access\n(bytes) for DNN inference workloads to analyze them from first principles.\nThese reveal unique, sometimes counter-intuitive, insights into the power and\nperformance behavior of DNN workloads on edge accelerators, e.g., the default\npower mode MAXN is not the most energy efficient and time efficiency implies\nenergy efficiency for all power modes. We also extend our analytical roofline\nmodels to DNN training. Finally, we apply these methods to tune the power mode\n(and hence the roofline) of the edge device to optimize the latency and energy\nfor DNN inference, with up to $15\\%$ lower energy and minimal degradation in\ninference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge accelerators such as Nvidia Jetsons are becoming an integral part of the\ncomputing continuum, and are often used for DNN inferencing and training.\nNvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power\nenvelope and offer $1000$s of power modes to customize CPU, GPU and memory\nfrequencies. Their widely varying power--performance trade-offs can be\nexploited for energy and power-constrained deployments. While data-driven\nmethods to predict the power and latency of DNN workloads for edge devices\nexist, there is a lack of principled study to understand why edge accelerators\nand their power modes perform the way they do. We develop a time roofline and a\nnovel energy roofline model for the Jetson Orin AGX for diverse power modes,\nand couple it with an analytical model of the compute (FLOP) and memory access\n(bytes) for DNN inference workloads to analyze them from first principles.\nThese reveal unique, sometimes counter-intuitive, insights into the power and\nperformance behavior of DNN workloads on edge accelerators, e.g., the default\npower mode MAXN is not the most energy efficient and time efficiency implies\nenergy efficiency for all power modes. We also extend our analytical roofline\nmodels to DNN training. Finally, we apply these methods to tune the power mode\n(and hence the roofline) of the edge device to optimize the latency and energy\nfor DNN inference, with up to $15\\%$ lower energy and minimal degradation in\ninference time."
                },
                "authors": [
                    {
                        "name": "Prashanthi S. K."
                    },
                    {
                        "name": "Kunal Kumar Sahoo"
                    },
                    {
                        "name": "Amartya Ranjan Saikia"
                    },
                    {
                        "name": "Pranav Gupta"
                    },
                    {
                        "name": "Atharva Vinay Joshi"
                    },
                    {
                        "name": "Priyanshu Pansari"
                    },
                    {
                        "name": "Yogesh Simmhan"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh Simmhan"
                },
                "author": "Yogesh Simmhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20186v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20186v2",
                "updated": "2025-09-25T10:55:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    55,
                    2,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-24T14:45:13Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    45,
                    13,
                    2,
                    267,
                    0
                ],
                "title": "Thinking Augmented Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Augmented Pre-training"
                },
                "summary": "This paper introduces a simple and scalable approach to improve the data\nefficiency of large language model (LLM) training by augmenting existing text\ndata with thinking trajectories. The compute for pre-training LLMs has been\ngrowing at an unprecedented rate, while the availability of high-quality data\nremains limited. Consequently, maximizing the utility of available data\nconstitutes a significant research challenge. A primary impediment is that\ncertain high-quality tokens are difficult to learn given a fixed model\ncapacity, as the underlying rationale for a single token can be exceptionally\ncomplex and deep. To address this issue, we propose Thinking augmented\nPre-Training (TPT), a universal methodology that augments text with\nautomatically generated thinking trajectories. Such augmentation effectively\nincreases the volume of the training data and makes high-quality tokens more\nlearnable through step-by-step reasoning and decomposition. We apply TPT across\ndiverse training configurations up to $100$B tokens, encompassing pre-training\nwith both constrained and abundant data, as well as mid-training from strong\nopen-source checkpoints. Experimental results indicate that our method\nsubstantially improves the performance of LLMs across various model sizes and\nfamilies. Notably, TPT enhances the data efficiency of LLM pre-training by a\nfactor of $3$. For a $3$B parameter model, it improves the post-training\nperformance by over $10\\%$ on several challenging reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a simple and scalable approach to improve the data\nefficiency of large language model (LLM) training by augmenting existing text\ndata with thinking trajectories. The compute for pre-training LLMs has been\ngrowing at an unprecedented rate, while the availability of high-quality data\nremains limited. Consequently, maximizing the utility of available data\nconstitutes a significant research challenge. A primary impediment is that\ncertain high-quality tokens are difficult to learn given a fixed model\ncapacity, as the underlying rationale for a single token can be exceptionally\ncomplex and deep. To address this issue, we propose Thinking augmented\nPre-Training (TPT), a universal methodology that augments text with\nautomatically generated thinking trajectories. Such augmentation effectively\nincreases the volume of the training data and makes high-quality tokens more\nlearnable through step-by-step reasoning and decomposition. We apply TPT across\ndiverse training configurations up to $100$B tokens, encompassing pre-training\nwith both constrained and abundant data, as well as mid-training from strong\nopen-source checkpoints. Experimental results indicate that our method\nsubstantially improves the performance of LLMs across various model sizes and\nfamilies. Notably, TPT enhances the data efficiency of LLM pre-training by a\nfactor of $3$. For a $3$B parameter model, it improves the post-training\nperformance by over $10\\%$ on several challenging reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20186v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20186v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20182v1",
                "updated": "2025-09-24T14:44:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    44,
                    28,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:44:28Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    44,
                    28,
                    2,
                    267,
                    0
                ],
                "title": "Automated Multi-Agent Workflows for RTL Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Multi-Agent Workflows for RTL Design"
                },
                "summary": "The rise of agentic AI workflows unlocks novel opportunities for computer\nsystems design and optimization. However, for specialized domains such as\nprogram synthesis, the relative scarcity of HDL and proprietary EDA resources\nonline compared to more common programming tasks introduces challenges, often\nnecessitating task-specific fine-tuning, high inference costs, and\nmanually-crafted agent orchestration. In this work, we present VeriMaAS, a\nmulti-agent framework designed to automatically compose agentic workflows for\nRTL code generation. Our key insight is to integrate formal verification\nfeedback from HDL tools directly into workflow generation, reducing the cost of\ngradient-based updates or prolonged reasoning traces. Our method improves\nsynthesis performance by 5-7% for pass@k over fine-tuned baselines, while\nrequiring only a few hundred training examples, representing an\norder-of-magnitude reduction in supervision cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of agentic AI workflows unlocks novel opportunities for computer\nsystems design and optimization. However, for specialized domains such as\nprogram synthesis, the relative scarcity of HDL and proprietary EDA resources\nonline compared to more common programming tasks introduces challenges, often\nnecessitating task-specific fine-tuning, high inference costs, and\nmanually-crafted agent orchestration. In this work, we present VeriMaAS, a\nmulti-agent framework designed to automatically compose agentic workflows for\nRTL code generation. Our key insight is to integrate formal verification\nfeedback from HDL tools directly into workflow generation, reducing the cost of\ngradient-based updates or prolonged reasoning traces. Our method improves\nsynthesis performance by 5-7% for pass@k over fine-tuned baselines, while\nrequiring only a few hundred training examples, representing an\norder-of-magnitude reduction in supervision cost."
                },
                "authors": [
                    {
                        "name": "Amulya Bhattaram"
                    },
                    {
                        "name": "Janani Ramamoorthy"
                    },
                    {
                        "name": "Ranit Gupta"
                    },
                    {
                        "name": "Diana Marculescu"
                    },
                    {
                        "name": "Dimitrios Stamoulis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Stamoulis"
                },
                "author": "Dimitrios Stamoulis",
                "arxiv_comment": "Accepted: ML for Systems Workshop NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20172v1",
                "updated": "2025-09-24T14:36:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    36,
                    44,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:36:44Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    36,
                    44,
                    2,
                    267,
                    0
                ],
                "title": "Benchmarking Web API Integration Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Web API Integration Code Generation"
                },
                "summary": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models~(LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present a\ndataset and evaluation pipeline designed to assess the ability of LLMs to\ngenerate web API invocation code. Our experiments with several open-source LLMs\nreveal that generating API invocations poses a significant challenge, resulting\nin hallucinated endpoints, incorrect argument usage, and other errors. None of\nthe evaluated open-source models were able to solve more than 40% of the tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models~(LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present a\ndataset and evaluation pipeline designed to assess the ability of LLMs to\ngenerate web API invocation code. Our experiments with several open-source LLMs\nreveal that generating API invocations poses a significant challenge, resulting\nin hallucinated endpoints, incorrect argument usage, and other errors. None of\nthe evaluated open-source models were able to solve more than 40% of the tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Maninger"
                    },
                    {
                        "name": "Leon Chemnitz"
                    },
                    {
                        "name": "Amir Molzam Sharifloo"
                    },
                    {
                        "name": "Jannis Brugger"
                    },
                    {
                        "name": "Mira Mezini"
                    }
                ],
                "author_detail": {
                    "name": "Mira Mezini"
                },
                "author": "Mira Mezini",
                "arxiv_comment": "To be published in Proceedings of 2nd ACM International Conference on\n  AI-powered Software, Benchmark & Dataset Track (AIware '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20168v1",
                "updated": "2025-09-24T14:34:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    34,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:34:17Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    34,
                    17,
                    2,
                    267,
                    0
                ],
                "title": "Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in\n  Persian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in\n  Persian"
                },
                "summary": "Multilingual Large Language Models (LLMs) are increasingly used worldwide,\nmaking it essential to ensure they are free from gender bias to prevent\nrepresentational harm. While prior studies have examined such biases in\nhigh-resource languages, low-resource languages remain understudied. In this\npaper, we propose a template-based probing methodology, validated against\nreal-world data, to uncover gender stereotypes in LLMs. As part of this\nframework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a\nmetric that quantifies deviations from gender parity. We evaluate four\nprominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B,\nacross four semantic domains, focusing on Persian, a low-resource language with\ndistinct linguistic features. Our results show that all models exhibit gender\nstereotypes, with greater disparities in Persian than in English across all\ndomains. Among these, sports reflect the most rigid gender biases. This study\nunderscores the need for inclusive NLP practices and provides a framework for\nassessing bias in other low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models (LLMs) are increasingly used worldwide,\nmaking it essential to ensure they are free from gender bias to prevent\nrepresentational harm. While prior studies have examined such biases in\nhigh-resource languages, low-resource languages remain understudied. In this\npaper, we propose a template-based probing methodology, validated against\nreal-world data, to uncover gender stereotypes in LLMs. As part of this\nframework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a\nmetric that quantifies deviations from gender parity. We evaluate four\nprominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B,\nacross four semantic domains, focusing on Persian, a low-resource language with\ndistinct linguistic features. Our results show that all models exhibit gender\nstereotypes, with greater disparities in Persian than in English across all\ndomains. Among these, sports reflect the most rigid gender biases. This study\nunderscores the need for inclusive NLP practices and provides a framework for\nassessing bias in other low-resource languages."
                },
                "authors": [
                    {
                        "name": "Ghazal Kalhor"
                    },
                    {
                        "name": "Behnam Bahrak"
                    }
                ],
                "author_detail": {
                    "name": "Behnam Bahrak"
                },
                "author": "Behnam Bahrak",
                "arxiv_comment": "Accepted and forthcoming at the Widening Natural Language Processing\n  Workshop (WiNLP 2025) at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20166v1",
                "updated": "2025-09-24T14:33:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    33,
                    7,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:33:07Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    33,
                    7,
                    2,
                    267,
                    0
                ],
                "title": "CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and\n  Threat Intelligence Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and\n  Threat Intelligence Reasoning"
                },
                "summary": "Today's cyber defenders are overwhelmed by a deluge of security alerts,\nthreat intelligence signals, and shifting business context, creating an urgent\nneed for AI systems to enhance operational security work. While Large Language\nModels (LLMs) have the potential to automate and scale Security Operations\nCenter (SOC) operations, existing evaluations do not fully assess the scenarios\nmost relevant to real-world defenders. This lack of informed evaluation impacts\nboth AI developers and those applying LLMs to SOC automation. Without clear\ninsight into LLM performance in real-world security scenarios, developers lack\na north star for development, and users cannot reliably select the most\neffective models. Meanwhile, malicious actors are using AI to scale cyber\nattacks, highlighting the need for open source benchmarks to drive adoption and\ncommunity-driven improvement among defenders and model developers. To address\nthis, we introduce CyberSOCEval, a new suite of open source benchmarks within\nCyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in\ntwo tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive\ndomains with inadequate coverage in current benchmarks. Our evaluations show\nthat larger, more modern LLMs tend to perform better, confirming the training\nscaling laws paradigm. We also find that reasoning models leveraging test time\nscaling do not achieve the same boost as in coding and math, suggesting these\nmodels have not been trained to reason about cybersecurity analysis, and\npointing to a key opportunity for improvement. Finally, current LLMs are far\nfrom saturating our evaluations, showing that CyberSOCEval presents a\nsignificant challenge for AI developers to improve cyber defense capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's cyber defenders are overwhelmed by a deluge of security alerts,\nthreat intelligence signals, and shifting business context, creating an urgent\nneed for AI systems to enhance operational security work. While Large Language\nModels (LLMs) have the potential to automate and scale Security Operations\nCenter (SOC) operations, existing evaluations do not fully assess the scenarios\nmost relevant to real-world defenders. This lack of informed evaluation impacts\nboth AI developers and those applying LLMs to SOC automation. Without clear\ninsight into LLM performance in real-world security scenarios, developers lack\na north star for development, and users cannot reliably select the most\neffective models. Meanwhile, malicious actors are using AI to scale cyber\nattacks, highlighting the need for open source benchmarks to drive adoption and\ncommunity-driven improvement among defenders and model developers. To address\nthis, we introduce CyberSOCEval, a new suite of open source benchmarks within\nCyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in\ntwo tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive\ndomains with inadequate coverage in current benchmarks. Our evaluations show\nthat larger, more modern LLMs tend to perform better, confirming the training\nscaling laws paradigm. We also find that reasoning models leveraging test time\nscaling do not achieve the same boost as in coding and math, suggesting these\nmodels have not been trained to reason about cybersecurity analysis, and\npointing to a key opportunity for improvement. Finally, current LLMs are far\nfrom saturating our evaluations, showing that CyberSOCEval presents a\nsignificant challenge for AI developers to improve cyber defense capabilities."
                },
                "authors": [
                    {
                        "name": "Lauren Deason"
                    },
                    {
                        "name": "Adam Bali"
                    },
                    {
                        "name": "Ciprian Bejean"
                    },
                    {
                        "name": "Diana Bolocan"
                    },
                    {
                        "name": "James Crnkovich"
                    },
                    {
                        "name": "Ioana Croitoru"
                    },
                    {
                        "name": "Krishna Durai"
                    },
                    {
                        "name": "Chase Midler"
                    },
                    {
                        "name": "Calin Miron"
                    },
                    {
                        "name": "David Molnar"
                    },
                    {
                        "name": "Brad Moon"
                    },
                    {
                        "name": "Bruno Ostarcevic"
                    },
                    {
                        "name": "Alberto Peltea"
                    },
                    {
                        "name": "Matt Rosenberg"
                    },
                    {
                        "name": "Catalin Sandu"
                    },
                    {
                        "name": "Arthur Saputkin"
                    },
                    {
                        "name": "Sagar Shah"
                    },
                    {
                        "name": "Daniel Stan"
                    },
                    {
                        "name": "Ernest Szocs"
                    },
                    {
                        "name": "Shengye Wan"
                    },
                    {
                        "name": "Spencer Whitman"
                    },
                    {
                        "name": "Sven Krasser"
                    },
                    {
                        "name": "Joshua Saxe"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Saxe"
                },
                "author": "Joshua Saxe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20162v1",
                "updated": "2025-09-24T14:30:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    30,
                    16,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:30:16Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    30,
                    16,
                    2,
                    267,
                    0
                ],
                "title": "Embedding Domain Knowledge for Large Language Models via Reinforcement\n  Learning from Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding Domain Knowledge for Large Language Models via Reinforcement\n  Learning from Augmented Generation"
                },
                "summary": "Large language models (LLMs) often exhibit limited performance on\ndomain-specific tasks due to the natural disproportionate representation of\nspecialized information in their training data and the static nature of these\ndatasets. Knowledge scarcity and temporal lag create knowledge gaps for domain\napplications. While post-training on domain datasets can embed knowledge into\nmodels, existing approaches have some limitations. Continual Pre-Training (CPT)\ntreats all tokens in domain documents with equal importance, failing to\nprioritize critical knowledge points, while supervised fine-tuning (SFT) with\nquestion-answer pairs struggles to develop the coherent knowledge structures\nnecessary for complex reasoning tasks. To address these challenges, we propose\nReinforcement Learning from Augmented Generation (RLAG). Our approach\niteratively cycles between sampling generations and optimizing the model\nthrough calculated rewards, effectively embedding critical and contextually\ncoherent domain knowledge. We select generated outputs with the highest log\nprobabilities as the sampling result, then compute three tailored reward\nmetrics to guide the optimization process. To comprehensively evaluate domain\nexpertise, we assess answer accuracy and the rationality of explanations\ngenerated for correctly answered questions. Experimental results across\nmedical, legal, astronomy, and current events datasets demonstrate that our\nproposed method significantly outperforms baseline approaches. Our code and\ndata are open sourced at https://github.com/ChaojunNie/RLAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit limited performance on\ndomain-specific tasks due to the natural disproportionate representation of\nspecialized information in their training data and the static nature of these\ndatasets. Knowledge scarcity and temporal lag create knowledge gaps for domain\napplications. While post-training on domain datasets can embed knowledge into\nmodels, existing approaches have some limitations. Continual Pre-Training (CPT)\ntreats all tokens in domain documents with equal importance, failing to\nprioritize critical knowledge points, while supervised fine-tuning (SFT) with\nquestion-answer pairs struggles to develop the coherent knowledge structures\nnecessary for complex reasoning tasks. To address these challenges, we propose\nReinforcement Learning from Augmented Generation (RLAG). Our approach\niteratively cycles between sampling generations and optimizing the model\nthrough calculated rewards, effectively embedding critical and contextually\ncoherent domain knowledge. We select generated outputs with the highest log\nprobabilities as the sampling result, then compute three tailored reward\nmetrics to guide the optimization process. To comprehensively evaluate domain\nexpertise, we assess answer accuracy and the rationality of explanations\ngenerated for correctly answered questions. Experimental results across\nmedical, legal, astronomy, and current events datasets demonstrate that our\nproposed method significantly outperforms baseline approaches. Our code and\ndata are open sourced at https://github.com/ChaojunNie/RLAG."
                },
                "authors": [
                    {
                        "name": "Chaojun Nie"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Guanxiang Wang"
                    },
                    {
                        "name": "Shisong Wud"
                    },
                    {
                        "name": "Zichen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zichen Wang"
                },
                "author": "Zichen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02003v4",
                "updated": "2025-09-24T14:26:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    26,
                    13,
                    2,
                    267,
                    0
                ],
                "published": "2025-03-03T19:26:04Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    19,
                    26,
                    4,
                    0,
                    62,
                    0
                ],
                "title": "HoT: Highlighted Chain of Thought for Referencing Supporting Facts from\n  Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoT: Highlighted Chain of Thought for Referencing Supporting Facts from\n  Inputs"
                },
                "summary": "An Achilles heel of Large Language Models (LLMs) is their tendency to\nhallucinate non-factual statements. A response mixed of factual and non-factual\nstatements poses a challenge for humans to verify and accurately base their\ndecisions on. To combat this problem, we propose Highlighted Chain-of-Thought\nPrompting (HoT), a technique for prompting LLMs to generate responses with XML\ntags that ground facts to those provided in the query. That is, given an input\nquestion, LLMs would first re-format the question to add XML tags highlighting\nkey facts, and then, generate a response with highlights over the facts\nreferenced from the input. Interestingly, in few-shot settings, HoT outperforms\nvanilla chain of thought prompting (CoT) on a wide range of 17 tasks from\narithmetic, reading comprehension to logical reasoning. When asking humans to\nverify LLM responses, highlights help time-limited participants to more\naccurately and efficiently recognize when LLMs are correct. Yet, surprisingly,\nwhen LLMs are wrong, HoTs tend to make users believe that an answer is correct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Achilles heel of Large Language Models (LLMs) is their tendency to\nhallucinate non-factual statements. A response mixed of factual and non-factual\nstatements poses a challenge for humans to verify and accurately base their\ndecisions on. To combat this problem, we propose Highlighted Chain-of-Thought\nPrompting (HoT), a technique for prompting LLMs to generate responses with XML\ntags that ground facts to those provided in the query. That is, given an input\nquestion, LLMs would first re-format the question to add XML tags highlighting\nkey facts, and then, generate a response with highlights over the facts\nreferenced from the input. Interestingly, in few-shot settings, HoT outperforms\nvanilla chain of thought prompting (CoT) on a wide range of 17 tasks from\narithmetic, reading comprehension to logical reasoning. When asking humans to\nverify LLM responses, highlights help time-limited participants to more\naccurately and efficiently recognize when LLMs are correct. Yet, surprisingly,\nwhen LLMs are wrong, HoTs tend to make users believe that an answer is correct."
                },
                "authors": [
                    {
                        "name": "Tin Nguyen"
                    },
                    {
                        "name": "Logan Bolton"
                    },
                    {
                        "name": "Mohammad Reza Taesiri"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Anh Totti Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Anh Totti Nguyen"
                },
                "author": "Anh Totti Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11228v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11228v3",
                "updated": "2025-09-24T14:22:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    22,
                    54,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-16T13:23:52Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    13,
                    23,
                    52,
                    4,
                    136,
                    0
                ],
                "title": "Learning hidden cascades via classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning hidden cascades via classification"
                },
                "summary": "The spreading dynamics in social networks are often studied under the\nassumption that individuals' statuses, whether informed or infected, are fully\nobservable. However, in many real-world situations, such statuses remain\nunobservable, which is crucial for determining an individual's potential to\nfurther spread the infection. While final statuses are hidden, intermediate\nindicators such as symptoms of infection are observable and provide useful\nrepresentations of the underlying diffusion process. We propose a partial\nobservability-aware Machine Learning framework to learn the characteristics of\nthe spreading model. We term the method Distribution Classification, which\nutilizes the power of classifiers to infer the underlying transmission\ndynamics. Through extensive benchmarking against Approximate Bayesian\nComputation and GNN-based baselines, our framework consistently outperforms\nthese state-of-the-art methods, delivering accurate parameter estimates across\ndiverse diffusion settings while scaling efficiently to large networks. We\nvalidate the method on synthetic networks and extend the study to a real-world\ninsider trading network, demonstrating its effectiveness in analyzing spreading\nphenomena where direct observation of individual statuses is not possible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spreading dynamics in social networks are often studied under the\nassumption that individuals' statuses, whether informed or infected, are fully\nobservable. However, in many real-world situations, such statuses remain\nunobservable, which is crucial for determining an individual's potential to\nfurther spread the infection. While final statuses are hidden, intermediate\nindicators such as symptoms of infection are observable and provide useful\nrepresentations of the underlying diffusion process. We propose a partial\nobservability-aware Machine Learning framework to learn the characteristics of\nthe spreading model. We term the method Distribution Classification, which\nutilizes the power of classifiers to infer the underlying transmission\ndynamics. Through extensive benchmarking against Approximate Bayesian\nComputation and GNN-based baselines, our framework consistently outperforms\nthese state-of-the-art methods, delivering accurate parameter estimates across\ndiverse diffusion settings while scaling efficiently to large networks. We\nvalidate the method on synthetic networks and extend the study to a real-world\ninsider trading network, demonstrating its effectiveness in analyzing spreading\nphenomena where direct observation of individual statuses is not possible."
                },
                "authors": [
                    {
                        "name": "Derrick Gilchrist Edward Manoharan"
                    },
                    {
                        "name": "Anubha Goel"
                    },
                    {
                        "name": "Alexandros Iosifidis"
                    },
                    {
                        "name": "Henri Hansen"
                    },
                    {
                        "name": "Juho Kanniainen"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kanniainen"
                },
                "author": "Juho Kanniainen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11228v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11228v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20153v2",
                "updated": "2025-09-25T10:43:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    43,
                    22,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-24T14:18:41Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    18,
                    41,
                    2,
                    267,
                    0
                ],
                "title": "Affective Computing and Emotional Data: Challenges and Implications in\n  Privacy Regulations, The AI Act, and Ethics in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affective Computing and Emotional Data: Challenges and Implications in\n  Privacy Regulations, The AI Act, and Ethics in Large Language Models"
                },
                "summary": "This paper examines the integration of emotional intelligence into artificial\nintelligence systems, with a focus on affective computing and the growing\ncapabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to\nrecognize and respond to human emotions. Drawing on interdisciplinary research\nthat combines computer science, psychology, and neuroscience, the study\nanalyzes foundational neural architectures - CNNs for processing facial\nexpressions and RNNs for sequential data, such as speech and text - that enable\nemotion recognition. It examines the transformation of human emotional\nexperiences into structured emotional data, addressing the distinction between\nexplicit emotional data collected with informed consent in research settings\nand implicit data gathered passively through everyday digital interactions.\nThat raises critical concerns about lawful processing, AI transparency, and\nindividual autonomy over emotional expressions in digital environments. The\npaper explores implications across various domains, including healthcare,\neducation, and customer service, while addressing challenges of cultural\nvariations in emotional expression and potential biases in emotion recognition\nsystems across different demographic groups. From a regulatory perspective, the\npaper examines emotional data in the context of the GDPR and the EU AI Act\nframeworks, highlighting how emotional data may be considered sensitive\npersonal data that requires robust safeguards, including purpose limitation,\ndata minimization, and meaningful consent mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the integration of emotional intelligence into artificial\nintelligence systems, with a focus on affective computing and the growing\ncapabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to\nrecognize and respond to human emotions. Drawing on interdisciplinary research\nthat combines computer science, psychology, and neuroscience, the study\nanalyzes foundational neural architectures - CNNs for processing facial\nexpressions and RNNs for sequential data, such as speech and text - that enable\nemotion recognition. It examines the transformation of human emotional\nexperiences into structured emotional data, addressing the distinction between\nexplicit emotional data collected with informed consent in research settings\nand implicit data gathered passively through everyday digital interactions.\nThat raises critical concerns about lawful processing, AI transparency, and\nindividual autonomy over emotional expressions in digital environments. The\npaper explores implications across various domains, including healthcare,\neducation, and customer service, while addressing challenges of cultural\nvariations in emotional expression and potential biases in emotion recognition\nsystems across different demographic groups. From a regulatory perspective, the\npaper examines emotional data in the context of the GDPR and the EU AI Act\nframeworks, highlighting how emotional data may be considered sensitive\npersonal data that requires robust safeguards, including purpose limitation,\ndata minimization, and meaningful consent mechanisms."
                },
                "authors": [
                    {
                        "name": "Nicola Fabiano"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Fabiano"
                },
                "author": "Nicola Fabiano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00096v2",
                "updated": "2025-09-24T14:16:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    16,
                    30,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-30T11:51:11Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    11,
                    51,
                    11,
                    4,
                    150,
                    0
                ],
                "title": "PathGene: Benchmarking Driver Gene Mutations and Exon Prediction Using\n  Multicenter Lung Cancer Histopathology Image Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PathGene: Benchmarking Driver Gene Mutations and Exon Prediction Using\n  Multicenter Lung Cancer Histopathology Image Dataset"
                },
                "summary": "Accurately predicting gene mutations, mutation subtypes and their exons in\nlung cancer is critical for personalized treatment planning and prognostic\nassessment. Faced with regional disparities in medical resources and the high\ncost of genomic assays, using artificial intelligence to infer these mutations\nand exon variants from routine histopathology images could greatly facilitate\nprecision therapy. Although some prior studies have shown that deep learning\ncan accelerate the prediction of key gene mutations from lung cancer pathology\nslides, their performance remains suboptimal and has so far been limited mainly\nto early screening tasks. To address these limitations, we have assembled\nPathGene, which comprises histopathology images paired with next-generation\nsequencing reports from 1,576 patients at the Second Xiangya Hospital, Central\nSouth University, and 448 TCGA-LUAD patients. This multi-center dataset links\nwhole-slide images to driver gene mutation status, mutation subtypes, exon, and\ntumor mutational burden (TMB) status, with the goal of leveraging pathology\nimages to predict mutations, subtypes, exon locations, and TMB for early\ngenetic screening and to advance precision oncology. Unlike existing datasets,\nwe provide molecular-level information related to histopathology images in\nPathGene to facilitate the development of biomarker prediction models. We\nbenchmarked 11 multiple-instance learning methods on PathGene for mutation,\nsubtype, exon, and TMB prediction tasks. These experimental methods provide\nvaluable alternatives for early genetic screening of lung cancer patients and\nassisting clinicians to quickly develop personalized precision targeted\ntreatment plans for patients. Code and data are available at\nhttps://github.com/panliangrui/NIPS2025/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately predicting gene mutations, mutation subtypes and their exons in\nlung cancer is critical for personalized treatment planning and prognostic\nassessment. Faced with regional disparities in medical resources and the high\ncost of genomic assays, using artificial intelligence to infer these mutations\nand exon variants from routine histopathology images could greatly facilitate\nprecision therapy. Although some prior studies have shown that deep learning\ncan accelerate the prediction of key gene mutations from lung cancer pathology\nslides, their performance remains suboptimal and has so far been limited mainly\nto early screening tasks. To address these limitations, we have assembled\nPathGene, which comprises histopathology images paired with next-generation\nsequencing reports from 1,576 patients at the Second Xiangya Hospital, Central\nSouth University, and 448 TCGA-LUAD patients. This multi-center dataset links\nwhole-slide images to driver gene mutation status, mutation subtypes, exon, and\ntumor mutational burden (TMB) status, with the goal of leveraging pathology\nimages to predict mutations, subtypes, exon locations, and TMB for early\ngenetic screening and to advance precision oncology. Unlike existing datasets,\nwe provide molecular-level information related to histopathology images in\nPathGene to facilitate the development of biomarker prediction models. We\nbenchmarked 11 multiple-instance learning methods on PathGene for mutation,\nsubtype, exon, and TMB prediction tasks. These experimental methods provide\nvaluable alternatives for early genetic screening of lung cancer patients and\nassisting clinicians to quickly develop personalized precision targeted\ntreatment plans for patients. Code and data are available at\nhttps://github.com/panliangrui/NIPS2025/."
                },
                "authors": [
                    {
                        "name": "Liangrui Pan"
                    },
                    {
                        "name": "Qingchun Liang"
                    },
                    {
                        "name": "Shen Zhao"
                    },
                    {
                        "name": "Songqing Fan"
                    },
                    {
                        "name": "Shaoliang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Shaoliang Peng"
                },
                "author": "Shaoliang Peng",
                "arxiv_comment": "This submission is being withdrawn because we identified issues in\n  the analysis that may affect the results. A corrected version will be\n  submitted in the future. The manuscript is withdrawn as it requires\n  substantial revision. An improved version will be submitted in the future",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18263v2",
                "updated": "2025-09-24T14:14:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    14,
                    34,
                    2,
                    267,
                    0
                ],
                "published": "2025-04-25T11:17:46Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    17,
                    46,
                    4,
                    115,
                    0
                ],
                "title": "COSINUS model-independent sensitivity to the DAMA/LIBRA dark matter\n  signal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSINUS model-independent sensitivity to the DAMA/LIBRA dark matter\n  signal"
                },
                "summary": "COSINUS is a dark matter direct detection experiment using NaI crystals as\ncryogenic scintillating calorimeters. If no signal is observed, this will\nconstrain the dark matter scattering rate in sodium iodide. We investigate how\nthis constraint can be used to infer that the annual modulation signal observed\nin the DAMA/LIBRA experiment cannot originate from dark matter nuclear recoil\nevents, independently of the dark matter model. We achieve this by unfolding\nthe DAMA modulation spectrum to obtain the implied unquenched nuclear recoil\nspectrum, which we then compare to the expected COSINUS sensitivity. We find\nthat assuming zero background in the signal region, a 1$\\sigma$, 2$\\sigma$ or\n3$\\sigma$ confidence limit exclusion can be obtained with 57, 130 or 250 kg day\nof exposure, respectively. A simple background model indicates that in the\npresence of background, the exposure requirements may increase by $\\sim30\\%$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSINUS is a dark matter direct detection experiment using NaI crystals as\ncryogenic scintillating calorimeters. If no signal is observed, this will\nconstrain the dark matter scattering rate in sodium iodide. We investigate how\nthis constraint can be used to infer that the annual modulation signal observed\nin the DAMA/LIBRA experiment cannot originate from dark matter nuclear recoil\nevents, independently of the dark matter model. We achieve this by unfolding\nthe DAMA modulation spectrum to obtain the implied unquenched nuclear recoil\nspectrum, which we then compare to the expected COSINUS sensitivity. We find\nthat assuming zero background in the signal region, a 1$\\sigma$, 2$\\sigma$ or\n3$\\sigma$ confidence limit exclusion can be obtained with 57, 130 or 250 kg day\nof exposure, respectively. A simple background model indicates that in the\npresence of background, the exposure requirements may increase by $\\sim30\\%$."
                },
                "authors": [
                    {
                        "name": "G. Angloher"
                    },
                    {
                        "name": "M. R. Bharadwaj"
                    },
                    {
                        "name": "A. Böhmer"
                    },
                    {
                        "name": "M. Cababie"
                    },
                    {
                        "name": "I. Colantoni"
                    },
                    {
                        "name": "I. Dafinei"
                    },
                    {
                        "name": "N. Di Marco"
                    },
                    {
                        "name": "C. Dittmar"
                    },
                    {
                        "name": "L. Einfalt"
                    },
                    {
                        "name": "F. Ferella"
                    },
                    {
                        "name": "F. Ferroni"
                    },
                    {
                        "name": "S. Fichtinger"
                    },
                    {
                        "name": "A. Filipponi"
                    },
                    {
                        "name": "T. Frank"
                    },
                    {
                        "name": "M. Friedl"
                    },
                    {
                        "name": "Z. Ge"
                    },
                    {
                        "name": "M. Heikinheimo"
                    },
                    {
                        "name": "M. N. Hughes"
                    },
                    {
                        "name": "K. Huitu"
                    },
                    {
                        "name": "M. Kellermann"
                    },
                    {
                        "name": "R. Maji"
                    },
                    {
                        "name": "M. Mancuso"
                    },
                    {
                        "name": "L. Pagnanini"
                    },
                    {
                        "name": "F. Petricca"
                    },
                    {
                        "name": "S. Pirro"
                    },
                    {
                        "name": "F. Pröbst"
                    },
                    {
                        "name": "G. Profeta"
                    },
                    {
                        "name": "A. Puiu"
                    },
                    {
                        "name": "F. Reindl"
                    },
                    {
                        "name": "K. Schäffner"
                    },
                    {
                        "name": "J. Schieck"
                    },
                    {
                        "name": "P. Schreiner"
                    },
                    {
                        "name": "C. Schwertner"
                    },
                    {
                        "name": "K. Shera"
                    },
                    {
                        "name": "M. Stahlberg"
                    },
                    {
                        "name": "A. Stendahl"
                    },
                    {
                        "name": "M. Stukel"
                    },
                    {
                        "name": "C. Tresca"
                    },
                    {
                        "name": "F. Wagner"
                    },
                    {
                        "name": "S. Yue"
                    },
                    {
                        "name": "V. Zema"
                    },
                    {
                        "name": "Y. Zhu"
                    },
                    {
                        "name": "The COSINUS Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "The COSINUS Collaboration"
                },
                "author": "The COSINUS Collaboration",
                "arxiv_comment": "11 pages, 7 figures, 1 table; v2 matches the published version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20149v1",
                "updated": "2025-09-24T14:14:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    14,
                    21,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:14:21Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    14,
                    21,
                    2,
                    267,
                    0
                ],
                "title": "Enhancing Requirement Traceability through Data Augmentation Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Requirement Traceability through Data Augmentation Using Large\n  Language Models"
                },
                "summary": "Requirements traceability is crucial in software engineering to ensure\nconsistency between requirements and code. However, existing automated\ntraceability methods are constrained by the scarcity of training data and\nchallenges in bridging the semantic gap between artifacts. This study aims to\naddress the data scarcity problem in requirements traceability by employing\nlarge language models (LLMs) for data augmentation. We propose a novel approach\nthat utilizes prompt-based techniques with LLMs to generate augmented\nrequirement-to-code trace links, thereby enhancing the training dataset. Four\nLLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both\nzero-shot and few-shot templates. Moreover, we optimized the encoder component\nof the tracing model to improve its efficiency and adaptability to augmented\ndata. The key contributions of this paper are: (1) proposing and evaluating\nfour prompt templates for data augmentation; (2) providing a comparative\nanalysis of four LLMs for generating trace links; (3) enhancing the model's\nencoder for improved adaptability to augmented datasets. Experimental results\nshow that our approach significantly enhances model performance, achieving an\nF1 score improvement of up to 28.59%, thus demonstrating its effectiveness and\npotential for practical application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requirements traceability is crucial in software engineering to ensure\nconsistency between requirements and code. However, existing automated\ntraceability methods are constrained by the scarcity of training data and\nchallenges in bridging the semantic gap between artifacts. This study aims to\naddress the data scarcity problem in requirements traceability by employing\nlarge language models (LLMs) for data augmentation. We propose a novel approach\nthat utilizes prompt-based techniques with LLMs to generate augmented\nrequirement-to-code trace links, thereby enhancing the training dataset. Four\nLLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both\nzero-shot and few-shot templates. Moreover, we optimized the encoder component\nof the tracing model to improve its efficiency and adaptability to augmented\ndata. The key contributions of this paper are: (1) proposing and evaluating\nfour prompt templates for data augmentation; (2) providing a comparative\nanalysis of four LLMs for generating trace links; (3) enhancing the model's\nencoder for improved adaptability to augmented datasets. Experimental results\nshow that our approach significantly enhances model performance, achieving an\nF1 score improvement of up to 28.59%, thus demonstrating its effectiveness and\npotential for practical application."
                },
                "authors": [
                    {
                        "name": "Jianzhang Zhang"
                    },
                    {
                        "name": "Jialong Zhou"
                    },
                    {
                        "name": "Nan Niu"
                    },
                    {
                        "name": "Chuang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Liu"
                },
                "author": "Chuang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09673v2",
                "updated": "2025-09-24T14:12:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    12,
                    54,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-11T17:59:12Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    12,
                    3,
                    254,
                    0
                ],
                "title": "Cosmology inference with perturbative forward modeling at the field\n  level: a comparison with joint power spectrum and bispectrum analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmology inference with perturbative forward modeling at the field\n  level: a comparison with joint power spectrum and bispectrum analyses"
                },
                "summary": "We extend field-level inference to jointly constrain the cosmological\nparameters $\\{A,\\omega_{\\rm cdm},H_0\\}$, in both real and redshift space. Our\nanalyses are based on mock data generated using a perturbative forward model,\nwith noise drawn from a Gaussian distribution with a constant power spectrum.\nThis idealized setting, where the field-level likelihood is exactly Gaussian,\nallows us to precisely quantify the information content in the nonlinear field\non large scales. We find that field-level inference accurately recovers all\ncosmological parameters in both real and redshift space, with uncertainties\nconsistent with perturbation theory expectations. We show that these error bars\nare comparable to those obtained from a joint power spectrum and bispectrum\nanalysis using the same perturbative model. Finally, we perform several tests\nusing the Gaussian field-level likelihood to fit the mock data where the true\nnoise model is non-Gaussian, and find significant biases in the inferred\ncosmological parameters. These results highlight that the success of\nfield-level inference critically depends on using the correct likelihood, which\nmay be the primary challenge for applying this method to smaller scales even in\nthe perturbative regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We extend field-level inference to jointly constrain the cosmological\nparameters $\\{A,\\omega_{\\rm cdm},H_0\\}$, in both real and redshift space. Our\nanalyses are based on mock data generated using a perturbative forward model,\nwith noise drawn from a Gaussian distribution with a constant power spectrum.\nThis idealized setting, where the field-level likelihood is exactly Gaussian,\nallows us to precisely quantify the information content in the nonlinear field\non large scales. We find that field-level inference accurately recovers all\ncosmological parameters in both real and redshift space, with uncertainties\nconsistent with perturbation theory expectations. We show that these error bars\nare comparable to those obtained from a joint power spectrum and bispectrum\nanalysis using the same perturbative model. Finally, we perform several tests\nusing the Gaussian field-level likelihood to fit the mock data where the true\nnoise model is non-Gaussian, and find significant biases in the inferred\ncosmological parameters. These results highlight that the success of\nfield-level inference critically depends on using the correct likelihood, which\nmay be the primary challenge for applying this method to smaller scales even in\nthe perturbative regime."
                },
                "authors": [
                    {
                        "name": "Kazuyuki Akitsu"
                    },
                    {
                        "name": "Marko Simonović"
                    },
                    {
                        "name": "Shi-Fan Chen"
                    },
                    {
                        "name": "Giovanni Cabass"
                    },
                    {
                        "name": "Matias Zaldarriaga"
                    }
                ],
                "author_detail": {
                    "name": "Matias Zaldarriaga"
                },
                "author": "Matias Zaldarriaga",
                "arxiv_comment": "50 pages, 27 figues, the code available at\n  https://github.com/kazakitsu/field-level-inference; v2 adds references and\n  corrects typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18991v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18991v5",
                "updated": "2025-09-25T02:38:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    2,
                    38,
                    14,
                    3,
                    268,
                    0
                ],
                "published": "2025-03-23T16:40:29Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    16,
                    40,
                    29,
                    6,
                    82,
                    0
                ],
                "title": "Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM\n  Alignment"
                },
                "summary": "Alignment is vital for safely deploying large language models (LLMs).\nExisting techniques are either reward-based (train a reward model on preference\npairs and optimize with reinforcement learning) or reward-free (directly\nfine-tune on ranked outputs). Recent research shows that well-tuned\nreward-based pipelines remain robust, and single-response demonstrations can\noutperform pairwise preference data. However, two challenges persist: (1)\nimbalanced safety datasets that overrepresent common hazards while neglecting\nlong-tail threats; and (2) static reward models that ignore task difficulty,\nlimiting optimization efficiency and attainable gains. We propose DR-IRL\n(Dynamically adjusting Rewards through Inverse Reinforcement Learning). We\nfirst train category-specific reward models using a balanced safety dataset\ncovering seven harmful categories via IRL. Then we enhance Group Relative\nPolicy Optimization (GRPO) by introducing dynamic reward scaling--adjusting\nrewards by task difficulty--data-level hardness by text encoder cosine\nsimilarity, model-level responsiveness by reward gaps. Extensive experiments\nacross various benchmarks and LLMs demonstrate that DR-IRL outperforms all\nbaseline methods in safety alignment while maintaining usefulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment is vital for safely deploying large language models (LLMs).\nExisting techniques are either reward-based (train a reward model on preference\npairs and optimize with reinforcement learning) or reward-free (directly\nfine-tune on ranked outputs). Recent research shows that well-tuned\nreward-based pipelines remain robust, and single-response demonstrations can\noutperform pairwise preference data. However, two challenges persist: (1)\nimbalanced safety datasets that overrepresent common hazards while neglecting\nlong-tail threats; and (2) static reward models that ignore task difficulty,\nlimiting optimization efficiency and attainable gains. We propose DR-IRL\n(Dynamically adjusting Rewards through Inverse Reinforcement Learning). We\nfirst train category-specific reward models using a balanced safety dataset\ncovering seven harmful categories via IRL. Then we enhance Group Relative\nPolicy Optimization (GRPO) by introducing dynamic reward scaling--adjusting\nrewards by task difficulty--data-level hardness by text encoder cosine\nsimilarity, model-level responsiveness by reward gaps. Extensive experiments\nacross various benchmarks and LLMs demonstrate that DR-IRL outperforms all\nbaseline methods in safety alignment while maintaining usefulness."
                },
                "authors": [
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Haoxuan Ma"
                    },
                    {
                        "name": "Weixin Wang"
                    },
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Jiexi Liu"
                    },
                    {
                        "name": "Xiaoshuang Jia"
                    },
                    {
                        "name": "Simeng Qin"
                    },
                    {
                        "name": "Xiaochun Cao"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xiaojun Jia"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Jia"
                },
                "author": "Xiaojun Jia",
                "arxiv_comment": "The first three authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18991v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18991v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01301v2",
                "updated": "2025-09-24T14:02:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    2,
                    11,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-01T09:39:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    9,
                    39,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "Culture is Everywhere: A Call for Intentionally Cultural Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culture is Everywhere: A Call for Intentionally Cultural Evaluation"
                },
                "summary": "The prevailing ``trivia-centered paradigm'' for evaluating the cultural\nalignment of large language models (LLMs) is increasingly inadequate as these\nmodels become more advanced and widely deployed. Existing approaches typically\nreduce culture to static facts or values, testing models via multiple-choice or\nshort-answer questions that treat culture as isolated trivia. Such methods\nneglect the pluralistic and interactive realities of culture, and overlook how\ncultural assumptions permeate even ostensibly ``neutral'' evaluation settings.\nIn this position paper, we argue for \\textbf{intentionally cultural\nevaluation}: an approach that systematically examines the cultural assumptions\nembedded in all aspects of evaluation, not just in explicitly cultural tasks.\nWe systematically characterize the what, how, and circumstances by which\nculturally contingent considerations arise in evaluation, and emphasize the\nimportance of researcher positionality for fostering inclusive, culturally\naligned NLP research. Finally, we discuss implications and future directions\nfor moving beyond current benchmarking practices, discovering important\napplications that we don't know exist, and involving communities in evaluation\ndesign through HCI-inspired participatory methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevailing ``trivia-centered paradigm'' for evaluating the cultural\nalignment of large language models (LLMs) is increasingly inadequate as these\nmodels become more advanced and widely deployed. Existing approaches typically\nreduce culture to static facts or values, testing models via multiple-choice or\nshort-answer questions that treat culture as isolated trivia. Such methods\nneglect the pluralistic and interactive realities of culture, and overlook how\ncultural assumptions permeate even ostensibly ``neutral'' evaluation settings.\nIn this position paper, we argue for \\textbf{intentionally cultural\nevaluation}: an approach that systematically examines the cultural assumptions\nembedded in all aspects of evaluation, not just in explicitly cultural tasks.\nWe systematically characterize the what, how, and circumstances by which\nculturally contingent considerations arise in evaluation, and emphasize the\nimportance of researcher positionality for fostering inclusive, culturally\naligned NLP research. Finally, we discuss implications and future directions\nfor moving beyond current benchmarking practices, discovering important\napplications that we don't know exist, and involving communities in evaluation\ndesign through HCI-inspired participatory methodologies."
                },
                "authors": [
                    {
                        "name": "Juhyun Oh"
                    },
                    {
                        "name": "Inha Cha"
                    },
                    {
                        "name": "Michael Saxon"
                    },
                    {
                        "name": "Hyunseung Lim"
                    },
                    {
                        "name": "Shaily Bhatt"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20136v1",
                "updated": "2025-09-24T14:01:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    1,
                    18,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:01:18Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    1,
                    18,
                    2,
                    267,
                    0
                ],
                "title": "V-GameGym: Visual Game Generation for Code Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-GameGym: Visual Game Generation for Code Large Language Models"
                },
                "summary": "Code large language models have demonstrated remarkable capabilities in\nprogramming tasks, yet current benchmarks primarily focus on single modality\nrather than visual game development. Most existing code-related benchmarks\nevaluate syntax correctness and execution accuracy, overlooking critical\ngame-specific metrics such as playability, visual aesthetics, and user\nengagement that are essential for real-world deployment. To address the gap\nbetween current LLM capabilities in algorithmic problem-solving and competitive\nprogramming versus the comprehensive requirements of practical game\ndevelopment, we present V-GameGym, a comprehensive benchmark comprising 2,219\nhigh-quality samples across 100 thematic clusters derived from real-world\nrepositories, adopting a novel clustering-based curation methodology to ensure\nboth diversity and structural completeness. Further, we introduce a multimodal\nevaluation framework with an automated LLM-driven pipeline for visual code\nsynthesis using complete UI sandbox environments. Our extensive analysis\nreveals that V-GameGym effectively bridges the gap between code generation\naccuracy and practical game development workflows, providing quantifiable\nquality metrics for visual programming and interactive element generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code large language models have demonstrated remarkable capabilities in\nprogramming tasks, yet current benchmarks primarily focus on single modality\nrather than visual game development. Most existing code-related benchmarks\nevaluate syntax correctness and execution accuracy, overlooking critical\ngame-specific metrics such as playability, visual aesthetics, and user\nengagement that are essential for real-world deployment. To address the gap\nbetween current LLM capabilities in algorithmic problem-solving and competitive\nprogramming versus the comprehensive requirements of practical game\ndevelopment, we present V-GameGym, a comprehensive benchmark comprising 2,219\nhigh-quality samples across 100 thematic clusters derived from real-world\nrepositories, adopting a novel clustering-based curation methodology to ensure\nboth diversity and structural completeness. Further, we introduce a multimodal\nevaluation framework with an automated LLM-driven pipeline for visual code\nsynthesis using complete UI sandbox environments. Our extensive analysis\nreveals that V-GameGym effectively bridges the gap between code generation\naccuracy and practical game development workflows, providing quantifiable\nquality metrics for visual programming and interactive element generation."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Jack Yang"
                    },
                    {
                        "name": "Renshuai Tao"
                    },
                    {
                        "name": "Lingzheng Chai"
                    },
                    {
                        "name": "Shawn Guo"
                    },
                    {
                        "name": "Jiajun Wu"
                    },
                    {
                        "name": "Xiaoming Chen"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Xander Xu"
                    },
                    {
                        "name": "Hu Wei"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17729v2",
                "updated": "2025-09-24T13:57:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    57,
                    51,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-22T12:59:18Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    59,
                    18,
                    0,
                    265,
                    0
                ],
                "title": "A Generative Conditional Distribution Equality Testing Framework and Its\n  Minimax Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative Conditional Distribution Equality Testing Framework and Its\n  Minimax Analysis"
                },
                "summary": "In this paper, we propose a general framework for testing the equality of the\nconditional distributions in a two-sample problem. This problem is most\nrelevant to transfer learning under covariate shift. Our framework is built on\nneural network-based generative methods and sample splitting techniques by\ntransforming the conditional distribution testing problem into an unconditional\none. We introduce two special tests: the generative permutation-based\nconditional distribution equality test and the generative classification\naccuracy-based conditional distribution equality test. Theoretically, we\nestablish a minimax lower bound for statistical inference in testing the\nequality of two conditional distributions under certain smoothness conditions.\nWe demonstrate that the generative permutation-based conditional distribution\nequality test and its modified version can attain this lower bound precisely or\nup to some iterated logarithmic factor. Moreover, we prove the testing\nconsistency of the generative classification accuracy-based conditional\ndistribution equality test. We also establish the convergence rate for the\nlearned conditional generator by deriving new results related to the\nrecently-developed offset Rademacher complexity and approximation properties\nusing neural networks. Empirically, we conduct numerical studies including\nsynthetic datasets and two real-world datasets, demonstrating the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a general framework for testing the equality of the\nconditional distributions in a two-sample problem. This problem is most\nrelevant to transfer learning under covariate shift. Our framework is built on\nneural network-based generative methods and sample splitting techniques by\ntransforming the conditional distribution testing problem into an unconditional\none. We introduce two special tests: the generative permutation-based\nconditional distribution equality test and the generative classification\naccuracy-based conditional distribution equality test. Theoretically, we\nestablish a minimax lower bound for statistical inference in testing the\nequality of two conditional distributions under certain smoothness conditions.\nWe demonstrate that the generative permutation-based conditional distribution\nequality test and its modified version can attain this lower bound precisely or\nup to some iterated logarithmic factor. Moreover, we prove the testing\nconsistency of the generative classification accuracy-based conditional\ndistribution equality test. We also establish the convergence rate for the\nlearned conditional generator by deriving new results related to the\nrecently-developed offset Rademacher complexity and approximation properties\nusing neural networks. Empirically, we conduct numerical studies including\nsynthetic datasets and two real-world datasets, demonstrating the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Siming Zheng"
                    },
                    {
                        "name": "Meifang Lan"
                    },
                    {
                        "name": "Tong Wang"
                    },
                    {
                        "name": "Yuanyuan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yuanyuan Lin"
                },
                "author": "Yuanyuan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08599v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08599v4",
                "updated": "2025-09-24T13:50:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    50,
                    49,
                    2,
                    267,
                    0
                ],
                "published": "2025-06-10T09:06:28Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    9,
                    6,
                    28,
                    1,
                    161,
                    0
                ],
                "title": "Geometric Hyperscanning of Affect under Active Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric Hyperscanning of Affect under Active Inference"
                },
                "summary": "Second-person neuroscience holds social cognition as embodied meaning\nco-regulation through reciprocal interaction, modeled here as coupled active\ninference with affect emerging as inference over identity-relevant surprise.\nEach agent maintains a self-model that tracks violations in its predictive\ncoherence while recursively modeling the other. Valence is computed from\nself-model prediction error, weighted by self-relevance, and modulated by prior\naffective states and by what we term temporal aiming, which captures affective\nappraisal over time. This accommodates shifts in the self-other boundary,\nallowing affect to emerge at individual and dyadic levels. We propose a novel\nmethod termed geometric hyperscanning, based on the Forman-Ricci curvature, to\nempirically operationalize these processes: it tracks topological\nreconfigurations in inter-brain networks, with its entro-py serving as a proxy\nfor affective phase transitions such as rupture, co-regulation, and\nre-attunement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Second-person neuroscience holds social cognition as embodied meaning\nco-regulation through reciprocal interaction, modeled here as coupled active\ninference with affect emerging as inference over identity-relevant surprise.\nEach agent maintains a self-model that tracks violations in its predictive\ncoherence while recursively modeling the other. Valence is computed from\nself-model prediction error, weighted by self-relevance, and modulated by prior\naffective states and by what we term temporal aiming, which captures affective\nappraisal over time. This accommodates shifts in the self-other boundary,\nallowing affect to emerge at individual and dyadic levels. We propose a novel\nmethod termed geometric hyperscanning, based on the Forman-Ricci curvature, to\nempirically operationalize these processes: it tracks topological\nreconfigurations in inter-brain networks, with its entro-py serving as a proxy\nfor affective phase transitions such as rupture, co-regulation, and\nre-attunement."
                },
                "authors": [
                    {
                        "name": "Nicolas Hinrichs"
                    },
                    {
                        "name": "Mahault Albarracin"
                    },
                    {
                        "name": "Dimitris Bolis"
                    },
                    {
                        "name": "Yuyue Jiang"
                    },
                    {
                        "name": "Leonardo Christov-Moore"
                    },
                    {
                        "name": "Leonhard Schilbach"
                    }
                ],
                "author_detail": {
                    "name": "Leonhard Schilbach"
                },
                "author": "Leonhard Schilbach",
                "arxiv_comment": "12 pages excl. references, 2 figures, and 2 appendixes. Accepted at\n  the 6th International Workshop on Active Inference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08599v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08599v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20124v1",
                "updated": "2025-09-24T13:49:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    49,
                    44,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:49:44Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    49,
                    44,
                    2,
                    267,
                    0
                ],
                "title": "Probability Signature: Bridging Data Semantics and Embedding Structure\n  in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probability Signature: Bridging Data Semantics and Embedding Structure\n  in Language Models"
                },
                "summary": "The embedding space of language models is widely believed to capture the\nsemantic relationships; for instance, embeddings of digits often exhibit an\nordered structure that corresponds to their natural sequence. However, the\nmechanisms driving the formation of such structures remain poorly understood.\nIn this work, we interpret the embedding structures via the data distribution.\nWe propose a set of probability signatures that reflect the semantic\nrelationships among tokens. Through experiments on the composite addition tasks\nusing the linear model and feedforward network, combined with theoretical\nanalysis of gradient flow dynamics, we reveal that these probability signatures\nsignificantly influence the embedding structures. We further generalize our\nanalysis to large language models (LLMs) by training the Qwen2.5 architecture\non the subsets of the Pile corpus. Our results show that the probability\nsignatures are faithfully aligned with the embedding structures, particularly\nin capturing strong pairwise similarities among embeddings. Our work uncovers\nthe mechanism of how data distribution guides the formation of embedding\nstructures, establishing a novel understanding of the relationship between\nembedding organization and semantic patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The embedding space of language models is widely believed to capture the\nsemantic relationships; for instance, embeddings of digits often exhibit an\nordered structure that corresponds to their natural sequence. However, the\nmechanisms driving the formation of such structures remain poorly understood.\nIn this work, we interpret the embedding structures via the data distribution.\nWe propose a set of probability signatures that reflect the semantic\nrelationships among tokens. Through experiments on the composite addition tasks\nusing the linear model and feedforward network, combined with theoretical\nanalysis of gradient flow dynamics, we reveal that these probability signatures\nsignificantly influence the embedding structures. We further generalize our\nanalysis to large language models (LLMs) by training the Qwen2.5 architecture\non the subsets of the Pile corpus. Our results show that the probability\nsignatures are faithfully aligned with the embedding structures, particularly\nin capturing strong pairwise similarities among embeddings. Our work uncovers\nthe mechanism of how data distribution guides the formation of embedding\nstructures, establishing a novel understanding of the relationship between\nembedding organization and semantic patterns."
                },
                "authors": [
                    {
                        "name": "Junjie Yao"
                    },
                    {
                        "name": "Zhi-Qin John Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Qin John Xu"
                },
                "author": "Zhi-Qin John Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20123v1",
                "updated": "2025-09-24T13:48:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    48,
                    14,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:48:14Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    48,
                    14,
                    2,
                    267,
                    0
                ],
                "title": "Can LLMs Forecast Internet Traffic from Social Media?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Forecast Internet Traffic from Social Media?"
                },
                "summary": "Societal events shape the Internet's behavior. The death of a prominent\npublic figure, a software launch, or a major sports match can trigger sudden\ndemand surges that overwhelm peering points and content delivery networks.\nAlthough these events fall outside regular traffic patterns, forecasting\nsystems still rely solely on those patterns and therefore miss these critical\nanomalies.\n  Thus, we argue for socio-technical systems that supplement technical\nmeasurements with an active understanding of the underlying drivers, including\nhow events and collective behavior shape digital demands. We propose traffic\nforecasting using signals from public discourse, such as headlines, forums, and\nsocial media, as early demand indicators.\n  To validate our intuition, we present a proof-of-concept system that\nautonomously scrapes online discussions, infers real-world events, clusters and\nenriches them semantically, and correlates them with traffic measurements at a\nmajor Internet Exchange Point. This prototype predicted between 56-92% of\nsociety-driven traffic spikes after scraping a moderate amount of online\ndiscussions.\n  We believe this approach opens new research opportunities in cross-domain\nforecasting, scheduling, demand anticipation, and society-informed decision\nmaking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Societal events shape the Internet's behavior. The death of a prominent\npublic figure, a software launch, or a major sports match can trigger sudden\ndemand surges that overwhelm peering points and content delivery networks.\nAlthough these events fall outside regular traffic patterns, forecasting\nsystems still rely solely on those patterns and therefore miss these critical\nanomalies.\n  Thus, we argue for socio-technical systems that supplement technical\nmeasurements with an active understanding of the underlying drivers, including\nhow events and collective behavior shape digital demands. We propose traffic\nforecasting using signals from public discourse, such as headlines, forums, and\nsocial media, as early demand indicators.\n  To validate our intuition, we present a proof-of-concept system that\nautonomously scrapes online discussions, infers real-world events, clusters and\nenriches them semantically, and correlates them with traffic measurements at a\nmajor Internet Exchange Point. This prototype predicted between 56-92% of\nsociety-driven traffic spikes after scraping a moderate amount of online\ndiscussions.\n  We believe this approach opens new research opportunities in cross-domain\nforecasting, scheduling, demand anticipation, and society-informed decision\nmaking."
                },
                "authors": [
                    {
                        "name": "Jonatan Langlet"
                    },
                    {
                        "name": "Mariano Scazzariello"
                    },
                    {
                        "name": "Flavio Luciani"
                    },
                    {
                        "name": "Marta Burocchi"
                    },
                    {
                        "name": "Dejan Kostić"
                    },
                    {
                        "name": "Marco Chiesa"
                    }
                ],
                "author_detail": {
                    "name": "Marco Chiesa"
                },
                "author": "Marco Chiesa",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20105v1",
                "updated": "2025-09-24T13:29:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    29,
                    53,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:29:53Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    29,
                    53,
                    2,
                    267,
                    0
                ],
                "title": "PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning\n  Traces in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning\n  Traces in LLMs"
                },
                "summary": "Large Language Models (LLMs) often struggle with maintaining coherent\nmulti-step reasoning traces, particularly in tasks that require a structured\nlogical flow. This work introduces a quantum-inspired approach to address the\nchallenge by incorporating a fidelity-based reward derived from Projected\nEntangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior\napproaches that use direct supervision or contrastive objectives, the proposed\nmethod guides learning through structural consistency, offering a novel\napproach to enforce global coherence in generated reasoning traces. The\nproposed framework is evaluated using multiple coherence-determining metrics on\ndiverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning\narithmetic, intuitive, and entailment-based reasoning. Results show that the\nproposed quantum-inspired approach offers significant improvements over\nsupervised, contrastive, and pretrained baseline approaches, highlighting the\neffectiveness of quantum-inspired fidelity as a foundation to improve reasoning\ntrace coherence in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with maintaining coherent\nmulti-step reasoning traces, particularly in tasks that require a structured\nlogical flow. This work introduces a quantum-inspired approach to address the\nchallenge by incorporating a fidelity-based reward derived from Projected\nEntangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior\napproaches that use direct supervision or contrastive objectives, the proposed\nmethod guides learning through structural consistency, offering a novel\napproach to enforce global coherence in generated reasoning traces. The\nproposed framework is evaluated using multiple coherence-determining metrics on\ndiverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning\narithmetic, intuitive, and entailment-based reasoning. Results show that the\nproposed quantum-inspired approach offers significant improvements over\nsupervised, contrastive, and pretrained baseline approaches, highlighting the\neffectiveness of quantum-inspired fidelity as a foundation to improve reasoning\ntrace coherence in LLMs."
                },
                "authors": [
                    {
                        "name": "Venkat Margapuri"
                    },
                    {
                        "name": "Garik Kazanjian"
                    },
                    {
                        "name": "Naren Kosaraju"
                    }
                ],
                "author_detail": {
                    "name": "Naren Kosaraju"
                },
                "author": "Naren Kosaraju",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20103v1",
                "updated": "2025-09-24T13:27:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    27,
                    46,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:27:46Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    27,
                    46,
                    2,
                    267,
                    0
                ],
                "title": "Enabling Multi-Species Bird Classification on Low-Power Bioacoustic\n  Loggers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Multi-Species Bird Classification on Low-Power Bioacoustic\n  Loggers"
                },
                "summary": "This paper introduces WrenNet, an efficient neural network enabling real-time\nmulti-species bird audio classification on low-power microcontrollers for\nscalable biodiversity monitoring. We propose a semi-learnable spectral feature\nextractor that adapts to avian vocalizations, outperforming standard mel-scale\nand fully-learnable alternatives. On an expert-curated 70-species dataset,\nWrenNet achieves up to 90.8\\% accuracy on acoustically distinctive species and\n70.1\\% on the full task. When deployed on an AudioMoth device ($\\leq$1MB RAM),\nit consumes only 77mJ per inference. Moreover, the proposed model is over 16x\nmore energy-efficient compared to Birdnet when running on a Raspberry Pi 3B+.\nThis work demonstrates the first practical framework for continuous,\nmulti-species acoustic monitoring on low-power edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces WrenNet, an efficient neural network enabling real-time\nmulti-species bird audio classification on low-power microcontrollers for\nscalable biodiversity monitoring. We propose a semi-learnable spectral feature\nextractor that adapts to avian vocalizations, outperforming standard mel-scale\nand fully-learnable alternatives. On an expert-curated 70-species dataset,\nWrenNet achieves up to 90.8\\% accuracy on acoustically distinctive species and\n70.1\\% on the full task. When deployed on an AudioMoth device ($\\leq$1MB RAM),\nit consumes only 77mJ per inference. Moreover, the proposed model is over 16x\nmore energy-efficient compared to Birdnet when running on a Raspberry Pi 3B+.\nThis work demonstrates the first practical framework for continuous,\nmulti-species acoustic monitoring on low-power edge devices."
                },
                "authors": [
                    {
                        "name": "Stefano Ciapponi"
                    },
                    {
                        "name": "Leonardo Mannini"
                    },
                    {
                        "name": "Jarek Scanferla"
                    },
                    {
                        "name": "Matteo Anderle"
                    },
                    {
                        "name": "Elisabetta Farella"
                    }
                ],
                "author_detail": {
                    "name": "Elisabetta Farella"
                },
                "author": "Elisabetta Farella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20102v1",
                "updated": "2025-09-24T13:27:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    27,
                    35,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:27:35Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    27,
                    35,
                    2,
                    267,
                    0
                ],
                "title": "Steerable Adversarial Scenario Generation through Test-Time Preference\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steerable Adversarial Scenario Generation through Test-Time Preference\n  Alignment"
                },
                "summary": "Adversarial scenario generation is a cost-effective approach for safety\nassessment of autonomous driving systems. However, existing methods are often\nconstrained to a single, fixed trade-off between competing objectives such as\nadversariality and realism. This yields behavior-specific models that cannot be\nsteered at inference time, lacking the efficiency and flexibility to generate\ntailored scenarios for diverse training and testing requirements. In view of\nthis, we reframe the task of adversarial scenario generation as a\nmulti-objective preference alignment problem and introduce a new framework\nnamed \\textbf{S}teerable \\textbf{A}dversarial scenario \\textbf{GE}nerator\n(SAGE). SAGE enables fine-grained test-time control over the trade-off between\nadversariality and realism without any retraining. We first propose\nhierarchical group-based preference optimization, a data-efficient offline\nalignment method that learns to balance competing objectives by decoupling hard\nfeasibility constraints from soft preferences. Instead of training a fixed\nmodel, SAGE fine-tunes two experts on opposing preferences and constructs a\ncontinuous spectrum of policies at inference time by linearly interpolating\ntheir weights. We provide theoretical justification for this framework through\nthe lens of linear mode connectivity. Extensive experiments demonstrate that\nSAGE not only generates scenarios with a superior balance of adversariality and\nrealism but also enables more effective closed-loop training of driving\npolicies. Project page: https://tongnie.github.io/SAGE/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial scenario generation is a cost-effective approach for safety\nassessment of autonomous driving systems. However, existing methods are often\nconstrained to a single, fixed trade-off between competing objectives such as\nadversariality and realism. This yields behavior-specific models that cannot be\nsteered at inference time, lacking the efficiency and flexibility to generate\ntailored scenarios for diverse training and testing requirements. In view of\nthis, we reframe the task of adversarial scenario generation as a\nmulti-objective preference alignment problem and introduce a new framework\nnamed \\textbf{S}teerable \\textbf{A}dversarial scenario \\textbf{GE}nerator\n(SAGE). SAGE enables fine-grained test-time control over the trade-off between\nadversariality and realism without any retraining. We first propose\nhierarchical group-based preference optimization, a data-efficient offline\nalignment method that learns to balance competing objectives by decoupling hard\nfeasibility constraints from soft preferences. Instead of training a fixed\nmodel, SAGE fine-tunes two experts on opposing preferences and constructs a\ncontinuous spectrum of policies at inference time by linearly interpolating\ntheir weights. We provide theoretical justification for this framework through\nthe lens of linear mode connectivity. Extensive experiments demonstrate that\nSAGE not only generates scenarios with a superior balance of adversariality and\nrealism but also enables more effective closed-loop training of driving\npolicies. Project page: https://tongnie.github.io/SAGE/."
                },
                "authors": [
                    {
                        "name": "Tong Nie"
                    },
                    {
                        "name": "Yuewen Mei"
                    },
                    {
                        "name": "Yihong Tang"
                    },
                    {
                        "name": "Junlin He"
                    },
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Haotian Shi"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Jian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jian Sun"
                },
                "author": "Jian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20097v1",
                "updated": "2025-09-24T13:20:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    20,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:20:37Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    20,
                    37,
                    2,
                    267,
                    0
                ],
                "title": "Integrated Framework for LLM Evaluation with Answer Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Framework for LLM Evaluation with Answer Generation"
                },
                "summary": "Reliable evaluation of large language models is essential to ensure their\napplicability in practical scenarios. Traditional benchmark-based evaluation\nmethods often rely on fixed reference answers, limiting their ability to\ncapture important qualitative aspects of generated responses. To address these\nshortcomings, we propose an integrated evaluation framework called\n\\textit{self-refining descriptive evaluation with expert-driven diagnostics},\nSPEED, which utilizes specialized functional experts to perform comprehensive,\ndescriptive analyses of model outputs. Unlike conventional approaches, SPEED\nactively incorporates expert feedback across multiple dimensions, including\nhallucination detection, toxicity assessment, and lexical-contextual\nappropriateness. Experimental results demonstrate that SPEED achieves robust\nand consistent evaluation performance across diverse domains and datasets.\nAdditionally, by employing relatively compact expert models, SPEED demonstrates\nsuperior resource efficiency compared to larger-scale evaluators. These\nfindings illustrate that SPEED significantly enhances fairness and\ninterpretability in LLM evaluations, offering a promising alternative to\nexisting evaluation methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable evaluation of large language models is essential to ensure their\napplicability in practical scenarios. Traditional benchmark-based evaluation\nmethods often rely on fixed reference answers, limiting their ability to\ncapture important qualitative aspects of generated responses. To address these\nshortcomings, we propose an integrated evaluation framework called\n\\textit{self-refining descriptive evaluation with expert-driven diagnostics},\nSPEED, which utilizes specialized functional experts to perform comprehensive,\ndescriptive analyses of model outputs. Unlike conventional approaches, SPEED\nactively incorporates expert feedback across multiple dimensions, including\nhallucination detection, toxicity assessment, and lexical-contextual\nappropriateness. Experimental results demonstrate that SPEED achieves robust\nand consistent evaluation performance across diverse domains and datasets.\nAdditionally, by employing relatively compact expert models, SPEED demonstrates\nsuperior resource efficiency compared to larger-scale evaluators. These\nfindings illustrate that SPEED significantly enhances fairness and\ninterpretability in LLM evaluations, offering a promising alternative to\nexisting evaluation methodologies."
                },
                "authors": [
                    {
                        "name": "Sujeong Lee"
                    },
                    {
                        "name": "Hayoung Lee"
                    },
                    {
                        "name": "Seongsoo Heo"
                    },
                    {
                        "name": "Wonik Choi"
                    }
                ],
                "author_detail": {
                    "name": "Wonik Choi"
                },
                "author": "Wonik Choi",
                "arxiv_comment": "16pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20091v1",
                "updated": "2025-09-24T13:11:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    11,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:11:37Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    11,
                    37,
                    2,
                    267,
                    0
                ],
                "title": "Unleashing the Potential of the Semantic Latent Space in Diffusion\n  Models for Image Dehazing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Semantic Latent Space in Diffusion\n  Models for Image Dehazing"
                },
                "summary": "Diffusion models have recently been investigated as powerful generative\nsolvers for image dehazing, owing to their remarkable capability to model the\ndata distribution. However, the massive computational burden imposed by the\nretraining of diffusion models, coupled with the extensive sampling steps\nduring the inference, limit the broader application of diffusion models in\nimage dehazing. To address these issues, we explore the properties of hazy\nimages in the semantic latent space of frozen pre-trained diffusion models, and\npropose a Diffusion Latent Inspired network for Image Dehazing, dubbed\nDiffLI$^2$D. Specifically, we first reveal that the semantic latent space of\npre-trained diffusion models can represent the content and haze characteristics\nof hazy images, as the diffusion time-step changes. Building upon this insight,\nwe integrate the diffusion latent representations at different time-steps into\na delicately designed dehazing network to provide instructions for image\ndehazing. Our DiffLI$^2$D avoids re-training diffusion models and iterative\nsampling process by effectively utilizing the informative representations\nderived from the pre-trained diffusion models, which also offers a novel\nperspective for introducing diffusion models to image dehazing. Extensive\nexperiments on multiple datasets demonstrate that the proposed method achieves\nsuperior performance to existing image dehazing methods. Code is available at\nhttps://github.com/aaaasan111/difflid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently been investigated as powerful generative\nsolvers for image dehazing, owing to their remarkable capability to model the\ndata distribution. However, the massive computational burden imposed by the\nretraining of diffusion models, coupled with the extensive sampling steps\nduring the inference, limit the broader application of diffusion models in\nimage dehazing. To address these issues, we explore the properties of hazy\nimages in the semantic latent space of frozen pre-trained diffusion models, and\npropose a Diffusion Latent Inspired network for Image Dehazing, dubbed\nDiffLI$^2$D. Specifically, we first reveal that the semantic latent space of\npre-trained diffusion models can represent the content and haze characteristics\nof hazy images, as the diffusion time-step changes. Building upon this insight,\nwe integrate the diffusion latent representations at different time-steps into\na delicately designed dehazing network to provide instructions for image\ndehazing. Our DiffLI$^2$D avoids re-training diffusion models and iterative\nsampling process by effectively utilizing the informative representations\nderived from the pre-trained diffusion models, which also offers a novel\nperspective for introducing diffusion models to image dehazing. Extensive\nexperiments on multiple datasets demonstrate that the proposed method achieves\nsuperior performance to existing image dehazing methods. Code is available at\nhttps://github.com/aaaasan111/difflid."
                },
                "authors": [
                    {
                        "name": "Zizheng Yang"
                    },
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20090v1",
                "updated": "2025-09-24T13:08:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    8,
                    20,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:08:20Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    8,
                    20,
                    2,
                    267,
                    0
                ],
                "title": "You Only Measure Once: On Designing Single-Shot Quantum Machine Learning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Only Measure Once: On Designing Single-Shot Quantum Machine Learning\n  Models"
                },
                "summary": "Quantum machine learning (QML) models conventionally rely on repeated\nmeasurements (shots) of observables to obtain reliable predictions. This\ndependence on large shot budgets leads to high inference cost and time\noverhead, which is particularly problematic as quantum hardware access is\ntypically priced proportionally to the number of shots. In this work we propose\nYou Only Measure Once (Yomo), a simple yet effective design that achieves\naccurate inference with dramatically fewer measurements, down to the\nsingle-shot regime. Yomo replaces Pauli expectation-value outputs with a\nprobability aggregation mechanism and introduces loss functions that encourage\nsharp predictions. Our theoretical analysis shows that Yomo avoids the\nshot-scaling limitations inherent to expectation-based models, and our\nexperiments on MNIST and CIFAR-10 confirm that Yomo consistently outperforms\nbaselines across different shot budgets and under simulations with depolarizing\nchannels. By enabling accurate single-shot inference, Yomo substantially\nreduces the financial and computational costs of deploying QML, thereby\nlowering the barrier to practical adoption of QML.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum machine learning (QML) models conventionally rely on repeated\nmeasurements (shots) of observables to obtain reliable predictions. This\ndependence on large shot budgets leads to high inference cost and time\noverhead, which is particularly problematic as quantum hardware access is\ntypically priced proportionally to the number of shots. In this work we propose\nYou Only Measure Once (Yomo), a simple yet effective design that achieves\naccurate inference with dramatically fewer measurements, down to the\nsingle-shot regime. Yomo replaces Pauli expectation-value outputs with a\nprobability aggregation mechanism and introduces loss functions that encourage\nsharp predictions. Our theoretical analysis shows that Yomo avoids the\nshot-scaling limitations inherent to expectation-based models, and our\nexperiments on MNIST and CIFAR-10 confirm that Yomo consistently outperforms\nbaselines across different shot budgets and under simulations with depolarizing\nchannels. By enabling accurate single-shot inference, Yomo substantially\nreduces the financial and computational costs of deploying QML, thereby\nlowering the barrier to practical adoption of QML."
                },
                "authors": [
                    {
                        "name": "Chen-Yu Liu"
                    },
                    {
                        "name": "Leonardo Placidi"
                    },
                    {
                        "name": "Kuan-Cheng Chen"
                    },
                    {
                        "name": "Samuel Yen-Chi Chen"
                    },
                    {
                        "name": "Gabriel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Matos"
                },
                "author": "Gabriel Matos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20088v1",
                "updated": "2025-09-24T13:06:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    6,
                    35,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:06:35Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    6,
                    35,
                    2,
                    267,
                    0
                ],
                "title": "Causal Understanding by LLMs: The Role of Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Understanding by LLMs: The Role of Uncertainty"
                },
                "summary": "Recent papers show LLMs achieve near-random accuracy in causal relation\nclassification, raising questions about whether such failures arise from\nlimited pretraining exposure or deeper representational gaps. We investigate\nthis under uncertainty-based evaluation, testing whether pretraining exposure\nto causal examples improves causal understanding >18K PubMed sentences -- half\nfrom The Pile corpus, half post-2024 -- across seven models\n(Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model\nbehavior through: (i) causal classification, where the model identifies causal\nrelationships in text, and (ii) verbatim memorization probing, where we assess\nwhether the model prefers previously seen causal statements over their\nparaphrases. Models perform four-way classification\n(direct/conditional/correlational/no-relationship) and select between originals\nand their generated paraphrases. Results show almost identical accuracy on\nseen/unseen sentences (p > 0.05), no memorization bias (24.8% original\nselection), and output distribution over the possible options is almost flat,\nwith entropic values near the maximum (1.35/1.39), confirming random guessing.\nInstruction-tuned models show severe miscalibration (Qwen: > 95% confidence,\n32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11%\nvs. direct). These findings suggest that failures in causal understanding arise\nfrom the lack of structured causal representation, rather than insufficient\nexposure to causal examples during pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent papers show LLMs achieve near-random accuracy in causal relation\nclassification, raising questions about whether such failures arise from\nlimited pretraining exposure or deeper representational gaps. We investigate\nthis under uncertainty-based evaluation, testing whether pretraining exposure\nto causal examples improves causal understanding >18K PubMed sentences -- half\nfrom The Pile corpus, half post-2024 -- across seven models\n(Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model\nbehavior through: (i) causal classification, where the model identifies causal\nrelationships in text, and (ii) verbatim memorization probing, where we assess\nwhether the model prefers previously seen causal statements over their\nparaphrases. Models perform four-way classification\n(direct/conditional/correlational/no-relationship) and select between originals\nand their generated paraphrases. Results show almost identical accuracy on\nseen/unseen sentences (p > 0.05), no memorization bias (24.8% original\nselection), and output distribution over the possible options is almost flat,\nwith entropic values near the maximum (1.35/1.39), confirming random guessing.\nInstruction-tuned models show severe miscalibration (Qwen: > 95% confidence,\n32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11%\nvs. direct). These findings suggest that failures in causal understanding arise\nfrom the lack of structured causal representation, rather than insufficient\nexposure to causal examples during pretraining."
                },
                "authors": [
                    {
                        "name": "Oscar Lithgow-Serrano"
                    },
                    {
                        "name": "Vani Kanjirangat"
                    },
                    {
                        "name": "Alessandro Antonucci"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Antonucci"
                },
                "author": "Alessandro Antonucci",
                "arxiv_comment": "Accepted in second UncertaiNLP workshop at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20086v1",
                "updated": "2025-09-24T13:05:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    5,
                    9,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:05:09Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    5,
                    9,
                    2,
                    267,
                    0
                ],
                "title": "OLaPh: Optimal Language Phonemizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OLaPh: Optimal Language Phonemizer"
                },
                "summary": "Phonemization, the conversion of text into phonemes, is a key step in\ntext-to-speech. Traditional approaches use rule-based transformations and\nlexicon lookups, while more advanced methods apply preprocessing techniques or\nneural networks for improved accuracy on out-of-domain vocabulary. However, all\nsystems struggle with names, loanwords, abbreviations, and homographs. This\nwork presents OLaPh (Optimal Language Phonemizer), a framework that combines\nlarge lexica, multiple NLP techniques, and compound resolution with a\nprobabilistic scoring function. Evaluations in German and English show improved\naccuracy over previous approaches, including on a challenging dataset. To\nfurther address unresolved cases, we train a large language model on\nOLaPh-generated data, which achieves even stronger generalization and\nperformance. Together, the framework and LLM improve phonemization consistency\nand provide a freely available resource for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phonemization, the conversion of text into phonemes, is a key step in\ntext-to-speech. Traditional approaches use rule-based transformations and\nlexicon lookups, while more advanced methods apply preprocessing techniques or\nneural networks for improved accuracy on out-of-domain vocabulary. However, all\nsystems struggle with names, loanwords, abbreviations, and homographs. This\nwork presents OLaPh (Optimal Language Phonemizer), a framework that combines\nlarge lexica, multiple NLP techniques, and compound resolution with a\nprobabilistic scoring function. Evaluations in German and English show improved\naccuracy over previous approaches, including on a challenging dataset. To\nfurther address unresolved cases, we train a large language model on\nOLaPh-generated data, which achieves even stronger generalization and\nperformance. Together, the framework and LLM improve phonemization consistency\nand provide a freely available resource for future research."
                },
                "authors": [
                    {
                        "name": "Johannes Wirth"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Wirth"
                },
                "author": "Johannes Wirth",
                "arxiv_comment": "5 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20083v1",
                "updated": "2025-09-24T13:03:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    3,
                    15,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:03:15Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    3,
                    15,
                    2,
                    267,
                    0
                ],
                "title": "Rethinking player evaluation in sports: Goals above expectation and\n  beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking player evaluation in sports: Goals above expectation and\n  beyond"
                },
                "summary": "A popular quantitative approach to evaluating player performance in sports\ninvolves comparing an observed outcome to the expected outcome ignoring player\ninvolvement, which is estimated using statistical or machine learning methods.\nIn soccer, for instance, goals above expectation (GAX) of a player measure how\noften shots of this player led to a goal compared to the model-derived expected\noutcome of the shots. Typically, sports data analysts rely on flexible machine\nlearning models, which are capable of handling complex nonlinear effects and\nfeature interactions, but fail to provide valid statistical inference due to\nfinite-sample bias and slow convergence rates. In this paper, we close this gap\nby presenting a framework for player evaluation with metrics derived from\ndifferences in actual and expected outcomes using flexible machine learning\nalgorithms, which nonetheless allows for valid frequentist inference. We first\nshow that the commonly used metrics are directly related to Rao's score test in\nparametric regression models for the expected outcome. Motivated by this\nfinding and recent developments in double machine learning, we then propose the\nuse of residualized versions of the original metrics. For GAX, the\nresidualization step corresponds to an additional regression predicting whether\na given player would take the shot under the circumstances described by the\nfeatures. We further relate metrics in the proposed framework to\nplayer-specific effect estimates in interpretable semiparametric regression\nmodels, allowing us to infer directional effects, e.g., to determine players\nthat have a positive impact on the outcome. Our primary use case are GAX in\nsoccer. We further apply our framework to evaluate goal-stopping ability of\ngoalkeepers, shooting skill in basketball, quarterback passing skill in\nAmerican football, and injury-proneness of soccer players.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A popular quantitative approach to evaluating player performance in sports\ninvolves comparing an observed outcome to the expected outcome ignoring player\ninvolvement, which is estimated using statistical or machine learning methods.\nIn soccer, for instance, goals above expectation (GAX) of a player measure how\noften shots of this player led to a goal compared to the model-derived expected\noutcome of the shots. Typically, sports data analysts rely on flexible machine\nlearning models, which are capable of handling complex nonlinear effects and\nfeature interactions, but fail to provide valid statistical inference due to\nfinite-sample bias and slow convergence rates. In this paper, we close this gap\nby presenting a framework for player evaluation with metrics derived from\ndifferences in actual and expected outcomes using flexible machine learning\nalgorithms, which nonetheless allows for valid frequentist inference. We first\nshow that the commonly used metrics are directly related to Rao's score test in\nparametric regression models for the expected outcome. Motivated by this\nfinding and recent developments in double machine learning, we then propose the\nuse of residualized versions of the original metrics. For GAX, the\nresidualization step corresponds to an additional regression predicting whether\na given player would take the shot under the circumstances described by the\nfeatures. We further relate metrics in the proposed framework to\nplayer-specific effect estimates in interpretable semiparametric regression\nmodels, allowing us to infer directional effects, e.g., to determine players\nthat have a positive impact on the outcome. Our primary use case are GAX in\nsoccer. We further apply our framework to evaluate goal-stopping ability of\ngoalkeepers, shooting skill in basketball, quarterback passing skill in\nAmerican football, and injury-proneness of soccer players."
                },
                "authors": [
                    {
                        "name": "Robert Bajons"
                    },
                    {
                        "name": "Lucas Kook"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Kook"
                },
                "author": "Lucas Kook",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13446v2",
                "updated": "2025-09-24T13:00:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    0,
                    59,
                    2,
                    267,
                    0
                ],
                "published": "2024-07-18T12:14:34Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    12,
                    14,
                    34,
                    3,
                    200,
                    0
                ],
                "title": "Subsampled One-Step Estimation for Fast Statistical Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subsampled One-Step Estimation for Fast Statistical Inference"
                },
                "summary": "Subsampling is an effective approach to alleviate the computational burden\nassociated with large-scale datasets. Nevertheless, existing subsampling\nestimators incur a substantial loss in estimation efficiency compared to\nestimators based on the full dataset. Specifically, the convergence rate of\nexisting subsampling estimators is typically $n^{-1/2}$ rather than $N^{-1/2}$,\nwhere $n$ and $N$ denote the subsample and full data sizes, respectively. This\npaper proposes a subsampled one-step (SOS) method to mitigate the estimation\nefficiency loss utilizing the asymptotic expansions of the subsampling and\nfull-data estimators. The resulting SOS estimator is computationally efficient\nand achieves a fast convergence rate of $\\max\\{n^{-1}, N^{-1/2}\\}$ rather than\n$n^{-1/2}$. We establish the asymptotic distribution of the SOS estimator,\nwhich can be non-normal in general, and construct confidence intervals on top\nof the asymptotic distribution. Furthermore, we prove that the SOS estimator is\nasymptotically normal and equivalent to the full data-based estimator when $n /\n\\sqrt{N} \\to \\infty$.Simulation studies and real data analyses were conducted\nto demonstrate the finite sample performance of the SOS estimator. Numerical\nresults suggest that the SOS estimator is almost as computationally efficient\nas the uniform subsampling estimator while achieving similar estimation\nefficiency to the full data-based estimator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subsampling is an effective approach to alleviate the computational burden\nassociated with large-scale datasets. Nevertheless, existing subsampling\nestimators incur a substantial loss in estimation efficiency compared to\nestimators based on the full dataset. Specifically, the convergence rate of\nexisting subsampling estimators is typically $n^{-1/2}$ rather than $N^{-1/2}$,\nwhere $n$ and $N$ denote the subsample and full data sizes, respectively. This\npaper proposes a subsampled one-step (SOS) method to mitigate the estimation\nefficiency loss utilizing the asymptotic expansions of the subsampling and\nfull-data estimators. The resulting SOS estimator is computationally efficient\nand achieves a fast convergence rate of $\\max\\{n^{-1}, N^{-1/2}\\}$ rather than\n$n^{-1/2}$. We establish the asymptotic distribution of the SOS estimator,\nwhich can be non-normal in general, and construct confidence intervals on top\nof the asymptotic distribution. Furthermore, we prove that the SOS estimator is\nasymptotically normal and equivalent to the full data-based estimator when $n /\n\\sqrt{N} \\to \\infty$.Simulation studies and real data analyses were conducted\nto demonstrate the finite sample performance of the SOS estimator. Numerical\nresults suggest that the SOS estimator is almost as computationally efficient\nas the uniform subsampling estimator while achieving similar estimation\nefficiency to the full data-based estimator."
                },
                "authors": [
                    {
                        "name": "Miaomiao Su"
                    },
                    {
                        "name": "Ruoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ruoyu Wang"
                },
                "author": "Ruoyu Wang",
                "arxiv_doi": "10.1111/sjos.70022",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1111/sjos.70022",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12044v2",
                "updated": "2025-09-24T12:57:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    57,
                    3,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-24T16:17:50Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    16,
                    17,
                    50,
                    5,
                    144,
                    0
                ],
                "title": "Why Do Some Inputs Break Low-Bit LLM Quantization?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do Some Inputs Break Low-Bit LLM Quantization?"
                },
                "summary": "Low-bit weight-only quantization significantly reduces the memory footprint\nof large language models (LLMs), but disproportionately affects certain\nexamples. We analyze diverse 3-4 bit methods on LLMs ranging from 7B-70B in\nsize and find that the quantization errors of 50 pairs of methods are strongly\ncorrelated (avg. 0.82) on FineWeb examples. Moreover, the residual stream\nmagnitudes of full-precision models are indicative of future quantization\nerrors. We further establish a hypothesis that relates the residual stream\nmagnitudes to error amplification and accumulation over layers. Using LLM\nlocalization techniques, early exiting, and activation patching, we show that\nexamples with large errors rely on precise residual activations in the late\nlayers, and that the outputs of MLP gates play a crucial role in maintaining\nthe perplexity. Our work reveals why certain examples result in large\nquantization errors and which model components are most critical for\nperformance preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-bit weight-only quantization significantly reduces the memory footprint\nof large language models (LLMs), but disproportionately affects certain\nexamples. We analyze diverse 3-4 bit methods on LLMs ranging from 7B-70B in\nsize and find that the quantization errors of 50 pairs of methods are strongly\ncorrelated (avg. 0.82) on FineWeb examples. Moreover, the residual stream\nmagnitudes of full-precision models are indicative of future quantization\nerrors. We further establish a hypothesis that relates the residual stream\nmagnitudes to error amplification and accumulation over layers. Using LLM\nlocalization techniques, early exiting, and activation patching, we show that\nexamples with large errors rely on precise residual activations in the late\nlayers, and that the outputs of MLP gates play a crucial role in maintaining\nthe perplexity. Our work reveals why certain examples result in large\nquantization errors and which model components are most critical for\nperformance preservation."
                },
                "authors": [
                    {
                        "name": "Ting-Yun Chang"
                    },
                    {
                        "name": "Muru Zhang"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Robin Jia"
                    }
                ],
                "author_detail": {
                    "name": "Robin Jia"
                },
                "author": "Robin Jia",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04341v2",
                "updated": "2025-09-24T12:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    56,
                    29,
                    2,
                    267,
                    0
                ],
                "published": "2025-01-08T08:26:56Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    26,
                    56,
                    2,
                    8,
                    0
                ],
                "title": "Understanding Before Reasoning: Enhancing Chain-of-Thought with\n  Iterative Summarization Pre-Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Before Reasoning: Enhancing Chain-of-Thought with\n  Iterative Summarization Pre-Prompting"
                },
                "summary": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language\nModels (LLMs) to enhance complex reasoning. It guides LLMs to present\nmulti-step reasoning, rather than generating the final answer directly.\nHowever, CoT encounters difficulties when key information required for\nreasoning is implicit or missing. This occurs because CoT emphasizes the\nsequence of reasoning steps while overlooking the early extraction of essential\ninformation. We propose a pre-prompting method called Iterative Summarization\nPre-Prompting (ISP^2) to refine LLM reasoning when key information is not\nexplicitly provided. First, entities and their corresponding descriptions are\nextracted to form potential key information pairs. Next, we use a reliability\nrating to assess these pairs, then merge the two lowest-ranked pairs into a new\nentity description. This process is repeated until a unique key information\npair is obtained. Finally, that pair, along with the original question, is fed\ninto LLMs to produce the answer. Extensive experiments demonstrate a 7.1%\nimprovement compared to existing methods. Unlike traditional prompting, ISP^2\nadopts an inductive approach with pre-prompting, offering flexible integration\ninto diverse reasoning frameworks. The code is available at\nhttps://github.com/zdhgreat/ISP-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language\nModels (LLMs) to enhance complex reasoning. It guides LLMs to present\nmulti-step reasoning, rather than generating the final answer directly.\nHowever, CoT encounters difficulties when key information required for\nreasoning is implicit or missing. This occurs because CoT emphasizes the\nsequence of reasoning steps while overlooking the early extraction of essential\ninformation. We propose a pre-prompting method called Iterative Summarization\nPre-Prompting (ISP^2) to refine LLM reasoning when key information is not\nexplicitly provided. First, entities and their corresponding descriptions are\nextracted to form potential key information pairs. Next, we use a reliability\nrating to assess these pairs, then merge the two lowest-ranked pairs into a new\nentity description. This process is repeated until a unique key information\npair is obtained. Finally, that pair, along with the original question, is fed\ninto LLMs to produce the answer. Extensive experiments demonstrate a 7.1%\nimprovement compared to existing methods. Unlike traditional prompting, ISP^2\nadopts an inductive approach with pre-prompting, offering flexible integration\ninto diverse reasoning frameworks. The code is available at\nhttps://github.com/zdhgreat/ISP-2."
                },
                "authors": [
                    {
                        "name": "Dong-Hai Zhu"
                    },
                    {
                        "name": "Yu-Jie Xiong"
                    },
                    {
                        "name": "Jia-Chen Zhang"
                    },
                    {
                        "name": "Xi-Jiong Xie"
                    },
                    {
                        "name": "Chun-Ming Xia"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Ming Xia"
                },
                "author": "Chun-Ming Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20072v2",
                "updated": "2025-09-25T09:23:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    9,
                    23,
                    12,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-24T12:44:26Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    44,
                    26,
                    2,
                    267,
                    0
                ],
                "title": "From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint\n  Training"
                },
                "summary": "Recent advances in large language models (LLMs) have attracted significant\ninterest in extending their capabilities to multimodal scenarios, particularly\nfor speech-to-speech conversational systems. However, existing multimodal\nmodels handling interleaved audio and text rely on autoregressive methods,\noverlooking that text depends on target-target relations whereas audio depends\nmainly on source-target relations. In this work, we propose Text-to-Talk (TtT),\na unified audio-text framework that integrates autoregressive (AR) text\ngeneration with non-autoregressive (NAR) audio diffusion in a single\nTransformer. By leveraging the any-order autoregressive property of absorbing\ndiscrete diffusion, our approach provides a unified training objective for text\nand audio. To support this hybrid generation paradigm, we design a\nmodality-aware attention mechanism that enforces causal decoding for text while\nallowing bidirectional modeling within audio spans, and further introduce three\ntraining strategies that reduce train-test discrepancies. During inference, TtT\nemploys block-wise diffusion to synthesize audio in parallel while flexibly\nhandling variable-length outputs. Extensive experiments across Audio-QA and ASR\ntasks demonstrate the effectiveness of our approach, with detailed ablation\nstudies validating each proposed component. We will open-source our models,\ndata and code to facilitate future research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have attracted significant\ninterest in extending their capabilities to multimodal scenarios, particularly\nfor speech-to-speech conversational systems. However, existing multimodal\nmodels handling interleaved audio and text rely on autoregressive methods,\noverlooking that text depends on target-target relations whereas audio depends\nmainly on source-target relations. In this work, we propose Text-to-Talk (TtT),\na unified audio-text framework that integrates autoregressive (AR) text\ngeneration with non-autoregressive (NAR) audio diffusion in a single\nTransformer. By leveraging the any-order autoregressive property of absorbing\ndiscrete diffusion, our approach provides a unified training objective for text\nand audio. To support this hybrid generation paradigm, we design a\nmodality-aware attention mechanism that enforces causal decoding for text while\nallowing bidirectional modeling within audio spans, and further introduce three\ntraining strategies that reduce train-test discrepancies. During inference, TtT\nemploys block-wise diffusion to synthesize audio in parallel while flexibly\nhandling variable-length outputs. Extensive experiments across Audio-QA and ASR\ntasks demonstrate the effectiveness of our approach, with detailed ablation\nstudies validating each proposed component. We will open-source our models,\ndata and code to facilitate future research in this direction."
                },
                "authors": [
                    {
                        "name": "Tianqiao Liu"
                    },
                    {
                        "name": "Xueyi Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Zhichao Chen"
                    },
                    {
                        "name": "Weiqi Luo"
                    },
                    {
                        "name": "Zitao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zitao Liu"
                },
                "author": "Zitao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20070v1",
                "updated": "2025-09-24T12:40:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    40,
                    57,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T12:40:57Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    40,
                    57,
                    2,
                    267,
                    0
                ],
                "title": "LLM Trainer: Automated Robotic Data Generating via Demonstration\n  Augmentation using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Trainer: Automated Robotic Data Generating via Demonstration\n  Augmentation using LLMs"
                },
                "summary": "We present LLM Trainer, a fully automated pipeline that leverages the world\nknowledge of Large Language Models (LLMs) to transform a small number of human\ndemonstrations (as few as one) into a large robot dataset for imitation\nlearning. Our approach decomposes demonstration generation into two steps: (1)\noffline demonstration annotation that extracts keyframes, salient objects, and\npose-object relations; and (2) online keypose retargeting that adapts those\nkeyframes to a new scene, given an initial observation. Using these modified\nkeypoints, our system warps the original demonstration to generate a new\ntrajectory, which is then executed, and the resulting demo, if successful, is\nsaved. Because the annotation is reusable across scenes, we use Thompson\nsampling to optimize the annotation, significantly improving generation success\nrate. We evaluate our method on a range of tasks, and find that our data\nannotation method consistently outperforms expert-engineered baselines. We\nfurther show an ensemble policy that combines the optimized LLM feed-forward\nplan with a learned feedback imitation learning controller. Finally, we\ndemonstrate hardware feasibility on a Franka Emika Panda robot. For additional\nmaterials and demonstration videos, please see the project website:\nhttps://sites.google.com/andrew.cmu.edu/llm-trainer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LLM Trainer, a fully automated pipeline that leverages the world\nknowledge of Large Language Models (LLMs) to transform a small number of human\ndemonstrations (as few as one) into a large robot dataset for imitation\nlearning. Our approach decomposes demonstration generation into two steps: (1)\noffline demonstration annotation that extracts keyframes, salient objects, and\npose-object relations; and (2) online keypose retargeting that adapts those\nkeyframes to a new scene, given an initial observation. Using these modified\nkeypoints, our system warps the original demonstration to generate a new\ntrajectory, which is then executed, and the resulting demo, if successful, is\nsaved. Because the annotation is reusable across scenes, we use Thompson\nsampling to optimize the annotation, significantly improving generation success\nrate. We evaluate our method on a range of tasks, and find that our data\nannotation method consistently outperforms expert-engineered baselines. We\nfurther show an ensemble policy that combines the optimized LLM feed-forward\nplan with a learned feedback imitation learning controller. Finally, we\ndemonstrate hardware feasibility on a Franka Emika Panda robot. For additional\nmaterials and demonstration videos, please see the project website:\nhttps://sites.google.com/andrew.cmu.edu/llm-trainer"
                },
                "authors": [
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "9 pages, 5 figures, 4 tables. Submitted to ICRA 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20067v2",
                "updated": "2025-09-25T03:59:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    59,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-24T12:37:11Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    37,
                    11,
                    2,
                    267,
                    0
                ],
                "title": "MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM"
                },
                "summary": "Large language models (LLMs) have demonstrated notable potential in medical\napplications, yet they face substantial challenges in handling complex\nreal-world clinical diagnoses using conventional prompting methods. Current\nprompt engineering and multi-agent approaches typically optimize isolated\ninferences, neglecting the accumulation of reusable clinical experience. To\naddress this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)\nframework, which allows LLMs to self-learn clinical knowledge via a multi-agent\npipeline that summarizes, refines, and applies diagnostic insights. It mirrors\nhow physicians develop expertise through experience, enabling more focused and\naccurate diagnosis on key disease-specific cues. We further extend it to a\nMACD-human collaborative workflow, where multiple LLM-based diagnostician\nagents engage in iterative consultations, supported by an evaluator agent and\nhuman oversight for cases where agreement is not reached. Evaluated on 4,390\nreal-world patient cases across seven diseases using diverse open-source LLMs\n(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves\nprimary diagnostic accuracy, outperforming established clinical guidelines with\ngains up to 22.3% (MACD). On the subset of the data, it achieves performance on\npar with or exceeding that of human physicians (up to 16% improvement over\nphysicians-only diagnosis). Additionally, on the MACD-human workflow, it\nachieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,\nself-learned knowledge exhibits strong cross-model stability, transferability,\nand model-specific personalization, while the system can generate traceable\nrationales, enhancing explainability. Consequently, this work presents a\nscalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap\nbetween the intrinsic knowledge of LLMs and real-world clinical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated notable potential in medical\napplications, yet they face substantial challenges in handling complex\nreal-world clinical diagnoses using conventional prompting methods. Current\nprompt engineering and multi-agent approaches typically optimize isolated\ninferences, neglecting the accumulation of reusable clinical experience. To\naddress this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)\nframework, which allows LLMs to self-learn clinical knowledge via a multi-agent\npipeline that summarizes, refines, and applies diagnostic insights. It mirrors\nhow physicians develop expertise through experience, enabling more focused and\naccurate diagnosis on key disease-specific cues. We further extend it to a\nMACD-human collaborative workflow, where multiple LLM-based diagnostician\nagents engage in iterative consultations, supported by an evaluator agent and\nhuman oversight for cases where agreement is not reached. Evaluated on 4,390\nreal-world patient cases across seven diseases using diverse open-source LLMs\n(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves\nprimary diagnostic accuracy, outperforming established clinical guidelines with\ngains up to 22.3% (MACD). On the subset of the data, it achieves performance on\npar with or exceeding that of human physicians (up to 16% improvement over\nphysicians-only diagnosis). Additionally, on the MACD-human workflow, it\nachieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,\nself-learned knowledge exhibits strong cross-model stability, transferability,\nand model-specific personalization, while the system can generate traceable\nrationales, enhancing explainability. Consequently, this work presents a\nscalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap\nbetween the intrinsic knowledge of LLMs and real-world clinical practice."
                },
                "authors": [
                    {
                        "name": "Wenliang Li"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Hongji Zhu"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Junjun Li"
                    },
                    {
                        "name": "Mengru Li"
                    },
                    {
                        "name": "Wei Cao"
                    },
                    {
                        "name": "Zihang Jiang"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Shaohua Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shaohua Kevin Zhou"
                },
                "author": "Shaohua Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15140v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15140v3",
                "updated": "2025-09-24T12:32:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    32,
                    47,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-21T05:54:37Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    5,
                    54,
                    37,
                    2,
                    141,
                    0
                ],
                "title": "EC-LDA : Label Distribution Inference Attack against Federated Graph\n  Learning with Embedding Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EC-LDA : Label Distribution Inference Attack against Federated Graph\n  Learning with Embedding Compression"
                },
                "summary": "Graph Neural Networks (GNNs) have been widely used for graph analysis.\nFederated Graph Learning (FGL) is an emerging learning framework to\ncollaboratively train graph data from various clients. Although FGL allows\nclient data to remain localized, a malicious server can still steal client\nprivate data information through uploaded gradient. In this paper, we for the\nfirst time propose label distribution attacks (LDAs) on FGL that aim to infer\nthe label distributions of the client-side data. Firstly, we observe that the\neffectiveness of LDA is closely related to the variance of node embeddings in\nGNNs. Next, we analyze the relation between them and propose a new attack named\nEC-LDA, which significantly improves the attack effectiveness by compressing\nnode embeddings. Then, extensive experiments on node classification and link\nprediction tasks across six widely used graph datasets show that EC-LDA\noutperforms the SOTA LDAs. Specifically, EC-LDA can achieve the Cos-sim as high\nas 1.0 under almost all cases. Finally, we explore the robustness of EC-LDA\nunder differential privacy protection and discuss the potential effective\ndefense methods to EC-LDA. Our code is available at\nhttps://github.com/cheng-t/EC-LDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have been widely used for graph analysis.\nFederated Graph Learning (FGL) is an emerging learning framework to\ncollaboratively train graph data from various clients. Although FGL allows\nclient data to remain localized, a malicious server can still steal client\nprivate data information through uploaded gradient. In this paper, we for the\nfirst time propose label distribution attacks (LDAs) on FGL that aim to infer\nthe label distributions of the client-side data. Firstly, we observe that the\neffectiveness of LDA is closely related to the variance of node embeddings in\nGNNs. Next, we analyze the relation between them and propose a new attack named\nEC-LDA, which significantly improves the attack effectiveness by compressing\nnode embeddings. Then, extensive experiments on node classification and link\nprediction tasks across six widely used graph datasets show that EC-LDA\noutperforms the SOTA LDAs. Specifically, EC-LDA can achieve the Cos-sim as high\nas 1.0 under almost all cases. Finally, we explore the robustness of EC-LDA\nunder differential privacy protection and discuss the potential effective\ndefense methods to EC-LDA. Our code is available at\nhttps://github.com/cheng-t/EC-LDA."
                },
                "authors": [
                    {
                        "name": "Tong Cheng"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Xinpeng Ling"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Zhili Chen"
                    },
                    {
                        "name": "Haifeng Qian"
                    },
                    {
                        "name": "Junqing Gong"
                    }
                ],
                "author_detail": {
                    "name": "Junqing Gong"
                },
                "author": "Junqing Gong",
                "arxiv_comment": "This paper has been accepted by 2025 IEEE International Conference on\n  Data Mining (ICDM 2025)",
                "arxiv_journal_ref": "ICDM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15140v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15140v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20062v1",
                "updated": "2025-09-24T12:31:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    31,
                    25,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T12:31:25Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    31,
                    25,
                    2,
                    267,
                    0
                ],
                "title": "\"Chat\" between extreme mass ratio inspirals and Galactic binaries in\n  LISA data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Chat\" between extreme mass ratio inspirals and Galactic binaries in\n  LISA data"
                },
                "summary": "The future space-based gravitational wave observatory, the Laser\nInterferometer Space Antenna, is expected to observe between a few and a few\nthousand extreme mass-ratio inspirals (EMRIs) per year. Due to the simultaneous\npresence of other gravitational wave signals in the data, it can be challenging\nto detect EMRIs and accurately estimate their parameters. In this work, we\ninvestigate the interaction between a gravitational signal from an EMRI and\nmillions of signals from inspiralling Galactic white dwarf binaries. We\ndemonstrate that bright Galactic binaries contaminate the detection and\ncharacterization of EMRIs. We perform Bayesian inference of EMRI parameters\nafter removing resolvable Galactic binaries and confirm an accuracy comparable\nto that expected in Gaussian noise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The future space-based gravitational wave observatory, the Laser\nInterferometer Space Antenna, is expected to observe between a few and a few\nthousand extreme mass-ratio inspirals (EMRIs) per year. Due to the simultaneous\npresence of other gravitational wave signals in the data, it can be challenging\nto detect EMRIs and accurately estimate their parameters. In this work, we\ninvestigate the interaction between a gravitational signal from an EMRI and\nmillions of signals from inspiralling Galactic white dwarf binaries. We\ndemonstrate that bright Galactic binaries contaminate the detection and\ncharacterization of EMRIs. We perform Bayesian inference of EMRI parameters\nafter removing resolvable Galactic binaries and confirm an accuracy comparable\nto that expected in Gaussian noise."
                },
                "authors": [
                    {
                        "name": "Sviatoslav Khukhlaev"
                    },
                    {
                        "name": "Stanislav Babak"
                    }
                ],
                "author_detail": {
                    "name": "Stanislav Babak"
                },
                "author": "Stanislav Babak",
                "arxiv_comment": "10 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20060v1",
                "updated": "2025-09-24T12:29:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    29,
                    31,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T12:29:31Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    29,
                    31,
                    2,
                    267,
                    0
                ],
                "title": "Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens"
                },
                "summary": "This paper introduces a discrete diffusion model (DDM) framework for\ntext-aligned speech tokenization and reconstruction. By replacing the\nauto-regressive speech decoder with a discrete diffusion counterpart, our model\nachieves significantly better reconstruction quality, stronger ASR performance,\nand faster inference. We provide a comprehensive analysis of applying DDMs to\nspeech reconstruction, examining sampler choices, inference steps, and\nrobustness to length-scale estimation errors. Furthermore, we improve the\noriginal TASTE by systematically comparing vector quantization modules, showing\nthat FSQ yields up to a 35% relative WER reduction and +0.14 UT-MOS improvement\nover RVQ for AR models, while also enhancing DDM performance. Our model\ngenerates speech in just 10 denoising steps and even supports single-step\ngeneration with only minor quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a discrete diffusion model (DDM) framework for\ntext-aligned speech tokenization and reconstruction. By replacing the\nauto-regressive speech decoder with a discrete diffusion counterpart, our model\nachieves significantly better reconstruction quality, stronger ASR performance,\nand faster inference. We provide a comprehensive analysis of applying DDMs to\nspeech reconstruction, examining sampler choices, inference steps, and\nrobustness to length-scale estimation errors. Furthermore, we improve the\noriginal TASTE by systematically comparing vector quantization modules, showing\nthat FSQ yields up to a 35% relative WER reduction and +0.14 UT-MOS improvement\nover RVQ for AR models, while also enhancing DDM performance. Our model\ngenerates speech in just 10 denoising steps and even supports single-step\ngeneration with only minor quality degradation."
                },
                "authors": [
                    {
                        "name": "Pin-Jui Ku"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Jean-Marie Lemercier"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Ante Jukić"
                    }
                ],
                "author_detail": {
                    "name": "Ante Jukić"
                },
                "author": "Ante Jukić",
                "arxiv_comment": "5 pages. submitted to ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20059v1",
                "updated": "2025-09-24T12:29:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    29,
                    16,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T12:29:16Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    29,
                    16,
                    2,
                    267,
                    0
                ],
                "title": "Joint Ex-Post Location Calibration and Radio Map Construction under\n  Biased Positioning Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Ex-Post Location Calibration and Radio Map Construction under\n  Biased Positioning Errors"
                },
                "summary": "This paper proposes a high-accuracy radio map construction method tailored\nfor environments where location information is affected by bursty errors. Radio\nmaps are an effective tool for visualizing wireless environments. Although\nextensive research has been conducted on accurate radio map construction, most\nexisting approaches assume noise-free location information during sensing. In\npractice, however, positioning errors ranging from a few to several tens of\nmeters can arise due to device-based positioning systems (e.g., GNSS). Ignoring\nsuch errors during inference can lead to significant degradation in radio map\naccuracy. This study highlights that these errors often tend to be biased when\nusing mobile devices as sensors. We introduce a novel framework that models\nthese errors together with spatial correlation in radio propagation by\nembedding them as tunable parameters in the marginal log-likelihood function.\nThis enables ex-post calibration of location uncertainty during radio map\nconstruction. Numerical results based on practical human mobility data\ndemonstrate that the proposed method can limit RMSE degradation to\napproximately 0.25-0.29 dB, compared with Gaussian process regression using\nnoise-free location data, whereas baseline methods suffer performance losses\nexceeding 1 dB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a high-accuracy radio map construction method tailored\nfor environments where location information is affected by bursty errors. Radio\nmaps are an effective tool for visualizing wireless environments. Although\nextensive research has been conducted on accurate radio map construction, most\nexisting approaches assume noise-free location information during sensing. In\npractice, however, positioning errors ranging from a few to several tens of\nmeters can arise due to device-based positioning systems (e.g., GNSS). Ignoring\nsuch errors during inference can lead to significant degradation in radio map\naccuracy. This study highlights that these errors often tend to be biased when\nusing mobile devices as sensors. We introduce a novel framework that models\nthese errors together with spatial correlation in radio propagation by\nembedding them as tunable parameters in the marginal log-likelihood function.\nThis enables ex-post calibration of location uncertainty during radio map\nconstruction. Numerical results based on practical human mobility data\ndemonstrate that the proposed method can limit RMSE degradation to\napproximately 0.25-0.29 dB, compared with Gaussian process regression using\nnoise-free location data, whereas baseline methods suffer performance losses\nexceeding 1 dB."
                },
                "authors": [
                    {
                        "name": "Koki Kanzaki"
                    },
                    {
                        "name": "Koya Sato"
                    }
                ],
                "author_detail": {
                    "name": "Koya Sato"
                },
                "arxiv_affiliation": "The University of Electro-Communications",
                "author": "Koya Sato",
                "arxiv_comment": "6 pages, 8 figures. Accepted for presentation at 2025 IEEE GLOBECOM\n  Workshops: Workshop on The Interplay of Digital Twins and Pervasive\n  Artificial Intelligence for Next-Generation IoT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.06140v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.06140v4",
                "updated": "2025-09-24T12:28:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    28,
                    52,
                    2,
                    267,
                    0
                ],
                "published": "2022-10-12T12:48:25Z",
                "published_parsed": [
                    2022,
                    10,
                    12,
                    12,
                    48,
                    25,
                    2,
                    285,
                    0
                ],
                "title": "Differentially Private Bootstrap: New Privacy Analysis and Inference\n  Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Bootstrap: New Privacy Analysis and Inference\n  Strategies"
                },
                "summary": "Differentially private (DP) mechanisms protect individual-level information\nby introducing randomness into the statistical analysis procedure. Despite the\navailability of numerous DP tools, there remains a lack of general techniques\nfor conducting statistical inference under DP. We examine a DP bootstrap\nprocedure that releases multiple private bootstrap estimates to infer the\nsampling distribution and construct confidence intervals (CIs). Our privacy\nanalysis presents new results on the privacy cost of a single DP bootstrap\nestimate, applicable to any DP mechanism, and identifies some misapplications\nof the bootstrap in the existing literature. For the composition of the DP\nbootstrap, we present a numerical method to compute the exact privacy cost of\nreleasing multiple DP bootstrap estimates, and using the Gaussian-DP (GDP)\nframework (Dong et al., 2022), we show that the release of $B$ DP bootstrap\nestimates from mechanisms satisfying $(\\mu/\\sqrt{(2-2/\\mathrm{e})B})$-GDP\nasymptotically satisfies $\\mu$-GDP as $B$ goes to infinity. Then, we perform\nprivate statistical inference by post-processing the DP bootstrap estimates. We\nprove that our point estimates are consistent, our standard CIs are\nasymptotically valid, and both enjoy optimal convergence rates. To further\nimprove the finite performance, we use deconvolution with DP bootstrap\nestimates to accurately infer the sampling distribution. We derive CIs for\ntasks such as population mean estimation, logistic regression, and quantile\nregression, and we compare them to existing methods using simulations and\nreal-world experiments on 2016 Canada Census data. Our private CIs achieve the\nnominal coverage level and offer the first approach to private inference for\nquantile regression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially private (DP) mechanisms protect individual-level information\nby introducing randomness into the statistical analysis procedure. Despite the\navailability of numerous DP tools, there remains a lack of general techniques\nfor conducting statistical inference under DP. We examine a DP bootstrap\nprocedure that releases multiple private bootstrap estimates to infer the\nsampling distribution and construct confidence intervals (CIs). Our privacy\nanalysis presents new results on the privacy cost of a single DP bootstrap\nestimate, applicable to any DP mechanism, and identifies some misapplications\nof the bootstrap in the existing literature. For the composition of the DP\nbootstrap, we present a numerical method to compute the exact privacy cost of\nreleasing multiple DP bootstrap estimates, and using the Gaussian-DP (GDP)\nframework (Dong et al., 2022), we show that the release of $B$ DP bootstrap\nestimates from mechanisms satisfying $(\\mu/\\sqrt{(2-2/\\mathrm{e})B})$-GDP\nasymptotically satisfies $\\mu$-GDP as $B$ goes to infinity. Then, we perform\nprivate statistical inference by post-processing the DP bootstrap estimates. We\nprove that our point estimates are consistent, our standard CIs are\nasymptotically valid, and both enjoy optimal convergence rates. To further\nimprove the finite performance, we use deconvolution with DP bootstrap\nestimates to accurately infer the sampling distribution. We derive CIs for\ntasks such as population mean estimation, logistic regression, and quantile\nregression, and we compare them to existing methods using simulations and\nreal-world experiments on 2016 Canada Census data. Our private CIs achieve the\nnominal coverage level and offer the first approach to private inference for\nquantile regression."
                },
                "authors": [
                    {
                        "name": "Zhanyu Wang"
                    },
                    {
                        "name": "Guang Cheng"
                    },
                    {
                        "name": "Jordan Awan"
                    }
                ],
                "author_detail": {
                    "name": "Jordan Awan"
                },
                "author": "Jordan Awan",
                "arxiv_comment": "22 pages before appendices and references. 50 pages total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.06140v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.06140v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20051v1",
                "updated": "2025-09-24T12:19:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    19,
                    18,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T12:19:18Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    19,
                    18,
                    2,
                    267,
                    0
                ],
                "title": "One Filters All: A Generalist Filter for State Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Filters All: A Generalist Filter for State Estimation"
                },
                "summary": "Estimating hidden states in dynamical systems, also known as optimal\nfiltering, is a long-standing problem in various fields of science and\nengineering. In this paper, we introduce a general filtering framework,\n\\textbf{LLM-Filter}, which leverages large language models (LLMs) for state\nestimation by embedding noisy observations with text prototypes. In various\nexperiments for classical dynamical systems, we find that first, state\nestimation can significantly benefit from the reasoning knowledge embedded in\npre-trained LLMs. By achieving proper modality alignment with the frozen LLM,\nLLM-Filter outperforms the state-of-the-art learning-based approaches. Second,\nwe carefully design the prompt structure, System-as-Prompt (SaP), incorporating\ntask instructions that enable the LLM to understand the estimation tasks.\nGuided by these prompts, LLM-Filter exhibits exceptional generalization,\ncapable of performing filtering tasks accurately in changed or even unseen\nenvironments. We further observe a scaling-law behavior in LLM-Filter, where\naccuracy improves with larger model sizes and longer training times. These\nfindings make LLM-Filter a promising foundation model of filtering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating hidden states in dynamical systems, also known as optimal\nfiltering, is a long-standing problem in various fields of science and\nengineering. In this paper, we introduce a general filtering framework,\n\\textbf{LLM-Filter}, which leverages large language models (LLMs) for state\nestimation by embedding noisy observations with text prototypes. In various\nexperiments for classical dynamical systems, we find that first, state\nestimation can significantly benefit from the reasoning knowledge embedded in\npre-trained LLMs. By achieving proper modality alignment with the frozen LLM,\nLLM-Filter outperforms the state-of-the-art learning-based approaches. Second,\nwe carefully design the prompt structure, System-as-Prompt (SaP), incorporating\ntask instructions that enable the LLM to understand the estimation tasks.\nGuided by these prompts, LLM-Filter exhibits exceptional generalization,\ncapable of performing filtering tasks accurately in changed or even unseen\nenvironments. We further observe a scaling-law behavior in LLM-Filter, where\naccuracy improves with larger model sizes and longer training times. These\nfindings make LLM-Filter a promising foundation model of filtering."
                },
                "authors": [
                    {
                        "name": "Shiqi Liu"
                    },
                    {
                        "name": "Wenhan Cao"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Shengbo Eben Li"
                    }
                ],
                "author_detail": {
                    "name": "Shengbo Eben Li"
                },
                "author": "Shengbo Eben Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20045v1",
                "updated": "2025-09-24T12:13:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    13,
                    53,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T12:13:53Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    13,
                    53,
                    2,
                    267,
                    0
                ],
                "title": "Tokenization and Representation Biases in Multilingual Models on\n  Dialectal NLP Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization and Representation Biases in Multilingual Models on\n  Dialectal NLP Tasks"
                },
                "summary": "Dialectal data are characterized by linguistic variation that appears small\nto humans but has a significant impact on the performance of models. This\ndialect gap has been related to various factors (e.g., data size, economic and\nsocial factors) whose impact, however, turns out to be inconsistent. In this\nwork, we investigate factors impacting the model performance more directly: we\ncorrelate Tokenization Parity (TP) and Information Parity (IP), as measures of\nrepresentational biases in pre-trained multilingual models, with the downstream\nperformance. We compare state-of-the-art decoder-only LLMs with encoder-based\nmodels across three tasks: dialect classification, topic classification, and\nextractive question answering, controlling for varying scripts (Latin vs.\nnon-Latin) and resource availability (high vs. low). Our analysis reveals that\nTP is a better predictor of the performance on tasks reliant on syntactic and\nmorphological cues (e.g., extractive QA), while IP better predicts performance\nin semantic tasks (e.g., topic classification). Complementary analyses,\nincluding tokenizer behavior, vocabulary coverage, and qualitative insights,\nreveal that the language support claims of LLMs often might mask deeper\nmismatches at the script or token level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialectal data are characterized by linguistic variation that appears small\nto humans but has a significant impact on the performance of models. This\ndialect gap has been related to various factors (e.g., data size, economic and\nsocial factors) whose impact, however, turns out to be inconsistent. In this\nwork, we investigate factors impacting the model performance more directly: we\ncorrelate Tokenization Parity (TP) and Information Parity (IP), as measures of\nrepresentational biases in pre-trained multilingual models, with the downstream\nperformance. We compare state-of-the-art decoder-only LLMs with encoder-based\nmodels across three tasks: dialect classification, topic classification, and\nextractive question answering, controlling for varying scripts (Latin vs.\nnon-Latin) and resource availability (high vs. low). Our analysis reveals that\nTP is a better predictor of the performance on tasks reliant on syntactic and\nmorphological cues (e.g., extractive QA), while IP better predicts performance\nin semantic tasks (e.g., topic classification). Complementary analyses,\nincluding tokenizer behavior, vocabulary coverage, and qualitative insights,\nreveal that the language support claims of LLMs often might mask deeper\nmismatches at the script or token level."
                },
                "authors": [
                    {
                        "name": "Vani Kanjirangat"
                    },
                    {
                        "name": "Tanja Samardžić"
                    },
                    {
                        "name": "Ljiljana Dolamic"
                    },
                    {
                        "name": "Fabio Rinaldi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Rinaldi"
                },
                "author": "Fabio Rinaldi",
                "arxiv_comment": "Accepted in EMNLP-2025 Main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10697v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10697v3",
                "updated": "2025-09-24T12:12:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    12,
                    51,
                    2,
                    267,
                    0
                ],
                "published": "2024-11-16T04:35:17Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    4,
                    35,
                    17,
                    5,
                    321,
                    0
                ],
                "title": "Language Model Evolutionary Algorithms for Recommender Systems:\n  Benchmarks and Algorithm Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model Evolutionary Algorithms for Recommender Systems:\n  Benchmarks and Algorithm Comparisons"
                },
                "summary": "In the evolutionary computing community, the remarkable language-handling\ncapabilities and reasoning power of large language models (LLMs) have\nsignificantly enhanced the functionality of evolutionary algorithms (EAs),\nenabling them to tackle optimization problems involving structured language or\nprogram code. Although this field is still in its early stages, its impressive\npotential has led to the development of various LLM-based EAs. To effectively\nevaluate the performance and practical applicability of these LLM-based EAs,\nbenchmarks with real-world relevance are essential. In this paper, we focus on\nLLM-based recommender systems (RSs) and introduce a benchmark problem set,\nnamed RSBench, specifically designed to assess the performance of LLM-based EAs\nin recommendation prompt optimization. RSBench emphasizes session-based\nrecommendations, aiming to discover a set of Pareto optimal prompts that guide\nthe recommendation process, providing accurate, diverse, and fair\nrecommendations. We develop three LLM-based EAs based on established EA\nframeworks and experimentally evaluate their performance using RSBench. Our\nstudy offers valuable insights into the application of EAs in LLM-based RSs.\nAdditionally, we explore key components that may influence the overall\nperformance of the RS, providing meaningful guidance for future research on the\ndevelopment of LLM-based EAs in RSs. The source code of the proposed RSBench\ncan be found at https://github.com/LiuJ-2023/RSBench/tree/main.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolutionary computing community, the remarkable language-handling\ncapabilities and reasoning power of large language models (LLMs) have\nsignificantly enhanced the functionality of evolutionary algorithms (EAs),\nenabling them to tackle optimization problems involving structured language or\nprogram code. Although this field is still in its early stages, its impressive\npotential has led to the development of various LLM-based EAs. To effectively\nevaluate the performance and practical applicability of these LLM-based EAs,\nbenchmarks with real-world relevance are essential. In this paper, we focus on\nLLM-based recommender systems (RSs) and introduce a benchmark problem set,\nnamed RSBench, specifically designed to assess the performance of LLM-based EAs\nin recommendation prompt optimization. RSBench emphasizes session-based\nrecommendations, aiming to discover a set of Pareto optimal prompts that guide\nthe recommendation process, providing accurate, diverse, and fair\nrecommendations. We develop three LLM-based EAs based on established EA\nframeworks and experimentally evaluate their performance using RSBench. Our\nstudy offers valuable insights into the application of EAs in LLM-based RSs.\nAdditionally, we explore key components that may influence the overall\nperformance of the RS, providing meaningful guidance for future research on the\ndevelopment of LLM-based EAs in RSs. The source code of the proposed RSBench\ncan be found at https://github.com/LiuJ-2023/RSBench/tree/main."
                },
                "authors": [
                    {
                        "name": "Jiao Liu"
                    },
                    {
                        "name": "Zhu Sun"
                    },
                    {
                        "name": "Shanshan Feng"
                    },
                    {
                        "name": "Caishun Chen"
                    },
                    {
                        "name": "Yew-Soon Ong"
                    }
                ],
                "author_detail": {
                    "name": "Yew-Soon Ong"
                },
                "author": "Yew-Soon Ong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10697v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10697v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08590v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08590v3",
                "updated": "2025-09-24T12:06:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    6,
                    14,
                    2,
                    267,
                    0
                ],
                "published": "2025-04-11T14:49:33Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    14,
                    49,
                    33,
                    4,
                    101,
                    0
                ],
                "title": "Playpen: An Environment for Exploring Learning Through Conversational\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Playpen: An Environment for Exploring Learning Through Conversational\n  Interaction"
                },
                "summary": "Interaction between learner and feedback-giver has come into focus recently\nfor post-training of Large Language Models (LLMs), through the use of reward\nmodels that judge the appropriateness of a model's response. In this paper, we\ninvestigate whether Dialogue Games -- goal-directed and rule-governed\nactivities driven predominantly by verbal actions -- can also serve as a source\nof feedback signals for learning. We introduce Playpen, an environment for off-\nand online learning through Dialogue Game self-play, and investigate a\nrepresentative set of post-training methods: supervised fine-tuning; direct\nalignment (DPO); and reinforcement learning with GRPO. We experiment with\npost-training a small LLM (Llama-3.1-8B-Instruct), evaluating performance on\nunseen instances of training games as well as unseen games, and on standard\nbenchmarks. We find that imitation learning through SFT improves performance on\nunseen instances, but negatively impacts other skills, while interactive\nlearning with GRPO shows balanced improvements without loss of skills. We\nrelease the framework and the baseline training setups to foster research in\nthe promising new direction of learning in (synthetic) interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interaction between learner and feedback-giver has come into focus recently\nfor post-training of Large Language Models (LLMs), through the use of reward\nmodels that judge the appropriateness of a model's response. In this paper, we\ninvestigate whether Dialogue Games -- goal-directed and rule-governed\nactivities driven predominantly by verbal actions -- can also serve as a source\nof feedback signals for learning. We introduce Playpen, an environment for off-\nand online learning through Dialogue Game self-play, and investigate a\nrepresentative set of post-training methods: supervised fine-tuning; direct\nalignment (DPO); and reinforcement learning with GRPO. We experiment with\npost-training a small LLM (Llama-3.1-8B-Instruct), evaluating performance on\nunseen instances of training games as well as unseen games, and on standard\nbenchmarks. We find that imitation learning through SFT improves performance on\nunseen instances, but negatively impacts other skills, while interactive\nlearning with GRPO shows balanced improvements without loss of skills. We\nrelease the framework and the baseline training setups to foster research in\nthe promising new direction of learning in (synthetic) interaction."
                },
                "authors": [
                    {
                        "name": "Nicola Horst"
                    },
                    {
                        "name": "Davide Mazzaccara"
                    },
                    {
                        "name": "Antonia Schmidt"
                    },
                    {
                        "name": "Michael Sullivan"
                    },
                    {
                        "name": "Filippo Momentè"
                    },
                    {
                        "name": "Luca Franceschetti"
                    },
                    {
                        "name": "Philipp Sadler"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "Alberto Testoni"
                    },
                    {
                        "name": "Raffaella Bernardi"
                    },
                    {
                        "name": "Raquel Fernández"
                    },
                    {
                        "name": "Alexander Koller"
                    },
                    {
                        "name": "Oliver Lemon"
                    },
                    {
                        "name": "David Schlangen"
                    },
                    {
                        "name": "Mario Giulianelli"
                    },
                    {
                        "name": "Alessandro Suglia"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Suglia"
                },
                "author": "Alessandro Suglia",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main) Source code:\n  https://github.com/lm-playpen/playpen Please send correspodence to:\n  lm-playschool@googlegroups.com",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08590v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08590v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14359v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14359v4",
                "updated": "2025-09-24T12:04:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    4,
                    54,
                    2,
                    267,
                    0
                ],
                "published": "2025-02-20T08:36:58Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    36,
                    58,
                    3,
                    51,
                    0
                ],
                "title": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive\n  Tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive\n  Tests"
                },
                "summary": "We examine three evaluation paradigms: standard benchmarks (e.g., MMLU and\nBBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests\n(e.g., for working memory or theory of mind). First, we investigate which of\nthe former two-benchmarks or games-is most effective at discriminating LLMs of\nvarying quality. Then, inspired by human cognitive assessments, we compile a\nsuite of targeted tests that measure cognitive abilities deemed essential for\neffective language use, and we investigate their correlation with model\nperformance in benchmarks and games. Our analyses reveal that interactive games\nare superior to standard benchmarks in discriminating models. Causal and\nlogical reasoning correlate with both static and interactive tests, while\ndifferences emerge regarding core executive functions and social/emotional\nskills, which correlate more with games. We advocate for the development of new\ninteractive benchmarks and targeted cognitive tasks inspired by assessing human\nabilities but designed specifically for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine three evaluation paradigms: standard benchmarks (e.g., MMLU and\nBBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests\n(e.g., for working memory or theory of mind). First, we investigate which of\nthe former two-benchmarks or games-is most effective at discriminating LLMs of\nvarying quality. Then, inspired by human cognitive assessments, we compile a\nsuite of targeted tests that measure cognitive abilities deemed essential for\neffective language use, and we investigate their correlation with model\nperformance in benchmarks and games. Our analyses reveal that interactive games\nare superior to standard benchmarks in discriminating models. Causal and\nlogical reasoning correlate with both static and interactive tests, while\ndifferences emerge regarding core executive functions and social/emotional\nskills, which correlate more with games. We advocate for the development of new\ninteractive benchmarks and targeted cognitive tasks inspired by assessing human\nabilities but designed specifically for LLMs."
                },
                "authors": [
                    {
                        "name": "Filippo Momentè"
                    },
                    {
                        "name": "Alessandro Suglia"
                    },
                    {
                        "name": "Mario Giulianelli"
                    },
                    {
                        "name": "Ambra Ferrari"
                    },
                    {
                        "name": "Alexander Koller"
                    },
                    {
                        "name": "Oliver Lemon"
                    },
                    {
                        "name": "David Schlangen"
                    },
                    {
                        "name": "Raquel Fernández"
                    },
                    {
                        "name": "Raffaella Bernardi"
                    }
                ],
                "author_detail": {
                    "name": "Raffaella Bernardi"
                },
                "author": "Raffaella Bernardi",
                "arxiv_comment": "Accepted at EMNLP 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14359v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14359v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23368v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23368v3",
                "updated": "2025-09-24T12:01:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    1,
                    2,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-29T11:47:18Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    11,
                    47,
                    18,
                    3,
                    149,
                    0
                ],
                "title": "Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain\n  Human Label Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain\n  Human Label Variation"
                },
                "summary": "The recent rise of reasoning-tuned Large Language Models (LLMs)--which\ngenerate chains of thought (CoTs) before giving the final answer--has attracted\nsignificant attention and offers new opportunities for gaining insights into\nhuman label variation, which refers to plausible differences in how multiple\nannotators label the same data instance. Prior work has shown that\nLLM-generated explanations can help align model predictions with human label\ndistributions, but typically adopt a reverse paradigm: producing explanations\nbased on given answers. In contrast, CoTs provide a forward reasoning path that\nmay implicitly embed rationales for each answer option, before generating the\nanswers. We thus propose a novel LLM-based pipeline enriched with\nlinguistically-grounded discourse segmenters to extract supporting and opposing\nstatements for each answer option from CoTs with improved accuracy. We also\npropose a rank-based HLV evaluation framework that prioritizes the ranking of\nanswers over exact scores, which instead favor direct comparison of label\ndistributions. Our method outperforms a direct generation method as well as\nbaselines on three datasets, and shows better alignment of ranking methods with\nhumans, highlighting the effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent rise of reasoning-tuned Large Language Models (LLMs)--which\ngenerate chains of thought (CoTs) before giving the final answer--has attracted\nsignificant attention and offers new opportunities for gaining insights into\nhuman label variation, which refers to plausible differences in how multiple\nannotators label the same data instance. Prior work has shown that\nLLM-generated explanations can help align model predictions with human label\ndistributions, but typically adopt a reverse paradigm: producing explanations\nbased on given answers. In contrast, CoTs provide a forward reasoning path that\nmay implicitly embed rationales for each answer option, before generating the\nanswers. We thus propose a novel LLM-based pipeline enriched with\nlinguistically-grounded discourse segmenters to extract supporting and opposing\nstatements for each answer option from CoTs with improved accuracy. We also\npropose a rank-based HLV evaluation framework that prioritizes the ranking of\nanswers over exact scores, which instead favor direct comparison of label\ndistributions. Our method outperforms a direct generation method as well as\nbaselines on three datasets, and shows better alignment of ranking methods with\nhumans, highlighting the effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Beiduo Chen"
                    },
                    {
                        "name": "Yang Janet Liu"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "Accepted by EMNLP 2025 Main (Oral), 25 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23368v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23368v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01143v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01143v3",
                "updated": "2025-09-24T11:57:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    57,
                    38,
                    2,
                    267,
                    0
                ],
                "published": "2025-03-03T03:49:38Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    3,
                    49,
                    38,
                    0,
                    62,
                    0
                ],
                "title": "Diffusion Classifier-Driven Reward for Offline Preference-based\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Classifier-Driven Reward for Offline Preference-based\n  Reinforcement Learning"
                },
                "summary": "Offline preference-based reinforcement learning (PbRL) mitigates the need for\nreward definition, aligning with human preferences via preference-driven reward\nfeedback without interacting with the environment. However, trajectory-wise\npreference labels are difficult to meet the precise learning of step-wise\nreward, thereby affecting the performance of downstream algorithms. To\nalleviate the insufficient step-wise reward caused by trajectory-wise\npreferences, we propose a novel preference-based reward acquisition method:\nDiffusion Preference-based Reward (DPR). DPR directly treats step-wise\npreference-based reward acquisition as a binary classification and utilizes the\nrobustness of diffusion classifiers to infer step-wise rewards\ndiscriminatively. In addition, to further utilize trajectory-wise preference\ninformation, we propose Conditional Diffusion Preference-based Reward (C-DPR),\nwhich conditions on trajectory-wise preference labels to enhance reward\ninference. We apply the above methods to existing offline RL algorithms, and a\nseries of experimental results demonstrate that the diffusion classifier-driven\nreward outperforms the previous reward acquisition method with the\nBradley-Terry model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline preference-based reinforcement learning (PbRL) mitigates the need for\nreward definition, aligning with human preferences via preference-driven reward\nfeedback without interacting with the environment. However, trajectory-wise\npreference labels are difficult to meet the precise learning of step-wise\nreward, thereby affecting the performance of downstream algorithms. To\nalleviate the insufficient step-wise reward caused by trajectory-wise\npreferences, we propose a novel preference-based reward acquisition method:\nDiffusion Preference-based Reward (DPR). DPR directly treats step-wise\npreference-based reward acquisition as a binary classification and utilizes the\nrobustness of diffusion classifiers to infer step-wise rewards\ndiscriminatively. In addition, to further utilize trajectory-wise preference\ninformation, we propose Conditional Diffusion Preference-based Reward (C-DPR),\nwhich conditions on trajectory-wise preference labels to enhance reward\ninference. We apply the above methods to existing offline RL algorithms, and a\nseries of experimental results demonstrate that the diffusion classifier-driven\nreward outperforms the previous reward acquisition method with the\nBradley-Terry model."
                },
                "authors": [
                    {
                        "name": "Teng Pang"
                    },
                    {
                        "name": "Bingzheng Wang"
                    },
                    {
                        "name": "Guoqiang Wu"
                    },
                    {
                        "name": "Yilong Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yilong Yin"
                },
                "author": "Yilong Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01143v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01143v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22729v2",
                "updated": "2025-09-24T11:55:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    55,
                    48,
                    2,
                    267,
                    0
                ],
                "published": "2025-07-30T14:49:30Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    49,
                    30,
                    2,
                    211,
                    0
                ],
                "title": "Resource-Efficient Adaptation of Large Language Models for Text\n  Embeddings via Prompt Engineering and Contrastive Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Adaptation of Large Language Models for Text\n  Embeddings via Prompt Engineering and Contrastive Fine-tuning"
                },
                "summary": "Large Language Models (LLMs) have become a cornerstone in Natural Language\nProcessing (NLP), achieving impressive performance in text generation. Their\ntoken-level representations capture rich, human-aligned semantics. However,\npooling these vectors into a text embedding discards crucial information.\nNevertheless, many non-generative downstream tasks, such as clustering,\nclassification, or retrieval, still depend on accurate and controllable\nsentence- or document-level embeddings. We explore several adaptation\nstrategies for pre-trained, decoder-only LLMs: (i) various aggregation\ntechniques for token embeddings, (ii) task-specific prompt engineering, and\n(iii) text-level augmentation via contrastive fine-tuning. Combining these\ncomponents yields competitive performance on the English clustering track of\nthe Massive Text Embedding Benchmark (MTEB). An analysis of the attention map\nfurther shows that fine-tuning shifts focus from prompt tokens to semantically\nrelevant words, indicating more effective compression of meaning into the final\nhidden state. Our experiments demonstrate that LLMs can be effectively adapted\nas text embedding models through a combination of prompt engineering and\nresource-efficient contrastive fine-tuning on synthetically generated positive\npairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become a cornerstone in Natural Language\nProcessing (NLP), achieving impressive performance in text generation. Their\ntoken-level representations capture rich, human-aligned semantics. However,\npooling these vectors into a text embedding discards crucial information.\nNevertheless, many non-generative downstream tasks, such as clustering,\nclassification, or retrieval, still depend on accurate and controllable\nsentence- or document-level embeddings. We explore several adaptation\nstrategies for pre-trained, decoder-only LLMs: (i) various aggregation\ntechniques for token embeddings, (ii) task-specific prompt engineering, and\n(iii) text-level augmentation via contrastive fine-tuning. Combining these\ncomponents yields competitive performance on the English clustering track of\nthe Massive Text Embedding Benchmark (MTEB). An analysis of the attention map\nfurther shows that fine-tuning shifts focus from prompt tokens to semantically\nrelevant words, indicating more effective compression of meaning into the final\nhidden state. Our experiments demonstrate that LLMs can be effectively adapted\nas text embedding models through a combination of prompt engineering and\nresource-efficient contrastive fine-tuning on synthetically generated positive\npairs."
                },
                "authors": [
                    {
                        "name": "Benedikt Roth"
                    },
                    {
                        "name": "Stephan Rappensperger"
                    },
                    {
                        "name": "Tianming Qiu"
                    },
                    {
                        "name": "Hamza Imamović"
                    },
                    {
                        "name": "Julian Wörmann"
                    },
                    {
                        "name": "Hao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Shen"
                },
                "author": "Hao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.20362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20362v1",
                "updated": "2025-09-24T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    59,
                    54,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:59:54Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    59,
                    54,
                    2,
                    267,
                    0
                ],
                "title": "FlyTrap: Physical Distance-Pulling Attack Towards Camera-based\n  Autonomous Target Tracking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlyTrap: Physical Distance-Pulling Attack Towards Camera-based\n  Autonomous Target Tracking Systems"
                },
                "summary": "Autonomous Target Tracking (ATT) systems, especially ATT drones, are widely\nused in applications such as surveillance, border control, and law enforcement,\nwhile also being misused in stalking and destructive actions. Thus, the\nsecurity of ATT is highly critical for real-world applications. Under the\nscope, we present a new type of attack: distance-pulling attacks (DPA) and a\nsystematic study of it, which exploits vulnerabilities in ATT systems to\ndangerously reduce tracking distances, leading to drone capturing, increased\nsusceptibility to sensor attacks, or even physical collisions. To achieve these\ngoals, we present FlyTrap, a novel physical-world attack framework that employs\nan adversarial umbrella as a deployable and domain-specific attack vector.\nFlyTrap is specifically designed to meet key desired objectives in attacking\nATT drones: physical deployability, closed-loop effectiveness, and\nspatial-temporal consistency. Through novel progressive distance-pulling\nstrategy and controllable spatial-temporal consistency designs, FlyTrap\nmanipulates ATT drones in real-world setups to achieve significant system-level\nimpacts. Our evaluations include new datasets, metrics, and closed-loop\nexperiments on real-world white-box and even commercial ATT drones, including\nDJI and HoverAir. Results demonstrate FlyTrap's ability to reduce tracking\ndistances within the range to be captured, sensor attacked, or even directly\ncrashed, highlighting urgent security risks and practical implications for the\nsafe deployment of ATT systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Target Tracking (ATT) systems, especially ATT drones, are widely\nused in applications such as surveillance, border control, and law enforcement,\nwhile also being misused in stalking and destructive actions. Thus, the\nsecurity of ATT is highly critical for real-world applications. Under the\nscope, we present a new type of attack: distance-pulling attacks (DPA) and a\nsystematic study of it, which exploits vulnerabilities in ATT systems to\ndangerously reduce tracking distances, leading to drone capturing, increased\nsusceptibility to sensor attacks, or even physical collisions. To achieve these\ngoals, we present FlyTrap, a novel physical-world attack framework that employs\nan adversarial umbrella as a deployable and domain-specific attack vector.\nFlyTrap is specifically designed to meet key desired objectives in attacking\nATT drones: physical deployability, closed-loop effectiveness, and\nspatial-temporal consistency. Through novel progressive distance-pulling\nstrategy and controllable spatial-temporal consistency designs, FlyTrap\nmanipulates ATT drones in real-world setups to achieve significant system-level\nimpacts. Our evaluations include new datasets, metrics, and closed-loop\nexperiments on real-world white-box and even commercial ATT drones, including\nDJI and HoverAir. Results demonstrate FlyTrap's ability to reduce tracking\ndistances within the range to be captured, sensor attacked, or even directly\ncrashed, highlighting urgent security risks and practical implications for the\nsafe deployment of ATT systems."
                },
                "authors": [
                    {
                        "name": "Shaoyuan Xie"
                    },
                    {
                        "name": "Mohamad Habib Fakih"
                    },
                    {
                        "name": "Junchi Lu"
                    },
                    {
                        "name": "Fayzah Alshammari"
                    },
                    {
                        "name": "Ningfei Wang"
                    },
                    {
                        "name": "Takami Sato"
                    },
                    {
                        "name": "Halima Bouzidi"
                    },
                    {
                        "name": "Mohammad Abdullah Al Faruque"
                    },
                    {
                        "name": "Qi Alfred Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qi Alfred Chen"
                },
                "author": "Qi Alfred Chen",
                "arxiv_comment": "An extended version of the paper accepted by NDSS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20349v1",
                "updated": "2025-09-24T17:42:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    42,
                    0,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:42:00Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    42,
                    0,
                    2,
                    267,
                    0
                ],
                "title": "Process-Informed Forecasting of Complex Thermal Dynamics in\n  Pharmaceutical Manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process-Informed Forecasting of Complex Thermal Dynamics in\n  Pharmaceutical Manufacturing"
                },
                "summary": "Accurate time-series forecasting for complex physical systems is the backbone\nof modern industrial monitoring and control. While deep learning models excel\nat capturing complex dynamics, currently, their deployment is limited due to\nphysical inconsistency and robustness, hence constraining their reliability in\nregulated environments. We introduce process-informed forecasting (PIF) models\nfor temperature in pharmaceutical lyophilization. We investigate a wide range\nof models, from classical ones such as Autoregressive Integrated Moving Average\nModel (ARIMA) and Exponential Smoothing Model (ETS), to modern deep learning\narchitectures, including Kolmogorov-Arnold Networks (KANs). We compare three\ndifferent loss function formulations that integrate a process-informed\ntrajectory prior: a fixed-weight loss, a dynamic uncertainty-based loss, and a\nResidual-Based Attention (RBA) mechanism. We evaluate all models not only for\naccuracy and physical consistency but also for robustness to sensor noise.\nFurthermore, we test the practical generalizability of the best model in a\ntransfer learning scenario on a new process. Our results show that PIF models\noutperform their data-driven counterparts in terms of accuracy, physical\nplausibility and noise resilience. This work provides a roadmap for developing\nreliable and generalizable forecasting solutions for critical applications in\nthe pharmaceutical manufacturing landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate time-series forecasting for complex physical systems is the backbone\nof modern industrial monitoring and control. While deep learning models excel\nat capturing complex dynamics, currently, their deployment is limited due to\nphysical inconsistency and robustness, hence constraining their reliability in\nregulated environments. We introduce process-informed forecasting (PIF) models\nfor temperature in pharmaceutical lyophilization. We investigate a wide range\nof models, from classical ones such as Autoregressive Integrated Moving Average\nModel (ARIMA) and Exponential Smoothing Model (ETS), to modern deep learning\narchitectures, including Kolmogorov-Arnold Networks (KANs). We compare three\ndifferent loss function formulations that integrate a process-informed\ntrajectory prior: a fixed-weight loss, a dynamic uncertainty-based loss, and a\nResidual-Based Attention (RBA) mechanism. We evaluate all models not only for\naccuracy and physical consistency but also for robustness to sensor noise.\nFurthermore, we test the practical generalizability of the best model in a\ntransfer learning scenario on a new process. Our results show that PIF models\noutperform their data-driven counterparts in terms of accuracy, physical\nplausibility and noise resilience. This work provides a roadmap for developing\nreliable and generalizable forecasting solutions for critical applications in\nthe pharmaceutical manufacturing landscape."
                },
                "authors": [
                    {
                        "name": "Ramona Rubini"
                    },
                    {
                        "name": "Siavash Khodakarami"
                    },
                    {
                        "name": "Aniruddha Bora"
                    },
                    {
                        "name": "George Em Karniadakis"
                    },
                    {
                        "name": "Michele Dassisti"
                    }
                ],
                "author_detail": {
                    "name": "Michele Dassisti"
                },
                "author": "Michele Dassisti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15074v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15074v3",
                "updated": "2025-09-24T17:25:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    25,
                    12,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-21T03:43:29Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    3,
                    43,
                    29,
                    2,
                    141,
                    0
                ],
                "title": "DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware\n  Reinforcement Learning on Imbalanced Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware\n  Reinforcement Learning on Imbalanced Data"
                },
                "summary": "Large Language Models (LLMs) are increasingly aligned with human preferences\nthrough Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,\nGroup Relative Policy Optimization (GRPO) has gained attention for its\nsimplicity and strong performance, notably eliminating the need for a learned\nvalue function. However, GRPO implicitly assumes a balanced domain distribution\nand uniform semantic alignment across groups, assumptions that rarely hold in\nreal-world datasets. When applied to multi-domain, imbalanced data, GRPO\ndisproportionately optimizes for dominant domains, neglecting underrepresented\nones and resulting in poor generalization and fairness. We propose\nDomain-Informed Self-Consistency Policy Optimization (DISCO), a principled\nextension to GRPO that addresses inter-group imbalance with two key\ninnovations. Domain-aware reward scaling counteracts frequency bias by\nreweighting optimization based on domain prevalence. Difficulty-aware reward\nscaling leverages prompt-level self-consistency to identify and prioritize\nuncertain prompts that offer greater learning value. Together, these strategies\npromote more equitable and effective policy learning across domains. Extensive\nexperiments across multiple LLMs and skewed training distributions show that\nDISCO improves generalization, outperforms existing GRPO variants by 5% on\nQwen3 models, and sets new state-of-the-art results on multi-domain alignment\nbenchmarks. Our code and data are available at\nhttps://github.com/Tonyzhou98/disco_grpo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly aligned with human preferences\nthrough Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,\nGroup Relative Policy Optimization (GRPO) has gained attention for its\nsimplicity and strong performance, notably eliminating the need for a learned\nvalue function. However, GRPO implicitly assumes a balanced domain distribution\nand uniform semantic alignment across groups, assumptions that rarely hold in\nreal-world datasets. When applied to multi-domain, imbalanced data, GRPO\ndisproportionately optimizes for dominant domains, neglecting underrepresented\nones and resulting in poor generalization and fairness. We propose\nDomain-Informed Self-Consistency Policy Optimization (DISCO), a principled\nextension to GRPO that addresses inter-group imbalance with two key\ninnovations. Domain-aware reward scaling counteracts frequency bias by\nreweighting optimization based on domain prevalence. Difficulty-aware reward\nscaling leverages prompt-level self-consistency to identify and prioritize\nuncertain prompts that offer greater learning value. Together, these strategies\npromote more equitable and effective policy learning across domains. Extensive\nexperiments across multiple LLMs and skewed training distributions show that\nDISCO improves generalization, outperforms existing GRPO variants by 5% on\nQwen3 models, and sets new state-of-the-art results on multi-domain alignment\nbenchmarks. Our code and data are available at\nhttps://github.com/Tonyzhou98/disco_grpo."
                },
                "authors": [
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Jing Zhu"
                    },
                    {
                        "name": "Shengyi Qian"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Xiyao Wang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Paiheng Xu"
                    },
                    {
                        "name": "Wei Ai"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang",
                "arxiv_comment": "Accepted by EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15074v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15074v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20336v1",
                "updated": "2025-09-24T17:25:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    25,
                    5,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:25:05Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    25,
                    5,
                    2,
                    267,
                    0
                ],
                "title": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit\n  Tracing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit\n  Tracing"
                },
                "summary": "Transformer-based LLMs demonstrate strong performance on graph reasoning\ntasks, yet their internal mechanisms remain underexplored. To uncover these\nreasoning process mechanisms in a fundamental and unified view, we set the\nbasic decoder-only transformers and explain them using the circuit-tracer\nframework. Through this lens, we visualize reasoning traces and identify two\ncore mechanisms in graph reasoning: token merging and structural memorization,\nwhich underlie both path reasoning and substructure extraction tasks. We\nfurther quantify these behaviors and analyze how they are influenced by graph\ndensity and model size. Our study provides a unified interpretability framework\nfor understanding structural reasoning in decoder-only Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs demonstrate strong performance on graph reasoning\ntasks, yet their internal mechanisms remain underexplored. To uncover these\nreasoning process mechanisms in a fundamental and unified view, we set the\nbasic decoder-only transformers and explain them using the circuit-tracer\nframework. Through this lens, we visualize reasoning traces and identify two\ncore mechanisms in graph reasoning: token merging and structural memorization,\nwhich underlie both path reasoning and substructure extraction tasks. We\nfurther quantify these behaviors and analyze how they are influenced by graph\ndensity and model size. Our study provides a unified interpretability framework\nfor understanding structural reasoning in decoder-only Transformers."
                },
                "authors": [
                    {
                        "name": "Xinnan Dai"
                    },
                    {
                        "name": "Chung-Hsiang Lo"
                    },
                    {
                        "name": "Kai Guo"
                    },
                    {
                        "name": "Shenglai Zeng"
                    },
                    {
                        "name": "Dongsheng Luo"
                    },
                    {
                        "name": "Jiliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Tang"
                },
                "author": "Jiliang Tang",
                "arxiv_comment": "Accepted by the Workshop on Efficient Reasoning, Neurips 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02876v2",
                "updated": "2025-09-24T17:23:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    23,
                    48,
                    2,
                    267,
                    0
                ],
                "published": "2025-04-02T00:19:05Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    0,
                    19,
                    5,
                    2,
                    92,
                    0
                ],
                "title": "Multimodal Reference Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Reference Visual Grounding"
                },
                "summary": "Visual grounding focuses on detecting objects from images based on language\nexpressions. Recent Large Vision-Language Models (LVLMs) have significantly\nadvanced visual grounding performance by training large models with large-scale\ndatasets. However, the problem remains challenging, especially when similar\nobjects appear in the input image. For example, an LVLM may not be able to\ndifferentiate Diet Coke and regular Coke in an image. In this case, if\nadditional reference images of Diet Coke and regular Coke are available, it can\nhelp the visual grounding of similar objects.\n  In this work, we introduce a new task named Multimodal Reference Visual\nGrounding (MRVG). In this task, a model has access to a set of reference images\nof objects in a database. Based on these reference images and a language\nexpression, the model is required to detect a target object from a query image.\nWe first introduce a new dataset to study the MRVG problem. Then we introduce a\nnovel method, named MRVG-Net, to solve this visual grounding problem. We show\nthat by efficiently using reference images with few-shot object detection and\nusing Large Language Models (LLMs) for object matching, our method achieves\nsuperior visual grounding performance compared to the state-of-the-art LVLMs\nsuch as Qwen2.5-VL-72B. Our approach bridges the gap between few-shot detection\nand visual grounding, unlocking new capabilities for visual understanding,\nwhich has wide applications in robotics. Project page with our video, code, and\ndataset: https://irvlutd.github.io/MultiGrounding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding focuses on detecting objects from images based on language\nexpressions. Recent Large Vision-Language Models (LVLMs) have significantly\nadvanced visual grounding performance by training large models with large-scale\ndatasets. However, the problem remains challenging, especially when similar\nobjects appear in the input image. For example, an LVLM may not be able to\ndifferentiate Diet Coke and regular Coke in an image. In this case, if\nadditional reference images of Diet Coke and regular Coke are available, it can\nhelp the visual grounding of similar objects.\n  In this work, we introduce a new task named Multimodal Reference Visual\nGrounding (MRVG). In this task, a model has access to a set of reference images\nof objects in a database. Based on these reference images and a language\nexpression, the model is required to detect a target object from a query image.\nWe first introduce a new dataset to study the MRVG problem. Then we introduce a\nnovel method, named MRVG-Net, to solve this visual grounding problem. We show\nthat by efficiently using reference images with few-shot object detection and\nusing Large Language Models (LLMs) for object matching, our method achieves\nsuperior visual grounding performance compared to the state-of-the-art LVLMs\nsuch as Qwen2.5-VL-72B. Our approach bridges the gap between few-shot detection\nand visual grounding, unlocking new capabilities for visual understanding,\nwhich has wide applications in robotics. Project page with our video, code, and\ndataset: https://irvlutd.github.io/MultiGrounding"
                },
                "authors": [
                    {
                        "name": "Yangxiao Lu"
                    },
                    {
                        "name": "Ruosen Li"
                    },
                    {
                        "name": "Liqiang Jing"
                    },
                    {
                        "name": "Jikai Wang"
                    },
                    {
                        "name": "Xinya Du"
                    },
                    {
                        "name": "Yunhui Guo"
                    },
                    {
                        "name": "Nicholas Ruozzi"
                    },
                    {
                        "name": "Yu Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Xiang"
                },
                "author": "Yu Xiang",
                "arxiv_comment": "Project page with our code and dataset:\n  https://irvlutd.github.io/MultiGrounding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20328v1",
                "updated": "2025-09-24T17:17:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    17,
                    27,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:17:27Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    17,
                    27,
                    2,
                    267,
                    0
                ],
                "title": "Video models are zero-shot learners and reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video models are zero-shot learners and reasoners"
                },
                "summary": "The remarkable zero-shot capabilities of Large Language Models (LLMs) have\npropelled natural language processing from task-specific models to unified,\ngeneralist foundation models. This transformation emerged from simple\nprimitives: large, generative models trained on web-scale data. Curiously, the\nsame primitives apply to today's generative video models. Could video models be\non a trajectory towards general-purpose vision understanding, much like LLMs\ndeveloped general-purpose language understanding? We demonstrate that Veo 3 can\nsolve a broad variety of tasks it wasn't explicitly trained for: segmenting\nobjects, detecting edges, editing images, understanding physical properties,\nrecognizing object affordances, simulating tool use, and more. These abilities\nto perceive, model, and manipulate the visual world enable early forms of\nvisual reasoning like maze and symmetry solving. Veo's emergent zero-shot\ncapabilities indicate that video models are on a path to becoming unified,\ngeneralist vision foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable zero-shot capabilities of Large Language Models (LLMs) have\npropelled natural language processing from task-specific models to unified,\ngeneralist foundation models. This transformation emerged from simple\nprimitives: large, generative models trained on web-scale data. Curiously, the\nsame primitives apply to today's generative video models. Could video models be\non a trajectory towards general-purpose vision understanding, much like LLMs\ndeveloped general-purpose language understanding? We demonstrate that Veo 3 can\nsolve a broad variety of tasks it wasn't explicitly trained for: segmenting\nobjects, detecting edges, editing images, understanding physical properties,\nrecognizing object affordances, simulating tool use, and more. These abilities\nto perceive, model, and manipulate the visual world enable early forms of\nvisual reasoning like maze and symmetry solving. Veo's emergent zero-shot\ncapabilities indicate that video models are on a path to becoming unified,\ngeneralist vision foundation models."
                },
                "authors": [
                    {
                        "name": "Thaddäus Wiedemer"
                    },
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Paul Vicol"
                    },
                    {
                        "name": "Shixiang Shane Gu"
                    },
                    {
                        "name": "Nick Matarese"
                    },
                    {
                        "name": "Kevin Swersky"
                    },
                    {
                        "name": "Been Kim"
                    },
                    {
                        "name": "Priyank Jaini"
                    },
                    {
                        "name": "Robert Geirhos"
                    }
                ],
                "author_detail": {
                    "name": "Robert Geirhos"
                },
                "author": "Robert Geirhos",
                "arxiv_comment": "Project page: https://video-zero-shot.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20324v1",
                "updated": "2025-09-24T17:11:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    11,
                    35,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:11:35Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    11,
                    35,
                    2,
                    267,
                    0
                ],
                "title": "RAG Security and Privacy: Formalizing the Threat Model and Attack\n  Surface",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG Security and Privacy: Formalizing the Threat Model and Attack\n  Surface"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is an emerging approach in natural\nlanguage processing that combines large language models (LLMs) with external\ndocument retrieval to produce more accurate and grounded responses. While RAG\nhas shown strong potential in reducing hallucinations and improving factual\nconsistency, it also introduces new privacy and security challenges that differ\nfrom those faced by traditional LLMs. Existing research has demonstrated that\nLLMs can leak sensitive information through training data memorization or\nadversarial prompts, and RAG systems inherit many of these vulnerabilities. At\nthe same time, reliance of RAG on an external knowledge base opens new attack\nsurfaces, including the potential for leaking information about the presence or\ncontent of retrieved documents, or for injecting malicious content to\nmanipulate model behavior. Despite these risks, there is currently no formal\nframework that defines the threat landscape for RAG systems. In this paper, we\naddress a critical gap in the literature by proposing, to the best of our\nknowledge, the first formal threat model for retrieval-RAG systems. We\nintroduce a structured taxonomy of adversary types based on their access to\nmodel components and data, and we formally define key threat vectors such as\ndocument-level membership inference and data poisoning, which pose serious\nprivacy and integrity risks in real-world deployments. By establishing formal\ndefinitions and attack models, our work lays the foundation for a more rigorous\nand principled understanding of privacy and security in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is an emerging approach in natural\nlanguage processing that combines large language models (LLMs) with external\ndocument retrieval to produce more accurate and grounded responses. While RAG\nhas shown strong potential in reducing hallucinations and improving factual\nconsistency, it also introduces new privacy and security challenges that differ\nfrom those faced by traditional LLMs. Existing research has demonstrated that\nLLMs can leak sensitive information through training data memorization or\nadversarial prompts, and RAG systems inherit many of these vulnerabilities. At\nthe same time, reliance of RAG on an external knowledge base opens new attack\nsurfaces, including the potential for leaking information about the presence or\ncontent of retrieved documents, or for injecting malicious content to\nmanipulate model behavior. Despite these risks, there is currently no formal\nframework that defines the threat landscape for RAG systems. In this paper, we\naddress a critical gap in the literature by proposing, to the best of our\nknowledge, the first formal threat model for retrieval-RAG systems. We\nintroduce a structured taxonomy of adversary types based on their access to\nmodel components and data, and we formally define key threat vectors such as\ndocument-level membership inference and data poisoning, which pose serious\nprivacy and integrity risks in real-world deployments. By establishing formal\ndefinitions and attack models, our work lays the foundation for a more rigorous\nand principled understanding of privacy and security in RAG systems."
                },
                "authors": [
                    {
                        "name": "Atousa Arzanipour"
                    },
                    {
                        "name": "Rouzbeh Behnia"
                    },
                    {
                        "name": "Reza Ebrahimi"
                    },
                    {
                        "name": "Kaushik Dutta"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Dutta"
                },
                "author": "Kaushik Dutta",
                "arxiv_comment": "Accepted at the 5th ICDM Workshop on September 20, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20321v1",
                "updated": "2025-09-24T17:08:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    8,
                    12,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:08:12Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    8,
                    12,
                    2,
                    267,
                    0
                ],
                "title": "DRES: Benchmarking LLMs for Disfluency Removal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRES: Benchmarking LLMs for Disfluency Removal"
                },
                "summary": "Disfluencies -- such as \"um,\" \"uh,\" interjections, parentheticals, and edited\nstatements -- remain a persistent challenge for speech-driven systems,\ndegrading accuracy in command interpretation, summarization, and conversational\nagents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled\ntext-level benchmark that establishes a reproducible semantic upper bound for\nthis task. DRES builds on human-annotated Switchboard transcripts, isolating\ndisfluency removal from ASR errors and acoustic variability. We systematically\nevaluate proprietary and open-source LLMs across scales, prompting strategies,\nand architectures. Our results reveal that (i) simple segmentation consistently\nimproves performance, even for long-context models; (ii) reasoning-oriented\nmodels tend to over-delete fluent tokens; and (iii) fine-tuning achieves near\nstate-of-the-art precision and recall but harms generalization abilities. We\nfurther present a set of LLM-specific error modes and offer nine practical\nrecommendations (R1-R9) for deploying disfluency removal in speech-driven\npipelines. DRES provides a reproducible, model-agnostic foundation for\nadvancing robust spoken-language systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disfluencies -- such as \"um,\" \"uh,\" interjections, parentheticals, and edited\nstatements -- remain a persistent challenge for speech-driven systems,\ndegrading accuracy in command interpretation, summarization, and conversational\nagents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled\ntext-level benchmark that establishes a reproducible semantic upper bound for\nthis task. DRES builds on human-annotated Switchboard transcripts, isolating\ndisfluency removal from ASR errors and acoustic variability. We systematically\nevaluate proprietary and open-source LLMs across scales, prompting strategies,\nand architectures. Our results reveal that (i) simple segmentation consistently\nimproves performance, even for long-context models; (ii) reasoning-oriented\nmodels tend to over-delete fluent tokens; and (iii) fine-tuning achieves near\nstate-of-the-art precision and recall but harms generalization abilities. We\nfurther present a set of LLM-specific error modes and offer nine practical\nrecommendations (R1-R9) for deploying disfluency removal in speech-driven\npipelines. DRES provides a reproducible, model-agnostic foundation for\nadvancing robust spoken-language systems."
                },
                "authors": [
                    {
                        "name": "Maria Teleki"
                    },
                    {
                        "name": "Sai Janjur"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Oliver Grabner"
                    },
                    {
                        "name": "Ketan Verma"
                    },
                    {
                        "name": "Thomas Docog"
                    },
                    {
                        "name": "Xiangjue Dong"
                    },
                    {
                        "name": "Lingfeng Shi"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Stephanie Birkelbach"
                    },
                    {
                        "name": "Jason Kim"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "James Caverlee"
                    }
                ],
                "author_detail": {
                    "name": "James Caverlee"
                },
                "author": "James Caverlee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13739v2",
                "updated": "2025-09-24T17:02:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    2,
                    50,
                    2,
                    267,
                    0
                ],
                "published": "2025-08-19T11:23:09Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    23,
                    9,
                    1,
                    231,
                    0
                ],
                "title": "Enhancing Targeted Adversarial Attacks on Large Vision-Language Models\n  via Intermediate Projector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Targeted Adversarial Attacks on Large Vision-Language Models\n  via Intermediate Projector"
                },
                "summary": "The growing deployment of Large Vision-Language Models (VLMs) raises safety\nconcerns, as adversaries may exploit model vulnerabilities to induce harmful\noutputs, with targeted black-box adversarial attacks posing a particularly\nsevere threat. However, existing methods primarily maximize encoder-level\nglobal similarity, which lacks the granularity for stealthy and practical\nfine-grained attacks, where only specific target should be altered (e.g.,\nmodifying a car while preserving its background). Moreover, they largely\nneglect the projector, a key semantic bridge in VLMs for multimodal alignment.\nTo address these limitations, we propose a novel black-box targeted attack\nframework that leverages the projector. Specifically, we utilize the widely\nadopted Querying Transformer (Q-Former) which transforms global image\nembeddings into fine-grained query outputs, to enhance attack effectiveness and\ngranularity. For standard global targeted attack scenarios, we propose the\nIntermediate Projector Guided Attack (IPGA), which aligns Q-Former fine-grained\nquery outputs with the target to enhance attack strength and exploits the\nintermediate pretrained Q-Former that is not fine-tuned for any specific Large\nLanguage Model (LLM) to improve attack transferability. For fine-grained attack\nscenarios, we augment IPGA with the Residual Query Alignment (RQA) module,\nwhich preserves unrelated content by constraining non-target query outputs to\nenhance attack granularity. Extensive experiments demonstrate that IPGA\nsignificantly outperforms baselines in global targeted attacks, and IPGA with\nRQA (IPGA-R) attains superior success rates and unrelated content preservation\nover baselines in fine-grained attacks. Our method also transfers effectively\nto commercial VLMs such as Google Gemini and OpenAI GPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing deployment of Large Vision-Language Models (VLMs) raises safety\nconcerns, as adversaries may exploit model vulnerabilities to induce harmful\noutputs, with targeted black-box adversarial attacks posing a particularly\nsevere threat. However, existing methods primarily maximize encoder-level\nglobal similarity, which lacks the granularity for stealthy and practical\nfine-grained attacks, where only specific target should be altered (e.g.,\nmodifying a car while preserving its background). Moreover, they largely\nneglect the projector, a key semantic bridge in VLMs for multimodal alignment.\nTo address these limitations, we propose a novel black-box targeted attack\nframework that leverages the projector. Specifically, we utilize the widely\nadopted Querying Transformer (Q-Former) which transforms global image\nembeddings into fine-grained query outputs, to enhance attack effectiveness and\ngranularity. For standard global targeted attack scenarios, we propose the\nIntermediate Projector Guided Attack (IPGA), which aligns Q-Former fine-grained\nquery outputs with the target to enhance attack strength and exploits the\nintermediate pretrained Q-Former that is not fine-tuned for any specific Large\nLanguage Model (LLM) to improve attack transferability. For fine-grained attack\nscenarios, we augment IPGA with the Residual Query Alignment (RQA) module,\nwhich preserves unrelated content by constraining non-target query outputs to\nenhance attack granularity. Extensive experiments demonstrate that IPGA\nsignificantly outperforms baselines in global targeted attacks, and IPGA with\nRQA (IPGA-R) attains superior success rates and unrelated content preservation\nover baselines in fine-grained attacks. Our method also transfers effectively\nto commercial VLMs such as Google Gemini and OpenAI GPT."
                },
                "authors": [
                    {
                        "name": "Yiming Cao"
                    },
                    {
                        "name": "Yanjie Li"
                    },
                    {
                        "name": "Kaisheng Liang"
                    },
                    {
                        "name": "Bin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Xiao"
                },
                "author": "Bin Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20319v1",
                "updated": "2025-09-24T17:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    2,
                    39,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    2,
                    39,
                    2,
                    267,
                    0
                ],
                "title": "Z-Scores: A Metric for Linguistically Assessing Disfluency Removal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Z-Scores: A Metric for Linguistically Assessing Disfluency Removal"
                },
                "summary": "Evaluating disfluency removal in speech requires more than aggregate\ntoken-level scores. Traditional word-based metrics such as precision, recall,\nand F1 (E-Scores) capture overall performance but cannot reveal why models\nsucceed or fail. We introduce Z-Scores, a span-level linguistically-grounded\nevaluation metric that categorizes system behavior across distinct disfluency\ntypes (EDITED, INTJ, PRN). Our deterministic alignment module enables robust\nmapping between generated text and disfluent transcripts, allowing Z-Scores to\nexpose systematic weaknesses that word-level metrics obscure. By providing\ncategory-specific diagnostics, Z-Scores enable researchers to identify model\nfailure modes and design targeted interventions -- such as tailored prompts or\ndata augmentation -- yielding measurable performance improvements. A case study\nwith LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies\nhidden in aggregate F1, directly informing model refinement strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating disfluency removal in speech requires more than aggregate\ntoken-level scores. Traditional word-based metrics such as precision, recall,\nand F1 (E-Scores) capture overall performance but cannot reveal why models\nsucceed or fail. We introduce Z-Scores, a span-level linguistically-grounded\nevaluation metric that categorizes system behavior across distinct disfluency\ntypes (EDITED, INTJ, PRN). Our deterministic alignment module enables robust\nmapping between generated text and disfluent transcripts, allowing Z-Scores to\nexpose systematic weaknesses that word-level metrics obscure. By providing\ncategory-specific diagnostics, Z-Scores enable researchers to identify model\nfailure modes and design targeted interventions -- such as tailored prompts or\ndata augmentation -- yielding measurable performance improvements. A case study\nwith LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies\nhidden in aggregate F1, directly informing model refinement strategies."
                },
                "authors": [
                    {
                        "name": "Maria Teleki"
                    },
                    {
                        "name": "Sai Janjur"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Oliver Grabner"
                    },
                    {
                        "name": "Ketan Verma"
                    },
                    {
                        "name": "Thomas Docog"
                    },
                    {
                        "name": "Xiangjue Dong"
                    },
                    {
                        "name": "Lingfeng Shi"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Stephanie Birkelbach"
                    },
                    {
                        "name": "Jason Kim"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "James Caverlee"
                    }
                ],
                "author_detail": {
                    "name": "James Caverlee"
                },
                "author": "James Caverlee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20318v1",
                "updated": "2025-09-24T17:01:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    1,
                    50,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:01:50Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    1,
                    50,
                    2,
                    267,
                    0
                ],
                "title": "A Comprehensive Evaluation of YOLO-based Deer Detection Performance on\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation of YOLO-based Deer Detection Performance on\n  Edge Devices"
                },
                "summary": "The escalating economic losses in agriculture due to deer intrusion,\nestimated to be in the hundreds of millions of dollars annually in the U.S.,\nhighlight the inadequacy of traditional mitigation strategies since these\nmethods are often labor-intensive, costly, and ineffective for modern farming\nsystems. To overcome this, there is a critical need for intelligent, autonomous\nsolutions which require accurate and efficient deer detection. But the progress\nin this field is impeded by a significant gap in the literature, mainly the\nlack of a domain-specific, practical dataset and limited study on the on-field\ndeployability of deer detection systems. Addressing this gap, this study\npresents a comprehensive evaluation of state-of-the-art deep learning models\nfor deer detection in challenging real-world scenarios. The contributions of\nthis work are threefold. First, we introduce a curated, publicly available\ndataset of 3,095 annotated images with bounding-box annotations of deer,\nderived from the Idaho Cameratraps project. Second, we provide an extensive\ncomparative analysis of 12 model variants across four recent YOLO\narchitectures(v8, v9, v10, and v11). Finally, we benchmarked performance on a\nhigh-end NVIDIA RTX 5090 GPU and evaluated on two representative edge computing\nplatforms: Raspberry Pi 5 and NVIDIA Jetson AGX Xavier. Results show that the\nreal-time detection is not feasible in Raspberry Pi without hardware-specific\nmodel optimization, while NVIDIA Jetson provides greater than 30 FPS with\nGPU-accelerated inference on 's' and 'n' series models. This study also reveals\nthat smaller, architecturally advanced models such as YOLOv11n, YOLOv8s, and\nYOLOv9s offer the optimal balance of high accuracy (AP@.5 > 0.85) and\ncomputational efficiency (FPS > 30). To support further research, both the\nsource code and datasets are publicly available at\nhttps://github.com/WinnerBishal/track-the-deer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The escalating economic losses in agriculture due to deer intrusion,\nestimated to be in the hundreds of millions of dollars annually in the U.S.,\nhighlight the inadequacy of traditional mitigation strategies since these\nmethods are often labor-intensive, costly, and ineffective for modern farming\nsystems. To overcome this, there is a critical need for intelligent, autonomous\nsolutions which require accurate and efficient deer detection. But the progress\nin this field is impeded by a significant gap in the literature, mainly the\nlack of a domain-specific, practical dataset and limited study on the on-field\ndeployability of deer detection systems. Addressing this gap, this study\npresents a comprehensive evaluation of state-of-the-art deep learning models\nfor deer detection in challenging real-world scenarios. The contributions of\nthis work are threefold. First, we introduce a curated, publicly available\ndataset of 3,095 annotated images with bounding-box annotations of deer,\nderived from the Idaho Cameratraps project. Second, we provide an extensive\ncomparative analysis of 12 model variants across four recent YOLO\narchitectures(v8, v9, v10, and v11). Finally, we benchmarked performance on a\nhigh-end NVIDIA RTX 5090 GPU and evaluated on two representative edge computing\nplatforms: Raspberry Pi 5 and NVIDIA Jetson AGX Xavier. Results show that the\nreal-time detection is not feasible in Raspberry Pi without hardware-specific\nmodel optimization, while NVIDIA Jetson provides greater than 30 FPS with\nGPU-accelerated inference on 's' and 'n' series models. This study also reveals\nthat smaller, architecturally advanced models such as YOLOv11n, YOLOv8s, and\nYOLOv9s offer the optimal balance of high accuracy (AP@.5 > 0.85) and\ncomputational efficiency (FPS > 30). To support further research, both the\nsource code and datasets are publicly available at\nhttps://github.com/WinnerBishal/track-the-deer."
                },
                "authors": [
                    {
                        "name": "Bishal Adhikari"
                    },
                    {
                        "name": "Jiajia Li"
                    },
                    {
                        "name": "Eric S. Michel"
                    },
                    {
                        "name": "Jacob Dykes"
                    },
                    {
                        "name": "Te-Ming Paul Tseng"
                    },
                    {
                        "name": "Mary Love Tagert"
                    },
                    {
                        "name": "Dong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Dong Chen"
                },
                "author": "Dong Chen",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20317v1",
                "updated": "2025-09-24T17:01:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    1,
                    32,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T17:01:32Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    1,
                    32,
                    2,
                    267,
                    0
                ],
                "title": "SIM-CoT: Supervised Implicit Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIM-CoT: Supervised Implicit Chain-of-Thought"
                },
                "summary": "Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient\nalternative to explicit CoT reasoning in Large Language Models (LLMs), but a\npersistent performance gap has limited the application of implicit CoT. We\nidentify a core latent instability issue by scaling the computational budget of\nimplicit CoT approaches: as we increase the number of implicit reasoning tokens\nto enhance performance, the training process often becomes unstable and\ncollapses. Our analysis reveals that this instability arises from the latent\nrepresentations becoming homogeneous and losing their semantic diversity, a\nfailure caused by insufficient step-level supervision in existing implicit CoT\napproaches. To address this issue, we propose SIM-CoT, a plug-and-play training\nmodule that introduces step-level supervision to stabilize and enrich the\nlatent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder\nduring training to align each implicit token with its corresponding explicit\nreasoning step, ensuring that latent states capture distinct and meaningful\ninformation. The proposed auxiliary decoder is removed during inference,\npreserving the computational efficiency of implicit CoT methods with no added\noverhead. In addition, the auxiliary decoder affords interpretability of\nimplicit reasoning by projecting each latent token onto an explicit reasoning\nvocabulary, enabling per-step visualization of semantic roles and diagnosis.\nSIM-CoT significantly enhances both the in-domain accuracy and out-of-domain\nstability of various implicit CoT methods, boosting baselines like Coconut by\n+8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong\nscalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1%\nwith 2.3\\times greater token efficiency, while substantially closing the\nperformance gap on larger models like LLaMA-3.1 8B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient\nalternative to explicit CoT reasoning in Large Language Models (LLMs), but a\npersistent performance gap has limited the application of implicit CoT. We\nidentify a core latent instability issue by scaling the computational budget of\nimplicit CoT approaches: as we increase the number of implicit reasoning tokens\nto enhance performance, the training process often becomes unstable and\ncollapses. Our analysis reveals that this instability arises from the latent\nrepresentations becoming homogeneous and losing their semantic diversity, a\nfailure caused by insufficient step-level supervision in existing implicit CoT\napproaches. To address this issue, we propose SIM-CoT, a plug-and-play training\nmodule that introduces step-level supervision to stabilize and enrich the\nlatent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder\nduring training to align each implicit token with its corresponding explicit\nreasoning step, ensuring that latent states capture distinct and meaningful\ninformation. The proposed auxiliary decoder is removed during inference,\npreserving the computational efficiency of implicit CoT methods with no added\noverhead. In addition, the auxiliary decoder affords interpretability of\nimplicit reasoning by projecting each latent token onto an explicit reasoning\nvocabulary, enabling per-step visualization of semantic roles and diagnosis.\nSIM-CoT significantly enhances both the in-domain accuracy and out-of-domain\nstability of various implicit CoT methods, boosting baselines like Coconut by\n+8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong\nscalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1%\nwith 2.3\\times greater token efficiency, while substantially closing the\nperformance gap on larger models like LLaMA-3.1 8B."
                },
                "authors": [
                    {
                        "name": "Xilin Wei"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01841v2",
                "updated": "2025-09-24T17:00:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    0,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2024-09-27T23:05:02Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    23,
                    5,
                    2,
                    4,
                    271,
                    0
                ],
                "title": "A GEN AI Framework for Medical Note Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A GEN AI Framework for Medical Note Generation"
                },
                "summary": "The increasing administrative burden of medical documentation, particularly\nthrough Electronic Health Records (EHR), significantly reduces the time\navailable for direct patient care and contributes to physician burnout. To\naddress this issue, we propose MediNotes, an advanced generative AI framework\ndesigned to automate the creation of SOAP (Subjective, Objective, Assessment,\nPlan) notes from medical conversations. MediNotes integrates Large Language\nModels (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech\nRecognition (ASR) to capture and process both text and voice inputs in real\ntime or from recorded audio, generating structured and contextually accurate\nmedical notes. The framework also incorporates advanced techniques like\nQuantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning\n(PEFT) for efficient model fine-tuning in resource-constrained environments.\nAdditionally, MediNotes offers a query-based retrieval system, allowing\nhealthcare providers and patients to access relevant medical information\nquickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate\nthat MediNotes significantly improves the accuracy, efficiency, and usability\nof automated medical documentation, offering a robust solution to reduce the\nadministrative burden on healthcare professionals while improving the quality\nof clinical workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing administrative burden of medical documentation, particularly\nthrough Electronic Health Records (EHR), significantly reduces the time\navailable for direct patient care and contributes to physician burnout. To\naddress this issue, we propose MediNotes, an advanced generative AI framework\ndesigned to automate the creation of SOAP (Subjective, Objective, Assessment,\nPlan) notes from medical conversations. MediNotes integrates Large Language\nModels (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech\nRecognition (ASR) to capture and process both text and voice inputs in real\ntime or from recorded audio, generating structured and contextually accurate\nmedical notes. The framework also incorporates advanced techniques like\nQuantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning\n(PEFT) for efficient model fine-tuning in resource-constrained environments.\nAdditionally, MediNotes offers a query-based retrieval system, allowing\nhealthcare providers and patients to access relevant medical information\nquickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate\nthat MediNotes significantly improves the accuracy, efficiency, and usability\nof automated medical documentation, offering a robust solution to reduce the\nadministrative burden on healthcare professionals while improving the quality\nof clinical workflows."
                },
                "authors": [
                    {
                        "name": "Hui Yi Leong"
                    },
                    {
                        "name": "Yi Fan Gao"
                    },
                    {
                        "name": "Shuai Ji"
                    },
                    {
                        "name": "Bora Kalaycioglu"
                    },
                    {
                        "name": "Uktu Pamuksuz"
                    }
                ],
                "author_detail": {
                    "name": "Uktu Pamuksuz"
                },
                "author": "Uktu Pamuksuz",
                "arxiv_comment": "8 Figures, 7 page, IEEE standard research paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09324v3",
                "updated": "2025-09-24T16:59:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    59,
                    19,
                    2,
                    267,
                    0
                ],
                "published": "2024-09-14T06:02:17Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    6,
                    2,
                    17,
                    5,
                    258,
                    0
                ],
                "title": "Efficient Fine-Tuning of Large Language Models for Automated Medical\n  Documentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Fine-Tuning of Large Language Models for Automated Medical\n  Documentation"
                },
                "summary": "Scientific research indicates that for every hour spent in direct patient\ncare, physicians spend nearly two additional hours on administrative tasks,\nparticularly on electronic health records (EHRs) and desk work. This excessive\nadministrative burden not only reduces the time available for patient care but\nalso contributes to physician burnout and inefficiencies in healthcare\ndelivery. To address these challenges, this study introduces MediGen, a\nfine-tuned large language model (LLM) designed to automate the generation of\nmedical reports from medical dialogues. By leveraging state-of-the-art\nmethodologies for fine-tuning open-source pretrained models, including\nLLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing\nclinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising\nresults, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating\nits effectiveness in generating accurate and clinically relevant medical\nreports. These findings suggest that MediGen has the potential to significantly\nreduce the administrative workload on physicians, improving both healthcare\nefficiency and physician well-being.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific research indicates that for every hour spent in direct patient\ncare, physicians spend nearly two additional hours on administrative tasks,\nparticularly on electronic health records (EHRs) and desk work. This excessive\nadministrative burden not only reduces the time available for patient care but\nalso contributes to physician burnout and inefficiencies in healthcare\ndelivery. To address these challenges, this study introduces MediGen, a\nfine-tuned large language model (LLM) designed to automate the generation of\nmedical reports from medical dialogues. By leveraging state-of-the-art\nmethodologies for fine-tuning open-source pretrained models, including\nLLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing\nclinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising\nresults, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating\nits effectiveness in generating accurate and clinically relevant medical\nreports. These findings suggest that MediGen has the potential to significantly\nreduce the administrative workload on physicians, improving both healthcare\nefficiency and physician well-being."
                },
                "authors": [
                    {
                        "name": "Hui Yi Leong"
                    },
                    {
                        "name": "Yi Fan Gao"
                    },
                    {
                        "name": "Ji Shuai"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Uktu Pamuksuz"
                    }
                ],
                "author_detail": {
                    "name": "Uktu Pamuksuz"
                },
                "author": "Uktu Pamuksuz",
                "arxiv_doi": "10.13140/RG.2.2.26884.74881",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.26884.74881",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.09324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "4 pages, 3 Figures, 3 Tables. The final version will be published in\n  the proceedings of the IEEE conference",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v2",
                "updated": "2025-09-24T16:56:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    56,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective"
                },
                "summary": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08540v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08540v3",
                "updated": "2025-09-24T16:51:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    51,
                    16,
                    2,
                    267,
                    0
                ],
                "published": "2025-07-11T12:39:25Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    39,
                    25,
                    4,
                    192,
                    0
                ],
                "title": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection"
                },
                "summary": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications."
                },
                "authors": [
                    {
                        "name": "Ioannis Lamprou"
                    },
                    {
                        "name": "Alexander Shevtsov"
                    },
                    {
                        "name": "Ioannis Arapakis"
                    },
                    {
                        "name": "Sotiris Ioannidis"
                    }
                ],
                "author_detail": {
                    "name": "Sotiris Ioannidis"
                },
                "author": "Sotiris Ioannidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08540v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08540v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22323v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22323v2",
                "updated": "2025-09-24T16:48:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    48,
                    33,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-28T13:09:47Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    9,
                    47,
                    2,
                    148,
                    0
                ],
                "title": "Advancing Expert Specialization for Better MoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Expert Specialization for Better MoE"
                },
                "summary": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community."
                },
                "authors": [
                    {
                        "name": "Hongcan Guo"
                    },
                    {
                        "name": "Haolang Lu"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Bolun Chu"
                    },
                    {
                        "name": "Jialin Zhuang"
                    },
                    {
                        "name": "Yuan Yang"
                    },
                    {
                        "name": "Wenhao Che"
                    },
                    {
                        "name": "Sicong Leng"
                    },
                    {
                        "name": "Qimei Cui"
                    },
                    {
                        "name": "Xudong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xudong Jiang"
                },
                "author": "Xudong Jiang",
                "arxiv_comment": "33pages, 6figures(Accepted by Neurips 2026 Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22323v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22323v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22931v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22931v3",
                "updated": "2025-09-24T16:41:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    41,
                    40,
                    2,
                    267,
                    0
                ],
                "published": "2025-07-24T13:46:51Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    46,
                    51,
                    3,
                    205,
                    0
                ],
                "title": "Enhancing RAG Efficiency with Adaptive Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing RAG Efficiency with Adaptive Context Compression"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy."
                },
                "authors": [
                    {
                        "name": "Shuyu Guo"
                    },
                    {
                        "name": "Shuo Zhang"
                    },
                    {
                        "name": "Zhaochun Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhaochun Ren"
                },
                "author": "Zhaochun Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22931v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22931v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09873v2",
                "updated": "2025-09-24T16:38:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    38,
                    38,
                    2,
                    267,
                    0
                ],
                "published": "2024-11-15T01:48:08Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    1,
                    48,
                    8,
                    4,
                    320,
                    0
                ],
                "title": "LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing\n  Online Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing\n  Online Learners"
                },
                "summary": "Intelligent tutoring systems (ITS) using artificial intelligence (AI)\ntechnology have shown promise in supporting learners with diverse abilities.\nLarge language models (LLMs) provide new opportunities to incorporate personas\nto AI-based tutors and support dynamic interactive dialogue. This paper\nexplores how DHH learners interact with LLM-powered AI tutors with different\nexperiences in DHH education as personas to identify their accessibility\npreferences. A user study with 16 DHH participants showed that they asked\nDHH-related questions based on background information and evaluated the AI\ntutors' cultural knowledge of the DHH communities in their responses.\nParticipants suggested providing more transparency in each AI tutor's position\nwithin the DHH community. Participants also pointed out the lack of support in\nthe multimodality of sign language in current LLMs. We discuss design\nimplications to support the diverse needs in interaction between DHH users and\nthe LLMs, such as offering supports in tuning language styles of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent tutoring systems (ITS) using artificial intelligence (AI)\ntechnology have shown promise in supporting learners with diverse abilities.\nLarge language models (LLMs) provide new opportunities to incorporate personas\nto AI-based tutors and support dynamic interactive dialogue. This paper\nexplores how DHH learners interact with LLM-powered AI tutors with different\nexperiences in DHH education as personas to identify their accessibility\npreferences. A user study with 16 DHH participants showed that they asked\nDHH-related questions based on background information and evaluated the AI\ntutors' cultural knowledge of the DHH communities in their responses.\nParticipants suggested providing more transparency in each AI tutor's position\nwithin the DHH community. Participants also pointed out the lack of support in\nthe multimodality of sign language in current LLMs. We discuss design\nimplications to support the diverse needs in interaction between DHH users and\nthe LLMs, such as offering supports in tuning language styles of LLMs."
                },
                "authors": [
                    {
                        "name": "Haocong Cheng"
                    },
                    {
                        "name": "Si Chen"
                    },
                    {
                        "name": "Christopher Perdriau"
                    },
                    {
                        "name": "Shriya Mokkapati"
                    },
                    {
                        "name": "Yun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yun Huang"
                },
                "author": "Yun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20293v1",
                "updated": "2025-09-24T16:26:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    26,
                    47,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T16:26:47Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    26,
                    47,
                    2,
                    267,
                    0
                ],
                "title": "When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks\n  Silently Undermine Validity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks\n  Silently Undermine Validity"
                },
                "summary": "LLM-judged benchmarks are increasingly used to evaluate complex model\nbehaviors, yet their design introduces failure modes absent in conventional\nground-truth based benchmarks. We argue that without tight objectives and\nverifiable constructions, benchmark rankings can produce high-confidence\nrankings that are in fact largely noise. We introduce two mechanisms to\ndiagnose these issues. Schematic adherence quantifies how much of a judge's\noverall verdict is explained by the explicit evaluation schema, revealing\nunexplained variance when judges deviate from their own rubric. Psychometric\nvalidity aggregates internal consistency and discriminant validity signals to\nquantify irreducible uncertainty in any benchmarking run. Applying these tools\nto Arena-Hard Auto, we find severe schema incoherence and factor collapse\nacross popular judges: for example, unexplained variance exceeding 90 percent\nfor DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We\nalso show that the ELO-style aggregation used by Arena-Hard Auto collapses and\nmasks genuine ranking uncertainty. Our results highlight design failures that\nundermine validity and offer actionable principles for building better-scoped,\nreliability-aware LLM-judged benchmarks. We release our code at\nhttps://anonymous.4open.science/r/judgment-to-noise-947D/README.md",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-judged benchmarks are increasingly used to evaluate complex model\nbehaviors, yet their design introduces failure modes absent in conventional\nground-truth based benchmarks. We argue that without tight objectives and\nverifiable constructions, benchmark rankings can produce high-confidence\nrankings that are in fact largely noise. We introduce two mechanisms to\ndiagnose these issues. Schematic adherence quantifies how much of a judge's\noverall verdict is explained by the explicit evaluation schema, revealing\nunexplained variance when judges deviate from their own rubric. Psychometric\nvalidity aggregates internal consistency and discriminant validity signals to\nquantify irreducible uncertainty in any benchmarking run. Applying these tools\nto Arena-Hard Auto, we find severe schema incoherence and factor collapse\nacross popular judges: for example, unexplained variance exceeding 90 percent\nfor DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We\nalso show that the ELO-style aggregation used by Arena-Hard Auto collapses and\nmasks genuine ranking uncertainty. Our results highlight design failures that\nundermine validity and offer actionable principles for building better-scoped,\nreliability-aware LLM-judged benchmarks. We release our code at\nhttps://anonymous.4open.science/r/judgment-to-noise-947D/README.md"
                },
                "authors": [
                    {
                        "name": "Benjamin Feuer"
                    },
                    {
                        "name": "Chiung-Yi Tseng"
                    },
                    {
                        "name": "Astitwa Sarthak Lathe"
                    },
                    {
                        "name": "Oussama Elachqar"
                    },
                    {
                        "name": "John P Dickerson"
                    }
                ],
                "author_detail": {
                    "name": "John P Dickerson"
                },
                "author": "John P Dickerson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20283v1",
                "updated": "2025-09-24T16:15:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    15,
                    51,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T16:15:51Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    15,
                    51,
                    2,
                    267,
                    0
                ],
                "title": "Monitoring Violations of Differential Privacy over Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring Violations of Differential Privacy over Time"
                },
                "summary": "Auditing differential privacy has emerged as an important area of research\nthat supports the design of privacy-preserving mechanisms. Privacy audits help\nto obtain empirical estimates of the privacy parameter, to expose flawed\nimplementations of algorithms and to compare practical with theoretical privacy\nguarantees. In this work, we investigate an unexplored facet of privacy\nauditing: the sustained auditing of a mechanism that can go through changes\nduring its development or deployment. Monitoring the privacy of algorithms over\ntime comes with specific challenges. Running state-of-the-art (static) auditors\nrepeatedly requires excessive sampling efforts, while the reliability of such\nmethods deteriorates over time without proper adjustments. To overcome these\nobstacles, we present a new monitoring procedure that extracts information from\nthe entire deployment history of the algorithm. This allows us to reduce\nsampling efforts, while sustaining reliable outcomes of our auditor. We derive\nformal guarantees with regard to the soundness of our methods and evaluate\ntheir performance for important mechanisms from the literature. Our theoretical\nfindings and experiments demonstrate the efficacy of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing differential privacy has emerged as an important area of research\nthat supports the design of privacy-preserving mechanisms. Privacy audits help\nto obtain empirical estimates of the privacy parameter, to expose flawed\nimplementations of algorithms and to compare practical with theoretical privacy\nguarantees. In this work, we investigate an unexplored facet of privacy\nauditing: the sustained auditing of a mechanism that can go through changes\nduring its development or deployment. Monitoring the privacy of algorithms over\ntime comes with specific challenges. Running state-of-the-art (static) auditors\nrepeatedly requires excessive sampling efforts, while the reliability of such\nmethods deteriorates over time without proper adjustments. To overcome these\nobstacles, we present a new monitoring procedure that extracts information from\nthe entire deployment history of the algorithm. This allows us to reduce\nsampling efforts, while sustaining reliable outcomes of our auditor. We derive\nformal guarantees with regard to the soundness of our methods and evaluate\ntheir performance for important mechanisms from the literature. Our theoretical\nfindings and experiments demonstrate the efficacy of our approach."
                },
                "authors": [
                    {
                        "name": "Önder Askin"
                    },
                    {
                        "name": "Tim Kutta"
                    },
                    {
                        "name": "Holger Dette"
                    }
                ],
                "author_detail": {
                    "name": "Holger Dette"
                },
                "author": "Holger Dette",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20278v1",
                "updated": "2025-09-24T16:15:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    15,
                    26,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T16:15:26Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    15,
                    26,
                    2,
                    267,
                    0
                ],
                "title": "Instruction Boundary: Quantifying Biases in LLM Reasoning under Various\n  Coverage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Boundary: Quantifying Biases in LLM Reasoning under Various\n  Coverage"
                },
                "summary": "Large-language-model (LLM) reasoning has long been regarded as a powerful\ntool for problem solving across domains, providing non-experts with valuable\nadvice. However, their limitations - especially those stemming from prompt\ndesign - remain underexplored. Because users may supply biased or incomplete\nprompts - often unintentionally - LLMs can be misled, undermining reliability\nand creating risks. We refer to this vulnerability as the Instruction Boundary.\nTo investigate the phenomenon, we distill it into eight concrete facets and\nintroduce BiasDetector, a framework that measures biases arising from three\ninstruction types: complete, redundant, and insufficient. We evaluate several\nmainstream LLMs and find that, despite high headline accuracy, substantial\nbiases persist in many downstream tasks as a direct consequence of prompt\ncoverage. Our empirical study confirms that LLM reasoning reliability can still\nbe significantly improved. We analyze the practical impact of these biases and\noutline mitigation strategies. Our findings underscore the need for developers\nto tackle biases and for users to craft options carefully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-language-model (LLM) reasoning has long been regarded as a powerful\ntool for problem solving across domains, providing non-experts with valuable\nadvice. However, their limitations - especially those stemming from prompt\ndesign - remain underexplored. Because users may supply biased or incomplete\nprompts - often unintentionally - LLMs can be misled, undermining reliability\nand creating risks. We refer to this vulnerability as the Instruction Boundary.\nTo investigate the phenomenon, we distill it into eight concrete facets and\nintroduce BiasDetector, a framework that measures biases arising from three\ninstruction types: complete, redundant, and insufficient. We evaluate several\nmainstream LLMs and find that, despite high headline accuracy, substantial\nbiases persist in many downstream tasks as a direct consequence of prompt\ncoverage. Our empirical study confirms that LLM reasoning reliability can still\nbe significantly improved. We analyze the practical impact of these biases and\noutline mitigation strategies. Our findings underscore the need for developers\nto tackle biases and for users to craft options carefully."
                },
                "authors": [
                    {
                        "name": "Zipeng Ling"
                    },
                    {
                        "name": "Yuehao Tang"
                    },
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Shuliang Liu"
                    },
                    {
                        "name": "Gaoyang Jiang"
                    },
                    {
                        "name": "Shenghong Fu"
                    },
                    {
                        "name": "Junqi Yang"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Jiawan Zhang"
                    },
                    {
                        "name": "Kejia Huang"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20277v1",
                "updated": "2025-09-24T16:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    15,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T16:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    15,
                    17,
                    2,
                    267,
                    0
                ],
                "title": "Investigating Security Implications of Automatically Generated Code on\n  the Software Supply Chain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Security Implications of Automatically Generated Code on\n  the Software Supply Chain"
                },
                "summary": "In recent years, various software supply chain (SSC) attacks have posed\nsignificant risks to the global community. Severe consequences may arise if\ndevelopers integrate insecure code snippets that are vulnerable to SSC attacks\ninto their products. Particularly, code generation techniques, such as large\nlanguage models (LLMs), have been widely utilized in the developer community.\nHowever, LLMs are known to suffer from inherent issues when generating code,\nincluding fabrication, misinformation, and reliance on outdated training data,\nall of which can result in serious software supply chain threats. In this\npaper, we investigate the security threats to the SSC that arise from these\ninherent issues. We examine three categories of threats, including eleven\npotential SSC-related threats, related to external components in source code,\nand continuous integration configuration files. We find some threats in\nLLM-generated code could enable attackers to hijack software and workflows,\nwhile some others might cause potential hidden threats that compromise the\nsecurity of the software over time. To understand these security impacts and\nseverity, we design a tool, SSCGuard, to generate 439,138 prompts based on\nSSC-related questions collected online, and analyze the responses of four\npopular LLMs from GPT and Llama. Our results show that all identified\nSSC-related threats persistently exist. To mitigate these risks, we propose a\nnovel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce\nfabrication, and a middleware-based defense that informs users of various SSC\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, various software supply chain (SSC) attacks have posed\nsignificant risks to the global community. Severe consequences may arise if\ndevelopers integrate insecure code snippets that are vulnerable to SSC attacks\ninto their products. Particularly, code generation techniques, such as large\nlanguage models (LLMs), have been widely utilized in the developer community.\nHowever, LLMs are known to suffer from inherent issues when generating code,\nincluding fabrication, misinformation, and reliance on outdated training data,\nall of which can result in serious software supply chain threats. In this\npaper, we investigate the security threats to the SSC that arise from these\ninherent issues. We examine three categories of threats, including eleven\npotential SSC-related threats, related to external components in source code,\nand continuous integration configuration files. We find some threats in\nLLM-generated code could enable attackers to hijack software and workflows,\nwhile some others might cause potential hidden threats that compromise the\nsecurity of the software over time. To understand these security impacts and\nseverity, we design a tool, SSCGuard, to generate 439,138 prompts based on\nSSC-related questions collected online, and analyze the responses of four\npopular LLMs from GPT and Llama. Our results show that all identified\nSSC-related threats persistently exist. To mitigate these risks, we propose a\nnovel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce\nfabrication, and a middleware-based defense that informs users of various SSC\nthreats."
                },
                "authors": [
                    {
                        "name": "Xiaofan Li"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03148v2",
                "updated": "2025-09-24T16:07:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    7,
                    19,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-03T08:57:10Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    57,
                    10,
                    2,
                    246,
                    0
                ],
                "title": "Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan,\n  Sutsilvan, Surmiran, Puter, and Vallader",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan,\n  Sutsilvan, Surmiran, Puter, and Vallader"
                },
                "summary": "The Romansh language, spoken in Switzerland, has limited resources for\nmachine translation evaluation. In this paper, we present a benchmark for six\nvarieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five\nregional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our\nreference translations were created by human translators based on the WMT24++\nbenchmark, which ensures parallelism with more than 55 other languages. An\nautomatic evaluation of existing MT systems and LLMs shows that translation out\nof Romansh into German is handled relatively well for all the varieties, but\ntranslation into Romansh is still challenging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Romansh language, spoken in Switzerland, has limited resources for\nmachine translation evaluation. In this paper, we present a benchmark for six\nvarieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five\nregional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our\nreference translations were created by human translators based on the WMT24++\nbenchmark, which ensures parallelism with more than 55 other languages. An\nautomatic evaluation of existing MT systems and LLMs shows that translation out\nof Romansh into German is handled relatively well for all the varieties, but\ntranslation into Romansh is still challenging."
                },
                "authors": [
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Ignacio Pérez Prat"
                    },
                    {
                        "name": "Not Battesta Soliva"
                    },
                    {
                        "name": "Sandra Baltermia-Guetg"
                    },
                    {
                        "name": "Andrina Beeli"
                    },
                    {
                        "name": "Simona Beeli"
                    },
                    {
                        "name": "Madlaina Capeder"
                    },
                    {
                        "name": "Laura Decurtins"
                    },
                    {
                        "name": "Gian Peder Gregori"
                    },
                    {
                        "name": "Flavia Hobi"
                    },
                    {
                        "name": "Gabriela Holderegger"
                    },
                    {
                        "name": "Arina Lazzarini"
                    },
                    {
                        "name": "Viviana Lazzarini"
                    },
                    {
                        "name": "Walter Rosselli"
                    },
                    {
                        "name": "Bettina Vital"
                    },
                    {
                        "name": "Anna Rutkiewicz"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "WMT25 (Open Language Data Initiative Shared Task)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08650v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08650v5",
                "updated": "2025-09-24T16:04:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    4,
                    24,
                    2,
                    267,
                    0
                ],
                "published": "2025-01-15T20:59:42Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    59,
                    42,
                    2,
                    15,
                    0
                ],
                "title": "Who is Responsible? The Data, Models, Users or Regulations? A\n  Comprehensive Survey on Responsible Generative AI for a Sustainable Future",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who is Responsible? The Data, Models, Users or Regulations? A\n  Comprehensive Survey on Responsible Generative AI for a Sustainable Future"
                },
                "summary": "Generative AI is moving rapidly from research into real world deployment\nacross sectors, which elevates the need for responsible development,\ndeployment, evaluation, and governance. To address this pressing challenge, in\nthis study, we synthesize the landscape of responsible generative AI across\nmethods, benchmarks, and policies, and connects governance expectations to\nconcrete engineering practice. We follow a prespecified search and screening\nprotocol focused on post-ChatGPT era with selective inclusion of foundational\nwork for definitions, and we conduct a narrative and thematic synthesis. Three\nfindings emerge; First, benchmark and practice coverage is dense for bias and\ntoxicity but relatively sparse for privacy and provenance, deepfake and media\nintegrity risk, and system level failure in tool using and agentic settings.\nSecond, many evaluations remain static and task local, which limits evidence\nportability for audit and lifecycle assurance. Third, documentation and metric\nvalidity are inconsistent, which complicates comparison across releases and\ndomains. We outline a research and practice agenda that prioritizes adaptive\nand multimodal evaluation, privacy and provenance testing, deepfake risk\nassessment, calibration and uncertainty reporting, versioned and documented\nartifacts, and continuous monitoring. Limitations include reliance on public\nartifacts and the focus period, which may under represent capabilities reported\nlater. The survey offers a path to align development and evaluation with\ngovernance needs and to support safe, transparent, and accountable deployment\nacross domains. Project page:\nhttps://anas-zafar.github.io/responsible-ai.github.io , GitHub:\nhttps://github.com/anas-zafar/Responsible-AI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI is moving rapidly from research into real world deployment\nacross sectors, which elevates the need for responsible development,\ndeployment, evaluation, and governance. To address this pressing challenge, in\nthis study, we synthesize the landscape of responsible generative AI across\nmethods, benchmarks, and policies, and connects governance expectations to\nconcrete engineering practice. We follow a prespecified search and screening\nprotocol focused on post-ChatGPT era with selective inclusion of foundational\nwork for definitions, and we conduct a narrative and thematic synthesis. Three\nfindings emerge; First, benchmark and practice coverage is dense for bias and\ntoxicity but relatively sparse for privacy and provenance, deepfake and media\nintegrity risk, and system level failure in tool using and agentic settings.\nSecond, many evaluations remain static and task local, which limits evidence\nportability for audit and lifecycle assurance. Third, documentation and metric\nvalidity are inconsistent, which complicates comparison across releases and\ndomains. We outline a research and practice agenda that prioritizes adaptive\nand multimodal evaluation, privacy and provenance testing, deepfake risk\nassessment, calibration and uncertainty reporting, versioned and documented\nartifacts, and continuous monitoring. Limitations include reliance on public\nartifacts and the focus period, which may under represent capabilities reported\nlater. The survey offers a path to align development and evaluation with\ngovernance needs and to support safe, transparent, and accountable deployment\nacross domains. Project page:\nhttps://anas-zafar.github.io/responsible-ai.github.io , GitHub:\nhttps://github.com/anas-zafar/Responsible-AI"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Rizwan Qureshi"
                    },
                    {
                        "name": "Anam Zahid"
                    },
                    {
                        "name": "Safiullah Kamawal"
                    },
                    {
                        "name": "Ferhat Sadak"
                    },
                    {
                        "name": "Joseph Fioresi"
                    },
                    {
                        "name": "Muhammaed Saeed"
                    },
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Aditya Jain"
                    },
                    {
                        "name": "Anas Zafar"
                    },
                    {
                        "name": "Muneeb Ul Hassan"
                    },
                    {
                        "name": "Aizan Zafar"
                    },
                    {
                        "name": "Hasan Maqbool"
                    },
                    {
                        "name": "Ashmal Vayani"
                    },
                    {
                        "name": "Jia Wu"
                    },
                    {
                        "name": "Maged Shoman"
                    }
                ],
                "author_detail": {
                    "name": "Maged Shoman"
                },
                "author": "Maged Shoman",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08650v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08650v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20270v1",
                "updated": "2025-09-24T16:04:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    4,
                    11,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T16:04:11Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    4,
                    11,
                    2,
                    267,
                    0
                ],
                "title": "Scan-do Attitude: Towards Autonomous CT Protocol Management using a\n  Large Language Model Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scan-do Attitude: Towards Autonomous CT Protocol Management using a\n  Large Language Model Agent"
                },
                "summary": "Managing scan protocols in Computed Tomography (CT), which includes adjusting\nacquisition parameters or configuring reconstructions, as well as selecting\npostprocessing tools in a patient-specific manner, is time-consuming and\nrequires clinical as well as technical expertise. At the same time, we observe\nan increasing shortage of skilled workforce in radiology. To address this\nissue, a Large Language Model (LLM)-based agent framework is proposed to assist\nwith the interpretation and execution of protocol configuration requests given\nin natural language or a structured, device-independent format, aiming to\nimprove the workflow efficiency and reduce technologists' workload. The agent\ncombines in-context-learning, instruction-following, and structured toolcalling\nabilities to identify relevant protocol elements and apply accurate\nmodifications. In a systematic evaluation, experimental results indicate that\nthe agent can effectively retrieve protocol components, generate device\ncompatible protocol definition files, and faithfully implement user requests.\nDespite demonstrating feasibility in principle, the approach faces limitations\nregarding syntactic and semantic validity due to lack of a unified device API,\nand challenges with ambiguous or complex requests. In summary, the findings\nshow a clear path towards LLM-based agents for supporting scan protocol\nmanagement in CT imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing scan protocols in Computed Tomography (CT), which includes adjusting\nacquisition parameters or configuring reconstructions, as well as selecting\npostprocessing tools in a patient-specific manner, is time-consuming and\nrequires clinical as well as technical expertise. At the same time, we observe\nan increasing shortage of skilled workforce in radiology. To address this\nissue, a Large Language Model (LLM)-based agent framework is proposed to assist\nwith the interpretation and execution of protocol configuration requests given\nin natural language or a structured, device-independent format, aiming to\nimprove the workflow efficiency and reduce technologists' workload. The agent\ncombines in-context-learning, instruction-following, and structured toolcalling\nabilities to identify relevant protocol elements and apply accurate\nmodifications. In a systematic evaluation, experimental results indicate that\nthe agent can effectively retrieve protocol components, generate device\ncompatible protocol definition files, and faithfully implement user requests.\nDespite demonstrating feasibility in principle, the approach faces limitations\nregarding syntactic and semantic validity due to lack of a unified device API,\nand challenges with ambiguous or complex requests. In summary, the findings\nshow a clear path towards LLM-based agents for supporting scan protocol\nmanagement in CT imaging."
                },
                "authors": [
                    {
                        "name": "Xingjian Kang"
                    },
                    {
                        "name": "Linda Vorberg"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Alexander Katzmann"
                    },
                    {
                        "name": "Oliver Taubmann"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Taubmann"
                },
                "author": "Oliver Taubmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20267v1",
                "updated": "2025-09-24T15:59:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    59,
                    6,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:59:06Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    59,
                    6,
                    2,
                    267,
                    0
                ],
                "title": "Radiation-induced Ionization Effects and Space Mission Requirements for\n  Silicon Photonic Mach-Zehnder Modulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiation-induced Ionization Effects and Space Mission Requirements for\n  Silicon Photonic Mach-Zehnder Modulators"
                },
                "summary": "Photonic integrated circuits have become essential for meeting the growing\nglobal demand for high-capacity information processing and transport. Assessing\ntheir radiation tolerance is essential for deploying systems in radiation prone\nenvironments - including in space, high-energy particle accelerators, and\ndefense radiation testing facilities - where the performance and compactness of\nphotonic integrated circuits are increasingly advantageous. This work\ninvestigates the analog and digital radio frequency electro-optic performance\nof Mach-Zehnder modulators (MZMs) subject to 10-keV X-ray irradiation, which\nmimics cumulative ionization effects in space flight. Silicon photonic MZMs\nserve as excellent exemplars since they are interferometric devices comprised\nof elements common to many integrated photonic circuits. Under standard bias\nconditions, the irradiated MZMs exhibited significantly reduced bandwidth, a\ncorresponding eye closure and baud rate dependent increases in the estimated\nerror rate. The observed performance degradation is attributed to total\nionizing dose effects which leads to hole trapping at the silicon/silicon\ndioxide waveguide interfaces as well as fast traps with energies near the\nconduction band edge. Notably, when MZMs were irradiated with all leads\ngrounded, no radiation sensitivity to the electro-optic response was observed\nhighlighting the importance of testing under standard operating conditions for\nground-based radiation testing as well as on-orbit studies. Understanding the\nradiation induced performance degradation of MZMs and other integrated photonic\ndevices is increasingly important for space and accelerator environments as\nperformance requirements and deployment opportunities increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photonic integrated circuits have become essential for meeting the growing\nglobal demand for high-capacity information processing and transport. Assessing\ntheir radiation tolerance is essential for deploying systems in radiation prone\nenvironments - including in space, high-energy particle accelerators, and\ndefense radiation testing facilities - where the performance and compactness of\nphotonic integrated circuits are increasingly advantageous. This work\ninvestigates the analog and digital radio frequency electro-optic performance\nof Mach-Zehnder modulators (MZMs) subject to 10-keV X-ray irradiation, which\nmimics cumulative ionization effects in space flight. Silicon photonic MZMs\nserve as excellent exemplars since they are interferometric devices comprised\nof elements common to many integrated photonic circuits. Under standard bias\nconditions, the irradiated MZMs exhibited significantly reduced bandwidth, a\ncorresponding eye closure and baud rate dependent increases in the estimated\nerror rate. The observed performance degradation is attributed to total\nionizing dose effects which leads to hole trapping at the silicon/silicon\ndioxide waveguide interfaces as well as fast traps with energies near the\nconduction band edge. Notably, when MZMs were irradiated with all leads\ngrounded, no radiation sensitivity to the electro-optic response was observed\nhighlighting the importance of testing under standard operating conditions for\nground-based radiation testing as well as on-orbit studies. Understanding the\nradiation induced performance degradation of MZMs and other integrated photonic\ndevices is increasingly important for space and accelerator environments as\nperformance requirements and deployment opportunities increase."
                },
                "authors": [
                    {
                        "name": "Kellen P. Arnold"
                    },
                    {
                        "name": "Joel B. Slaby"
                    },
                    {
                        "name": "Nathaniel J. Karom"
                    },
                    {
                        "name": "Anurag R. Veluri"
                    },
                    {
                        "name": "C. Alex Kaylor"
                    },
                    {
                        "name": "Andrew L. Sternberg"
                    },
                    {
                        "name": "Dennis R. Ball"
                    },
                    {
                        "name": "Ronald D. Schrimpf"
                    },
                    {
                        "name": "Daniel M. Fleetwood"
                    },
                    {
                        "name": "Stephen E. Ralph"
                    },
                    {
                        "name": "Robert A. Reed"
                    },
                    {
                        "name": "Sharon M. Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Sharon M. Weiss"
                },
                "author": "Sharon M. Weiss",
                "arxiv_comment": "22 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14264v2",
                "updated": "2025-09-24T15:47:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    47,
                    49,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-20T12:13:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    12,
                    13,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "AAPO: Enhancing the Reasoning Capabilities of LLMs with Advantage\n  Momentum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AAPO: Enhancing the Reasoning Capabilities of LLMs with Advantage\n  Momentum"
                },
                "summary": "Reinforcement learning (RL) has emerged as an effective approach for\nenhancing the reasoning capabilities of large language models (LLMs),\nespecially in scenarios where supervised fine-tuning (SFT) falls short due to\nlimited chain-of-thought (CoT) data. Among RL-based post-training methods,\ngroup relative advantage estimation, as exemplified by Group Relative Policy\nOptimization (GRPO), has attracted considerable attention for eliminating the\ndependency on the value model, thereby simplifying training compared to\ntraditional approaches like Proximal Policy Optimization (PPO). However, we\nobserve that exsiting group relative advantage estimation method still suffers\nfrom training inefficiencies, particularly when the estimated advantage\napproaches zero. To address this limitation, we propose Advantage-Augmented\nPolicy Optimization (AAPO), a novel RL algorithm that optimizes the\ncross-entropy (CE) loss using advantages enhanced through a momentum-based\nestimation scheme. This approach effectively mitigates the inefficiencies\nassociated with group relative advantage estimation. Experimental results on\nmultiple mathematical reasoning benchmarks demonstrate the superior performance\nof AAPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has emerged as an effective approach for\nenhancing the reasoning capabilities of large language models (LLMs),\nespecially in scenarios where supervised fine-tuning (SFT) falls short due to\nlimited chain-of-thought (CoT) data. Among RL-based post-training methods,\ngroup relative advantage estimation, as exemplified by Group Relative Policy\nOptimization (GRPO), has attracted considerable attention for eliminating the\ndependency on the value model, thereby simplifying training compared to\ntraditional approaches like Proximal Policy Optimization (PPO). However, we\nobserve that exsiting group relative advantage estimation method still suffers\nfrom training inefficiencies, particularly when the estimated advantage\napproaches zero. To address this limitation, we propose Advantage-Augmented\nPolicy Optimization (AAPO), a novel RL algorithm that optimizes the\ncross-entropy (CE) loss using advantages enhanced through a momentum-based\nestimation scheme. This approach effectively mitigates the inefficiencies\nassociated with group relative advantage estimation. Experimental results on\nmultiple mathematical reasoning benchmarks demonstrate the superior performance\nof AAPO."
                },
                "authors": [
                    {
                        "name": "Jian Xiong"
                    },
                    {
                        "name": "Jingbo Zhou"
                    },
                    {
                        "name": "Jingyong Ye"
                    },
                    {
                        "name": "Qiang Huang"
                    },
                    {
                        "name": "Dejing Dou"
                    }
                ],
                "author_detail": {
                    "name": "Dejing Dou"
                },
                "author": "Dejing Dou",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03547v2",
                "updated": "2025-09-24T15:41:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    41,
                    44,
                    2,
                    267,
                    0
                ],
                "published": "2025-08-05T15:15:35Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    15,
                    15,
                    35,
                    1,
                    217,
                    0
                ],
                "title": "Guided Reality: Generating Visually-Enriched AR Task Guidance with LLMs\n  and Vision Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided Reality: Generating Visually-Enriched AR Task Guidance with LLMs\n  and Vision Models"
                },
                "summary": "Large language models (LLMs) have enabled the automatic generation of\nstep-by-step augmented reality (AR) instructions for a wide range of physical\ntasks. However, existing LLM-based AR guidance often lacks rich visual\naugmentations to effectively embed instructions into spatial context for a\nbetter user understanding. We present Guided Reality, a fully automated AR\nsystem that generates embedded and dynamic visual guidance based on\nstep-by-step instructions. Our system integrates LLMs and vision models to: 1)\ngenerate multi-step instructions from user queries, 2) identify appropriate\ntypes of visual guidance, 3) extract spatial information about key interaction\npoints in the real world, and 4) embed visual guidance in physical space to\nsupport task execution. Drawing from a corpus of user manuals, we define five\ncategories of visual guidance and propose an identification strategy based on\nthe current step. We evaluate the system through a user study (N=16),\ncompleting real-world tasks and exploring the system in the wild. Additionally,\nfour instructors shared insights on how Guided Reality could be integrated into\ntheir training workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have enabled the automatic generation of\nstep-by-step augmented reality (AR) instructions for a wide range of physical\ntasks. However, existing LLM-based AR guidance often lacks rich visual\naugmentations to effectively embed instructions into spatial context for a\nbetter user understanding. We present Guided Reality, a fully automated AR\nsystem that generates embedded and dynamic visual guidance based on\nstep-by-step instructions. Our system integrates LLMs and vision models to: 1)\ngenerate multi-step instructions from user queries, 2) identify appropriate\ntypes of visual guidance, 3) extract spatial information about key interaction\npoints in the real world, and 4) embed visual guidance in physical space to\nsupport task execution. Drawing from a corpus of user manuals, we define five\ncategories of visual guidance and propose an identification strategy based on\nthe current step. We evaluate the system through a user study (N=16),\ncompleting real-world tasks and exploring the system in the wild. Additionally,\nfour instructors shared insights on how Guided Reality could be integrated into\ntheir training workflows."
                },
                "authors": [
                    {
                        "name": "Ada Yi Zhao"
                    },
                    {
                        "name": "Aditya Gunturu"
                    },
                    {
                        "name": "Ellen Yi-Luen Do"
                    },
                    {
                        "name": "Ryo Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Ryo Suzuki"
                },
                "author": "Ryo Suzuki",
                "arxiv_doi": "10.1145/3746059.3747784",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746059.3747784",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.03547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear at UIST 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21179v2",
                "updated": "2025-09-24T15:38:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    38,
                    14,
                    2,
                    267,
                    0
                ],
                "published": "2025-07-26T15:50:08Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    50,
                    8,
                    5,
                    207,
                    0
                ],
                "title": "CANDLE: A Cross-Modal Agentic Knowledge Distillation Framework for\n  Interpretable Sarcopenia Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CANDLE: A Cross-Modal Agentic Knowledge Distillation Framework for\n  Interpretable Sarcopenia Diagnosis"
                },
                "summary": "Background and Aims: Large language models (LLMs) have shown remarkable\ngeneralization and transfer capabilities by learning from vast corpora of text\nand web data. Their semantic representations allow cross-task knowledge\ntransfer and reasoning, offering promising opportunities for data-scarce and\nheterogeneous domains such as clinical medicine. Yet, in diagnostic tasks like\nsarcopenia, major challenges remain: interpretability, transparency, and\ndeployment efficiency. Traditional machine learning (TML) models provide stable\nperformance and feature-level attribution, ensuring traceable and auditable\ndecision logic, but lack semantic breadth. Conversely, LLMs enable flexible\ninference but often function as opaque predictors. Existing integration\nstrategies remain shallow, rarely embedding the structured reasoning of TML\ninto LLM inference. Methods: Using sarcopenia diagnosis as a case study,\nSHapley Additive exPlanations (SHAP) were extracted from a baseline XGBoost\nmodel and transformed into structured, LLM-compatible representations. An\nactor-critic reinforcement learning (RL) strategy guided the LLM to reason over\nthese SHAP-based inputs, producing calibrated rationales and refined decision\nrules. The distilled reasoning was consolidated into a structured knowledge\nrepository and deployed via retrieval-augmented generation (RAG) for case-based\ninference. Results: (Omitted here.) Conclusion: By coupling SHAP-derived\nstatistical evidence with reinforcement-trained LLM reasoning, CANDLE mitigates\nthe interpretability-performance trade-off, enhances predictive accuracy, and\npreserves high decision consistency. The framework offers a scalable approach\nto knowledge assetization of TML models, enabling interpretable, reproducible,\nand clinically aligned decision support in sarcopenia and potentially broader\nmedical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background and Aims: Large language models (LLMs) have shown remarkable\ngeneralization and transfer capabilities by learning from vast corpora of text\nand web data. Their semantic representations allow cross-task knowledge\ntransfer and reasoning, offering promising opportunities for data-scarce and\nheterogeneous domains such as clinical medicine. Yet, in diagnostic tasks like\nsarcopenia, major challenges remain: interpretability, transparency, and\ndeployment efficiency. Traditional machine learning (TML) models provide stable\nperformance and feature-level attribution, ensuring traceable and auditable\ndecision logic, but lack semantic breadth. Conversely, LLMs enable flexible\ninference but often function as opaque predictors. Existing integration\nstrategies remain shallow, rarely embedding the structured reasoning of TML\ninto LLM inference. Methods: Using sarcopenia diagnosis as a case study,\nSHapley Additive exPlanations (SHAP) were extracted from a baseline XGBoost\nmodel and transformed into structured, LLM-compatible representations. An\nactor-critic reinforcement learning (RL) strategy guided the LLM to reason over\nthese SHAP-based inputs, producing calibrated rationales and refined decision\nrules. The distilled reasoning was consolidated into a structured knowledge\nrepository and deployed via retrieval-augmented generation (RAG) for case-based\ninference. Results: (Omitted here.) Conclusion: By coupling SHAP-derived\nstatistical evidence with reinforcement-trained LLM reasoning, CANDLE mitigates\nthe interpretability-performance trade-off, enhances predictive accuracy, and\npreserves high decision consistency. The framework offers a scalable approach\nto knowledge assetization of TML models, enabling interpretable, reproducible,\nand clinically aligned decision support in sarcopenia and potentially broader\nmedical domains."
                },
                "authors": [
                    {
                        "name": "Yuqi Jin"
                    },
                    {
                        "name": "Zhenhao Shuai"
                    },
                    {
                        "name": "Zihan Hu"
                    },
                    {
                        "name": "Weiteng Zhang"
                    },
                    {
                        "name": "Weihao Xie"
                    },
                    {
                        "name": "Jianwei Shuai"
                    },
                    {
                        "name": "Xian Shen"
                    },
                    {
                        "name": "Zhen Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Feng"
                },
                "author": "Zhen Feng",
                "arxiv_comment": "11 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20241v1",
                "updated": "2025-09-24T15:32:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    32,
                    1,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:32:01Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    32,
                    1,
                    2,
                    267,
                    0
                ],
                "title": "Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute"
                },
                "summary": "As AI inference scales to billions of queries and emerging reasoning and\nagentic workflows increase token demand, reliable estimates of per-query energy\nuse are increasingly important for capacity planning, emissions accounting, and\nefficiency prioritization. Many public estimates are inconsistent and overstate\nenergy use, because they extrapolate from limited benchmarks and fail to\nreflect efficiency gains achievable at scale. In this perspective, we introduce\na bottom-up methodology to estimate the per-query energy of large-scale LLM\nsystems based on token throughput. For models running on an H100 node under\nrealistic workloads, GPU utilization and PUE constraints, we estimate a median\nenergy per query of 0.34 Wh (IQR: 0.18-0.67) for frontier-scale models (>200\nbillion parameters). These results are consistent with measurements using\nproduction-scale configurations and show that non-production estimates and\nassumptions can overstate energy use by 4-20x. Extending to test-time scaling\nscenarios with 15x more tokens per typical query, the median energy rises 13x\nto 4.32 Wh, indicating that targeting efficiency in this regime will deliver\nthe largest fleet-wide savings. We quantify achievable efficiency gains at the\nmodel, serving platform, and hardware levels, finding individual median\nreductions of 1.5-3.5x in energy per query, while combined advances can\nplausibly deliver 8-20x reductions. To illustrate the system-level impact, we\nestimate the baseline daily energy use of a deployment serving 1 billion\nqueries to be 0.8 GWh/day. If 10% are long queries, demand could grow to 1.8\nGWh/day. With targeted efficiency interventions, it falls to 0.9 GWh/day,\nsimilar to the energy footprint of web search at that scale. This echoes how\ndata centers historically tempered energy growth through efficiency gains\nduring the internet and cloud build-up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI inference scales to billions of queries and emerging reasoning and\nagentic workflows increase token demand, reliable estimates of per-query energy\nuse are increasingly important for capacity planning, emissions accounting, and\nefficiency prioritization. Many public estimates are inconsistent and overstate\nenergy use, because they extrapolate from limited benchmarks and fail to\nreflect efficiency gains achievable at scale. In this perspective, we introduce\na bottom-up methodology to estimate the per-query energy of large-scale LLM\nsystems based on token throughput. For models running on an H100 node under\nrealistic workloads, GPU utilization and PUE constraints, we estimate a median\nenergy per query of 0.34 Wh (IQR: 0.18-0.67) for frontier-scale models (>200\nbillion parameters). These results are consistent with measurements using\nproduction-scale configurations and show that non-production estimates and\nassumptions can overstate energy use by 4-20x. Extending to test-time scaling\nscenarios with 15x more tokens per typical query, the median energy rises 13x\nto 4.32 Wh, indicating that targeting efficiency in this regime will deliver\nthe largest fleet-wide savings. We quantify achievable efficiency gains at the\nmodel, serving platform, and hardware levels, finding individual median\nreductions of 1.5-3.5x in energy per query, while combined advances can\nplausibly deliver 8-20x reductions. To illustrate the system-level impact, we\nestimate the baseline daily energy use of a deployment serving 1 billion\nqueries to be 0.8 GWh/day. If 10% are long queries, demand could grow to 1.8\nGWh/day. With targeted efficiency interventions, it falls to 0.9 GWh/day,\nsimilar to the energy footprint of web search at that scale. This echoes how\ndata centers historically tempered energy growth through efficiency gains\nduring the internet and cloud build-up."
                },
                "authors": [
                    {
                        "name": "Felipe Oviedo"
                    },
                    {
                        "name": "Fiodar Kazhamiaka"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Allen Kim"
                    },
                    {
                        "name": "Amy Luers"
                    },
                    {
                        "name": "Melanie Nakagawa"
                    },
                    {
                        "name": "Ricardo Bianchini"
                    },
                    {
                        "name": "Juan M. Lavista Ferres"
                    }
                ],
                "author_detail": {
                    "name": "Juan M. Lavista Ferres"
                },
                "author": "Juan M. Lavista Ferres",
                "arxiv_comment": "A preprint version with DOI is available at Zenodo:\n  https://doi.org/10.5281/zenodo.17188770",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18920v2",
                "updated": "2025-09-24T15:30:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    30,
                    3,
                    2,
                    267,
                    0
                ],
                "published": "2024-07-09T07:02:57Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    7,
                    2,
                    57,
                    1,
                    191,
                    0
                ],
                "title": "Context-Masked Meta-Prompting for Privacy-Preserving LLM Adaptation in\n  Finance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Masked Meta-Prompting for Privacy-Preserving LLM Adaptation in\n  Finance"
                },
                "summary": "The increasing reliance on Large Language Models (LLMs) in sensitive domains\nlike finance necessitates robust methods for privacy preservation and\nregulatory compliance. This paper presents an iterative meta-prompting\nmethodology designed to optimise hard prompts without exposing proprietary or\nconfidential context to the LLM. Through a novel regeneration process involving\nfeeder and propagation methods, we demonstrate significant improvements in\nprompt efficacy. Evaluated on public datasets serving as proxies for financial\ntasks such as SQuAD for extractive financial Q&A, CNN/DailyMail for news\nsummarisation, and SAMSum for client interaction summarisation, our approach,\nutilising GPT-3.5 Turbo, achieved a 103.87% improvement in ROUGE-L F1 for\nquestion answering. This work highlights a practical, low-cost strategy for\nadapting LLMs to financial applications while upholding critical privacy and\nauditability standards, offering a compelling case for its relevance in the\nevolving landscape of generative AI in finance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing reliance on Large Language Models (LLMs) in sensitive domains\nlike finance necessitates robust methods for privacy preservation and\nregulatory compliance. This paper presents an iterative meta-prompting\nmethodology designed to optimise hard prompts without exposing proprietary or\nconfidential context to the LLM. Through a novel regeneration process involving\nfeeder and propagation methods, we demonstrate significant improvements in\nprompt efficacy. Evaluated on public datasets serving as proxies for financial\ntasks such as SQuAD for extractive financial Q&A, CNN/DailyMail for news\nsummarisation, and SAMSum for client interaction summarisation, our approach,\nutilising GPT-3.5 Turbo, achieved a 103.87% improvement in ROUGE-L F1 for\nquestion answering. This work highlights a practical, low-cost strategy for\nadapting LLMs to financial applications while upholding critical privacy and\nauditability standards, offering a compelling case for its relevance in the\nevolving landscape of generative AI in finance."
                },
                "authors": [
                    {
                        "name": "Sayash Raaj Hiraou"
                    }
                ],
                "author_detail": {
                    "name": "Sayash Raaj Hiraou"
                },
                "author": "Sayash Raaj Hiraou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04931v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04931v3",
                "updated": "2025-09-24T15:24:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    24,
                    43,
                    2,
                    267,
                    0
                ],
                "published": "2023-12-08T09:48:36Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    9,
                    48,
                    36,
                    4,
                    342,
                    0
                ],
                "title": "Long Video Understanding with Learnable Retrieval in Video-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Video Understanding with Learnable Retrieval in Video-Language\n  Models"
                },
                "summary": "The remarkable natural language understanding, reasoning, and generation\ncapabilities of large language models (LLMs) have made them attractive for\napplication to video understanding, utilizing video tokens as contextual input.\nHowever, employing LLMs for long video understanding presents significant\nchallenges. The extensive number of video tokens leads to considerable\ncomputational costs for LLMs while using aggregated tokens results in loss of\nvision details. Moreover, the presence of abundant question-irrelevant tokens\nintroduces noise to the video reasoning process. To address these issues, we\nintroduce a simple yet effective learnable retrieval-based video-language model\n(R-VLM) for efficient long video understanding. Specifically, given a question\n(query) and a long video, our model identifies and selects the most relevant K\nvideo chunks and uses their associated visual tokens to serve as context for\nthe LLM inference. This effectively reduces the number of video tokens,\neliminates noise interference, and enhances system performance. We achieve this\nby incorporating a learnable lightweight MLP block to facilitate the efficient\nretrieval of question-relevant chunks, through the end-to-end training of our\nvideo-language model with a proposed soft matching loss. Our experimental\nresults on multiple zero-shot video question answering datasets validate the\neffectiveness of our framework for comprehending long videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable natural language understanding, reasoning, and generation\ncapabilities of large language models (LLMs) have made them attractive for\napplication to video understanding, utilizing video tokens as contextual input.\nHowever, employing LLMs for long video understanding presents significant\nchallenges. The extensive number of video tokens leads to considerable\ncomputational costs for LLMs while using aggregated tokens results in loss of\nvision details. Moreover, the presence of abundant question-irrelevant tokens\nintroduces noise to the video reasoning process. To address these issues, we\nintroduce a simple yet effective learnable retrieval-based video-language model\n(R-VLM) for efficient long video understanding. Specifically, given a question\n(query) and a long video, our model identifies and selects the most relevant K\nvideo chunks and uses their associated visual tokens to serve as context for\nthe LLM inference. This effectively reduces the number of video tokens,\neliminates noise interference, and enhances system performance. We achieve this\nby incorporating a learnable lightweight MLP block to facilitate the efficient\nretrieval of question-relevant chunks, through the end-to-end training of our\nvideo-language model with a proposed soft matching loss. Our experimental\nresults on multiple zero-shot video question answering datasets validate the\neffectiveness of our framework for comprehending long videos."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xu"
                    },
                    {
                        "name": "Cuiling Lan"
                    },
                    {
                        "name": "Wenxuan Xie"
                    },
                    {
                        "name": "Xuejin Chen"
                    },
                    {
                        "name": "Yan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Lu"
                },
                "author": "Yan Lu",
                "arxiv_comment": "Accepted by IEEE Transactions on Multimedia (TMM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04931v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04931v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20230v1",
                "updated": "2025-09-24T15:23:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    23,
                    46,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:23:46Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    23,
                    46,
                    2,
                    267,
                    0
                ],
                "title": "Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided\n  Multi-Point Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided\n  Multi-Point Optimization"
                },
                "summary": "Current LLM unlearning methods face a critical security vulnerability that\nundermines their fundamental purpose: while they appear to successfully remove\nsensitive or harmful knowledge, this ``forgotten\" information remains\nprecariously recoverable through relearning attacks. We identify that the root\ncause is that conventional methods optimizing the forgetting loss at individual\ndata points will drive model parameters toward sharp minima in the loss\nlandscape. In these unstable regions, even minimal parameter perturbations can\ndrastically alter the model's behaviors. Consequently, relearning attacks\nexploit this vulnerability by using just a few fine-tuning samples to navigate\nthe steep gradients surrounding these unstable regions, thereby rapidly\nrecovering knowledge that was supposedly erased. This exposes a critical\nrobustness gap between apparent unlearning and actual knowledge removal. To\naddress this issue, we propose StableUN, a bi-level feedback-guided\noptimization framework that explicitly seeks more stable parameter regions via\nneighborhood-aware optimization. It integrates forgetting feedback, which uses\nadversarial perturbations to probe parameter neighborhoods, with remembering\nfeedback to preserve model utility, aligning the two objectives through\ngradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that\nour method is significantly more robust against both relearning and\njailbreaking attacks while maintaining competitive utility performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM unlearning methods face a critical security vulnerability that\nundermines their fundamental purpose: while they appear to successfully remove\nsensitive or harmful knowledge, this ``forgotten\" information remains\nprecariously recoverable through relearning attacks. We identify that the root\ncause is that conventional methods optimizing the forgetting loss at individual\ndata points will drive model parameters toward sharp minima in the loss\nlandscape. In these unstable regions, even minimal parameter perturbations can\ndrastically alter the model's behaviors. Consequently, relearning attacks\nexploit this vulnerability by using just a few fine-tuning samples to navigate\nthe steep gradients surrounding these unstable regions, thereby rapidly\nrecovering knowledge that was supposedly erased. This exposes a critical\nrobustness gap between apparent unlearning and actual knowledge removal. To\naddress this issue, we propose StableUN, a bi-level feedback-guided\noptimization framework that explicitly seeks more stable parameter regions via\nneighborhood-aware optimization. It integrates forgetting feedback, which uses\nadversarial perturbations to probe parameter neighborhoods, with remembering\nfeedback to preserve model utility, aligning the two objectives through\ngradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that\nour method is significantly more robust against both relearning and\njailbreaking attacks while maintaining competitive utility performance."
                },
                "authors": [
                    {
                        "name": "Wenhan Wu"
                    },
                    {
                        "name": "Zheyuan Liu"
                    },
                    {
                        "name": "Chongyang Gao"
                    },
                    {
                        "name": "Ren Wang"
                    },
                    {
                        "name": "Kaize Ding"
                    }
                ],
                "author_detail": {
                    "name": "Kaize Ding"
                },
                "author": "Kaize Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20229v1",
                "updated": "2025-09-24T15:22:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    22,
                    32,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:22:32Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    22,
                    32,
                    2,
                    267,
                    0
                ],
                "title": "Techno-Economic analysis for Smart Hangar inspection operations through\n  Sensing and Localisation at scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techno-Economic analysis for Smart Hangar inspection operations through\n  Sensing and Localisation at scale"
                },
                "summary": "The accuracy, resilience, and affordability of localisation are fundamental\nto autonomous robotic inspection within aircraft maintenance and overhaul (MRO)\nhangars. Hangars typically feature tall ceilings and are often made of\nmaterials such as metal. Due to its nature, it is considered a GPS-denied\nenvironment, with extensive multipath effects and stringent operational\nconstraints that collectively create a uniquely challenging environment. This\npersistent gap highlights the need for domain-specific comparative studies,\nincluding rigorous cost, accuracy, and integration assessments, to inform a\nreliable and scalable deployment of a localisation system in the Smart Hangar.\nThis paper presents the first techno-economic roadmap that benchmarks motion\ncapture (MoCap), ultra-wideband (UWB), and a ceiling-mounted camera network\nacross three operational scenarios: robot localisation, asset tracking, and\nsurface defect detection within a 40x50 m hangar bay. A dual-layer optimisation\nfor camera selection and positioning framework is introduced, which couples\nmarket-based camera-lens selection with an optimisation solver, producing\ncamera layouts that minimise hardware while meeting accuracy targets. The\nroadmap equips MRO planners with an actionable method to balance accuracy,\ncoverage, and budget, demonstrating that an optimised vision architecture has\nthe potential to unlock robust and cost-effective sensing for next-generation\nSmart Hangars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The accuracy, resilience, and affordability of localisation are fundamental\nto autonomous robotic inspection within aircraft maintenance and overhaul (MRO)\nhangars. Hangars typically feature tall ceilings and are often made of\nmaterials such as metal. Due to its nature, it is considered a GPS-denied\nenvironment, with extensive multipath effects and stringent operational\nconstraints that collectively create a uniquely challenging environment. This\npersistent gap highlights the need for domain-specific comparative studies,\nincluding rigorous cost, accuracy, and integration assessments, to inform a\nreliable and scalable deployment of a localisation system in the Smart Hangar.\nThis paper presents the first techno-economic roadmap that benchmarks motion\ncapture (MoCap), ultra-wideband (UWB), and a ceiling-mounted camera network\nacross three operational scenarios: robot localisation, asset tracking, and\nsurface defect detection within a 40x50 m hangar bay. A dual-layer optimisation\nfor camera selection and positioning framework is introduced, which couples\nmarket-based camera-lens selection with an optimisation solver, producing\ncamera layouts that minimise hardware while meeting accuracy targets. The\nroadmap equips MRO planners with an actionable method to balance accuracy,\ncoverage, and budget, demonstrating that an optimised vision architecture has\nthe potential to unlock robust and cost-effective sensing for next-generation\nSmart Hangars."
                },
                "authors": [
                    {
                        "name": "Angelos Plastropoulos"
                    },
                    {
                        "name": "Nicolas P. Avdelidis"
                    },
                    {
                        "name": "Argyrios Zolotas"
                    }
                ],
                "author_detail": {
                    "name": "Argyrios Zolotas"
                },
                "author": "Argyrios Zolotas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20218v1",
                "updated": "2025-09-24T15:15:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    15,
                    5,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:15:05Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    15,
                    5,
                    2,
                    267,
                    0
                ],
                "title": "Design Insights and Comparative Evaluation of a Hardware-Based\n  Cooperative Perception Architecture for Lane Change Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design Insights and Comparative Evaluation of a Hardware-Based\n  Cooperative Perception Architecture for Lane Change Prediction"
                },
                "summary": "Research on lane change prediction has gained attention in the last few\nyears. Most existing works in this area have been conducted in simulation\nenvironments or with pre-recorded datasets, these works often rely on\nsimplified assumptions about sensing, communication, and traffic behavior that\ndo not always hold in practice. Real-world deployments of lane-change\nprediction systems are relatively rare, and when they are reported, the\npractical challenges, limitations, and lessons learned are often\nunder-documented. This study explores cooperative lane-change prediction\nthrough a real hardware deployment in mixed traffic and shares the insights\nthat emerged during implementation and testing. We highlight the practical\nchallenges we faced, including bottlenecks, reliability issues, and operational\nconstraints that shaped the behavior of the system. By documenting these\nexperiences, the study provides guidance for others working on similar\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on lane change prediction has gained attention in the last few\nyears. Most existing works in this area have been conducted in simulation\nenvironments or with pre-recorded datasets, these works often rely on\nsimplified assumptions about sensing, communication, and traffic behavior that\ndo not always hold in practice. Real-world deployments of lane-change\nprediction systems are relatively rare, and when they are reported, the\npractical challenges, limitations, and lessons learned are often\nunder-documented. This study explores cooperative lane-change prediction\nthrough a real hardware deployment in mixed traffic and shares the insights\nthat emerged during implementation and testing. We highlight the practical\nchallenges we faced, including bottlenecks, reliability issues, and operational\nconstraints that shaped the behavior of the system. By documenting these\nexperiences, the study provides guidance for others working on similar\npipelines."
                },
                "authors": [
                    {
                        "name": "Mohamed Manzour"
                    },
                    {
                        "name": "Catherine M. Elias"
                    },
                    {
                        "name": "Omar M. Shehata"
                    },
                    {
                        "name": "Rubén Izquierdo"
                    },
                    {
                        "name": "Miguel Ángel Sotelo"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Ángel Sotelo"
                },
                "author": "Miguel Ángel Sotelo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20215v1",
                "updated": "2025-09-24T15:12:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    12,
                    21,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:12:21Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    12,
                    21,
                    2,
                    267,
                    0
                ],
                "title": "The Cream Rises to the Top: Efficient Reranking Method for Verilog Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Cream Rises to the Top: Efficient Reranking Method for Verilog Code\n  Generation"
                },
                "summary": "LLMs face significant challenges in Verilog generation due to limited\ndomain-specific knowledge. While sampling techniques improve pass@k metrics,\nhardware engineers need one trustworthy solution rather than uncertain\ncandidates. To bridge this gap, we formulate it as a semantic alignment problem\nbetween requirements and Verilog implementations, and propose VCD-RNK, a\ndiscriminator model tailored for efficient Verilog code reranking.\nSpecifically, VCD-RNKincorporates Verilog-specific reasoning by distilling\nexpert knowledge across three dimensions: code semantic analysis, test case\ngeneration, and functional correctness assessment. By explicitly simulating the\nabove reasoning processes during inference, VCD-RNK effectively avoids\ncomputationally intensive test execution in existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs face significant challenges in Verilog generation due to limited\ndomain-specific knowledge. While sampling techniques improve pass@k metrics,\nhardware engineers need one trustworthy solution rather than uncertain\ncandidates. To bridge this gap, we formulate it as a semantic alignment problem\nbetween requirements and Verilog implementations, and propose VCD-RNK, a\ndiscriminator model tailored for efficient Verilog code reranking.\nSpecifically, VCD-RNKincorporates Verilog-specific reasoning by distilling\nexpert knowledge across three dimensions: code semantic analysis, test case\ngeneration, and functional correctness assessment. By explicitly simulating the\nabove reasoning processes during inference, VCD-RNK effectively avoids\ncomputationally intensive test execution in existing methods."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Wei Zheng"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Fengji Zhang"
                    },
                    {
                        "name": "Terry Yue Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Terry Yue Zhuo"
                },
                "author": "Terry Yue Zhuo",
                "arxiv_comment": "Under review ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20214v1",
                "updated": "2025-09-24T15:10:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    10,
                    44,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:10:44Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    10,
                    44,
                    2,
                    267,
                    0
                ],
                "title": "Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for\n  Efficient LLM Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for\n  Efficient LLM Deployment"
                },
                "summary": "We study weight-only post-training quantization (PTQ), which quantizes the\nweights of a large language model (LLM) without retraining, using little or no\ncalibration data. Weight-only PTQ is crucial for reducing the memory footprint\nand latency of LLM inference, especially in memory-bound, small-batch inference\nscenarios, such as personalized inference on edge devices. Despite its\nimportance, irregular weight distributions with heavy-tailed outliers in LLMs\ncomplicate quantization, recently motivating rotation-based methods that\ntransform weights into near-Gaussian distributions, which are more regular with\nfewer outliers, thereby reducing quantization error. In this work, we first\nderive the information-theoretically optimal bit allocation for Gaussianized\nweights under given bit budgets, revealing that fine-grained fractional-bit\nquantizers approaching the Gaussian distortion-rate bound are essential to\nachieve near-optimal quantization performance. To bridge this theoretical\ninsight and practical implementation, we introduce Q-Palette, a versatile\ncollection of fractional-bit quantizers that range from trellis-coded\nquantizers offering near-optimal distortion to simpler vector and scalar\nquantizers optimized for faster inference, all efficiently implemented with\noptimized CUDA kernels across various bitwidths. Furthermore, leveraging\nQ-Palette as a foundational component, we propose a novel mixed-scheme\nquantization framework, jointly optimizing quantizer choices and layer fusion\ndecisions given resource constraints. The code is available at\nhttps://github.com/snu-mllab/Q-Palette.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study weight-only post-training quantization (PTQ), which quantizes the\nweights of a large language model (LLM) without retraining, using little or no\ncalibration data. Weight-only PTQ is crucial for reducing the memory footprint\nand latency of LLM inference, especially in memory-bound, small-batch inference\nscenarios, such as personalized inference on edge devices. Despite its\nimportance, irregular weight distributions with heavy-tailed outliers in LLMs\ncomplicate quantization, recently motivating rotation-based methods that\ntransform weights into near-Gaussian distributions, which are more regular with\nfewer outliers, thereby reducing quantization error. In this work, we first\nderive the information-theoretically optimal bit allocation for Gaussianized\nweights under given bit budgets, revealing that fine-grained fractional-bit\nquantizers approaching the Gaussian distortion-rate bound are essential to\nachieve near-optimal quantization performance. To bridge this theoretical\ninsight and practical implementation, we introduce Q-Palette, a versatile\ncollection of fractional-bit quantizers that range from trellis-coded\nquantizers offering near-optimal distortion to simpler vector and scalar\nquantizers optimized for faster inference, all efficiently implemented with\noptimized CUDA kernels across various bitwidths. Furthermore, leveraging\nQ-Palette as a foundational component, we propose a novel mixed-scheme\nquantization framework, jointly optimizing quantizer choices and layer fusion\ndecisions given resource constraints. The code is available at\nhttps://github.com/snu-mllab/Q-Palette."
                },
                "authors": [
                    {
                        "name": "Deokjae Lee"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00978v2",
                "updated": "2025-09-24T15:07:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    7,
                    8,
                    2,
                    267,
                    0
                ],
                "published": "2025-07-01T17:29:12Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    29,
                    12,
                    1,
                    182,
                    0
                ],
                "title": "Decentralised Multi-Manager Fund Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralised Multi-Manager Fund Framework"
                },
                "summary": "We introduce a decentralised, algorithmic framework for permissionless,\nmulti-strategy capital allocation via tokenised, automated vaults. The system\nis designed to function analogously to a multi-strategy asset management\ncompany, but implemented entirely on-chain through a modular architecture\ncomprising four interacting layers. The first, the capitalisation layer,\ncomposed of vaults that facilitate multi-asset deposits, tokenises investor\nparticipation, and specifies high level risk limits and admissible venues for\ndeployment. The second, the strategy layer, enables the submission of\nstrategies by human developers or autonomous agents, creating a decentralised\nmarketplace governed by a validation mechanism incorporating adversarial and\ngamified elements. The third, the execution layer, operationalises strategy\ndeployment using the host blockchain network's services. The fourth layer, the\nvalidated allocation layer, assesses and allocates capital among validated\nstrategies, dynamically rebalancing toward those exhibiting superior\nrisk-adjusted performance. In the framework, each admitted strategy acts as a\nmanager for the \"fund\", encapsulated in a smart contract vault that issues\ntransferable V-Tokens, conveying fractional ownership of the real-time\nportfolio operated by the vault. The system is designed to be open to\nparticipation by both human and AI agents, who collectively perform the roles\nof capital allocators, strategy developers, and validated allocators. The\nresulting structure is a self-regulating asset management ecosystem capable of\ndecentralised, cooperative optimisation across traditional and digital\nfinancial domains. This framework is facilitated by a host chain network, which\noffers native automation and data oracle services enabling vault entities to\nautonomously operate on-chain, paving the way for being self sufficient in\ndynamic allocation of capital.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a decentralised, algorithmic framework for permissionless,\nmulti-strategy capital allocation via tokenised, automated vaults. The system\nis designed to function analogously to a multi-strategy asset management\ncompany, but implemented entirely on-chain through a modular architecture\ncomprising four interacting layers. The first, the capitalisation layer,\ncomposed of vaults that facilitate multi-asset deposits, tokenises investor\nparticipation, and specifies high level risk limits and admissible venues for\ndeployment. The second, the strategy layer, enables the submission of\nstrategies by human developers or autonomous agents, creating a decentralised\nmarketplace governed by a validation mechanism incorporating adversarial and\ngamified elements. The third, the execution layer, operationalises strategy\ndeployment using the host blockchain network's services. The fourth layer, the\nvalidated allocation layer, assesses and allocates capital among validated\nstrategies, dynamically rebalancing toward those exhibiting superior\nrisk-adjusted performance. In the framework, each admitted strategy acts as a\nmanager for the \"fund\", encapsulated in a smart contract vault that issues\ntransferable V-Tokens, conveying fractional ownership of the real-time\nportfolio operated by the vault. The system is designed to be open to\nparticipation by both human and AI agents, who collectively perform the roles\nof capital allocators, strategy developers, and validated allocators. The\nresulting structure is a self-regulating asset management ecosystem capable of\ndecentralised, cooperative optimisation across traditional and digital\nfinancial domains. This framework is facilitated by a host chain network, which\noffers native automation and data oracle services enabling vault entities to\nautonomously operate on-chain, paving the way for being self sufficient in\ndynamic allocation of capital."
                },
                "authors": [
                    {
                        "name": "Arman Abgaryan"
                    },
                    {
                        "name": "Utkarsh Sharma"
                    },
                    {
                        "name": "Joshua Tobkin"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Tobkin"
                },
                "author": "Joshua Tobkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11543v2",
                "updated": "2025-09-24T15:05:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    5,
                    34,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-15T03:24:08Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    3,
                    24,
                    8,
                    0,
                    258,
                    0
                ],
                "title": "UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning"
                },
                "summary": "Graphical User Interface (GUI) agents have demonstrated remarkable progress\nin automating complex user interface interactions through reinforcement\nlearning. However, current approaches face a fundamental dilemma: offline RL\nenables stable training on pre-collected trajectories, but struggles with\nmulti-step task execution for lack of trajectory-level reward signals; online\nRL captures these signals through environment interaction, but suffers from\nsparse rewards and prohibitive deployment costs. To address it, we present\nSemi-online Reinforcement Learning, a novel paradigm that simulates online RL\non offline trajectories. During each rollout process, we preserve the original\nmodel output within the multi-turn dialogue, where a Patch Module adaptively\nrecovers the divergence between rollout and expert trajectories. To capture\nlong-term training signals, Semi-online RL introduces discounted future returns\ninto the reward computation and optimizes the policy with weighted step-level\nand episode-level advantages. We further introduce Semi-Online Performance\n(SOP), a metric that aligns better with true online performance, serving as a\npractical and effective proxy for real-world evaluation. Experiments show that\nours Semi-online RL achieves SOTA performance among 7B models across four\ndynamic benchmarks, with significant gains over the base model (e.g., +12.0% on\nAndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging\nthe gap between offline training efficiency and online multi-turn reasoning.\nThe code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical User Interface (GUI) agents have demonstrated remarkable progress\nin automating complex user interface interactions through reinforcement\nlearning. However, current approaches face a fundamental dilemma: offline RL\nenables stable training on pre-collected trajectories, but struggles with\nmulti-step task execution for lack of trajectory-level reward signals; online\nRL captures these signals through environment interaction, but suffers from\nsparse rewards and prohibitive deployment costs. To address it, we present\nSemi-online Reinforcement Learning, a novel paradigm that simulates online RL\non offline trajectories. During each rollout process, we preserve the original\nmodel output within the multi-turn dialogue, where a Patch Module adaptively\nrecovers the divergence between rollout and expert trajectories. To capture\nlong-term training signals, Semi-online RL introduces discounted future returns\ninto the reward computation and optimizes the policy with weighted step-level\nand episode-level advantages. We further introduce Semi-Online Performance\n(SOP), a metric that aligns better with true online performance, serving as a\npractical and effective proxy for real-world evaluation. Experiments show that\nours Semi-online RL achieves SOTA performance among 7B models across four\ndynamic benchmarks, with significant gains over the base model (e.g., +12.0% on\nAndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging\nthe gap between offline training efficiency and online multi-turn reasoning.\nThe code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1."
                },
                "authors": [
                    {
                        "name": "Zhengxi Lu"
                    },
                    {
                        "name": "Jiabo Ye"
                    },
                    {
                        "name": "Fei Tang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Haiyang Xu"
                    },
                    {
                        "name": "Ziwei Zheng"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "arxiv_comment": "22 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20208v1",
                "updated": "2025-09-24T15:02:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    2,
                    33,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T15:02:33Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    15,
                    2,
                    33,
                    2,
                    267,
                    0
                ],
                "title": "Play by the Type Rules: Inferring Constraints for LLM Functions in\n  Declarative Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Play by the Type Rules: Inferring Constraints for LLM Functions in\n  Declarative Programs"
                },
                "summary": "Integrating LLM powered operators in declarative query languages allows for\nthe combination of cheap and interpretable functions with powerful,\ngeneralizable language model reasoning. However, in order to benefit from the\noptimized execution of a database query language like SQL, generated outputs\nmust align with the rules enforced by both type checkers and database contents.\nCurrent approaches address this challenge with orchestrations consisting of\nmany LLM-based post-processing calls to ensure alignment between generated\noutputs and database values, introducing performance bottlenecks. We perform a\nstudy on the ability of various sized open-source language models to both parse\nand execute functions within a query language based on SQL, showing that small\nlanguage models can excel as function executors over hybrid data sources. Then,\nwe propose an efficient solution to enforce the well-typedness of LLM\nfunctions, demonstrating 7% accuracy improvement on a multi-hop question\nanswering dataset with 53% improvement in latency over comparable solutions. We\nmake our implementation available at https://github.com/parkervg/blendsql",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating LLM powered operators in declarative query languages allows for\nthe combination of cheap and interpretable functions with powerful,\ngeneralizable language model reasoning. However, in order to benefit from the\noptimized execution of a database query language like SQL, generated outputs\nmust align with the rules enforced by both type checkers and database contents.\nCurrent approaches address this challenge with orchestrations consisting of\nmany LLM-based post-processing calls to ensure alignment between generated\noutputs and database values, introducing performance bottlenecks. We perform a\nstudy on the ability of various sized open-source language models to both parse\nand execute functions within a query language based on SQL, showing that small\nlanguage models can excel as function executors over hybrid data sources. Then,\nwe propose an efficient solution to enforce the well-typedness of LLM\nfunctions, demonstrating 7% accuracy improvement on a multi-hop question\nanswering dataset with 53% improvement in latency over comparable solutions. We\nmake our implementation available at https://github.com/parkervg/blendsql"
                },
                "authors": [
                    {
                        "name": "Parker Glenn"
                    },
                    {
                        "name": "Alfy Samuel"
                    },
                    {
                        "name": "Daben Liu"
                    }
                ],
                "author_detail": {
                    "name": "Daben Liu"
                },
                "author": "Daben Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05926v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05926v3",
                "updated": "2025-09-24T14:58:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    58,
                    32,
                    2,
                    267,
                    0
                ],
                "published": "2025-01-10T12:46:39Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    46,
                    39,
                    4,
                    10,
                    0
                ],
                "title": "LLMs Reproduce Stereotypes of Sexual and Gender Minorities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Reproduce Stereotypes of Sexual and Gender Minorities"
                },
                "summary": "A large body of research has found substantial gender bias in NLP systems.\nMost of this research takes a binary, essentialist view of gender: limiting its\nvariation to the categories _men_ and _women_, conflating gender with sex, and\nignoring different sexual identities. But gender and sexuality exist on a\nspectrum, so in this paper we study the biases of large language models (LLMs)\ntowards sexual and gender minorities beyond binary categories. Grounding our\nstudy in a widely used social psychology model -- the Stereotype Content Model\n-- we demonstrate that English-language survey questions about social\nperceptions elicit more negative stereotypes of sexual and gender minorities\nfrom both humans and LLMs. We then extend this framework to a more realistic\nuse case: text generation. Our analysis shows that LLMs generate stereotyped\nrepresentations of sexual and gender minorities in this setting, showing that\nthey amplify representational harms in creative writing, a widely advertised\nuse for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large body of research has found substantial gender bias in NLP systems.\nMost of this research takes a binary, essentialist view of gender: limiting its\nvariation to the categories _men_ and _women_, conflating gender with sex, and\nignoring different sexual identities. But gender and sexuality exist on a\nspectrum, so in this paper we study the biases of large language models (LLMs)\ntowards sexual and gender minorities beyond binary categories. Grounding our\nstudy in a widely used social psychology model -- the Stereotype Content Model\n-- we demonstrate that English-language survey questions about social\nperceptions elicit more negative stereotypes of sexual and gender minorities\nfrom both humans and LLMs. We then extend this framework to a more realistic\nuse case: text generation. Our analysis shows that LLMs generate stereotyped\nrepresentations of sexual and gender minorities in this setting, showing that\nthey amplify representational harms in creative writing, a widely advertised\nuse for LLMs."
                },
                "authors": [
                    {
                        "name": "Ruby Ostrow"
                    },
                    {
                        "name": "Adam Lopez"
                    }
                ],
                "author_detail": {
                    "name": "Adam Lopez"
                },
                "author": "Adam Lopez",
                "arxiv_comment": "13 pages, 5 figures, 9 tables (including bibliography and appendix).\n  Accepted to Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05926v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05926v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11045v2",
                "updated": "2025-09-24T14:57:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    57,
                    25,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-21T14:10:06Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    10,
                    6,
                    2,
                    141,
                    0
                ],
                "title": "Procedural Environment Generation for Tool-Use Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Environment Generation for Tool-Use Agents"
                },
                "summary": "Although the power of LLM tool-use agents has ignited a flurry of recent\nresearch in this area, the curation of tool-use training data remains an open\nproblem$-$especially for online RL training. Existing approaches to synthetic\ntool-use data generation tend to be non-interactive, and/or non-compositional.\nWe introduce RandomWorld, a pipeline for the procedural generation of\ninteractive tools and compositional tool-use data. We show that models tuned\nvia SFT and RL on synthetic RandomWorld data improve on a range of tool-use\nbenchmarks, and set the new SoTA for two metrics on the NESTFUL dataset.\nFurther experiments show that downstream performance scales with the amount of\nRandomWorld-generated training data, opening up the possibility of further\nimprovement through the use of entirely synthetic data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the power of LLM tool-use agents has ignited a flurry of recent\nresearch in this area, the curation of tool-use training data remains an open\nproblem$-$especially for online RL training. Existing approaches to synthetic\ntool-use data generation tend to be non-interactive, and/or non-compositional.\nWe introduce RandomWorld, a pipeline for the procedural generation of\ninteractive tools and compositional tool-use data. We show that models tuned\nvia SFT and RL on synthetic RandomWorld data improve on a range of tool-use\nbenchmarks, and set the new SoTA for two metrics on the NESTFUL dataset.\nFurther experiments show that downstream performance scales with the amount of\nRandomWorld-generated training data, opening up the possibility of further\nimprovement through the use of entirely synthetic data."
                },
                "authors": [
                    {
                        "name": "Michael Sullivan"
                    },
                    {
                        "name": "Mareike Hartmann"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "arxiv_comment": "16 pages, 3 figures; accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20196v1",
                "updated": "2025-09-24T14:52:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    52,
                    1,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:52:01Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    52,
                    1,
                    2,
                    267,
                    0
                ],
                "title": "Universal Camouflage Attack on Vision-Language Models for Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Camouflage Attack on Vision-Language Models for Autonomous\n  Driving"
                },
                "summary": "Visual language modeling for automated driving is emerging as a promising\nresearch direction with substantial improvements in multimodal reasoning\ncapabilities. Despite its advanced reasoning abilities, VLM-AD remains\nvulnerable to serious security threats from adversarial attacks, which involve\nmisleading model decisions through carefully crafted perturbations. Existing\nattacks have obvious challenges: 1) Physical adversarial attacks primarily\ntarget vision modules. They are difficult to directly transfer to VLM-AD\nsystems because they typically attack low-level perceptual components. 2)\nAdversarial attacks against VLM-AD have largely concentrated on the digital\nlevel. To address these challenges, we propose the first Universal Camouflage\nAttack (UCA) framework for VLM-AD. Unlike previous methods that focus on\noptimizing the logit layer, UCA operates in the feature space to generate\nphysically realizable camouflage textures that exhibit strong generalization\nacross different user commands and model architectures. Motivated by the\nobserved vulnerability of encoder and projection layers in VLM-AD, UCA\nintroduces a feature divergence loss (FDL) that maximizes the representational\ndiscrepancy between clean and adversarial images. In addition, UCA incorporates\na multi-scale learning strategy and adjusts the sampling ratio to enhance its\nadaptability to changes in scale and viewpoint diversity in real-world\nscenarios, thereby improving training stability. Extensive experiments\ndemonstrate that UCA can induce incorrect driving commands across various\nVLM-AD models and driving scenarios, significantly surpassing existing\nstate-of-the-art attack methods (improving 30\\% in 3-P metrics). Furthermore,\nUCA exhibits strong attack robustness under diverse viewpoints and dynamic\nconditions, indicating high potential for practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual language modeling for automated driving is emerging as a promising\nresearch direction with substantial improvements in multimodal reasoning\ncapabilities. Despite its advanced reasoning abilities, VLM-AD remains\nvulnerable to serious security threats from adversarial attacks, which involve\nmisleading model decisions through carefully crafted perturbations. Existing\nattacks have obvious challenges: 1) Physical adversarial attacks primarily\ntarget vision modules. They are difficult to directly transfer to VLM-AD\nsystems because they typically attack low-level perceptual components. 2)\nAdversarial attacks against VLM-AD have largely concentrated on the digital\nlevel. To address these challenges, we propose the first Universal Camouflage\nAttack (UCA) framework for VLM-AD. Unlike previous methods that focus on\noptimizing the logit layer, UCA operates in the feature space to generate\nphysically realizable camouflage textures that exhibit strong generalization\nacross different user commands and model architectures. Motivated by the\nobserved vulnerability of encoder and projection layers in VLM-AD, UCA\nintroduces a feature divergence loss (FDL) that maximizes the representational\ndiscrepancy between clean and adversarial images. In addition, UCA incorporates\na multi-scale learning strategy and adjusts the sampling ratio to enhance its\nadaptability to changes in scale and viewpoint diversity in real-world\nscenarios, thereby improving training stability. Extensive experiments\ndemonstrate that UCA can induce incorrect driving commands across various\nVLM-AD models and driving scenarios, significantly surpassing existing\nstate-of-the-art attack methods (improving 30\\% in 3-P metrics). Furthermore,\nUCA exhibits strong attack robustness under diverse viewpoints and dynamic\nconditions, indicating high potential for practical deployment."
                },
                "authors": [
                    {
                        "name": "Dehong Kong"
                    },
                    {
                        "name": "Sifan Yu"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Jiawei Liang"
                    },
                    {
                        "name": "Jianhou Gan"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04137v2",
                "updated": "2025-09-24T14:48:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    48,
                    30,
                    2,
                    267,
                    0
                ],
                "published": "2025-07-05T19:20:59Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    19,
                    20,
                    59,
                    5,
                    186,
                    0
                ],
                "title": "Detecting Token-Level Hallucinations Using Variance Signals: A\n  Reference-Free Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Token-Level Hallucinations Using Variance Signals: A\n  Reference-Free Approach"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive generative\ncapabilities across diverse tasks but remain susceptible to hallucinations,\nconfidently generated yet factually incorrect outputs. We introduce a\nreference-free, token-level hallucination detection framework that leverages\nthe variance in token log-probabilities across multiple stochastic generations.\nUnlike prior methods that require ground-truth references or sentence-level\nverification, our approach is model-agnostic, interpretable, and suited for\nreal-time or post-hoc analysis. We evaluate our method on unanswerable question\nprompts from the SQuAD v2 dataset and benchmark across three autoregressive\nmodels of varying scales: GPT-Neo 125M, Falcon 1B, and Mistral 7B. Through both\nquantitative metrics and visual diagnostics, we show that token-level variance\nreliably highlights instability in model outputs and correlates with\nhallucination patterns. Our framework is lightweight, reproducible, and\nadaptable to multiple domains, offering a valuable diagnostic tool for\nanalyzing generative reliability in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive generative\ncapabilities across diverse tasks but remain susceptible to hallucinations,\nconfidently generated yet factually incorrect outputs. We introduce a\nreference-free, token-level hallucination detection framework that leverages\nthe variance in token log-probabilities across multiple stochastic generations.\nUnlike prior methods that require ground-truth references or sentence-level\nverification, our approach is model-agnostic, interpretable, and suited for\nreal-time or post-hoc analysis. We evaluate our method on unanswerable question\nprompts from the SQuAD v2 dataset and benchmark across three autoregressive\nmodels of varying scales: GPT-Neo 125M, Falcon 1B, and Mistral 7B. Through both\nquantitative metrics and visual diagnostics, we show that token-level variance\nreliably highlights instability in model outputs and correlates with\nhallucination patterns. Our framework is lightweight, reproducible, and\nadaptable to multiple domains, offering a valuable diagnostic tool for\nanalyzing generative reliability in LLMs."
                },
                "authors": [
                    {
                        "name": "Keshav Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Keshav Kumar"
                },
                "author": "Keshav Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20190v1",
                "updated": "2025-09-24T14:46:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    46,
                    42,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:46:42Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    46,
                    42,
                    2,
                    267,
                    0
                ],
                "title": "STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test\n  Generation"
                },
                "summary": "In modern automotive development, security testing is critical for\nsafeguarding systems against increasingly advanced threats. Attack trees are\nwidely used to systematically represent potential attack vectors, but\ngenerating comprehensive test cases from these trees remains a labor-intensive,\nerror-prone task that has seen limited automation in the context of testing\nvehicular systems. This paper introduces STAF (Security Test Automation\nFramework), a novel approach to automating security test case generation.\nLeveraging Large Language Models (LLMs) and a four-step self-corrective\nRetrieval-Augmented Generation (RAG) framework, STAF automates the generation\nof executable security test cases from attack trees, providing an end-to-end\nsolution that encompasses the entire attack surface. We particularly show the\nelements and processes needed to provide an LLM to actually produce sensible\nand executable automotive security test suites, along with the integration with\nan automated testing framework. We further compare our tailored approach with\ngeneral purpose (vanilla) LLMs and the performance of different LLMs (namely\nGPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our\noperation step-by-step in a concrete case study. Our results show significant\nimprovements in efficiency, accuracy, scalability, and easy integration in any\nworkflow, marking a substantial advancement in automating automotive security\ntesting methodologies. Using TARAs as an input for verfication tests, we create\nsynergies by connecting two vital elements of a secure automotive development\nprocess.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern automotive development, security testing is critical for\nsafeguarding systems against increasingly advanced threats. Attack trees are\nwidely used to systematically represent potential attack vectors, but\ngenerating comprehensive test cases from these trees remains a labor-intensive,\nerror-prone task that has seen limited automation in the context of testing\nvehicular systems. This paper introduces STAF (Security Test Automation\nFramework), a novel approach to automating security test case generation.\nLeveraging Large Language Models (LLMs) and a four-step self-corrective\nRetrieval-Augmented Generation (RAG) framework, STAF automates the generation\nof executable security test cases from attack trees, providing an end-to-end\nsolution that encompasses the entire attack surface. We particularly show the\nelements and processes needed to provide an LLM to actually produce sensible\nand executable automotive security test suites, along with the integration with\nan automated testing framework. We further compare our tailored approach with\ngeneral purpose (vanilla) LLMs and the performance of different LLMs (namely\nGPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our\noperation step-by-step in a concrete case study. Our results show significant\nimprovements in efficiency, accuracy, scalability, and easy integration in any\nworkflow, marking a substantial advancement in automating automotive security\ntesting methodologies. Using TARAs as an input for verfication tests, we create\nsynergies by connecting two vital elements of a secure automotive development\nprocess."
                },
                "authors": [
                    {
                        "name": "Tanmay Khule"
                    },
                    {
                        "name": "Stefan Marksteiner"
                    },
                    {
                        "name": "Jose Alguindigue"
                    },
                    {
                        "name": "Hannes Fuchs"
                    },
                    {
                        "name": "Sebastian Fischmeister"
                    },
                    {
                        "name": "Apurva Narayan"
                    }
                ],
                "author_detail": {
                    "name": "Apurva Narayan"
                },
                "author": "Apurva Narayan",
                "arxiv_comment": "18 pages, 2 figures, accepted for 23rd escar Europe (Nov 05-06, 2025,\n  Frankfurt, Germany)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20189v1",
                "updated": "2025-09-24T14:46:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    46,
                    7,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:46:07Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    46,
                    7,
                    2,
                    267,
                    0
                ],
                "title": "Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge\n  Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge\n  Accelerators"
                },
                "summary": "Edge accelerators such as Nvidia Jetsons are becoming an integral part of the\ncomputing continuum, and are often used for DNN inferencing and training.\nNvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power\nenvelope and offer $1000$s of power modes to customize CPU, GPU and memory\nfrequencies. Their widely varying power--performance trade-offs can be\nexploited for energy and power-constrained deployments. While data-driven\nmethods to predict the power and latency of DNN workloads for edge devices\nexist, there is a lack of principled study to understand why edge accelerators\nand their power modes perform the way they do. We develop a time roofline and a\nnovel energy roofline model for the Jetson Orin AGX for diverse power modes,\nand couple it with an analytical model of the compute (FLOP) and memory access\n(bytes) for DNN inference workloads to analyze them from first principles.\nThese reveal unique, sometimes counter-intuitive, insights into the power and\nperformance behavior of DNN workloads on edge accelerators, e.g., the default\npower mode MAXN is not the most energy efficient and time efficiency implies\nenergy efficiency for all power modes. We also extend our analytical roofline\nmodels to DNN training. Finally, we apply these methods to tune the power mode\n(and hence the roofline) of the edge device to optimize the latency and energy\nfor DNN inference, with up to $15\\%$ lower energy and minimal degradation in\ninference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge accelerators such as Nvidia Jetsons are becoming an integral part of the\ncomputing continuum, and are often used for DNN inferencing and training.\nNvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power\nenvelope and offer $1000$s of power modes to customize CPU, GPU and memory\nfrequencies. Their widely varying power--performance trade-offs can be\nexploited for energy and power-constrained deployments. While data-driven\nmethods to predict the power and latency of DNN workloads for edge devices\nexist, there is a lack of principled study to understand why edge accelerators\nand their power modes perform the way they do. We develop a time roofline and a\nnovel energy roofline model for the Jetson Orin AGX for diverse power modes,\nand couple it with an analytical model of the compute (FLOP) and memory access\n(bytes) for DNN inference workloads to analyze them from first principles.\nThese reveal unique, sometimes counter-intuitive, insights into the power and\nperformance behavior of DNN workloads on edge accelerators, e.g., the default\npower mode MAXN is not the most energy efficient and time efficiency implies\nenergy efficiency for all power modes. We also extend our analytical roofline\nmodels to DNN training. Finally, we apply these methods to tune the power mode\n(and hence the roofline) of the edge device to optimize the latency and energy\nfor DNN inference, with up to $15\\%$ lower energy and minimal degradation in\ninference time."
                },
                "authors": [
                    {
                        "name": "Prashanthi S. K."
                    },
                    {
                        "name": "Kunal Kumar Sahoo"
                    },
                    {
                        "name": "Amartya Ranjan Saikia"
                    },
                    {
                        "name": "Pranav Gupta"
                    },
                    {
                        "name": "Atharva Vinay Joshi"
                    },
                    {
                        "name": "Priyanshu Pansari"
                    },
                    {
                        "name": "Yogesh Simmhan"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh Simmhan"
                },
                "author": "Yogesh Simmhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20186v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20186v2",
                "updated": "2025-09-25T10:55:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    55,
                    2,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-24T14:45:13Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    45,
                    13,
                    2,
                    267,
                    0
                ],
                "title": "Thinking Augmented Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Augmented Pre-training"
                },
                "summary": "This paper introduces a simple and scalable approach to improve the data\nefficiency of large language model (LLM) training by augmenting existing text\ndata with thinking trajectories. The compute for pre-training LLMs has been\ngrowing at an unprecedented rate, while the availability of high-quality data\nremains limited. Consequently, maximizing the utility of available data\nconstitutes a significant research challenge. A primary impediment is that\ncertain high-quality tokens are difficult to learn given a fixed model\ncapacity, as the underlying rationale for a single token can be exceptionally\ncomplex and deep. To address this issue, we propose Thinking augmented\nPre-Training (TPT), a universal methodology that augments text with\nautomatically generated thinking trajectories. Such augmentation effectively\nincreases the volume of the training data and makes high-quality tokens more\nlearnable through step-by-step reasoning and decomposition. We apply TPT across\ndiverse training configurations up to $100$B tokens, encompassing pre-training\nwith both constrained and abundant data, as well as mid-training from strong\nopen-source checkpoints. Experimental results indicate that our method\nsubstantially improves the performance of LLMs across various model sizes and\nfamilies. Notably, TPT enhances the data efficiency of LLM pre-training by a\nfactor of $3$. For a $3$B parameter model, it improves the post-training\nperformance by over $10\\%$ on several challenging reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a simple and scalable approach to improve the data\nefficiency of large language model (LLM) training by augmenting existing text\ndata with thinking trajectories. The compute for pre-training LLMs has been\ngrowing at an unprecedented rate, while the availability of high-quality data\nremains limited. Consequently, maximizing the utility of available data\nconstitutes a significant research challenge. A primary impediment is that\ncertain high-quality tokens are difficult to learn given a fixed model\ncapacity, as the underlying rationale for a single token can be exceptionally\ncomplex and deep. To address this issue, we propose Thinking augmented\nPre-Training (TPT), a universal methodology that augments text with\nautomatically generated thinking trajectories. Such augmentation effectively\nincreases the volume of the training data and makes high-quality tokens more\nlearnable through step-by-step reasoning and decomposition. We apply TPT across\ndiverse training configurations up to $100$B tokens, encompassing pre-training\nwith both constrained and abundant data, as well as mid-training from strong\nopen-source checkpoints. Experimental results indicate that our method\nsubstantially improves the performance of LLMs across various model sizes and\nfamilies. Notably, TPT enhances the data efficiency of LLM pre-training by a\nfactor of $3$. For a $3$B parameter model, it improves the post-training\nperformance by over $10\\%$ on several challenging reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20186v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20186v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20172v1",
                "updated": "2025-09-24T14:36:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    36,
                    44,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:36:44Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    36,
                    44,
                    2,
                    267,
                    0
                ],
                "title": "Benchmarking Web API Integration Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Web API Integration Code Generation"
                },
                "summary": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models~(LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present a\ndataset and evaluation pipeline designed to assess the ability of LLMs to\ngenerate web API invocation code. Our experiments with several open-source LLMs\nreveal that generating API invocations poses a significant challenge, resulting\nin hallucinated endpoints, incorrect argument usage, and other errors. None of\nthe evaluated open-source models were able to solve more than 40% of the tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models~(LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present a\ndataset and evaluation pipeline designed to assess the ability of LLMs to\ngenerate web API invocation code. Our experiments with several open-source LLMs\nreveal that generating API invocations poses a significant challenge, resulting\nin hallucinated endpoints, incorrect argument usage, and other errors. None of\nthe evaluated open-source models were able to solve more than 40% of the tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Maninger"
                    },
                    {
                        "name": "Leon Chemnitz"
                    },
                    {
                        "name": "Amir Molzam Sharifloo"
                    },
                    {
                        "name": "Jannis Brugger"
                    },
                    {
                        "name": "Mira Mezini"
                    }
                ],
                "author_detail": {
                    "name": "Mira Mezini"
                },
                "author": "Mira Mezini",
                "arxiv_comment": "To be published in Proceedings of 2nd ACM International Conference on\n  AI-powered Software, Benchmark & Dataset Track (AIware '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20168v1",
                "updated": "2025-09-24T14:34:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    34,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:34:17Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    34,
                    17,
                    2,
                    267,
                    0
                ],
                "title": "Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in\n  Persian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in\n  Persian"
                },
                "summary": "Multilingual Large Language Models (LLMs) are increasingly used worldwide,\nmaking it essential to ensure they are free from gender bias to prevent\nrepresentational harm. While prior studies have examined such biases in\nhigh-resource languages, low-resource languages remain understudied. In this\npaper, we propose a template-based probing methodology, validated against\nreal-world data, to uncover gender stereotypes in LLMs. As part of this\nframework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a\nmetric that quantifies deviations from gender parity. We evaluate four\nprominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B,\nacross four semantic domains, focusing on Persian, a low-resource language with\ndistinct linguistic features. Our results show that all models exhibit gender\nstereotypes, with greater disparities in Persian than in English across all\ndomains. Among these, sports reflect the most rigid gender biases. This study\nunderscores the need for inclusive NLP practices and provides a framework for\nassessing bias in other low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models (LLMs) are increasingly used worldwide,\nmaking it essential to ensure they are free from gender bias to prevent\nrepresentational harm. While prior studies have examined such biases in\nhigh-resource languages, low-resource languages remain understudied. In this\npaper, we propose a template-based probing methodology, validated against\nreal-world data, to uncover gender stereotypes in LLMs. As part of this\nframework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a\nmetric that quantifies deviations from gender parity. We evaluate four\nprominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B,\nacross four semantic domains, focusing on Persian, a low-resource language with\ndistinct linguistic features. Our results show that all models exhibit gender\nstereotypes, with greater disparities in Persian than in English across all\ndomains. Among these, sports reflect the most rigid gender biases. This study\nunderscores the need for inclusive NLP practices and provides a framework for\nassessing bias in other low-resource languages."
                },
                "authors": [
                    {
                        "name": "Ghazal Kalhor"
                    },
                    {
                        "name": "Behnam Bahrak"
                    }
                ],
                "author_detail": {
                    "name": "Behnam Bahrak"
                },
                "author": "Behnam Bahrak",
                "arxiv_comment": "Accepted and forthcoming at the Widening Natural Language Processing\n  Workshop (WiNLP 2025) at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20166v1",
                "updated": "2025-09-24T14:33:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    33,
                    7,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:33:07Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    33,
                    7,
                    2,
                    267,
                    0
                ],
                "title": "CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and\n  Threat Intelligence Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and\n  Threat Intelligence Reasoning"
                },
                "summary": "Today's cyber defenders are overwhelmed by a deluge of security alerts,\nthreat intelligence signals, and shifting business context, creating an urgent\nneed for AI systems to enhance operational security work. While Large Language\nModels (LLMs) have the potential to automate and scale Security Operations\nCenter (SOC) operations, existing evaluations do not fully assess the scenarios\nmost relevant to real-world defenders. This lack of informed evaluation impacts\nboth AI developers and those applying LLMs to SOC automation. Without clear\ninsight into LLM performance in real-world security scenarios, developers lack\na north star for development, and users cannot reliably select the most\neffective models. Meanwhile, malicious actors are using AI to scale cyber\nattacks, highlighting the need for open source benchmarks to drive adoption and\ncommunity-driven improvement among defenders and model developers. To address\nthis, we introduce CyberSOCEval, a new suite of open source benchmarks within\nCyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in\ntwo tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive\ndomains with inadequate coverage in current benchmarks. Our evaluations show\nthat larger, more modern LLMs tend to perform better, confirming the training\nscaling laws paradigm. We also find that reasoning models leveraging test time\nscaling do not achieve the same boost as in coding and math, suggesting these\nmodels have not been trained to reason about cybersecurity analysis, and\npointing to a key opportunity for improvement. Finally, current LLMs are far\nfrom saturating our evaluations, showing that CyberSOCEval presents a\nsignificant challenge for AI developers to improve cyber defense capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's cyber defenders are overwhelmed by a deluge of security alerts,\nthreat intelligence signals, and shifting business context, creating an urgent\nneed for AI systems to enhance operational security work. While Large Language\nModels (LLMs) have the potential to automate and scale Security Operations\nCenter (SOC) operations, existing evaluations do not fully assess the scenarios\nmost relevant to real-world defenders. This lack of informed evaluation impacts\nboth AI developers and those applying LLMs to SOC automation. Without clear\ninsight into LLM performance in real-world security scenarios, developers lack\na north star for development, and users cannot reliably select the most\neffective models. Meanwhile, malicious actors are using AI to scale cyber\nattacks, highlighting the need for open source benchmarks to drive adoption and\ncommunity-driven improvement among defenders and model developers. To address\nthis, we introduce CyberSOCEval, a new suite of open source benchmarks within\nCyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in\ntwo tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive\ndomains with inadequate coverage in current benchmarks. Our evaluations show\nthat larger, more modern LLMs tend to perform better, confirming the training\nscaling laws paradigm. We also find that reasoning models leveraging test time\nscaling do not achieve the same boost as in coding and math, suggesting these\nmodels have not been trained to reason about cybersecurity analysis, and\npointing to a key opportunity for improvement. Finally, current LLMs are far\nfrom saturating our evaluations, showing that CyberSOCEval presents a\nsignificant challenge for AI developers to improve cyber defense capabilities."
                },
                "authors": [
                    {
                        "name": "Lauren Deason"
                    },
                    {
                        "name": "Adam Bali"
                    },
                    {
                        "name": "Ciprian Bejean"
                    },
                    {
                        "name": "Diana Bolocan"
                    },
                    {
                        "name": "James Crnkovich"
                    },
                    {
                        "name": "Ioana Croitoru"
                    },
                    {
                        "name": "Krishna Durai"
                    },
                    {
                        "name": "Chase Midler"
                    },
                    {
                        "name": "Calin Miron"
                    },
                    {
                        "name": "David Molnar"
                    },
                    {
                        "name": "Brad Moon"
                    },
                    {
                        "name": "Bruno Ostarcevic"
                    },
                    {
                        "name": "Alberto Peltea"
                    },
                    {
                        "name": "Matt Rosenberg"
                    },
                    {
                        "name": "Catalin Sandu"
                    },
                    {
                        "name": "Arthur Saputkin"
                    },
                    {
                        "name": "Sagar Shah"
                    },
                    {
                        "name": "Daniel Stan"
                    },
                    {
                        "name": "Ernest Szocs"
                    },
                    {
                        "name": "Shengye Wan"
                    },
                    {
                        "name": "Spencer Whitman"
                    },
                    {
                        "name": "Sven Krasser"
                    },
                    {
                        "name": "Joshua Saxe"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Saxe"
                },
                "author": "Joshua Saxe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20162v1",
                "updated": "2025-09-24T14:30:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    30,
                    16,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:30:16Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    30,
                    16,
                    2,
                    267,
                    0
                ],
                "title": "Embedding Domain Knowledge for Large Language Models via Reinforcement\n  Learning from Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding Domain Knowledge for Large Language Models via Reinforcement\n  Learning from Augmented Generation"
                },
                "summary": "Large language models (LLMs) often exhibit limited performance on\ndomain-specific tasks due to the natural disproportionate representation of\nspecialized information in their training data and the static nature of these\ndatasets. Knowledge scarcity and temporal lag create knowledge gaps for domain\napplications. While post-training on domain datasets can embed knowledge into\nmodels, existing approaches have some limitations. Continual Pre-Training (CPT)\ntreats all tokens in domain documents with equal importance, failing to\nprioritize critical knowledge points, while supervised fine-tuning (SFT) with\nquestion-answer pairs struggles to develop the coherent knowledge structures\nnecessary for complex reasoning tasks. To address these challenges, we propose\nReinforcement Learning from Augmented Generation (RLAG). Our approach\niteratively cycles between sampling generations and optimizing the model\nthrough calculated rewards, effectively embedding critical and contextually\ncoherent domain knowledge. We select generated outputs with the highest log\nprobabilities as the sampling result, then compute three tailored reward\nmetrics to guide the optimization process. To comprehensively evaluate domain\nexpertise, we assess answer accuracy and the rationality of explanations\ngenerated for correctly answered questions. Experimental results across\nmedical, legal, astronomy, and current events datasets demonstrate that our\nproposed method significantly outperforms baseline approaches. Our code and\ndata are open sourced at https://github.com/ChaojunNie/RLAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit limited performance on\ndomain-specific tasks due to the natural disproportionate representation of\nspecialized information in their training data and the static nature of these\ndatasets. Knowledge scarcity and temporal lag create knowledge gaps for domain\napplications. While post-training on domain datasets can embed knowledge into\nmodels, existing approaches have some limitations. Continual Pre-Training (CPT)\ntreats all tokens in domain documents with equal importance, failing to\nprioritize critical knowledge points, while supervised fine-tuning (SFT) with\nquestion-answer pairs struggles to develop the coherent knowledge structures\nnecessary for complex reasoning tasks. To address these challenges, we propose\nReinforcement Learning from Augmented Generation (RLAG). Our approach\niteratively cycles between sampling generations and optimizing the model\nthrough calculated rewards, effectively embedding critical and contextually\ncoherent domain knowledge. We select generated outputs with the highest log\nprobabilities as the sampling result, then compute three tailored reward\nmetrics to guide the optimization process. To comprehensively evaluate domain\nexpertise, we assess answer accuracy and the rationality of explanations\ngenerated for correctly answered questions. Experimental results across\nmedical, legal, astronomy, and current events datasets demonstrate that our\nproposed method significantly outperforms baseline approaches. Our code and\ndata are open sourced at https://github.com/ChaojunNie/RLAG."
                },
                "authors": [
                    {
                        "name": "Chaojun Nie"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Guanxiang Wang"
                    },
                    {
                        "name": "Shisong Wud"
                    },
                    {
                        "name": "Zichen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zichen Wang"
                },
                "author": "Zichen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02003v4",
                "updated": "2025-09-24T14:26:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    26,
                    13,
                    2,
                    267,
                    0
                ],
                "published": "2025-03-03T19:26:04Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    19,
                    26,
                    4,
                    0,
                    62,
                    0
                ],
                "title": "HoT: Highlighted Chain of Thought for Referencing Supporting Facts from\n  Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoT: Highlighted Chain of Thought for Referencing Supporting Facts from\n  Inputs"
                },
                "summary": "An Achilles heel of Large Language Models (LLMs) is their tendency to\nhallucinate non-factual statements. A response mixed of factual and non-factual\nstatements poses a challenge for humans to verify and accurately base their\ndecisions on. To combat this problem, we propose Highlighted Chain-of-Thought\nPrompting (HoT), a technique for prompting LLMs to generate responses with XML\ntags that ground facts to those provided in the query. That is, given an input\nquestion, LLMs would first re-format the question to add XML tags highlighting\nkey facts, and then, generate a response with highlights over the facts\nreferenced from the input. Interestingly, in few-shot settings, HoT outperforms\nvanilla chain of thought prompting (CoT) on a wide range of 17 tasks from\narithmetic, reading comprehension to logical reasoning. When asking humans to\nverify LLM responses, highlights help time-limited participants to more\naccurately and efficiently recognize when LLMs are correct. Yet, surprisingly,\nwhen LLMs are wrong, HoTs tend to make users believe that an answer is correct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Achilles heel of Large Language Models (LLMs) is their tendency to\nhallucinate non-factual statements. A response mixed of factual and non-factual\nstatements poses a challenge for humans to verify and accurately base their\ndecisions on. To combat this problem, we propose Highlighted Chain-of-Thought\nPrompting (HoT), a technique for prompting LLMs to generate responses with XML\ntags that ground facts to those provided in the query. That is, given an input\nquestion, LLMs would first re-format the question to add XML tags highlighting\nkey facts, and then, generate a response with highlights over the facts\nreferenced from the input. Interestingly, in few-shot settings, HoT outperforms\nvanilla chain of thought prompting (CoT) on a wide range of 17 tasks from\narithmetic, reading comprehension to logical reasoning. When asking humans to\nverify LLM responses, highlights help time-limited participants to more\naccurately and efficiently recognize when LLMs are correct. Yet, surprisingly,\nwhen LLMs are wrong, HoTs tend to make users believe that an answer is correct."
                },
                "authors": [
                    {
                        "name": "Tin Nguyen"
                    },
                    {
                        "name": "Logan Bolton"
                    },
                    {
                        "name": "Mohammad Reza Taesiri"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Anh Totti Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Anh Totti Nguyen"
                },
                "author": "Anh Totti Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20153v2",
                "updated": "2025-09-25T10:43:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    43,
                    22,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-24T14:18:41Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    18,
                    41,
                    2,
                    267,
                    0
                ],
                "title": "Affective Computing and Emotional Data: Challenges and Implications in\n  Privacy Regulations, The AI Act, and Ethics in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affective Computing and Emotional Data: Challenges and Implications in\n  Privacy Regulations, The AI Act, and Ethics in Large Language Models"
                },
                "summary": "This paper examines the integration of emotional intelligence into artificial\nintelligence systems, with a focus on affective computing and the growing\ncapabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to\nrecognize and respond to human emotions. Drawing on interdisciplinary research\nthat combines computer science, psychology, and neuroscience, the study\nanalyzes foundational neural architectures - CNNs for processing facial\nexpressions and RNNs for sequential data, such as speech and text - that enable\nemotion recognition. It examines the transformation of human emotional\nexperiences into structured emotional data, addressing the distinction between\nexplicit emotional data collected with informed consent in research settings\nand implicit data gathered passively through everyday digital interactions.\nThat raises critical concerns about lawful processing, AI transparency, and\nindividual autonomy over emotional expressions in digital environments. The\npaper explores implications across various domains, including healthcare,\neducation, and customer service, while addressing challenges of cultural\nvariations in emotional expression and potential biases in emotion recognition\nsystems across different demographic groups. From a regulatory perspective, the\npaper examines emotional data in the context of the GDPR and the EU AI Act\nframeworks, highlighting how emotional data may be considered sensitive\npersonal data that requires robust safeguards, including purpose limitation,\ndata minimization, and meaningful consent mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the integration of emotional intelligence into artificial\nintelligence systems, with a focus on affective computing and the growing\ncapabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to\nrecognize and respond to human emotions. Drawing on interdisciplinary research\nthat combines computer science, psychology, and neuroscience, the study\nanalyzes foundational neural architectures - CNNs for processing facial\nexpressions and RNNs for sequential data, such as speech and text - that enable\nemotion recognition. It examines the transformation of human emotional\nexperiences into structured emotional data, addressing the distinction between\nexplicit emotional data collected with informed consent in research settings\nand implicit data gathered passively through everyday digital interactions.\nThat raises critical concerns about lawful processing, AI transparency, and\nindividual autonomy over emotional expressions in digital environments. The\npaper explores implications across various domains, including healthcare,\neducation, and customer service, while addressing challenges of cultural\nvariations in emotional expression and potential biases in emotion recognition\nsystems across different demographic groups. From a regulatory perspective, the\npaper examines emotional data in the context of the GDPR and the EU AI Act\nframeworks, highlighting how emotional data may be considered sensitive\npersonal data that requires robust safeguards, including purpose limitation,\ndata minimization, and meaningful consent mechanisms."
                },
                "authors": [
                    {
                        "name": "Nicola Fabiano"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Fabiano"
                },
                "author": "Nicola Fabiano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20149v1",
                "updated": "2025-09-24T14:14:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    14,
                    21,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:14:21Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    14,
                    21,
                    2,
                    267,
                    0
                ],
                "title": "Enhancing Requirement Traceability through Data Augmentation Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Requirement Traceability through Data Augmentation Using Large\n  Language Models"
                },
                "summary": "Requirements traceability is crucial in software engineering to ensure\nconsistency between requirements and code. However, existing automated\ntraceability methods are constrained by the scarcity of training data and\nchallenges in bridging the semantic gap between artifacts. This study aims to\naddress the data scarcity problem in requirements traceability by employing\nlarge language models (LLMs) for data augmentation. We propose a novel approach\nthat utilizes prompt-based techniques with LLMs to generate augmented\nrequirement-to-code trace links, thereby enhancing the training dataset. Four\nLLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both\nzero-shot and few-shot templates. Moreover, we optimized the encoder component\nof the tracing model to improve its efficiency and adaptability to augmented\ndata. The key contributions of this paper are: (1) proposing and evaluating\nfour prompt templates for data augmentation; (2) providing a comparative\nanalysis of four LLMs for generating trace links; (3) enhancing the model's\nencoder for improved adaptability to augmented datasets. Experimental results\nshow that our approach significantly enhances model performance, achieving an\nF1 score improvement of up to 28.59%, thus demonstrating its effectiveness and\npotential for practical application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requirements traceability is crucial in software engineering to ensure\nconsistency between requirements and code. However, existing automated\ntraceability methods are constrained by the scarcity of training data and\nchallenges in bridging the semantic gap between artifacts. This study aims to\naddress the data scarcity problem in requirements traceability by employing\nlarge language models (LLMs) for data augmentation. We propose a novel approach\nthat utilizes prompt-based techniques with LLMs to generate augmented\nrequirement-to-code trace links, thereby enhancing the training dataset. Four\nLLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both\nzero-shot and few-shot templates. Moreover, we optimized the encoder component\nof the tracing model to improve its efficiency and adaptability to augmented\ndata. The key contributions of this paper are: (1) proposing and evaluating\nfour prompt templates for data augmentation; (2) providing a comparative\nanalysis of four LLMs for generating trace links; (3) enhancing the model's\nencoder for improved adaptability to augmented datasets. Experimental results\nshow that our approach significantly enhances model performance, achieving an\nF1 score improvement of up to 28.59%, thus demonstrating its effectiveness and\npotential for practical application."
                },
                "authors": [
                    {
                        "name": "Jianzhang Zhang"
                    },
                    {
                        "name": "Jialong Zhou"
                    },
                    {
                        "name": "Nan Niu"
                    },
                    {
                        "name": "Chuang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Liu"
                },
                "author": "Chuang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18991v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18991v5",
                "updated": "2025-09-25T02:38:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    2,
                    38,
                    14,
                    3,
                    268,
                    0
                ],
                "published": "2025-03-23T16:40:29Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    16,
                    40,
                    29,
                    6,
                    82,
                    0
                ],
                "title": "Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM\n  Alignment"
                },
                "summary": "Alignment is vital for safely deploying large language models (LLMs).\nExisting techniques are either reward-based (train a reward model on preference\npairs and optimize with reinforcement learning) or reward-free (directly\nfine-tune on ranked outputs). Recent research shows that well-tuned\nreward-based pipelines remain robust, and single-response demonstrations can\noutperform pairwise preference data. However, two challenges persist: (1)\nimbalanced safety datasets that overrepresent common hazards while neglecting\nlong-tail threats; and (2) static reward models that ignore task difficulty,\nlimiting optimization efficiency and attainable gains. We propose DR-IRL\n(Dynamically adjusting Rewards through Inverse Reinforcement Learning). We\nfirst train category-specific reward models using a balanced safety dataset\ncovering seven harmful categories via IRL. Then we enhance Group Relative\nPolicy Optimization (GRPO) by introducing dynamic reward scaling--adjusting\nrewards by task difficulty--data-level hardness by text encoder cosine\nsimilarity, model-level responsiveness by reward gaps. Extensive experiments\nacross various benchmarks and LLMs demonstrate that DR-IRL outperforms all\nbaseline methods in safety alignment while maintaining usefulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment is vital for safely deploying large language models (LLMs).\nExisting techniques are either reward-based (train a reward model on preference\npairs and optimize with reinforcement learning) or reward-free (directly\nfine-tune on ranked outputs). Recent research shows that well-tuned\nreward-based pipelines remain robust, and single-response demonstrations can\noutperform pairwise preference data. However, two challenges persist: (1)\nimbalanced safety datasets that overrepresent common hazards while neglecting\nlong-tail threats; and (2) static reward models that ignore task difficulty,\nlimiting optimization efficiency and attainable gains. We propose DR-IRL\n(Dynamically adjusting Rewards through Inverse Reinforcement Learning). We\nfirst train category-specific reward models using a balanced safety dataset\ncovering seven harmful categories via IRL. Then we enhance Group Relative\nPolicy Optimization (GRPO) by introducing dynamic reward scaling--adjusting\nrewards by task difficulty--data-level hardness by text encoder cosine\nsimilarity, model-level responsiveness by reward gaps. Extensive experiments\nacross various benchmarks and LLMs demonstrate that DR-IRL outperforms all\nbaseline methods in safety alignment while maintaining usefulness."
                },
                "authors": [
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Haoxuan Ma"
                    },
                    {
                        "name": "Weixin Wang"
                    },
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Jiexi Liu"
                    },
                    {
                        "name": "Xiaoshuang Jia"
                    },
                    {
                        "name": "Simeng Qin"
                    },
                    {
                        "name": "Xiaochun Cao"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xiaojun Jia"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Jia"
                },
                "author": "Xiaojun Jia",
                "arxiv_comment": "The first three authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18991v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18991v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01301v2",
                "updated": "2025-09-24T14:02:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    2,
                    11,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-01T09:39:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    9,
                    39,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "Culture is Everywhere: A Call for Intentionally Cultural Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culture is Everywhere: A Call for Intentionally Cultural Evaluation"
                },
                "summary": "The prevailing ``trivia-centered paradigm'' for evaluating the cultural\nalignment of large language models (LLMs) is increasingly inadequate as these\nmodels become more advanced and widely deployed. Existing approaches typically\nreduce culture to static facts or values, testing models via multiple-choice or\nshort-answer questions that treat culture as isolated trivia. Such methods\nneglect the pluralistic and interactive realities of culture, and overlook how\ncultural assumptions permeate even ostensibly ``neutral'' evaluation settings.\nIn this position paper, we argue for \\textbf{intentionally cultural\nevaluation}: an approach that systematically examines the cultural assumptions\nembedded in all aspects of evaluation, not just in explicitly cultural tasks.\nWe systematically characterize the what, how, and circumstances by which\nculturally contingent considerations arise in evaluation, and emphasize the\nimportance of researcher positionality for fostering inclusive, culturally\naligned NLP research. Finally, we discuss implications and future directions\nfor moving beyond current benchmarking practices, discovering important\napplications that we don't know exist, and involving communities in evaluation\ndesign through HCI-inspired participatory methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevailing ``trivia-centered paradigm'' for evaluating the cultural\nalignment of large language models (LLMs) is increasingly inadequate as these\nmodels become more advanced and widely deployed. Existing approaches typically\nreduce culture to static facts or values, testing models via multiple-choice or\nshort-answer questions that treat culture as isolated trivia. Such methods\nneglect the pluralistic and interactive realities of culture, and overlook how\ncultural assumptions permeate even ostensibly ``neutral'' evaluation settings.\nIn this position paper, we argue for \\textbf{intentionally cultural\nevaluation}: an approach that systematically examines the cultural assumptions\nembedded in all aspects of evaluation, not just in explicitly cultural tasks.\nWe systematically characterize the what, how, and circumstances by which\nculturally contingent considerations arise in evaluation, and emphasize the\nimportance of researcher positionality for fostering inclusive, culturally\naligned NLP research. Finally, we discuss implications and future directions\nfor moving beyond current benchmarking practices, discovering important\napplications that we don't know exist, and involving communities in evaluation\ndesign through HCI-inspired participatory methodologies."
                },
                "authors": [
                    {
                        "name": "Juhyun Oh"
                    },
                    {
                        "name": "Inha Cha"
                    },
                    {
                        "name": "Michael Saxon"
                    },
                    {
                        "name": "Hyunseung Lim"
                    },
                    {
                        "name": "Shaily Bhatt"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20136v1",
                "updated": "2025-09-24T14:01:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    1,
                    18,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T14:01:18Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    14,
                    1,
                    18,
                    2,
                    267,
                    0
                ],
                "title": "V-GameGym: Visual Game Generation for Code Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-GameGym: Visual Game Generation for Code Large Language Models"
                },
                "summary": "Code large language models have demonstrated remarkable capabilities in\nprogramming tasks, yet current benchmarks primarily focus on single modality\nrather than visual game development. Most existing code-related benchmarks\nevaluate syntax correctness and execution accuracy, overlooking critical\ngame-specific metrics such as playability, visual aesthetics, and user\nengagement that are essential for real-world deployment. To address the gap\nbetween current LLM capabilities in algorithmic problem-solving and competitive\nprogramming versus the comprehensive requirements of practical game\ndevelopment, we present V-GameGym, a comprehensive benchmark comprising 2,219\nhigh-quality samples across 100 thematic clusters derived from real-world\nrepositories, adopting a novel clustering-based curation methodology to ensure\nboth diversity and structural completeness. Further, we introduce a multimodal\nevaluation framework with an automated LLM-driven pipeline for visual code\nsynthesis using complete UI sandbox environments. Our extensive analysis\nreveals that V-GameGym effectively bridges the gap between code generation\naccuracy and practical game development workflows, providing quantifiable\nquality metrics for visual programming and interactive element generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code large language models have demonstrated remarkable capabilities in\nprogramming tasks, yet current benchmarks primarily focus on single modality\nrather than visual game development. Most existing code-related benchmarks\nevaluate syntax correctness and execution accuracy, overlooking critical\ngame-specific metrics such as playability, visual aesthetics, and user\nengagement that are essential for real-world deployment. To address the gap\nbetween current LLM capabilities in algorithmic problem-solving and competitive\nprogramming versus the comprehensive requirements of practical game\ndevelopment, we present V-GameGym, a comprehensive benchmark comprising 2,219\nhigh-quality samples across 100 thematic clusters derived from real-world\nrepositories, adopting a novel clustering-based curation methodology to ensure\nboth diversity and structural completeness. Further, we introduce a multimodal\nevaluation framework with an automated LLM-driven pipeline for visual code\nsynthesis using complete UI sandbox environments. Our extensive analysis\nreveals that V-GameGym effectively bridges the gap between code generation\naccuracy and practical game development workflows, providing quantifiable\nquality metrics for visual programming and interactive element generation."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Jack Yang"
                    },
                    {
                        "name": "Renshuai Tao"
                    },
                    {
                        "name": "Lingzheng Chai"
                    },
                    {
                        "name": "Shawn Guo"
                    },
                    {
                        "name": "Jiajun Wu"
                    },
                    {
                        "name": "Xiaoming Chen"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Xander Xu"
                    },
                    {
                        "name": "Hu Wei"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20124v1",
                "updated": "2025-09-24T13:49:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    49,
                    44,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:49:44Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    49,
                    44,
                    2,
                    267,
                    0
                ],
                "title": "Probability Signature: Bridging Data Semantics and Embedding Structure\n  in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probability Signature: Bridging Data Semantics and Embedding Structure\n  in Language Models"
                },
                "summary": "The embedding space of language models is widely believed to capture the\nsemantic relationships; for instance, embeddings of digits often exhibit an\nordered structure that corresponds to their natural sequence. However, the\nmechanisms driving the formation of such structures remain poorly understood.\nIn this work, we interpret the embedding structures via the data distribution.\nWe propose a set of probability signatures that reflect the semantic\nrelationships among tokens. Through experiments on the composite addition tasks\nusing the linear model and feedforward network, combined with theoretical\nanalysis of gradient flow dynamics, we reveal that these probability signatures\nsignificantly influence the embedding structures. We further generalize our\nanalysis to large language models (LLMs) by training the Qwen2.5 architecture\non the subsets of the Pile corpus. Our results show that the probability\nsignatures are faithfully aligned with the embedding structures, particularly\nin capturing strong pairwise similarities among embeddings. Our work uncovers\nthe mechanism of how data distribution guides the formation of embedding\nstructures, establishing a novel understanding of the relationship between\nembedding organization and semantic patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The embedding space of language models is widely believed to capture the\nsemantic relationships; for instance, embeddings of digits often exhibit an\nordered structure that corresponds to their natural sequence. However, the\nmechanisms driving the formation of such structures remain poorly understood.\nIn this work, we interpret the embedding structures via the data distribution.\nWe propose a set of probability signatures that reflect the semantic\nrelationships among tokens. Through experiments on the composite addition tasks\nusing the linear model and feedforward network, combined with theoretical\nanalysis of gradient flow dynamics, we reveal that these probability signatures\nsignificantly influence the embedding structures. We further generalize our\nanalysis to large language models (LLMs) by training the Qwen2.5 architecture\non the subsets of the Pile corpus. Our results show that the probability\nsignatures are faithfully aligned with the embedding structures, particularly\nin capturing strong pairwise similarities among embeddings. Our work uncovers\nthe mechanism of how data distribution guides the formation of embedding\nstructures, establishing a novel understanding of the relationship between\nembedding organization and semantic patterns."
                },
                "authors": [
                    {
                        "name": "Junjie Yao"
                    },
                    {
                        "name": "Zhi-Qin John Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Qin John Xu"
                },
                "author": "Zhi-Qin John Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17336v2",
                "updated": "2025-09-24T13:49:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    49,
                    27,
                    2,
                    267,
                    0
                ],
                "published": "2025-07-23T09:05:13Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    9,
                    5,
                    13,
                    2,
                    204,
                    0
                ],
                "title": "Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian\n  Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian\n  Splatting"
                },
                "summary": "Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed\nrendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric\nvideos. However, the large number of Gaussians, substantial temporal\nredundancies, and especially the absence of an entropy-aware compression\nframework result in large storage requirements. Consequently, this poses\nsignificant challenges for practical deployment, efficient edge-device\nprocessing, and data transmission. In this paper, we introduce a novel\nend-to-end RD-optimized compression framework tailored for 4DGS, aiming to\nenable flexible, high-fidelity rendering across varied computational platforms.\nLeveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the\nstate-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS\ncompression methods for compatibility while effectively addressing additional\nchallenges introduced by the temporal axis. In particular, instead of storing\nmotion trajectories independently per point, we employ a wavelet transform to\nreflect the real-world smoothness prior, significantly enhancing storage\nefficiency. This approach yields significantly improved compression ratios and\nprovides a user-controlled balance between compression efficiency and rendering\nquality. Extensive experiments demonstrate the effectiveness of our method,\nachieving up to 91$\\times$ compression compared to the original Ex4DGS model\nwhile maintaining high visual fidelity. These results highlight the\napplicability of our framework for real-time dynamic scene rendering in diverse\nscenarios, from resource-constrained edge devices to high-performance\nenvironments. The source code is available at\nhttps://github.com/HyeongminLEE/RD4DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed\nrendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric\nvideos. However, the large number of Gaussians, substantial temporal\nredundancies, and especially the absence of an entropy-aware compression\nframework result in large storage requirements. Consequently, this poses\nsignificant challenges for practical deployment, efficient edge-device\nprocessing, and data transmission. In this paper, we introduce a novel\nend-to-end RD-optimized compression framework tailored for 4DGS, aiming to\nenable flexible, high-fidelity rendering across varied computational platforms.\nLeveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the\nstate-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS\ncompression methods for compatibility while effectively addressing additional\nchallenges introduced by the temporal axis. In particular, instead of storing\nmotion trajectories independently per point, we employ a wavelet transform to\nreflect the real-world smoothness prior, significantly enhancing storage\nefficiency. This approach yields significantly improved compression ratios and\nprovides a user-controlled balance between compression efficiency and rendering\nquality. Extensive experiments demonstrate the effectiveness of our method,\nachieving up to 91$\\times$ compression compared to the original Ex4DGS model\nwhile maintaining high visual fidelity. These results highlight the\napplicability of our framework for real-time dynamic scene rendering in diverse\nscenarios, from resource-constrained edge devices to high-performance\nenvironments. The source code is available at\nhttps://github.com/HyeongminLEE/RD4DGS."
                },
                "authors": [
                    {
                        "name": "Hyeongmin Lee"
                    },
                    {
                        "name": "Kyungjune Baek"
                    }
                ],
                "author_detail": {
                    "name": "Kyungjune Baek"
                },
                "author": "Kyungjune Baek",
                "arxiv_comment": "24 pages, 10 figures, NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20123v1",
                "updated": "2025-09-24T13:48:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    48,
                    14,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:48:14Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    48,
                    14,
                    2,
                    267,
                    0
                ],
                "title": "Can LLMs Forecast Internet Traffic from Social Media?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Forecast Internet Traffic from Social Media?"
                },
                "summary": "Societal events shape the Internet's behavior. The death of a prominent\npublic figure, a software launch, or a major sports match can trigger sudden\ndemand surges that overwhelm peering points and content delivery networks.\nAlthough these events fall outside regular traffic patterns, forecasting\nsystems still rely solely on those patterns and therefore miss these critical\nanomalies.\n  Thus, we argue for socio-technical systems that supplement technical\nmeasurements with an active understanding of the underlying drivers, including\nhow events and collective behavior shape digital demands. We propose traffic\nforecasting using signals from public discourse, such as headlines, forums, and\nsocial media, as early demand indicators.\n  To validate our intuition, we present a proof-of-concept system that\nautonomously scrapes online discussions, infers real-world events, clusters and\nenriches them semantically, and correlates them with traffic measurements at a\nmajor Internet Exchange Point. This prototype predicted between 56-92% of\nsociety-driven traffic spikes after scraping a moderate amount of online\ndiscussions.\n  We believe this approach opens new research opportunities in cross-domain\nforecasting, scheduling, demand anticipation, and society-informed decision\nmaking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Societal events shape the Internet's behavior. The death of a prominent\npublic figure, a software launch, or a major sports match can trigger sudden\ndemand surges that overwhelm peering points and content delivery networks.\nAlthough these events fall outside regular traffic patterns, forecasting\nsystems still rely solely on those patterns and therefore miss these critical\nanomalies.\n  Thus, we argue for socio-technical systems that supplement technical\nmeasurements with an active understanding of the underlying drivers, including\nhow events and collective behavior shape digital demands. We propose traffic\nforecasting using signals from public discourse, such as headlines, forums, and\nsocial media, as early demand indicators.\n  To validate our intuition, we present a proof-of-concept system that\nautonomously scrapes online discussions, infers real-world events, clusters and\nenriches them semantically, and correlates them with traffic measurements at a\nmajor Internet Exchange Point. This prototype predicted between 56-92% of\nsociety-driven traffic spikes after scraping a moderate amount of online\ndiscussions.\n  We believe this approach opens new research opportunities in cross-domain\nforecasting, scheduling, demand anticipation, and society-informed decision\nmaking."
                },
                "authors": [
                    {
                        "name": "Jonatan Langlet"
                    },
                    {
                        "name": "Mariano Scazzariello"
                    },
                    {
                        "name": "Flavio Luciani"
                    },
                    {
                        "name": "Marta Burocchi"
                    },
                    {
                        "name": "Dejan Kostić"
                    },
                    {
                        "name": "Marco Chiesa"
                    }
                ],
                "author_detail": {
                    "name": "Marco Chiesa"
                },
                "author": "Marco Chiesa",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20105v1",
                "updated": "2025-09-24T13:29:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    29,
                    53,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:29:53Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    29,
                    53,
                    2,
                    267,
                    0
                ],
                "title": "PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning\n  Traces in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning\n  Traces in LLMs"
                },
                "summary": "Large Language Models (LLMs) often struggle with maintaining coherent\nmulti-step reasoning traces, particularly in tasks that require a structured\nlogical flow. This work introduces a quantum-inspired approach to address the\nchallenge by incorporating a fidelity-based reward derived from Projected\nEntangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior\napproaches that use direct supervision or contrastive objectives, the proposed\nmethod guides learning through structural consistency, offering a novel\napproach to enforce global coherence in generated reasoning traces. The\nproposed framework is evaluated using multiple coherence-determining metrics on\ndiverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning\narithmetic, intuitive, and entailment-based reasoning. Results show that the\nproposed quantum-inspired approach offers significant improvements over\nsupervised, contrastive, and pretrained baseline approaches, highlighting the\neffectiveness of quantum-inspired fidelity as a foundation to improve reasoning\ntrace coherence in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with maintaining coherent\nmulti-step reasoning traces, particularly in tasks that require a structured\nlogical flow. This work introduces a quantum-inspired approach to address the\nchallenge by incorporating a fidelity-based reward derived from Projected\nEntangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior\napproaches that use direct supervision or contrastive objectives, the proposed\nmethod guides learning through structural consistency, offering a novel\napproach to enforce global coherence in generated reasoning traces. The\nproposed framework is evaluated using multiple coherence-determining metrics on\ndiverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning\narithmetic, intuitive, and entailment-based reasoning. Results show that the\nproposed quantum-inspired approach offers significant improvements over\nsupervised, contrastive, and pretrained baseline approaches, highlighting the\neffectiveness of quantum-inspired fidelity as a foundation to improve reasoning\ntrace coherence in LLMs."
                },
                "authors": [
                    {
                        "name": "Venkat Margapuri"
                    },
                    {
                        "name": "Garik Kazanjian"
                    },
                    {
                        "name": "Naren Kosaraju"
                    }
                ],
                "author_detail": {
                    "name": "Naren Kosaraju"
                },
                "author": "Naren Kosaraju",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20099v2",
                "updated": "2025-09-25T11:14:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    11,
                    14,
                    50,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-24T13:23:03Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    23,
                    3,
                    2,
                    267,
                    0
                ],
                "title": "Cascade! Human in the loop shortcomings can increase the risk of\n  failures in recommender systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascade! Human in the loop shortcomings can increase the risk of\n  failures in recommender systems"
                },
                "summary": "Recommender systems are among the most commonly deployed systems today.\nSystems design approaches to AI-powered recommender systems have done well to\nurge recommender system developers to follow more intentional data collection,\ncuration, and management procedures. So too has the \"human-in-the-loop\"\nparadigm been widely adopted, primarily to address the issue of accountability.\nHowever, in this paper, we take the position that human oversight in\nrecommender system design also entails novel risks that have yet to be fully\ndescribed. These risks are \"codetermined\" by the information context in which\nsuch systems are often deployed. Furthermore, new knowledge of the shortcomings\nof \"human-in-the-loop\" practices to deliver meaningful oversight of other AI\nsystems suggest that they may also be inadequate for achieving socially\nresponsible recommendations. We review how the limitations of human oversight\nmay increase the chances of a specific kind of failure: a \"cascade\" or\n\"compound\" failure. We then briefly explore how the unique dynamics of three\ncommon deployment contexts can make humans in the loop more likely to fail in\ntheir oversight duties. We then conclude with two recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems are among the most commonly deployed systems today.\nSystems design approaches to AI-powered recommender systems have done well to\nurge recommender system developers to follow more intentional data collection,\ncuration, and management procedures. So too has the \"human-in-the-loop\"\nparadigm been widely adopted, primarily to address the issue of accountability.\nHowever, in this paper, we take the position that human oversight in\nrecommender system design also entails novel risks that have yet to be fully\ndescribed. These risks are \"codetermined\" by the information context in which\nsuch systems are often deployed. Furthermore, new knowledge of the shortcomings\nof \"human-in-the-loop\" practices to deliver meaningful oversight of other AI\nsystems suggest that they may also be inadequate for achieving socially\nresponsible recommendations. We review how the limitations of human oversight\nmay increase the chances of a specific kind of failure: a \"cascade\" or\n\"compound\" failure. We then briefly explore how the unique dynamics of three\ncommon deployment contexts can make humans in the loop more likely to fail in\ntheir oversight duties. We then conclude with two recommendations."
                },
                "authors": [
                    {
                        "name": "Wm. Matthew Kennedy"
                    },
                    {
                        "name": "Nishanshi Shukla"
                    },
                    {
                        "name": "Cigdem Patlak"
                    },
                    {
                        "name": "Blake Chambers"
                    },
                    {
                        "name": "Theodora Skeadas"
                    },
                    {
                        "name": "Tuesday"
                    },
                    {
                        "name": "Kingsley Owadara"
                    },
                    {
                        "name": "Aayush Dhanotiya"
                    }
                ],
                "author_detail": {
                    "name": "Aayush Dhanotiya"
                },
                "author": "Aayush Dhanotiya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20097v1",
                "updated": "2025-09-24T13:20:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    20,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:20:37Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    20,
                    37,
                    2,
                    267,
                    0
                ],
                "title": "Integrated Framework for LLM Evaluation with Answer Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Framework for LLM Evaluation with Answer Generation"
                },
                "summary": "Reliable evaluation of large language models is essential to ensure their\napplicability in practical scenarios. Traditional benchmark-based evaluation\nmethods often rely on fixed reference answers, limiting their ability to\ncapture important qualitative aspects of generated responses. To address these\nshortcomings, we propose an integrated evaluation framework called\n\\textit{self-refining descriptive evaluation with expert-driven diagnostics},\nSPEED, which utilizes specialized functional experts to perform comprehensive,\ndescriptive analyses of model outputs. Unlike conventional approaches, SPEED\nactively incorporates expert feedback across multiple dimensions, including\nhallucination detection, toxicity assessment, and lexical-contextual\nappropriateness. Experimental results demonstrate that SPEED achieves robust\nand consistent evaluation performance across diverse domains and datasets.\nAdditionally, by employing relatively compact expert models, SPEED demonstrates\nsuperior resource efficiency compared to larger-scale evaluators. These\nfindings illustrate that SPEED significantly enhances fairness and\ninterpretability in LLM evaluations, offering a promising alternative to\nexisting evaluation methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable evaluation of large language models is essential to ensure their\napplicability in practical scenarios. Traditional benchmark-based evaluation\nmethods often rely on fixed reference answers, limiting their ability to\ncapture important qualitative aspects of generated responses. To address these\nshortcomings, we propose an integrated evaluation framework called\n\\textit{self-refining descriptive evaluation with expert-driven diagnostics},\nSPEED, which utilizes specialized functional experts to perform comprehensive,\ndescriptive analyses of model outputs. Unlike conventional approaches, SPEED\nactively incorporates expert feedback across multiple dimensions, including\nhallucination detection, toxicity assessment, and lexical-contextual\nappropriateness. Experimental results demonstrate that SPEED achieves robust\nand consistent evaluation performance across diverse domains and datasets.\nAdditionally, by employing relatively compact expert models, SPEED demonstrates\nsuperior resource efficiency compared to larger-scale evaluators. These\nfindings illustrate that SPEED significantly enhances fairness and\ninterpretability in LLM evaluations, offering a promising alternative to\nexisting evaluation methodologies."
                },
                "authors": [
                    {
                        "name": "Sujeong Lee"
                    },
                    {
                        "name": "Hayoung Lee"
                    },
                    {
                        "name": "Seongsoo Heo"
                    },
                    {
                        "name": "Wonik Choi"
                    }
                ],
                "author_detail": {
                    "name": "Wonik Choi"
                },
                "author": "Wonik Choi",
                "arxiv_comment": "16pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20088v1",
                "updated": "2025-09-24T13:06:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    6,
                    35,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:06:35Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    6,
                    35,
                    2,
                    267,
                    0
                ],
                "title": "Causal Understanding by LLMs: The Role of Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Understanding by LLMs: The Role of Uncertainty"
                },
                "summary": "Recent papers show LLMs achieve near-random accuracy in causal relation\nclassification, raising questions about whether such failures arise from\nlimited pretraining exposure or deeper representational gaps. We investigate\nthis under uncertainty-based evaluation, testing whether pretraining exposure\nto causal examples improves causal understanding >18K PubMed sentences -- half\nfrom The Pile corpus, half post-2024 -- across seven models\n(Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model\nbehavior through: (i) causal classification, where the model identifies causal\nrelationships in text, and (ii) verbatim memorization probing, where we assess\nwhether the model prefers previously seen causal statements over their\nparaphrases. Models perform four-way classification\n(direct/conditional/correlational/no-relationship) and select between originals\nand their generated paraphrases. Results show almost identical accuracy on\nseen/unseen sentences (p > 0.05), no memorization bias (24.8% original\nselection), and output distribution over the possible options is almost flat,\nwith entropic values near the maximum (1.35/1.39), confirming random guessing.\nInstruction-tuned models show severe miscalibration (Qwen: > 95% confidence,\n32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11%\nvs. direct). These findings suggest that failures in causal understanding arise\nfrom the lack of structured causal representation, rather than insufficient\nexposure to causal examples during pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent papers show LLMs achieve near-random accuracy in causal relation\nclassification, raising questions about whether such failures arise from\nlimited pretraining exposure or deeper representational gaps. We investigate\nthis under uncertainty-based evaluation, testing whether pretraining exposure\nto causal examples improves causal understanding >18K PubMed sentences -- half\nfrom The Pile corpus, half post-2024 -- across seven models\n(Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model\nbehavior through: (i) causal classification, where the model identifies causal\nrelationships in text, and (ii) verbatim memorization probing, where we assess\nwhether the model prefers previously seen causal statements over their\nparaphrases. Models perform four-way classification\n(direct/conditional/correlational/no-relationship) and select between originals\nand their generated paraphrases. Results show almost identical accuracy on\nseen/unseen sentences (p > 0.05), no memorization bias (24.8% original\nselection), and output distribution over the possible options is almost flat,\nwith entropic values near the maximum (1.35/1.39), confirming random guessing.\nInstruction-tuned models show severe miscalibration (Qwen: > 95% confidence,\n32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11%\nvs. direct). These findings suggest that failures in causal understanding arise\nfrom the lack of structured causal representation, rather than insufficient\nexposure to causal examples during pretraining."
                },
                "authors": [
                    {
                        "name": "Oscar Lithgow-Serrano"
                    },
                    {
                        "name": "Vani Kanjirangat"
                    },
                    {
                        "name": "Alessandro Antonucci"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Antonucci"
                },
                "author": "Alessandro Antonucci",
                "arxiv_comment": "Accepted in second UncertaiNLP workshop at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20086v1",
                "updated": "2025-09-24T13:05:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    5,
                    9,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T13:05:09Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    13,
                    5,
                    9,
                    2,
                    267,
                    0
                ],
                "title": "OLaPh: Optimal Language Phonemizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OLaPh: Optimal Language Phonemizer"
                },
                "summary": "Phonemization, the conversion of text into phonemes, is a key step in\ntext-to-speech. Traditional approaches use rule-based transformations and\nlexicon lookups, while more advanced methods apply preprocessing techniques or\nneural networks for improved accuracy on out-of-domain vocabulary. However, all\nsystems struggle with names, loanwords, abbreviations, and homographs. This\nwork presents OLaPh (Optimal Language Phonemizer), a framework that combines\nlarge lexica, multiple NLP techniques, and compound resolution with a\nprobabilistic scoring function. Evaluations in German and English show improved\naccuracy over previous approaches, including on a challenging dataset. To\nfurther address unresolved cases, we train a large language model on\nOLaPh-generated data, which achieves even stronger generalization and\nperformance. Together, the framework and LLM improve phonemization consistency\nand provide a freely available resource for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phonemization, the conversion of text into phonemes, is a key step in\ntext-to-speech. Traditional approaches use rule-based transformations and\nlexicon lookups, while more advanced methods apply preprocessing techniques or\nneural networks for improved accuracy on out-of-domain vocabulary. However, all\nsystems struggle with names, loanwords, abbreviations, and homographs. This\nwork presents OLaPh (Optimal Language Phonemizer), a framework that combines\nlarge lexica, multiple NLP techniques, and compound resolution with a\nprobabilistic scoring function. Evaluations in German and English show improved\naccuracy over previous approaches, including on a challenging dataset. To\nfurther address unresolved cases, we train a large language model on\nOLaPh-generated data, which achieves even stronger generalization and\nperformance. Together, the framework and LLM improve phonemization consistency\nand provide a freely available resource for future research."
                },
                "authors": [
                    {
                        "name": "Johannes Wirth"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Wirth"
                },
                "author": "Johannes Wirth",
                "arxiv_comment": "5 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12044v2",
                "updated": "2025-09-24T12:57:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    57,
                    3,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-24T16:17:50Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    16,
                    17,
                    50,
                    5,
                    144,
                    0
                ],
                "title": "Why Do Some Inputs Break Low-Bit LLM Quantization?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do Some Inputs Break Low-Bit LLM Quantization?"
                },
                "summary": "Low-bit weight-only quantization significantly reduces the memory footprint\nof large language models (LLMs), but disproportionately affects certain\nexamples. We analyze diverse 3-4 bit methods on LLMs ranging from 7B-70B in\nsize and find that the quantization errors of 50 pairs of methods are strongly\ncorrelated (avg. 0.82) on FineWeb examples. Moreover, the residual stream\nmagnitudes of full-precision models are indicative of future quantization\nerrors. We further establish a hypothesis that relates the residual stream\nmagnitudes to error amplification and accumulation over layers. Using LLM\nlocalization techniques, early exiting, and activation patching, we show that\nexamples with large errors rely on precise residual activations in the late\nlayers, and that the outputs of MLP gates play a crucial role in maintaining\nthe perplexity. Our work reveals why certain examples result in large\nquantization errors and which model components are most critical for\nperformance preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-bit weight-only quantization significantly reduces the memory footprint\nof large language models (LLMs), but disproportionately affects certain\nexamples. We analyze diverse 3-4 bit methods on LLMs ranging from 7B-70B in\nsize and find that the quantization errors of 50 pairs of methods are strongly\ncorrelated (avg. 0.82) on FineWeb examples. Moreover, the residual stream\nmagnitudes of full-precision models are indicative of future quantization\nerrors. We further establish a hypothesis that relates the residual stream\nmagnitudes to error amplification and accumulation over layers. Using LLM\nlocalization techniques, early exiting, and activation patching, we show that\nexamples with large errors rely on precise residual activations in the late\nlayers, and that the outputs of MLP gates play a crucial role in maintaining\nthe perplexity. Our work reveals why certain examples result in large\nquantization errors and which model components are most critical for\nperformance preservation."
                },
                "authors": [
                    {
                        "name": "Ting-Yun Chang"
                    },
                    {
                        "name": "Muru Zhang"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Robin Jia"
                    }
                ],
                "author_detail": {
                    "name": "Robin Jia"
                },
                "author": "Robin Jia",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04341v2",
                "updated": "2025-09-24T12:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    56,
                    29,
                    2,
                    267,
                    0
                ],
                "published": "2025-01-08T08:26:56Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    26,
                    56,
                    2,
                    8,
                    0
                ],
                "title": "Understanding Before Reasoning: Enhancing Chain-of-Thought with\n  Iterative Summarization Pre-Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Before Reasoning: Enhancing Chain-of-Thought with\n  Iterative Summarization Pre-Prompting"
                },
                "summary": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language\nModels (LLMs) to enhance complex reasoning. It guides LLMs to present\nmulti-step reasoning, rather than generating the final answer directly.\nHowever, CoT encounters difficulties when key information required for\nreasoning is implicit or missing. This occurs because CoT emphasizes the\nsequence of reasoning steps while overlooking the early extraction of essential\ninformation. We propose a pre-prompting method called Iterative Summarization\nPre-Prompting (ISP^2) to refine LLM reasoning when key information is not\nexplicitly provided. First, entities and their corresponding descriptions are\nextracted to form potential key information pairs. Next, we use a reliability\nrating to assess these pairs, then merge the two lowest-ranked pairs into a new\nentity description. This process is repeated until a unique key information\npair is obtained. Finally, that pair, along with the original question, is fed\ninto LLMs to produce the answer. Extensive experiments demonstrate a 7.1%\nimprovement compared to existing methods. Unlike traditional prompting, ISP^2\nadopts an inductive approach with pre-prompting, offering flexible integration\ninto diverse reasoning frameworks. The code is available at\nhttps://github.com/zdhgreat/ISP-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language\nModels (LLMs) to enhance complex reasoning. It guides LLMs to present\nmulti-step reasoning, rather than generating the final answer directly.\nHowever, CoT encounters difficulties when key information required for\nreasoning is implicit or missing. This occurs because CoT emphasizes the\nsequence of reasoning steps while overlooking the early extraction of essential\ninformation. We propose a pre-prompting method called Iterative Summarization\nPre-Prompting (ISP^2) to refine LLM reasoning when key information is not\nexplicitly provided. First, entities and their corresponding descriptions are\nextracted to form potential key information pairs. Next, we use a reliability\nrating to assess these pairs, then merge the two lowest-ranked pairs into a new\nentity description. This process is repeated until a unique key information\npair is obtained. Finally, that pair, along with the original question, is fed\ninto LLMs to produce the answer. Extensive experiments demonstrate a 7.1%\nimprovement compared to existing methods. Unlike traditional prompting, ISP^2\nadopts an inductive approach with pre-prompting, offering flexible integration\ninto diverse reasoning frameworks. The code is available at\nhttps://github.com/zdhgreat/ISP-2."
                },
                "authors": [
                    {
                        "name": "Dong-Hai Zhu"
                    },
                    {
                        "name": "Yu-Jie Xiong"
                    },
                    {
                        "name": "Jia-Chen Zhang"
                    },
                    {
                        "name": "Xi-Jiong Xie"
                    },
                    {
                        "name": "Chun-Ming Xia"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Ming Xia"
                },
                "author": "Chun-Ming Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20072v2",
                "updated": "2025-09-25T09:23:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    9,
                    23,
                    12,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-24T12:44:26Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    44,
                    26,
                    2,
                    267,
                    0
                ],
                "title": "From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint\n  Training"
                },
                "summary": "Recent advances in large language models (LLMs) have attracted significant\ninterest in extending their capabilities to multimodal scenarios, particularly\nfor speech-to-speech conversational systems. However, existing multimodal\nmodels handling interleaved audio and text rely on autoregressive methods,\noverlooking that text depends on target-target relations whereas audio depends\nmainly on source-target relations. In this work, we propose Text-to-Talk (TtT),\na unified audio-text framework that integrates autoregressive (AR) text\ngeneration with non-autoregressive (NAR) audio diffusion in a single\nTransformer. By leveraging the any-order autoregressive property of absorbing\ndiscrete diffusion, our approach provides a unified training objective for text\nand audio. To support this hybrid generation paradigm, we design a\nmodality-aware attention mechanism that enforces causal decoding for text while\nallowing bidirectional modeling within audio spans, and further introduce three\ntraining strategies that reduce train-test discrepancies. During inference, TtT\nemploys block-wise diffusion to synthesize audio in parallel while flexibly\nhandling variable-length outputs. Extensive experiments across Audio-QA and ASR\ntasks demonstrate the effectiveness of our approach, with detailed ablation\nstudies validating each proposed component. We will open-source our models,\ndata and code to facilitate future research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have attracted significant\ninterest in extending their capabilities to multimodal scenarios, particularly\nfor speech-to-speech conversational systems. However, existing multimodal\nmodels handling interleaved audio and text rely on autoregressive methods,\noverlooking that text depends on target-target relations whereas audio depends\nmainly on source-target relations. In this work, we propose Text-to-Talk (TtT),\na unified audio-text framework that integrates autoregressive (AR) text\ngeneration with non-autoregressive (NAR) audio diffusion in a single\nTransformer. By leveraging the any-order autoregressive property of absorbing\ndiscrete diffusion, our approach provides a unified training objective for text\nand audio. To support this hybrid generation paradigm, we design a\nmodality-aware attention mechanism that enforces causal decoding for text while\nallowing bidirectional modeling within audio spans, and further introduce three\ntraining strategies that reduce train-test discrepancies. During inference, TtT\nemploys block-wise diffusion to synthesize audio in parallel while flexibly\nhandling variable-length outputs. Extensive experiments across Audio-QA and ASR\ntasks demonstrate the effectiveness of our approach, with detailed ablation\nstudies validating each proposed component. We will open-source our models,\ndata and code to facilitate future research in this direction."
                },
                "authors": [
                    {
                        "name": "Tianqiao Liu"
                    },
                    {
                        "name": "Xueyi Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Zhichao Chen"
                    },
                    {
                        "name": "Weiqi Luo"
                    },
                    {
                        "name": "Zitao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zitao Liu"
                },
                "author": "Zitao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20070v1",
                "updated": "2025-09-24T12:40:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    40,
                    57,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T12:40:57Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    40,
                    57,
                    2,
                    267,
                    0
                ],
                "title": "LLM Trainer: Automated Robotic Data Generating via Demonstration\n  Augmentation using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Trainer: Automated Robotic Data Generating via Demonstration\n  Augmentation using LLMs"
                },
                "summary": "We present LLM Trainer, a fully automated pipeline that leverages the world\nknowledge of Large Language Models (LLMs) to transform a small number of human\ndemonstrations (as few as one) into a large robot dataset for imitation\nlearning. Our approach decomposes demonstration generation into two steps: (1)\noffline demonstration annotation that extracts keyframes, salient objects, and\npose-object relations; and (2) online keypose retargeting that adapts those\nkeyframes to a new scene, given an initial observation. Using these modified\nkeypoints, our system warps the original demonstration to generate a new\ntrajectory, which is then executed, and the resulting demo, if successful, is\nsaved. Because the annotation is reusable across scenes, we use Thompson\nsampling to optimize the annotation, significantly improving generation success\nrate. We evaluate our method on a range of tasks, and find that our data\nannotation method consistently outperforms expert-engineered baselines. We\nfurther show an ensemble policy that combines the optimized LLM feed-forward\nplan with a learned feedback imitation learning controller. Finally, we\ndemonstrate hardware feasibility on a Franka Emika Panda robot. For additional\nmaterials and demonstration videos, please see the project website:\nhttps://sites.google.com/andrew.cmu.edu/llm-trainer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LLM Trainer, a fully automated pipeline that leverages the world\nknowledge of Large Language Models (LLMs) to transform a small number of human\ndemonstrations (as few as one) into a large robot dataset for imitation\nlearning. Our approach decomposes demonstration generation into two steps: (1)\noffline demonstration annotation that extracts keyframes, salient objects, and\npose-object relations; and (2) online keypose retargeting that adapts those\nkeyframes to a new scene, given an initial observation. Using these modified\nkeypoints, our system warps the original demonstration to generate a new\ntrajectory, which is then executed, and the resulting demo, if successful, is\nsaved. Because the annotation is reusable across scenes, we use Thompson\nsampling to optimize the annotation, significantly improving generation success\nrate. We evaluate our method on a range of tasks, and find that our data\nannotation method consistently outperforms expert-engineered baselines. We\nfurther show an ensemble policy that combines the optimized LLM feed-forward\nplan with a learned feedback imitation learning controller. Finally, we\ndemonstrate hardware feasibility on a Franka Emika Panda robot. For additional\nmaterials and demonstration videos, please see the project website:\nhttps://sites.google.com/andrew.cmu.edu/llm-trainer"
                },
                "authors": [
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "9 pages, 5 figures, 4 tables. Submitted to ICRA 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20068v1",
                "updated": "2025-09-24T12:37:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    37,
                    18,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T12:37:18Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    37,
                    18,
                    2,
                    267,
                    0
                ],
                "title": "A Novel Short-Term Anomaly Prediction for IIoT with Software Defined\n  Twin Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Short-Term Anomaly Prediction for IIoT with Software Defined\n  Twin Network"
                },
                "summary": "Secure monitoring and dynamic control in an IIoT environment are major\nrequirements for current development goals. We believe that dynamic, secure\nmonitoring of the IIoT environment can be achieved through integration with the\nSoftware-Defined Network (SDN) and Digital Twin (DT) paradigms. The current\nliterature lacks implementation details for SDN-based DT and time-aware\nintelligent model training for short-term anomaly detection against IIoT\nthreats. Therefore, we have proposed a novel framework for short-term anomaly\ndetection that uses an SDN-based DT. Using a comprehensive dataset, time-aware\nlabeling of features, and a comprehensive evaluation of various machine\nlearning models, we propose a novel SD-TWIN-based anomaly detection algorithm.\nAccording to the performance of a new real-time SD-TWIN deployment, the GPU-\naccelerated LightGBM model is particularly effective, achieving a balance of\nhigh recall and strong classification performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure monitoring and dynamic control in an IIoT environment are major\nrequirements for current development goals. We believe that dynamic, secure\nmonitoring of the IIoT environment can be achieved through integration with the\nSoftware-Defined Network (SDN) and Digital Twin (DT) paradigms. The current\nliterature lacks implementation details for SDN-based DT and time-aware\nintelligent model training for short-term anomaly detection against IIoT\nthreats. Therefore, we have proposed a novel framework for short-term anomaly\ndetection that uses an SDN-based DT. Using a comprehensive dataset, time-aware\nlabeling of features, and a comprehensive evaluation of various machine\nlearning models, we propose a novel SD-TWIN-based anomaly detection algorithm.\nAccording to the performance of a new real-time SD-TWIN deployment, the GPU-\naccelerated LightGBM model is particularly effective, achieving a balance of\nhigh recall and strong classification performance."
                },
                "authors": [
                    {
                        "name": "Bilal Dalgic"
                    },
                    {
                        "name": "Betul Sen"
                    },
                    {
                        "name": "Muge Erel-Ozcevik"
                    }
                ],
                "author_detail": {
                    "name": "Muge Erel-Ozcevik"
                },
                "arxiv_affiliation": "Manisa Celal Bayar University, Turkey",
                "author": "Muge Erel-Ozcevik",
                "arxiv_comment": "Accepted by 2025 IEEE Globecom Workshops-TwinNetApp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20067v2",
                "updated": "2025-09-25T03:59:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    59,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-24T12:37:11Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    37,
                    11,
                    2,
                    267,
                    0
                ],
                "title": "MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM"
                },
                "summary": "Large language models (LLMs) have demonstrated notable potential in medical\napplications, yet they face substantial challenges in handling complex\nreal-world clinical diagnoses using conventional prompting methods. Current\nprompt engineering and multi-agent approaches typically optimize isolated\ninferences, neglecting the accumulation of reusable clinical experience. To\naddress this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)\nframework, which allows LLMs to self-learn clinical knowledge via a multi-agent\npipeline that summarizes, refines, and applies diagnostic insights. It mirrors\nhow physicians develop expertise through experience, enabling more focused and\naccurate diagnosis on key disease-specific cues. We further extend it to a\nMACD-human collaborative workflow, where multiple LLM-based diagnostician\nagents engage in iterative consultations, supported by an evaluator agent and\nhuman oversight for cases where agreement is not reached. Evaluated on 4,390\nreal-world patient cases across seven diseases using diverse open-source LLMs\n(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves\nprimary diagnostic accuracy, outperforming established clinical guidelines with\ngains up to 22.3% (MACD). On the subset of the data, it achieves performance on\npar with or exceeding that of human physicians (up to 16% improvement over\nphysicians-only diagnosis). Additionally, on the MACD-human workflow, it\nachieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,\nself-learned knowledge exhibits strong cross-model stability, transferability,\nand model-specific personalization, while the system can generate traceable\nrationales, enhancing explainability. Consequently, this work presents a\nscalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap\nbetween the intrinsic knowledge of LLMs and real-world clinical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated notable potential in medical\napplications, yet they face substantial challenges in handling complex\nreal-world clinical diagnoses using conventional prompting methods. Current\nprompt engineering and multi-agent approaches typically optimize isolated\ninferences, neglecting the accumulation of reusable clinical experience. To\naddress this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)\nframework, which allows LLMs to self-learn clinical knowledge via a multi-agent\npipeline that summarizes, refines, and applies diagnostic insights. It mirrors\nhow physicians develop expertise through experience, enabling more focused and\naccurate diagnosis on key disease-specific cues. We further extend it to a\nMACD-human collaborative workflow, where multiple LLM-based diagnostician\nagents engage in iterative consultations, supported by an evaluator agent and\nhuman oversight for cases where agreement is not reached. Evaluated on 4,390\nreal-world patient cases across seven diseases using diverse open-source LLMs\n(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves\nprimary diagnostic accuracy, outperforming established clinical guidelines with\ngains up to 22.3% (MACD). On the subset of the data, it achieves performance on\npar with or exceeding that of human physicians (up to 16% improvement over\nphysicians-only diagnosis). Additionally, on the MACD-human workflow, it\nachieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,\nself-learned knowledge exhibits strong cross-model stability, transferability,\nand model-specific personalization, while the system can generate traceable\nrationales, enhancing explainability. Consequently, this work presents a\nscalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap\nbetween the intrinsic knowledge of LLMs and real-world clinical practice."
                },
                "authors": [
                    {
                        "name": "Wenliang Li"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Hongji Zhu"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Junjun Li"
                    },
                    {
                        "name": "Mengru Li"
                    },
                    {
                        "name": "Wei Cao"
                    },
                    {
                        "name": "Zihang Jiang"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Shaohua Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shaohua Kevin Zhou"
                },
                "author": "Shaohua Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20051v1",
                "updated": "2025-09-24T12:19:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    19,
                    18,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T12:19:18Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    19,
                    18,
                    2,
                    267,
                    0
                ],
                "title": "One Filters All: A Generalist Filter for State Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Filters All: A Generalist Filter for State Estimation"
                },
                "summary": "Estimating hidden states in dynamical systems, also known as optimal\nfiltering, is a long-standing problem in various fields of science and\nengineering. In this paper, we introduce a general filtering framework,\n\\textbf{LLM-Filter}, which leverages large language models (LLMs) for state\nestimation by embedding noisy observations with text prototypes. In various\nexperiments for classical dynamical systems, we find that first, state\nestimation can significantly benefit from the reasoning knowledge embedded in\npre-trained LLMs. By achieving proper modality alignment with the frozen LLM,\nLLM-Filter outperforms the state-of-the-art learning-based approaches. Second,\nwe carefully design the prompt structure, System-as-Prompt (SaP), incorporating\ntask instructions that enable the LLM to understand the estimation tasks.\nGuided by these prompts, LLM-Filter exhibits exceptional generalization,\ncapable of performing filtering tasks accurately in changed or even unseen\nenvironments. We further observe a scaling-law behavior in LLM-Filter, where\naccuracy improves with larger model sizes and longer training times. These\nfindings make LLM-Filter a promising foundation model of filtering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating hidden states in dynamical systems, also known as optimal\nfiltering, is a long-standing problem in various fields of science and\nengineering. In this paper, we introduce a general filtering framework,\n\\textbf{LLM-Filter}, which leverages large language models (LLMs) for state\nestimation by embedding noisy observations with text prototypes. In various\nexperiments for classical dynamical systems, we find that first, state\nestimation can significantly benefit from the reasoning knowledge embedded in\npre-trained LLMs. By achieving proper modality alignment with the frozen LLM,\nLLM-Filter outperforms the state-of-the-art learning-based approaches. Second,\nwe carefully design the prompt structure, System-as-Prompt (SaP), incorporating\ntask instructions that enable the LLM to understand the estimation tasks.\nGuided by these prompts, LLM-Filter exhibits exceptional generalization,\ncapable of performing filtering tasks accurately in changed or even unseen\nenvironments. We further observe a scaling-law behavior in LLM-Filter, where\naccuracy improves with larger model sizes and longer training times. These\nfindings make LLM-Filter a promising foundation model of filtering."
                },
                "authors": [
                    {
                        "name": "Shiqi Liu"
                    },
                    {
                        "name": "Wenhan Cao"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Shengbo Eben Li"
                    }
                ],
                "author_detail": {
                    "name": "Shengbo Eben Li"
                },
                "author": "Shengbo Eben Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20045v1",
                "updated": "2025-09-24T12:13:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    13,
                    53,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T12:13:53Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    13,
                    53,
                    2,
                    267,
                    0
                ],
                "title": "Tokenization and Representation Biases in Multilingual Models on\n  Dialectal NLP Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization and Representation Biases in Multilingual Models on\n  Dialectal NLP Tasks"
                },
                "summary": "Dialectal data are characterized by linguistic variation that appears small\nto humans but has a significant impact on the performance of models. This\ndialect gap has been related to various factors (e.g., data size, economic and\nsocial factors) whose impact, however, turns out to be inconsistent. In this\nwork, we investigate factors impacting the model performance more directly: we\ncorrelate Tokenization Parity (TP) and Information Parity (IP), as measures of\nrepresentational biases in pre-trained multilingual models, with the downstream\nperformance. We compare state-of-the-art decoder-only LLMs with encoder-based\nmodels across three tasks: dialect classification, topic classification, and\nextractive question answering, controlling for varying scripts (Latin vs.\nnon-Latin) and resource availability (high vs. low). Our analysis reveals that\nTP is a better predictor of the performance on tasks reliant on syntactic and\nmorphological cues (e.g., extractive QA), while IP better predicts performance\nin semantic tasks (e.g., topic classification). Complementary analyses,\nincluding tokenizer behavior, vocabulary coverage, and qualitative insights,\nreveal that the language support claims of LLMs often might mask deeper\nmismatches at the script or token level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialectal data are characterized by linguistic variation that appears small\nto humans but has a significant impact on the performance of models. This\ndialect gap has been related to various factors (e.g., data size, economic and\nsocial factors) whose impact, however, turns out to be inconsistent. In this\nwork, we investigate factors impacting the model performance more directly: we\ncorrelate Tokenization Parity (TP) and Information Parity (IP), as measures of\nrepresentational biases in pre-trained multilingual models, with the downstream\nperformance. We compare state-of-the-art decoder-only LLMs with encoder-based\nmodels across three tasks: dialect classification, topic classification, and\nextractive question answering, controlling for varying scripts (Latin vs.\nnon-Latin) and resource availability (high vs. low). Our analysis reveals that\nTP is a better predictor of the performance on tasks reliant on syntactic and\nmorphological cues (e.g., extractive QA), while IP better predicts performance\nin semantic tasks (e.g., topic classification). Complementary analyses,\nincluding tokenizer behavior, vocabulary coverage, and qualitative insights,\nreveal that the language support claims of LLMs often might mask deeper\nmismatches at the script or token level."
                },
                "authors": [
                    {
                        "name": "Vani Kanjirangat"
                    },
                    {
                        "name": "Tanja Samardžić"
                    },
                    {
                        "name": "Ljiljana Dolamic"
                    },
                    {
                        "name": "Fabio Rinaldi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Rinaldi"
                },
                "author": "Fabio Rinaldi",
                "arxiv_comment": "Accepted in EMNLP-2025 Main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10697v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10697v3",
                "updated": "2025-09-24T12:12:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    12,
                    51,
                    2,
                    267,
                    0
                ],
                "published": "2024-11-16T04:35:17Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    4,
                    35,
                    17,
                    5,
                    321,
                    0
                ],
                "title": "Language Model Evolutionary Algorithms for Recommender Systems:\n  Benchmarks and Algorithm Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model Evolutionary Algorithms for Recommender Systems:\n  Benchmarks and Algorithm Comparisons"
                },
                "summary": "In the evolutionary computing community, the remarkable language-handling\ncapabilities and reasoning power of large language models (LLMs) have\nsignificantly enhanced the functionality of evolutionary algorithms (EAs),\nenabling them to tackle optimization problems involving structured language or\nprogram code. Although this field is still in its early stages, its impressive\npotential has led to the development of various LLM-based EAs. To effectively\nevaluate the performance and practical applicability of these LLM-based EAs,\nbenchmarks with real-world relevance are essential. In this paper, we focus on\nLLM-based recommender systems (RSs) and introduce a benchmark problem set,\nnamed RSBench, specifically designed to assess the performance of LLM-based EAs\nin recommendation prompt optimization. RSBench emphasizes session-based\nrecommendations, aiming to discover a set of Pareto optimal prompts that guide\nthe recommendation process, providing accurate, diverse, and fair\nrecommendations. We develop three LLM-based EAs based on established EA\nframeworks and experimentally evaluate their performance using RSBench. Our\nstudy offers valuable insights into the application of EAs in LLM-based RSs.\nAdditionally, we explore key components that may influence the overall\nperformance of the RS, providing meaningful guidance for future research on the\ndevelopment of LLM-based EAs in RSs. The source code of the proposed RSBench\ncan be found at https://github.com/LiuJ-2023/RSBench/tree/main.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolutionary computing community, the remarkable language-handling\ncapabilities and reasoning power of large language models (LLMs) have\nsignificantly enhanced the functionality of evolutionary algorithms (EAs),\nenabling them to tackle optimization problems involving structured language or\nprogram code. Although this field is still in its early stages, its impressive\npotential has led to the development of various LLM-based EAs. To effectively\nevaluate the performance and practical applicability of these LLM-based EAs,\nbenchmarks with real-world relevance are essential. In this paper, we focus on\nLLM-based recommender systems (RSs) and introduce a benchmark problem set,\nnamed RSBench, specifically designed to assess the performance of LLM-based EAs\nin recommendation prompt optimization. RSBench emphasizes session-based\nrecommendations, aiming to discover a set of Pareto optimal prompts that guide\nthe recommendation process, providing accurate, diverse, and fair\nrecommendations. We develop three LLM-based EAs based on established EA\nframeworks and experimentally evaluate their performance using RSBench. Our\nstudy offers valuable insights into the application of EAs in LLM-based RSs.\nAdditionally, we explore key components that may influence the overall\nperformance of the RS, providing meaningful guidance for future research on the\ndevelopment of LLM-based EAs in RSs. The source code of the proposed RSBench\ncan be found at https://github.com/LiuJ-2023/RSBench/tree/main."
                },
                "authors": [
                    {
                        "name": "Jiao Liu"
                    },
                    {
                        "name": "Zhu Sun"
                    },
                    {
                        "name": "Shanshan Feng"
                    },
                    {
                        "name": "Caishun Chen"
                    },
                    {
                        "name": "Yew-Soon Ong"
                    }
                ],
                "author_detail": {
                    "name": "Yew-Soon Ong"
                },
                "author": "Yew-Soon Ong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10697v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10697v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08590v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08590v3",
                "updated": "2025-09-24T12:06:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    6,
                    14,
                    2,
                    267,
                    0
                ],
                "published": "2025-04-11T14:49:33Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    14,
                    49,
                    33,
                    4,
                    101,
                    0
                ],
                "title": "Playpen: An Environment for Exploring Learning Through Conversational\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Playpen: An Environment for Exploring Learning Through Conversational\n  Interaction"
                },
                "summary": "Interaction between learner and feedback-giver has come into focus recently\nfor post-training of Large Language Models (LLMs), through the use of reward\nmodels that judge the appropriateness of a model's response. In this paper, we\ninvestigate whether Dialogue Games -- goal-directed and rule-governed\nactivities driven predominantly by verbal actions -- can also serve as a source\nof feedback signals for learning. We introduce Playpen, an environment for off-\nand online learning through Dialogue Game self-play, and investigate a\nrepresentative set of post-training methods: supervised fine-tuning; direct\nalignment (DPO); and reinforcement learning with GRPO. We experiment with\npost-training a small LLM (Llama-3.1-8B-Instruct), evaluating performance on\nunseen instances of training games as well as unseen games, and on standard\nbenchmarks. We find that imitation learning through SFT improves performance on\nunseen instances, but negatively impacts other skills, while interactive\nlearning with GRPO shows balanced improvements without loss of skills. We\nrelease the framework and the baseline training setups to foster research in\nthe promising new direction of learning in (synthetic) interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interaction between learner and feedback-giver has come into focus recently\nfor post-training of Large Language Models (LLMs), through the use of reward\nmodels that judge the appropriateness of a model's response. In this paper, we\ninvestigate whether Dialogue Games -- goal-directed and rule-governed\nactivities driven predominantly by verbal actions -- can also serve as a source\nof feedback signals for learning. We introduce Playpen, an environment for off-\nand online learning through Dialogue Game self-play, and investigate a\nrepresentative set of post-training methods: supervised fine-tuning; direct\nalignment (DPO); and reinforcement learning with GRPO. We experiment with\npost-training a small LLM (Llama-3.1-8B-Instruct), evaluating performance on\nunseen instances of training games as well as unseen games, and on standard\nbenchmarks. We find that imitation learning through SFT improves performance on\nunseen instances, but negatively impacts other skills, while interactive\nlearning with GRPO shows balanced improvements without loss of skills. We\nrelease the framework and the baseline training setups to foster research in\nthe promising new direction of learning in (synthetic) interaction."
                },
                "authors": [
                    {
                        "name": "Nicola Horst"
                    },
                    {
                        "name": "Davide Mazzaccara"
                    },
                    {
                        "name": "Antonia Schmidt"
                    },
                    {
                        "name": "Michael Sullivan"
                    },
                    {
                        "name": "Filippo Momentè"
                    },
                    {
                        "name": "Luca Franceschetti"
                    },
                    {
                        "name": "Philipp Sadler"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "Alberto Testoni"
                    },
                    {
                        "name": "Raffaella Bernardi"
                    },
                    {
                        "name": "Raquel Fernández"
                    },
                    {
                        "name": "Alexander Koller"
                    },
                    {
                        "name": "Oliver Lemon"
                    },
                    {
                        "name": "David Schlangen"
                    },
                    {
                        "name": "Mario Giulianelli"
                    },
                    {
                        "name": "Alessandro Suglia"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Suglia"
                },
                "author": "Alessandro Suglia",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main) Source code:\n  https://github.com/lm-playpen/playpen Please send correspodence to:\n  lm-playschool@googlegroups.com",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08590v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08590v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14359v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14359v4",
                "updated": "2025-09-24T12:04:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    4,
                    54,
                    2,
                    267,
                    0
                ],
                "published": "2025-02-20T08:36:58Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    36,
                    58,
                    3,
                    51,
                    0
                ],
                "title": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive\n  Tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive\n  Tests"
                },
                "summary": "We examine three evaluation paradigms: standard benchmarks (e.g., MMLU and\nBBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests\n(e.g., for working memory or theory of mind). First, we investigate which of\nthe former two-benchmarks or games-is most effective at discriminating LLMs of\nvarying quality. Then, inspired by human cognitive assessments, we compile a\nsuite of targeted tests that measure cognitive abilities deemed essential for\neffective language use, and we investigate their correlation with model\nperformance in benchmarks and games. Our analyses reveal that interactive games\nare superior to standard benchmarks in discriminating models. Causal and\nlogical reasoning correlate with both static and interactive tests, while\ndifferences emerge regarding core executive functions and social/emotional\nskills, which correlate more with games. We advocate for the development of new\ninteractive benchmarks and targeted cognitive tasks inspired by assessing human\nabilities but designed specifically for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine three evaluation paradigms: standard benchmarks (e.g., MMLU and\nBBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests\n(e.g., for working memory or theory of mind). First, we investigate which of\nthe former two-benchmarks or games-is most effective at discriminating LLMs of\nvarying quality. Then, inspired by human cognitive assessments, we compile a\nsuite of targeted tests that measure cognitive abilities deemed essential for\neffective language use, and we investigate their correlation with model\nperformance in benchmarks and games. Our analyses reveal that interactive games\nare superior to standard benchmarks in discriminating models. Causal and\nlogical reasoning correlate with both static and interactive tests, while\ndifferences emerge regarding core executive functions and social/emotional\nskills, which correlate more with games. We advocate for the development of new\ninteractive benchmarks and targeted cognitive tasks inspired by assessing human\nabilities but designed specifically for LLMs."
                },
                "authors": [
                    {
                        "name": "Filippo Momentè"
                    },
                    {
                        "name": "Alessandro Suglia"
                    },
                    {
                        "name": "Mario Giulianelli"
                    },
                    {
                        "name": "Ambra Ferrari"
                    },
                    {
                        "name": "Alexander Koller"
                    },
                    {
                        "name": "Oliver Lemon"
                    },
                    {
                        "name": "David Schlangen"
                    },
                    {
                        "name": "Raquel Fernández"
                    },
                    {
                        "name": "Raffaella Bernardi"
                    }
                ],
                "author_detail": {
                    "name": "Raffaella Bernardi"
                },
                "author": "Raffaella Bernardi",
                "arxiv_comment": "Accepted at EMNLP 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14359v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14359v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23368v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23368v3",
                "updated": "2025-09-24T12:01:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    1,
                    2,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-29T11:47:18Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    11,
                    47,
                    18,
                    3,
                    149,
                    0
                ],
                "title": "Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain\n  Human Label Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain\n  Human Label Variation"
                },
                "summary": "The recent rise of reasoning-tuned Large Language Models (LLMs)--which\ngenerate chains of thought (CoTs) before giving the final answer--has attracted\nsignificant attention and offers new opportunities for gaining insights into\nhuman label variation, which refers to plausible differences in how multiple\nannotators label the same data instance. Prior work has shown that\nLLM-generated explanations can help align model predictions with human label\ndistributions, but typically adopt a reverse paradigm: producing explanations\nbased on given answers. In contrast, CoTs provide a forward reasoning path that\nmay implicitly embed rationales for each answer option, before generating the\nanswers. We thus propose a novel LLM-based pipeline enriched with\nlinguistically-grounded discourse segmenters to extract supporting and opposing\nstatements for each answer option from CoTs with improved accuracy. We also\npropose a rank-based HLV evaluation framework that prioritizes the ranking of\nanswers over exact scores, which instead favor direct comparison of label\ndistributions. Our method outperforms a direct generation method as well as\nbaselines on three datasets, and shows better alignment of ranking methods with\nhumans, highlighting the effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent rise of reasoning-tuned Large Language Models (LLMs)--which\ngenerate chains of thought (CoTs) before giving the final answer--has attracted\nsignificant attention and offers new opportunities for gaining insights into\nhuman label variation, which refers to plausible differences in how multiple\nannotators label the same data instance. Prior work has shown that\nLLM-generated explanations can help align model predictions with human label\ndistributions, but typically adopt a reverse paradigm: producing explanations\nbased on given answers. In contrast, CoTs provide a forward reasoning path that\nmay implicitly embed rationales for each answer option, before generating the\nanswers. We thus propose a novel LLM-based pipeline enriched with\nlinguistically-grounded discourse segmenters to extract supporting and opposing\nstatements for each answer option from CoTs with improved accuracy. We also\npropose a rank-based HLV evaluation framework that prioritizes the ranking of\nanswers over exact scores, which instead favor direct comparison of label\ndistributions. Our method outperforms a direct generation method as well as\nbaselines on three datasets, and shows better alignment of ranking methods with\nhumans, highlighting the effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Beiduo Chen"
                    },
                    {
                        "name": "Yang Janet Liu"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "Accepted by EMNLP 2025 Main (Oral), 25 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23368v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23368v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20036v1",
                "updated": "2025-09-24T12:00:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    0,
                    34,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T12:00:34Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    12,
                    0,
                    34,
                    2,
                    267,
                    0
                ],
                "title": "MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation\n  Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation\n  Mapping"
                },
                "summary": "Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have\ndemonstrated impressive performance on challenging terrains, allowing robots to\nexecute complex skills such as climbing, running, and jumping. However,\nexisting blind locomotion controllers often struggle to ensure safety and\nefficient traversal through risky gap terrains, which are typically highly\ncomplex, requiring robots to perceive terrain information and select\nappropriate footholds during locomotion accurately. Meanwhile, existing\nperception-based controllers still present several practical limitations,\nincluding a complex multi-sensor deployment system and expensive computing\nresource requirements. This paper proposes a DRL controller named MAstering\nRisky Gap Terrains (MARG), which integrates terrain maps and proprioception to\ndynamically adjust the action and enhance the robot's stability in these tasks.\nDuring the training phase, our controller accelerates policy optimization by\nselectively incorporating privileged information (e.g., center of mass,\nfriction coefficients) that are available in simulation but unmeasurable\ndirectly in real-world deployments due to sensor limitations. We also designed\nthree foot-related rewards to encourage the robot to explore safe footholds.\nMore importantly, a terrain map generation (TMG) model is proposed to reduce\nthe drift existing in mapping and provide accurate terrain maps using only one\nLiDAR, providing a foundation for zero-shot transfer of the learned policy. The\nexperimental results indicate that MARG maintains stability in various risky\nterrain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have\ndemonstrated impressive performance on challenging terrains, allowing robots to\nexecute complex skills such as climbing, running, and jumping. However,\nexisting blind locomotion controllers often struggle to ensure safety and\nefficient traversal through risky gap terrains, which are typically highly\ncomplex, requiring robots to perceive terrain information and select\nappropriate footholds during locomotion accurately. Meanwhile, existing\nperception-based controllers still present several practical limitations,\nincluding a complex multi-sensor deployment system and expensive computing\nresource requirements. This paper proposes a DRL controller named MAstering\nRisky Gap Terrains (MARG), which integrates terrain maps and proprioception to\ndynamically adjust the action and enhance the robot's stability in these tasks.\nDuring the training phase, our controller accelerates policy optimization by\nselectively incorporating privileged information (e.g., center of mass,\nfriction coefficients) that are available in simulation but unmeasurable\ndirectly in real-world deployments due to sensor limitations. We also designed\nthree foot-related rewards to encourage the robot to explore safe footholds.\nMore importantly, a terrain map generation (TMG) model is proposed to reduce\nthe drift existing in mapping and provide accurate terrain maps using only one\nLiDAR, providing a foundation for zero-shot transfer of the learned policy. The\nexperimental results indicate that MARG maintains stability in various risky\nterrain tasks."
                },
                "authors": [
                    {
                        "name": "Yinzhao Dong"
                    },
                    {
                        "name": "Ji Ma"
                    },
                    {
                        "name": "Liu Zhao"
                    },
                    {
                        "name": "Wanyue Li"
                    },
                    {
                        "name": "Peng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Peng Lu"
                },
                "author": "Peng Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22729v2",
                "updated": "2025-09-24T11:55:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    55,
                    48,
                    2,
                    267,
                    0
                ],
                "published": "2025-07-30T14:49:30Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    49,
                    30,
                    2,
                    211,
                    0
                ],
                "title": "Resource-Efficient Adaptation of Large Language Models for Text\n  Embeddings via Prompt Engineering and Contrastive Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Adaptation of Large Language Models for Text\n  Embeddings via Prompt Engineering and Contrastive Fine-tuning"
                },
                "summary": "Large Language Models (LLMs) have become a cornerstone in Natural Language\nProcessing (NLP), achieving impressive performance in text generation. Their\ntoken-level representations capture rich, human-aligned semantics. However,\npooling these vectors into a text embedding discards crucial information.\nNevertheless, many non-generative downstream tasks, such as clustering,\nclassification, or retrieval, still depend on accurate and controllable\nsentence- or document-level embeddings. We explore several adaptation\nstrategies for pre-trained, decoder-only LLMs: (i) various aggregation\ntechniques for token embeddings, (ii) task-specific prompt engineering, and\n(iii) text-level augmentation via contrastive fine-tuning. Combining these\ncomponents yields competitive performance on the English clustering track of\nthe Massive Text Embedding Benchmark (MTEB). An analysis of the attention map\nfurther shows that fine-tuning shifts focus from prompt tokens to semantically\nrelevant words, indicating more effective compression of meaning into the final\nhidden state. Our experiments demonstrate that LLMs can be effectively adapted\nas text embedding models through a combination of prompt engineering and\nresource-efficient contrastive fine-tuning on synthetically generated positive\npairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become a cornerstone in Natural Language\nProcessing (NLP), achieving impressive performance in text generation. Their\ntoken-level representations capture rich, human-aligned semantics. However,\npooling these vectors into a text embedding discards crucial information.\nNevertheless, many non-generative downstream tasks, such as clustering,\nclassification, or retrieval, still depend on accurate and controllable\nsentence- or document-level embeddings. We explore several adaptation\nstrategies for pre-trained, decoder-only LLMs: (i) various aggregation\ntechniques for token embeddings, (ii) task-specific prompt engineering, and\n(iii) text-level augmentation via contrastive fine-tuning. Combining these\ncomponents yields competitive performance on the English clustering track of\nthe Massive Text Embedding Benchmark (MTEB). An analysis of the attention map\nfurther shows that fine-tuning shifts focus from prompt tokens to semantically\nrelevant words, indicating more effective compression of meaning into the final\nhidden state. Our experiments demonstrate that LLMs can be effectively adapted\nas text embedding models through a combination of prompt engineering and\nresource-efficient contrastive fine-tuning on synthetically generated positive\npairs."
                },
                "authors": [
                    {
                        "name": "Benedikt Roth"
                    },
                    {
                        "name": "Stephan Rappensperger"
                    },
                    {
                        "name": "Tianming Qiu"
                    },
                    {
                        "name": "Hamza Imamović"
                    },
                    {
                        "name": "Julian Wörmann"
                    },
                    {
                        "name": "Hao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Shen"
                },
                "author": "Hao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21625v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21625v5",
                "updated": "2025-09-24T11:52:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    52,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2025-04-30T13:28:19Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    28,
                    19,
                    2,
                    120,
                    0
                ],
                "title": "Meeseeks: A Feedback-Driven, Iterative Self-Correction Benchmark\n  evaluating LLMs' Instruction Following Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meeseeks: A Feedback-Driven, Iterative Self-Correction Benchmark\n  evaluating LLMs' Instruction Following Capability"
                },
                "summary": "The capability to precisely adhere to instructions is a cornerstone for Large\nLanguage Models (LLMs) to function as dependable agents in real-world\nscenarios. However, confronted with complex prompts, LLMs frequently encounter\ndifficulties in fulfilling all specified requirements within a single response.\nDrawing inspiration from recent advancements in Chain-of-Thought (CoT)\nprompting and self-correction methodologies, we introduce Meeseeks (The name is\ninspired by Mr. Meeseeks from \"Rick and Morty,\" a character renowned for\nefficiently accomplishing assigned tasks. See:\nhttps://en.wikipedia.org/wiki/Mr._Meeseeks), a fully automated iterative\ninstruction-following benchmark equipped with an integrated feedback mechanism.\nMeeseeks identifies erroneous components in model responses and provides\ncorresponding feedback accurately, thereby iteratively guiding the model toward\nself-correction. The dataset contains over 700 curated instances annotated by\n32 distinct capability tags in Chinese and English. Extensive experimental\nresults reveal that different state-of-the-art commercial and open-source LLMs\nexhibit vastly disparate performance, and even after 20 turns of iterative\nfeedback-driven self-correction, nearly all models demonstrate suboptimal\nperformance. We conducted comprehensive analysis from both macro and instance\nlevels, uncovering numerous common issues prevalent in current state-of-the-art\nmodels, as well as several counterintuitive phenomena. We've open-sourced our\nwork on https://github.com/ADoublLEN/Meeseeks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capability to precisely adhere to instructions is a cornerstone for Large\nLanguage Models (LLMs) to function as dependable agents in real-world\nscenarios. However, confronted with complex prompts, LLMs frequently encounter\ndifficulties in fulfilling all specified requirements within a single response.\nDrawing inspiration from recent advancements in Chain-of-Thought (CoT)\nprompting and self-correction methodologies, we introduce Meeseeks (The name is\ninspired by Mr. Meeseeks from \"Rick and Morty,\" a character renowned for\nefficiently accomplishing assigned tasks. See:\nhttps://en.wikipedia.org/wiki/Mr._Meeseeks), a fully automated iterative\ninstruction-following benchmark equipped with an integrated feedback mechanism.\nMeeseeks identifies erroneous components in model responses and provides\ncorresponding feedback accurately, thereby iteratively guiding the model toward\nself-correction. The dataset contains over 700 curated instances annotated by\n32 distinct capability tags in Chinese and English. Extensive experimental\nresults reveal that different state-of-the-art commercial and open-source LLMs\nexhibit vastly disparate performance, and even after 20 turns of iterative\nfeedback-driven self-correction, nearly all models demonstrate suboptimal\nperformance. We conducted comprehensive analysis from both macro and instance\nlevels, uncovering numerous common issues prevalent in current state-of-the-art\nmodels, as well as several counterintuitive phenomena. We've open-sourced our\nwork on https://github.com/ADoublLEN/Meeseeks."
                },
                "authors": [
                    {
                        "name": "Jiaming wang"
                    },
                    {
                        "name": "Yunke Zhao"
                    },
                    {
                        "name": "Peng Ding"
                    },
                    {
                        "name": "Jun Kuang"
                    },
                    {
                        "name": "Yibin Shen"
                    },
                    {
                        "name": "Zhe Tang"
                    },
                    {
                        "name": "Yilin Jin"
                    },
                    {
                        "name": "ZongYu Wang"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21625v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21625v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12744v2",
                "updated": "2025-09-24T11:46:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    46,
                    54,
                    2,
                    267,
                    0
                ],
                "published": "2025-07-17T02:59:35Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    2,
                    59,
                    35,
                    3,
                    198,
                    0
                ],
                "title": "ASC-SW: Atrous strip convolution network with sliding windows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASC-SW: Atrous strip convolution network with sliding windows"
                },
                "summary": "With the rapid development of lightweight visual neural network\narchitectures, traditional high-performance vision models have undergone\nsignificant compression, enhancing their computational and energy efficiency\nand enabling deployment on resource-constrained edge devices. In order to\nenable the mobile robot to avoid the ground wires, we propose a visual-assisted\nnavigation framework called Atrous Strip Convolution Sliding Window (ASC-SW).\nThis framework compensates for the limitations of traditional light detection\nand range (LiDAR) sensors to detect ground-level obstacles such as wires. A\nlightweight and efficient segmentation model, Atrous Strip Convolution Network\n(ASCnet) was proposed, for detecting deformable linear objects (DLOs). Atrous\nStrip Convolution Spatial Pyramid Pooling (ASCSPP) is designed to extract DLOs\nfeatures effectively. Atrous Strip Convolution is integrated into ASCSPP to\naccurately identify the linear structure of DLOs with low computational cost.\nAdditionally, a Sliding Window (SW) post processing module is proposed to\ndenoise the output in complex environments, improving recognition accuracy.\nASC-SW achieves 75.3% MIoU at 217 FPS on a self-built real world dataset and\nreal-robot experiment was demonstrated that our proposed framework. It can be\nsuccessfully verified on the real-robot on the edge device(Jetson platform) at\nthat were originally inoperable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of lightweight visual neural network\narchitectures, traditional high-performance vision models have undergone\nsignificant compression, enhancing their computational and energy efficiency\nand enabling deployment on resource-constrained edge devices. In order to\nenable the mobile robot to avoid the ground wires, we propose a visual-assisted\nnavigation framework called Atrous Strip Convolution Sliding Window (ASC-SW).\nThis framework compensates for the limitations of traditional light detection\nand range (LiDAR) sensors to detect ground-level obstacles such as wires. A\nlightweight and efficient segmentation model, Atrous Strip Convolution Network\n(ASCnet) was proposed, for detecting deformable linear objects (DLOs). Atrous\nStrip Convolution Spatial Pyramid Pooling (ASCSPP) is designed to extract DLOs\nfeatures effectively. Atrous Strip Convolution is integrated into ASCSPP to\naccurately identify the linear structure of DLOs with low computational cost.\nAdditionally, a Sliding Window (SW) post processing module is proposed to\ndenoise the output in complex environments, improving recognition accuracy.\nASC-SW achieves 75.3% MIoU at 217 FPS on a self-built real world dataset and\nreal-robot experiment was demonstrated that our proposed framework. It can be\nsuccessfully verified on the real-robot on the edge device(Jetson platform) at\nthat were originally inoperable."
                },
                "authors": [
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Fan Zhu"
                    },
                    {
                        "name": "Yifeng Xu"
                    },
                    {
                        "name": "Baoru Huang"
                    },
                    {
                        "name": "Mohd Rizal Arshad"
                    }
                ],
                "author_detail": {
                    "name": "Mohd Rizal Arshad"
                },
                "author": "Mohd Rizal Arshad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23745v2",
                "updated": "2025-09-24T11:40:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    40,
                    31,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-29T17:59:01Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    1,
                    3,
                    149,
                    0
                ],
                "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Trust Or Not To Trust Your Vision-Language Model's Prediction"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated strong capabilities in\naligning visual and textual modalities, enabling a wide range of applications\nin multimodal understanding and generation. While they excel in zero-shot and\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\noften yielding confident yet incorrect predictions. This limitation poses a\nsignificant risk in safety-critical domains, where erroneous predictions can\nlead to severe consequences. In this work, we introduce TrustVLM, a\ntraining-free framework designed to address the critical challenge of\nestimating when VLM's predictions can be trusted. Motivated by the observed\nmodality gap in VLMs and the insight that certain concepts are more distinctly\nrepresented in the image embedding space, we propose a novel confidence-scoring\nfunction that leverages this space to improve misclassification detection. We\nrigorously evaluate our approach across 17 diverse datasets, employing 4\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\ncompared to existing baselines. By improving the reliability of the model\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\nVLMs in real-world applications. The code is available at\nhttps://github.com/EPFL-IMOS/TrustVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated strong capabilities in\naligning visual and textual modalities, enabling a wide range of applications\nin multimodal understanding and generation. While they excel in zero-shot and\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\noften yielding confident yet incorrect predictions. This limitation poses a\nsignificant risk in safety-critical domains, where erroneous predictions can\nlead to severe consequences. In this work, we introduce TrustVLM, a\ntraining-free framework designed to address the critical challenge of\nestimating when VLM's predictions can be trusted. Motivated by the observed\nmodality gap in VLMs and the insight that certain concepts are more distinctly\nrepresented in the image embedding space, we propose a novel confidence-scoring\nfunction that leverages this space to improve misclassification detection. We\nrigorously evaluate our approach across 17 diverse datasets, employing 4\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\ncompared to existing baselines. By improving the reliability of the model\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\nVLMs in real-world applications. The code is available at\nhttps://github.com/EPFL-IMOS/TrustVLM."
                },
                "authors": [
                    {
                        "name": "Hao Dong"
                    },
                    {
                        "name": "Moru Liu"
                    },
                    {
                        "name": "Jian Liang"
                    },
                    {
                        "name": "Eleni Chatzi"
                    },
                    {
                        "name": "Olga Fink"
                    }
                ],
                "author_detail": {
                    "name": "Olga Fink"
                },
                "author": "Olga Fink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20021v1",
                "updated": "2025-09-24T11:37:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    37,
                    48,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T11:37:48Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    37,
                    48,
                    2,
                    267,
                    0
                ],
                "title": "Embodied AI: From LLMs to World Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI: From LLMs to World Models"
                },
                "summary": "Embodied Artificial Intelligence (AI) is an intelligent system paradigm for\nachieving Artificial General Intelligence (AGI), serving as the cornerstone for\nvarious applications and driving the evolution from cyberspace to physical\nsystems. Recent breakthroughs in Large Language Models (LLMs) and World Models\n(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs\nempower embodied AI via semantic reasoning and task decomposition, bringing\nhigh-level natural language instructions and low-level natural language actions\ninto embodied cognition. On the other hand, WMs empower embodied AI by building\ninternal representations and future predictions of the external world,\nfacilitating physical law-compliant embodied interactions. As such, this paper\ncomprehensively explores the literature in embodied AI from basics to advances,\ncovering both LLM driven and WM driven works. In particular, we first present\nthe history, key technologies, key components, and hardware systems of embodied\nAI, as well as discuss its development via looking from unimodal to multimodal\nangle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,\nembodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,\nmeticulously delineating their indispensable roles in end-to-end embodied\ncognition and physical laws-driven embodied interactions. Building upon the\nabove advances, we further share our insights on the necessity of the joint\nMLLM-WM driven embodied AI architecture, shedding light on its profound\nsignificance in enabling complex tasks within physical worlds. In addition, we\nexamine representative applications of embodied AI, demonstrating its wide\napplicability in real-world scenarios. Last but not least, we point out future\nresearch directions of embodied AI that deserve further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Artificial Intelligence (AI) is an intelligent system paradigm for\nachieving Artificial General Intelligence (AGI), serving as the cornerstone for\nvarious applications and driving the evolution from cyberspace to physical\nsystems. Recent breakthroughs in Large Language Models (LLMs) and World Models\n(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs\nempower embodied AI via semantic reasoning and task decomposition, bringing\nhigh-level natural language instructions and low-level natural language actions\ninto embodied cognition. On the other hand, WMs empower embodied AI by building\ninternal representations and future predictions of the external world,\nfacilitating physical law-compliant embodied interactions. As such, this paper\ncomprehensively explores the literature in embodied AI from basics to advances,\ncovering both LLM driven and WM driven works. In particular, we first present\nthe history, key technologies, key components, and hardware systems of embodied\nAI, as well as discuss its development via looking from unimodal to multimodal\nangle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,\nembodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,\nmeticulously delineating their indispensable roles in end-to-end embodied\ncognition and physical laws-driven embodied interactions. Building upon the\nabove advances, we further share our insights on the necessity of the joint\nMLLM-WM driven embodied AI architecture, shedding light on its profound\nsignificance in enabling complex tasks within physical worlds. In addition, we\nexamine representative applications of embodied AI, demonstrating its wide\napplicability in real-world scenarios. Last but not least, we point out future\nresearch directions of embodied AI that deserve further investigation."
                },
                "authors": [
                    {
                        "name": "Tongtong Feng"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    },
                    {
                        "name": "Wenwu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Zhu"
                },
                "author": "Wenwu Zhu",
                "arxiv_comment": "Accepted by IEEE CASM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09505v2",
                "updated": "2025-09-24T11:31:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    31,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-11T14:49:50Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    49,
                    50,
                    3,
                    254,
                    0
                ],
                "title": "Combating the Memory Walls: Optimization Pathways for Long-Context\n  Agentic LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combating the Memory Walls: Optimization Pathways for Long-Context\n  Agentic LLM Inference"
                },
                "summary": "LLMs now form the backbone of AI agents for a diverse array of applications,\nincluding tool use, command-line agents, and web or computer use agents. These\nagentic LLM inference tasks are fundamentally different from chatbot-focused\ninference -- they often have much larger context lengths to capture complex,\nprolonged inputs, such as entire webpage DOMs or complicated tool call\ntrajectories. This, in turn, generates significant off-chip memory traffic for\nthe underlying hardware at the inference stage and causes the workload to be\nconstrained by two memory walls, namely the bandwidth and capacity memory\nwalls, preventing the on-chip compute units from achieving high utilization.\n  In this paper, we introduce PLENA, a hardware-software co-designed system\nthat applies three core optimization pathways to tackle these challenges. PLENA\nincludes an efficient hardware implementation of compute and memory units\nsupporting an asymmetric quantization scheme. PLENA also features a novel\nflattened systolic array architecture that has native support for\nFlashAttention to tackle these memory walls in the scenario of inference\nserving for long-context LLMs. Additionally, PLENA is developed with a complete\nstack, including a custom ISA, a compiler, a cycle-emulated simulator, and an\nautomated design space exploration flow. The simulated results show that PLENA\nachieves up to 8.5x higher utilization than existing accelerators, and delivers\n2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the\nTPU v6e, under the same multiplier count and memory settings. The full PLENA\nsystem will also be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs now form the backbone of AI agents for a diverse array of applications,\nincluding tool use, command-line agents, and web or computer use agents. These\nagentic LLM inference tasks are fundamentally different from chatbot-focused\ninference -- they often have much larger context lengths to capture complex,\nprolonged inputs, such as entire webpage DOMs or complicated tool call\ntrajectories. This, in turn, generates significant off-chip memory traffic for\nthe underlying hardware at the inference stage and causes the workload to be\nconstrained by two memory walls, namely the bandwidth and capacity memory\nwalls, preventing the on-chip compute units from achieving high utilization.\n  In this paper, we introduce PLENA, a hardware-software co-designed system\nthat applies three core optimization pathways to tackle these challenges. PLENA\nincludes an efficient hardware implementation of compute and memory units\nsupporting an asymmetric quantization scheme. PLENA also features a novel\nflattened systolic array architecture that has native support for\nFlashAttention to tackle these memory walls in the scenario of inference\nserving for long-context LLMs. Additionally, PLENA is developed with a complete\nstack, including a custom ISA, a compiler, a cycle-emulated simulator, and an\nautomated design space exploration flow. The simulated results show that PLENA\nachieves up to 8.5x higher utilization than existing accelerators, and delivers\n2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the\nTPU v6e, under the same multiplier count and memory settings. The full PLENA\nsystem will also be open-sourced."
                },
                "authors": [
                    {
                        "name": "Haoran Wu"
                    },
                    {
                        "name": "Can Xiao"
                    },
                    {
                        "name": "Jiayi Nie"
                    },
                    {
                        "name": "Xuan Guo"
                    },
                    {
                        "name": "Binglei Lou"
                    },
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Przemyslaw Forys"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Timothy M. Jones"
                    },
                    {
                        "name": "Rika Antonova"
                    },
                    {
                        "name": "Robert Mullins"
                    },
                    {
                        "name": "Aaron Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Zhao"
                },
                "author": "Aaron Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19170v2",
                "updated": "2025-09-24T11:28:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    28,
                    42,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-23T15:43:47Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    43,
                    47,
                    1,
                    266,
                    0
                ],
                "title": "Soft Tokens, Hard Truths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Tokens, Hard Truths"
                },
                "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model."
                },
                "authors": [
                    {
                        "name": "Natasha Butt"
                    },
                    {
                        "name": "Ariel Kwiatkowski"
                    },
                    {
                        "name": "Ismail Labiad"
                    },
                    {
                        "name": "Julia Kempe"
                    },
                    {
                        "name": "Yann Ollivier"
                    }
                ],
                "author_detail": {
                    "name": "Yann Ollivier"
                },
                "author": "Yann Ollivier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20009v1",
                "updated": "2025-09-24T11:28:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    28,
                    10,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T11:28:10Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    28,
                    10,
                    2,
                    267,
                    0
                ],
                "title": "Lidar-based Tracking of Traffic Participants with Sensor Nodes in\n  Existing Urban Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lidar-based Tracking of Traffic Participants with Sensor Nodes in\n  Existing Urban Infrastructure"
                },
                "summary": "This paper presents a lidar-only state estimation and tracking framework,\nalong with a roadside sensing unit for integration with existing urban\ninfrastructure. Urban deployments demand scalable, real-time tracking\nsolutions, yet traditional remote sensing remains costly and computationally\nintensive, especially under perceptually degraded conditions. Our sensor node\ncouples a single lidar with an edge computing unit and runs a computationally\nefficient, GPU-free observer that simultaneously estimates object state, class,\ndimensions, and existence probability. The pipeline performs: (i) state updates\nvia an extended Kalman filter, (ii) dimension estimation using a 1D\ngrid-map/Bayesian update, (iii) class updates via a lookup table driven by the\nmost probable footprint, and (iv) existence estimation from track age and\nbounding-box consistency. Experiments in dynamic urban-like scenes with diverse\ntraffic participants demonstrate real-time performance and high precision: The\ncomplete end-to-end pipeline finishes within \\SI{100}{\\milli\\second} for\n\\SI{99.88}{\\%} of messages, with an excellent detection rate. Robustness is\nfurther confirmed under simulated wind and sensor vibration. These results\nindicate that reliable, real-time roadside tracking is feasible on CPU-only\nedge hardware, enabling scalable, privacy-friendly deployments within existing\ncity infrastructure. The framework integrates with existing poles, traffic\nlights, and buildings, reducing deployment costs and simplifying large-scale\nurban rollouts and maintenance efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a lidar-only state estimation and tracking framework,\nalong with a roadside sensing unit for integration with existing urban\ninfrastructure. Urban deployments demand scalable, real-time tracking\nsolutions, yet traditional remote sensing remains costly and computationally\nintensive, especially under perceptually degraded conditions. Our sensor node\ncouples a single lidar with an edge computing unit and runs a computationally\nefficient, GPU-free observer that simultaneously estimates object state, class,\ndimensions, and existence probability. The pipeline performs: (i) state updates\nvia an extended Kalman filter, (ii) dimension estimation using a 1D\ngrid-map/Bayesian update, (iii) class updates via a lookup table driven by the\nmost probable footprint, and (iv) existence estimation from track age and\nbounding-box consistency. Experiments in dynamic urban-like scenes with diverse\ntraffic participants demonstrate real-time performance and high precision: The\ncomplete end-to-end pipeline finishes within \\SI{100}{\\milli\\second} for\n\\SI{99.88}{\\%} of messages, with an excellent detection rate. Robustness is\nfurther confirmed under simulated wind and sensor vibration. These results\nindicate that reliable, real-time roadside tracking is feasible on CPU-only\nedge hardware, enabling scalable, privacy-friendly deployments within existing\ncity infrastructure. The framework integrates with existing poles, traffic\nlights, and buildings, reducing deployment costs and simplifying large-scale\nurban rollouts and maintenance efforts."
                },
                "authors": [
                    {
                        "name": "Simon Schäfer"
                    },
                    {
                        "name": "Bassam Alrifaee"
                    },
                    {
                        "name": "Ehsan Hashemi"
                    }
                ],
                "author_detail": {
                    "name": "Ehsan Hashemi"
                },
                "author": "Ehsan Hashemi",
                "arxiv_comment": "21 pages, 9 figures, this work was submitted to Wileys'Advanced\n  Intelligent Systems for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20007v1",
                "updated": "2025-09-24T11:27:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    27,
                    7,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T11:27:07Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    27,
                    7,
                    2,
                    267,
                    0
                ],
                "title": "DiffNator: Generating Structured Explanations of Time-Series Differences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffNator: Generating Structured Explanations of Time-Series Differences"
                },
                "summary": "In many IoT applications, the central interest lies not in individual sensor\nsignals but in their differences, yet interpreting such differences requires\nexpert knowledge. We propose DiffNator, a framework for structured explanations\nof differences between two time series. We first design a JSON schema that\ncaptures the essential properties of such differences. Using the Time-series\nObservations of Real-world IoT (TORI) dataset, we generate paired sequences and\ntrain a model that combine a time-series encoder with a frozen LLM to output\nJSON-formatted explanations. Experimental results show that DiffNator generates\naccurate difference explanations and substantially outperforms both a visual\nquestion answering (VQA) baseline and a retrieval method using a pre-trained\ntime-series encoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many IoT applications, the central interest lies not in individual sensor\nsignals but in their differences, yet interpreting such differences requires\nexpert knowledge. We propose DiffNator, a framework for structured explanations\nof differences between two time series. We first design a JSON schema that\ncaptures the essential properties of such differences. Using the Time-series\nObservations of Real-world IoT (TORI) dataset, we generate paired sequences and\ntrain a model that combine a time-series encoder with a frozen LLM to output\nJSON-formatted explanations. Experimental results show that DiffNator generates\naccurate difference explanations and substantially outperforms both a visual\nquestion answering (VQA) baseline and a retrieval method using a pre-trained\ntime-series encoder."
                },
                "authors": [
                    {
                        "name": "Kota Dohi"
                    },
                    {
                        "name": "Tomoya Nishida"
                    },
                    {
                        "name": "Harsh Purohit"
                    },
                    {
                        "name": "Takashi Endo"
                    },
                    {
                        "name": "Yohei Kawaguchi"
                    }
                ],
                "author_detail": {
                    "name": "Yohei Kawaguchi"
                },
                "author": "Yohei Kawaguchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20004v1",
                "updated": "2025-09-24T11:24:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    24,
                    49,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T11:24:49Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    24,
                    49,
                    2,
                    267,
                    0
                ],
                "title": "The Knowledge-Behaviour Disconnect in LLM-based Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Knowledge-Behaviour Disconnect in LLM-based Chatbots"
                },
                "summary": "Large language model-based artificial conversational agents (like ChatGPT)\ngive answers to all kinds of questions, and often enough these answers are\ncorrect. Just on the basis of that capacity alone, we may attribute knowledge\nto them. But do these models use this knowledge as a basis for their own\nconversational behaviour? I argue this is not the case, and I will refer to\nthis failure as a `disconnect'. I further argue this disconnect is fundamental\nin the sense that with more data and more training of the LLM on which a\nconversational chatbot is based, it will not disappear. The reason is, as I\nwill claim, that the core technique used to train LLMs does not allow for the\nestablishment of the connection we are after. The disconnect reflects a\nfundamental limitation on the capacities of LLMs, and explains the source of\nhallucinations. I will furthermore consider the ethical version of the\ndisconnect (ethical conversational knowledge not being aligned with ethical\nconversational behaviour), since in this domain researchers have come up with\nseveral additional techniques to influence a chatbot's behaviour. I will\ndiscuss how these techniques do nothing to solve the disconnect and can make it\nworse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based artificial conversational agents (like ChatGPT)\ngive answers to all kinds of questions, and often enough these answers are\ncorrect. Just on the basis of that capacity alone, we may attribute knowledge\nto them. But do these models use this knowledge as a basis for their own\nconversational behaviour? I argue this is not the case, and I will refer to\nthis failure as a `disconnect'. I further argue this disconnect is fundamental\nin the sense that with more data and more training of the LLM on which a\nconversational chatbot is based, it will not disappear. The reason is, as I\nwill claim, that the core technique used to train LLMs does not allow for the\nestablishment of the connection we are after. The disconnect reflects a\nfundamental limitation on the capacities of LLMs, and explains the source of\nhallucinations. I will furthermore consider the ethical version of the\ndisconnect (ethical conversational knowledge not being aligned with ethical\nconversational behaviour), since in this domain researchers have come up with\nseveral additional techniques to influence a chatbot's behaviour. I will\ndiscuss how these techniques do nothing to solve the disconnect and can make it\nworse."
                },
                "authors": [
                    {
                        "name": "Jan Broersen"
                    }
                ],
                "author_detail": {
                    "name": "Jan Broersen"
                },
                "author": "Jan Broersen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01413v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01413v7",
                "updated": "2025-09-24T11:19:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    19,
                    59,
                    2,
                    267,
                    0
                ],
                "published": "2025-06-02T08:11:44Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    11,
                    44,
                    0,
                    153,
                    0
                ],
                "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models"
                },
                "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose RAIF, a systematic method to boost LLMs in dealing with\ncomplex instructions via incentivizing reasoning for test-time compute scaling.\nFirst, we stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on\nOOD constraints also confirms the generalizability of our RAIF. Codes and data\nare available at https://github.com/yuleiqin/RAIF.\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose RAIF, a systematic method to boost LLMs in dealing with\ncomplex instructions via incentivizing reasoning for test-time compute scaling.\nFirst, we stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on\nOOD constraints also confirms the generalizability of our RAIF. Codes and data\nare available at https://github.com/yuleiqin/RAIF.\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions"
                },
                "authors": [
                    {
                        "name": "Yulei Qin"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Zongyi Li"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Yuchen Shi"
                    },
                    {
                        "name": "Zhekai Lin"
                    },
                    {
                        "name": "Xiao Cui"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "Accepted to NeurIPS 2025; 15 pages of main body, 5 tables, 5 figures,\n  42 pages of appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01413v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01413v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11856v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11856v2",
                "updated": "2025-09-24T11:06:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    6,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2025-02-17T14:48:18Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    48,
                    18,
                    0,
                    48,
                    0
                ],
                "title": "LLMs as a synthesis between symbolic and distributed approaches to\n  language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as a synthesis between symbolic and distributed approaches to\n  language"
                },
                "summary": "Since the middle of the 20th century, a fierce battle is being fought between\nsymbolic and distributed approaches to language and cognition. The success of\ndeep learning models, and LLMs in particular, has been alternatively taken as\nshowing that the distributed camp has won, or dismissed as an irrelevant\nengineering development. In this position paper, I argue that deep learning\nmodels for language actually represent a synthesis between the two traditions.\nThis is because 1) deep learning architectures allow for both\ndistributed/continuous/fuzzy and symbolic/discrete/categorical-like\nrepresentations and processing; 2) models trained on language make use of this\nflexibility. In particular, I review recent research in interpretability that\nshowcases how a substantial part of morphosyntactic knowledge is encoded in a\nnear-discrete fashion in LLMs. This line of research suggests that different\nbehaviors arise in an emergent fashion, and models flexibly alternate between\nthe two modes (and everything in between) as needed. This is possibly one of\nthe main reasons for their wild success; and it makes them particularly\ninteresting for the study of language. Is it time for peace?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the middle of the 20th century, a fierce battle is being fought between\nsymbolic and distributed approaches to language and cognition. The success of\ndeep learning models, and LLMs in particular, has been alternatively taken as\nshowing that the distributed camp has won, or dismissed as an irrelevant\nengineering development. In this position paper, I argue that deep learning\nmodels for language actually represent a synthesis between the two traditions.\nThis is because 1) deep learning architectures allow for both\ndistributed/continuous/fuzzy and symbolic/discrete/categorical-like\nrepresentations and processing; 2) models trained on language make use of this\nflexibility. In particular, I review recent research in interpretability that\nshowcases how a substantial part of morphosyntactic knowledge is encoded in a\nnear-discrete fashion in LLMs. This line of research suggests that different\nbehaviors arise in an emergent fashion, and models flexibly alternate between\nthe two modes (and everything in between) as needed. This is possibly one of\nthe main reasons for their wild success; and it makes them particularly\ninteresting for the study of language. Is it time for peace?"
                },
                "authors": [
                    {
                        "name": "Gemma Boleda"
                    }
                ],
                "author_detail": {
                    "name": "Gemma Boleda"
                },
                "author": "Gemma Boleda",
                "arxiv_comment": "Final version to appear in Findings of the ACL (significantly revised\n  wrt v1). 15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11856v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11856v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11683v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11683v6",
                "updated": "2025-09-24T11:02:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    2,
                    49,
                    2,
                    267,
                    0
                ],
                "published": "2024-11-18T16:09:26Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    9,
                    26,
                    0,
                    323,
                    0
                ],
                "title": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic\n  Manipulation"
                },
                "summary": "Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link https://trojanrobot.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link https://trojanrobot.github.io."
                },
                "authors": [
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Hewen Pan"
                    },
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Yunpeng Jiang"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    },
                    {
                        "name": "Xiaohua Jia"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Jia"
                },
                "author": "Xiaohua Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11683v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11683v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19984v1",
                "updated": "2025-09-24T10:47:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    10,
                    47,
                    20,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T10:47:20Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    10,
                    47,
                    20,
                    2,
                    267,
                    0
                ],
                "title": "First Data of the 3000 km$^2$ Radio Detector at the Pierre Auger\n  Observatory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Data of the 3000 km$^2$ Radio Detector at the Pierre Auger\n  Observatory"
                },
                "summary": "In this contribution, we present the status and first data from the Radio\nDetector (RD) at the Pierre Auger Observatory, consisting of $1660$ radio\nantennas deployed across the $3000$ km$^2$ surface detector array. These\nantennas measure the radio emission from extensive air showers in the $30-80$\nMHz band, enabling electromagnetic energy measurements for air showers with\nzenith angles above $65\\deg$. Combined with the muonic measurements from the\nwater-Cherenkov detectors (WCDs), this allows mass composition studies at the\nhighest energies. The large-scale deployment of the RD began in November 2023\nand was completed in November 2024. A full end-to-end calibration shows\nconsistency between Galactic and in-lab calibration to better than $5$\\% and\nincludes continuous monitoring for hardware failures, ensuring, for example,\nantenna alignment within $5\\deg$. We present the first data, demonstrating a\nstrong correlation between the electromagnetic energy measured by the RD and\nthe total shower energy measured by the WCD, confirming that the detector chain\n- including triggering, data readout, absolute calibration, and reconstruction\nis well understood. We highlight a particularly impressive $32$ EeV shower at a\nzenith angle of $85\\deg$, producing a $50$ km-long radio footprint, showcasing\nthe unique capabilities of this detector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this contribution, we present the status and first data from the Radio\nDetector (RD) at the Pierre Auger Observatory, consisting of $1660$ radio\nantennas deployed across the $3000$ km$^2$ surface detector array. These\nantennas measure the radio emission from extensive air showers in the $30-80$\nMHz band, enabling electromagnetic energy measurements for air showers with\nzenith angles above $65\\deg$. Combined with the muonic measurements from the\nwater-Cherenkov detectors (WCDs), this allows mass composition studies at the\nhighest energies. The large-scale deployment of the RD began in November 2023\nand was completed in November 2024. A full end-to-end calibration shows\nconsistency between Galactic and in-lab calibration to better than $5$\\% and\nincludes continuous monitoring for hardware failures, ensuring, for example,\nantenna alignment within $5\\deg$. We present the first data, demonstrating a\nstrong correlation between the electromagnetic energy measured by the RD and\nthe total shower energy measured by the WCD, confirming that the detector chain\n- including triggering, data readout, absolute calibration, and reconstruction\nis well understood. We highlight a particularly impressive $32$ EeV shower at a\nzenith angle of $85\\deg$, producing a $50$ km-long radio footprint, showcasing\nthe unique capabilities of this detector."
                },
                "authors": [
                    {
                        "name": "Bjarni Pont"
                    }
                ],
                "author_detail": {
                    "name": "Bjarni Pont"
                },
                "arxiv_affiliation": "for the Pierre Auger Collaboration",
                "author": "Bjarni Pont",
                "arxiv_comment": "Presented at the 39th International Cosmic Ray Conference (ICRC\n  2025). 8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.16056v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.16056v4",
                "updated": "2025-09-24T10:44:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    10,
                    44,
                    26,
                    2,
                    267,
                    0
                ],
                "published": "2023-05-25T13:38:53Z",
                "published_parsed": [
                    2023,
                    5,
                    25,
                    13,
                    38,
                    53,
                    3,
                    145,
                    0
                ],
                "title": "Markov Decision Processes under External Temporal Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Decision Processes under External Temporal Processes"
                },
                "summary": "Reinforcement Learning Algorithms are predominantly developed for stationary\nenvironments, and the limited literature that considers nonstationary\nenvironments often involves specific assumptions about changes that can occur\nin transition probability matrices and reward functions. Considering that\nreal-world applications involve environments that continuously evolve due to\nvarious external events, and humans make decisions by discerning patterns in\nhistorical events, this study investigates Markov Decision Processes under the\ninfluence of an external temporal process. We establish the conditions under\nwhich the problem becomes tractable, allowing it to be addressed by considering\nonly a finite history of events, based on the properties of the perturbations\nintroduced by the exogenous process. We propose and theoretically analyze a\npolicy iteration algorithm to tackle this problem, which learns policies\ncontingent upon the current state of the environment, as well as a finite\nhistory of prior events of the exogenous process. We show that such an\nalgorithm is not guaranteed to converge. However, we provide a guarantee for\npolicy improvement in regions of the state space determined by the\napproximation error induced by considering tractable policies and value\nfunctions. We also establish the sample complexity of least-squares policy\nevaluation and policy improvement algorithms that consider approximations due\nto the incorporation of only a finite history of temporal events. While our\nresults are applicable to general discrete-time processes satisfying certain\nconditions on the rate of decay of the influence of their events, we further\nanalyze the case of discrete-time Hawkes processes with Gaussian marks. We\nperformed experiments to demonstrate our findings for policy evaluation and\ndeployment in traditional control environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Algorithms are predominantly developed for stationary\nenvironments, and the limited literature that considers nonstationary\nenvironments often involves specific assumptions about changes that can occur\nin transition probability matrices and reward functions. Considering that\nreal-world applications involve environments that continuously evolve due to\nvarious external events, and humans make decisions by discerning patterns in\nhistorical events, this study investigates Markov Decision Processes under the\ninfluence of an external temporal process. We establish the conditions under\nwhich the problem becomes tractable, allowing it to be addressed by considering\nonly a finite history of events, based on the properties of the perturbations\nintroduced by the exogenous process. We propose and theoretically analyze a\npolicy iteration algorithm to tackle this problem, which learns policies\ncontingent upon the current state of the environment, as well as a finite\nhistory of prior events of the exogenous process. We show that such an\nalgorithm is not guaranteed to converge. However, we provide a guarantee for\npolicy improvement in regions of the state space determined by the\napproximation error induced by considering tractable policies and value\nfunctions. We also establish the sample complexity of least-squares policy\nevaluation and policy improvement algorithms that consider approximations due\nto the incorporation of only a finite history of temporal events. While our\nresults are applicable to general discrete-time processes satisfying certain\nconditions on the rate of decay of the influence of their events, we further\nanalyze the case of discrete-time Hawkes processes with Gaussian marks. We\nperformed experiments to demonstrate our findings for policy evaluation and\ndeployment in traditional control environments."
                },
                "authors": [
                    {
                        "name": "Ranga Shaarad Ayyagari"
                    },
                    {
                        "name": "Revanth Raj Eega"
                    },
                    {
                        "name": "Ambedkar Dukkipati"
                    }
                ],
                "author_detail": {
                    "name": "Ambedkar Dukkipati"
                },
                "author": "Ambedkar Dukkipati",
                "arxiv_comment": "45 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.16056v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.16056v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22157v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22157v3",
                "updated": "2025-09-24T10:42:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    10,
                    42,
                    38,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-28T09:22:25Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    22,
                    25,
                    2,
                    148,
                    0
                ],
                "title": "LASER: Stratified Selective Sampling for Instruction Tuning with\n  Dedicated Scoring Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LASER: Stratified Selective Sampling for Instruction Tuning with\n  Dedicated Scoring Strategy"
                },
                "summary": "Recent work shows that post-training datasets for LLMs can be substantially\ndownsampled without noticeably deteriorating performance. However, data\nselection often incurs high computational costs or is limited to narrow\ndomains. In this paper, we demonstrate that data selection can be both --\nefficient and universal -- by using a multi-step pipeline in which we\nefficiently bin data points into groups, estimate quality using specialized\nmodels, and score difficulty with a robust, lightweight method. Task-based\ncategorization allows us to control the composition of our final data --\ncrucial for finetuning multi-purpose models. To guarantee diversity, we improve\nupon previous work using embedding models and a clustering algorithm. This\nintegrated strategy enables high-performance fine-tuning with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work shows that post-training datasets for LLMs can be substantially\ndownsampled without noticeably deteriorating performance. However, data\nselection often incurs high computational costs or is limited to narrow\ndomains. In this paper, we demonstrate that data selection can be both --\nefficient and universal -- by using a multi-step pipeline in which we\nefficiently bin data points into groups, estimate quality using specialized\nmodels, and score difficulty with a robust, lightweight method. Task-based\ncategorization allows us to control the composition of our final data --\ncrucial for finetuning multi-purpose models. To guarantee diversity, we improve\nupon previous work using embedding models and a clustering algorithm. This\nintegrated strategy enables high-performance fine-tuning with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Paramita Mirza"
                    },
                    {
                        "name": "Lucas Weber"
                    },
                    {
                        "name": "Fabian Küch"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Küch"
                },
                "author": "Fabian Küch",
                "arxiv_comment": "Accepted at Findings of the Association for Computational\n  Linguistics: EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22157v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22157v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16088v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16088v3",
                "updated": "2025-09-24T10:35:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    10,
                    35,
                    24,
                    2,
                    267,
                    0
                ],
                "published": "2025-05-22T00:06:29Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    0,
                    6,
                    29,
                    3,
                    142,
                    0
                ],
                "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal\n  Reasoning"
                },
                "summary": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 $\\rightarrow$ 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future time\nperiods; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year $\\rightarrow$ month $\\rightarrow$\nday). Our datasets and code are made publicly available\n\\href{https://github.com/gagan3012/date-fragments}{here}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 $\\rightarrow$ 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future time\nperiods; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year $\\rightarrow$ month $\\rightarrow$\nday). Our datasets and code are made publicly available\n\\href{https://github.com/gagan3012/date-fragments}{here}."
                },
                "authors": [
                    {
                        "name": "Gagan Bhatia"
                    },
                    {
                        "name": "Maxime Peyrard"
                    },
                    {
                        "name": "Wei Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhao"
                },
                "author": "Wei Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16088v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16088v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19965v1",
                "updated": "2025-09-24T10:21:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    10,
                    21,
                    29,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T10:21:29Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    10,
                    21,
                    29,
                    2,
                    267,
                    0
                ],
                "title": "SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation\n  via Multi-Modal Emotion Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation\n  via Multi-Modal Emotion Embedding"
                },
                "summary": "Audio-driven talking face generation has received growing interest,\nparticularly for applications requiring expressive and natural human-avatar\ninteraction. However, most existing emotion-aware methods rely on a single\nmodality (either audio or image) for emotion embedding, limiting their ability\nto capture nuanced affective cues. Additionally, most methods condition on a\nsingle reference image, restricting the model's ability to represent dynamic\nchanges in actions or attributes across time. To address these issues, we\nintroduce SynchroRaMa, a novel framework that integrates a multi-modal emotion\nembedding by combining emotional signals from text (via sentiment analysis) and\naudio (via speech-based emotion recognition and audio-derived valence-arousal\nfeatures), enabling the generation of talking face videos with richer and more\nauthentic emotional expressiveness and fidelity. To ensure natural head motion\nand accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M)\nmodule that generates motion frames aligned with the input audio. Finally,\nSynchroRaMa incorporates scene descriptions generated by Large Language Model\n(LLM) as additional textual input, enabling it to capture dynamic actions and\nhigh-level semantic attributes. Conditioning the model on both visual and\ntextual cues enhances temporal consistency and visual realism. Quantitative and\nqualitative experiments on benchmark datasets demonstrate that SynchroRaMa\noutperforms the state-of-the-art, achieving improvements in image quality,\nexpression preservation, and motion realism. A user study further confirms that\nSynchroRaMa achieves higher subjective ratings than competing methods in\noverall naturalness, motion diversity, and video smoothness. Our project page\nis available at <https://novicemm.github.io/synchrorama>.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-driven talking face generation has received growing interest,\nparticularly for applications requiring expressive and natural human-avatar\ninteraction. However, most existing emotion-aware methods rely on a single\nmodality (either audio or image) for emotion embedding, limiting their ability\nto capture nuanced affective cues. Additionally, most methods condition on a\nsingle reference image, restricting the model's ability to represent dynamic\nchanges in actions or attributes across time. To address these issues, we\nintroduce SynchroRaMa, a novel framework that integrates a multi-modal emotion\nembedding by combining emotional signals from text (via sentiment analysis) and\naudio (via speech-based emotion recognition and audio-derived valence-arousal\nfeatures), enabling the generation of talking face videos with richer and more\nauthentic emotional expressiveness and fidelity. To ensure natural head motion\nand accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M)\nmodule that generates motion frames aligned with the input audio. Finally,\nSynchroRaMa incorporates scene descriptions generated by Large Language Model\n(LLM) as additional textual input, enabling it to capture dynamic actions and\nhigh-level semantic attributes. Conditioning the model on both visual and\ntextual cues enhances temporal consistency and visual realism. Quantitative and\nqualitative experiments on benchmark datasets demonstrate that SynchroRaMa\noutperforms the state-of-the-art, achieving improvements in image quality,\nexpression preservation, and motion realism. A user study further confirms that\nSynchroRaMa achieves higher subjective ratings than competing methods in\noverall naturalness, motion diversity, and video smoothness. Our project page\nis available at <https://novicemm.github.io/synchrorama>."
                },
                "authors": [
                    {
                        "name": "Phyo Thet Yee"
                    },
                    {
                        "name": "Dimitrios Kollias"
                    },
                    {
                        "name": "Sudeepta Mishra"
                    },
                    {
                        "name": "Abhinav Dhall"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Dhall"
                },
                "author": "Abhinav Dhall",
                "arxiv_comment": "Accepted at WACV 2026, project page :\n  https://novicemm.github.io/synchrorama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00795v2",
                "updated": "2025-09-24T09:40:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    9,
                    40,
                    29,
                    2,
                    267,
                    0
                ],
                "published": "2025-02-02T13:27:59Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    13,
                    27,
                    59,
                    6,
                    33,
                    0
                ],
                "title": "Data Fusion for Full-Range Response Reconstruction via Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Fusion for Full-Range Response Reconstruction via Diffusion Models"
                },
                "summary": "Accurately capturing the full-range response of structures is crucial in\nstructural health monitoring (SHM) for ensuring safety and operational\nintegrity. However, limited sensor deployment due to cost, accessibility, or\nscale often hinders comprehensive monitoring. This paper presents a generative\ndata fusion framework utilizing diffusion models, to reconstruct the full-range\nstructural response from sparse and heterogeneous sensor measurements. We\nincorporate Diffusion Posterior Sampling (DPS) into the reconstruction\nframework, using sensor measurements as probabilistic constraints to guide the\nsampling process. Three forward models are designed: Direct Observation Mapping\n(DOM), Channel-based Observation Mapping (COM), and Neural Network Forward\nModel (NNFM), enabling flexible adaptation to different sensor placement\nconditions and reconstruction targets. The proposed framework is validated on a\nsteel plate shear wall exhibiting nonlinear responses. By simultaneously\nsampling 100 realizations and averaging them as the ensemble prediction result,\nthe three forward models achieve Weighted Mean Absolute Percentage Errors of\n1.62% (DOM), 3.27% (COM), and 3.49% (NNFM). Sensitivity analyses further\ndemonstrate robust performance under varying hyperparameters, sensor\nconfigurations, and noise levels. The proposed framework shows new\npossibilities for probabilistic modeling and decision-making in SHM by\nharnessing the capabilities of diffusion models, offering a novel data fusion\napproach for full-range monitoring of structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately capturing the full-range response of structures is crucial in\nstructural health monitoring (SHM) for ensuring safety and operational\nintegrity. However, limited sensor deployment due to cost, accessibility, or\nscale often hinders comprehensive monitoring. This paper presents a generative\ndata fusion framework utilizing diffusion models, to reconstruct the full-range\nstructural response from sparse and heterogeneous sensor measurements. We\nincorporate Diffusion Posterior Sampling (DPS) into the reconstruction\nframework, using sensor measurements as probabilistic constraints to guide the\nsampling process. Three forward models are designed: Direct Observation Mapping\n(DOM), Channel-based Observation Mapping (COM), and Neural Network Forward\nModel (NNFM), enabling flexible adaptation to different sensor placement\nconditions and reconstruction targets. The proposed framework is validated on a\nsteel plate shear wall exhibiting nonlinear responses. By simultaneously\nsampling 100 realizations and averaging them as the ensemble prediction result,\nthe three forward models achieve Weighted Mean Absolute Percentage Errors of\n1.62% (DOM), 3.27% (COM), and 3.49% (NNFM). Sensitivity analyses further\ndemonstrate robust performance under varying hyperparameters, sensor\nconfigurations, and noise levels. The proposed framework shows new\npossibilities for probabilistic modeling and decision-making in SHM by\nharnessing the capabilities of diffusion models, offering a novel data fusion\napproach for full-range monitoring of structures."
                },
                "authors": [
                    {
                        "name": "Wingho Feng"
                    },
                    {
                        "name": "Quanwang Li"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Jian-sheng Fan"
                    }
                ],
                "author_detail": {
                    "name": "Jian-sheng Fan"
                },
                "author": "Jian-sheng Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19931v1",
                "updated": "2025-09-24T09:38:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    9,
                    38,
                    48,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T09:38:48Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    9,
                    38,
                    48,
                    2,
                    267,
                    0
                ],
                "title": "Documentation Retrieval Improves Planning Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Documentation Retrieval Improves Planning Language Generation"
                },
                "summary": "Certain strong LLMs have shown promise for zero-shot formal planning by\ngenerating planning languages like PDDL. Yet, performance of most open-source\nmodels under 50B parameters has been reported to be close to zero due to the\nlow-resource nature of these languages. We significantly improve their\nperformance via a series of lightweight pipelines that integrates documentation\nretrieval with modular code generation and error refinement. With models like\nLlama-4-Maverick, our best pipeline improves plan correctness from 0\\% to over\n80\\% on the common BlocksWorld domain. However, while syntactic errors are\nsubstantially reduced, semantic errors persist in more challenging domains,\nrevealing fundamental limitations in current models' reasoning\ncapabilities.\\footnote{Our code and data can be found at\nhttps://github.com/Nangxxxxx/PDDL-RAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certain strong LLMs have shown promise for zero-shot formal planning by\ngenerating planning languages like PDDL. Yet, performance of most open-source\nmodels under 50B parameters has been reported to be close to zero due to the\nlow-resource nature of these languages. We significantly improve their\nperformance via a series of lightweight pipelines that integrates documentation\nretrieval with modular code generation and error refinement. With models like\nLlama-4-Maverick, our best pipeline improves plan correctness from 0\\% to over\n80\\% on the common BlocksWorld domain. However, while syntactic errors are\nsubstantially reduced, semantic errors persist in more challenging domains,\nrevealing fundamental limitations in current models' reasoning\ncapabilities.\\footnote{Our code and data can be found at\nhttps://github.com/Nangxxxxx/PDDL-RAG"
                },
                "authors": [
                    {
                        "name": "Renxiang Wang"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "arxiv_comment": "12 pages, 14 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19926v1",
                "updated": "2025-09-24T09:33:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    9,
                    33,
                    46,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T09:33:46Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    9,
                    33,
                    46,
                    2,
                    267,
                    0
                ],
                "title": "MMSE-Calibrated Few-Shot Prompting for Alzheimer's Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMSE-Calibrated Few-Shot Prompting for Alzheimer's Detection"
                },
                "summary": "Prompting large language models is a training-free method for detecting\nAlzheimer's disease from speech transcripts. Using the ADReSS dataset, we\nrevisit zero-shot prompting and study few-shot prompting with a class-balanced\nprotocol using nested interleave and a strict schema, sweeping up to 20\nexamples per class. We evaluate two variants achieving state-of-the-art\nprompting results. (i) MMSE-Proxy Prompting: each few-shot example carries a\nprobability anchored to Mini-Mental State Examination bands via a deterministic\nmapping, enabling AUC computing; this reaches 0.82 accuracy and 0.86 AUC (ii)\nReasoning-augmented Prompting: few-shot examples pool is generated with a\nmultimodal LLM (GPT-5) that takes as input the Cookie Theft image, transcript,\nand MMSE to output a reasoning and MMSE-aligned probability; evaluation remains\ntranscript-only and reaches 0.82 accuracy and 0.83 AUC. To our knowledge, this\nis the first ADReSS study to anchor elicited probabilities to MMSE and to use\nmultimodal construction to improve interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting large language models is a training-free method for detecting\nAlzheimer's disease from speech transcripts. Using the ADReSS dataset, we\nrevisit zero-shot prompting and study few-shot prompting with a class-balanced\nprotocol using nested interleave and a strict schema, sweeping up to 20\nexamples per class. We evaluate two variants achieving state-of-the-art\nprompting results. (i) MMSE-Proxy Prompting: each few-shot example carries a\nprobability anchored to Mini-Mental State Examination bands via a deterministic\nmapping, enabling AUC computing; this reaches 0.82 accuracy and 0.86 AUC (ii)\nReasoning-augmented Prompting: few-shot examples pool is generated with a\nmultimodal LLM (GPT-5) that takes as input the Cookie Theft image, transcript,\nand MMSE to output a reasoning and MMSE-aligned probability; evaluation remains\ntranscript-only and reaches 0.82 accuracy and 0.83 AUC. To our knowledge, this\nis the first ADReSS study to anchor elicited probabilities to MMSE and to use\nmultimodal construction to improve interpretability."
                },
                "authors": [
                    {
                        "name": "Jana Sweidan"
                    },
                    {
                        "name": "Mounim A. El-Yacoubi"
                    },
                    {
                        "name": "Nasredine Semmar"
                    }
                ],
                "author_detail": {
                    "name": "Nasredine Semmar"
                },
                "author": "Nasredine Semmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]